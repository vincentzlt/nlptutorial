Europe began 	 0.2
Stef Slembrouck 	 1.0
The USAF 	 0.005208333333333333
, domain 	 0.0005614823133071309
, their 	 0.0005614823133071309
weights equal 	 0.2
intervening punctuation 	 1.0
is definite 	 0.0020325203252032522
LexRank score 	 0.08333333333333333
either a 	 0.2
accurate results 	 0.14285714285714285
extract the 	 0.25
modeling are 	 0.14285714285714285
first evaluation 	 0.030303030303030304
expressed in 	 0.16666666666666666
<s> Imagine 	 0.0007686395080707148
technologies have 	 0.25
only one 	 0.02631578947368421
boundaries . 	 0.36363636363636365
in Star 	 0.0018726591760299626
language other 	 0.006756756756756757
used together 	 0.008849557522123894
suitable department 	 0.25
availability of 	 1.0
where POS 	 0.02857142857142857
past-tense verb 	 1.0
disabilities can 	 0.25
a lattice 	 0.001226993865030675
large number 	 0.08695652173913043
what sound 	 0.03125
the HMM 	 0.0006920415224913495
AI systems 	 0.6666666666666666
-LRB- not 	 0.0027100271002710027
as many 	 0.006968641114982578
tagger proceeds 	 0.1111111111111111
his famous 	 0.08333333333333333
<s> Glass-box 	 0.0007686395080707148
must take 	 0.07142857142857142
speaker-dependent system 	 1.0
international ATC 	 0.5
Once these 	 0.2
<s> Behind 	 0.0007686395080707148
conclusions . 	 1.0
vs. independence 	 0.08333333333333333
static shape 	 1.0
Example-based Main 	 0.3333333333333333
to write 	 0.0013280212483399733
built a 	 0.3333333333333333
compensate for 	 1.0
that Piron 	 0.0035460992907801418
Unfortunately , 	 1.0
delta-delta coefficients 	 1.0
NIST 's 	 0.5
pauses between 	 0.5
been devoted 	 0.014705882352941176
Typical stages 	 0.5
state-of-the-art abstractive 	 0.5
doing extensive 	 0.5
unweighted edges 	 1.0
that merely 	 0.0035460992907801418
results were 	 0.047619047619047616
because they 	 0.13333333333333333
task-effectiveness well 	 0.5
semantic theory 	 0.09523809523809523
language-specific changes 	 1.0
summaries and 	 0.046511627906976744
distorted by 	 0.5
prestige '' 	 1.0
tasks is 	 0.03125
that describe 	 0.0035460992907801418
to as 	 0.005312084993359893
elements containing 	 0.25
Beginning with 	 0.5
<s> E. 	 0.0007686395080707148
to keep 	 0.0026560424966799467
letter that 	 0.16666666666666666
context-free , 	 0.09090909090909091
positive and 	 0.2857142857142857
and sources 	 0.001445086705202312
= masculine 	 0.1111111111111111
recognition performance 	 0.03305785123966942
unless the 	 1.0
of process 	 0.00089126559714795
letter to 	 0.16666666666666666
are grouped 	 0.004149377593360996
Isles and 	 1.0
more often 	 0.010526315789473684
dictionaries , 	 1.0
which even 	 0.007246376811594203
<s> DARPA 	 0.0007686395080707148
decision-support aids 	 1.0
cases and 	 0.05555555555555555
effectiveness in 	 0.3333333333333333
, as 	 0.01291409320606401
indeed answer 	 0.3333333333333333
, partially 	 0.0005614823133071309
of phonetic 	 0.00089126559714795
star scale 	 0.5
typically involve 	 0.05555555555555555
to parsers 	 0.0013280212483399733
sometimes open-ended 	 0.07692307692307693
post-process the 	 1.0
and languages 	 0.001445086705202312
which soon 	 0.007246376811594203
being developed 	 0.05555555555555555
additionally requires 	 1.0
cross-lingual questions 	 0.5
course be 	 0.3333333333333333
speech signal 	 0.006578947368421052
is able 	 0.0020325203252032522
of automated 	 0.00089126559714795
judge , 	 0.25
injuries to 	 1.0
conflicting objectives 	 1.0
Number = 	 1.0
language as 	 0.006756756756756757
represent analog 	 0.1111111111111111
demonstrates the 	 1.0
user-specified or 	 0.5
processing systems 	 0.05555555555555555
translate five 	 0.16666666666666666
soft decisions 	 0.5
installed at 	 0.6666666666666666
very sophisticated 	 0.024390243902439025
One approach 	 0.07692307692307693
Wall Street 	 1.0
The choice 	 0.005208333333333333
ARCHILES technique 	 1.0
a linguistic 	 0.00245398773006135
of good 	 0.00089126559714795
, adverbs 	 0.0005614823133071309
appropriate perspective 	 0.25
, enables 	 0.0005614823133071309
that integrated 	 0.0035460992907801418
evaluation process 	 0.018518518518518517
correct output 	 0.06666666666666667
with certain 	 0.00546448087431694
, Type 	 0.0005614823133071309
computer forecasts 	 0.022727272727272728
a corporation 	 0.001226993865030675
burden on 	 1.0
standard telegraph 	 0.07142857142857142
variant of 	 1.0
the questioner 	 0.002768166089965398
on recognition 	 0.0047169811320754715
as horoscope 	 0.003484320557491289
syllables , 	 0.5
Harris 's 	 0.2222222222222222
These range 	 0.058823529411764705
PC can 	 0.25
include papers 	 0.037037037037037035
shapes . 	 0.3333333333333333
after Fourier 	 0.08333333333333333
: e.g. 	 0.0196078431372549
`` words 	 0.005291005291005291
NLP problem 	 0.0425531914893617
about human 	 0.025
sophisticated questioners 	 0.14285714285714285
rules generated 	 0.023255813953488372
Conferences in 	 0.5
chosen domains 	 0.2
makes the 	 0.25
knowledge about 	 0.1111111111111111
relating to 	 1.0
identify ambiguities 	 0.08333333333333333
to compare 	 0.005312084993359893
determine whether 	 0.043478260869565216
hit you 	 1.0
optimizes parameters 	 1.0
rich information 	 0.2
after , 	 0.08333333333333333
algorithms . 	 0.11428571428571428
hand it 	 0.07142857142857142
large , 	 0.043478260869565216
one font 	 0.015384615384615385
Deese , 	 1.0
learner . 	 0.5
Thompson , 	 1.0
an isolated 	 0.007575757575757576
as Lisp 	 0.003484320557491289
and ATIS 	 0.001445086705202312
environment in 	 0.16666666666666666
Answer formulation 	 0.3333333333333333
flatbed scanner 	 1.0
and his 	 0.001445086705202312
a bunch 	 0.001226993865030675
Latin-script , 	 1.0
the two 	 0.0034602076124567475
open letter 	 0.25
these numbers 	 0.047619047619047616
data has 	 0.012987012987012988
the optical 	 0.0006920415224913495
apple is 	 0.6666666666666666
and replicated 	 0.001445086705202312
punctuation , 	 0.2857142857142857
often requires 	 0.022727272727272728
this kind 	 0.01098901098901099
each of 	 0.1111111111111111
working in 	 0.2857142857142857
ask him 	 0.25
D. , 	 0.4
or length 	 0.0045045045045045045
limit the 	 0.5
Again , 	 1.0
's estimate 	 0.0196078431372549
, stemming 	 0.0005614823133071309
word co-occurrence 	 0.016666666666666666
for mental 	 0.0036101083032490976
with n 	 0.00546448087431694
are measured 	 0.004149377593360996
system recognizes 	 0.010752688172043012
capturing data 	 1.0
the appropriate 	 0.0020761245674740486
<s> Contrary 	 0.0015372790161414297
on extractive 	 0.0047169811320754715
which focuses 	 0.007246376811594203
process new 	 0.027777777777777776
<s> Shepard 	 0.0007686395080707148
around the 	 0.375
by highlighting 	 0.005714285714285714
with equivalent 	 0.01092896174863388
the gradual 	 0.0006920415224913495
`` Red 	 0.005291005291005291
compactly , 	 1.0
domain-independent and 	 1.0
scanner and 	 0.3333333333333333
, Elinor 	 0.0005614823133071309
languages -RRB- 	 0.04
expansion Automated 	 0.3333333333333333
other domains 	 0.014285714285714285
one within 	 0.015384615384615385
male-female normalization 	 1.0
HLDA -RRB- 	 1.0
analysis as 	 0.015384615384615385
, Words 	 0.0005614823133071309
other 10 	 0.014285714285714285
Wilensky , 	 1.0
Like the 	 0.5
of elaborate 	 0.00089126559714795
reliable hits 	 0.25
well enough 	 0.03571428571428571
-- are 	 0.08
-RRB- one 	 0.0027100271002710027
email address 	 0.5
with attribute 	 0.00546448087431694
discouraged the 	 1.0
this context 	 0.02197802197802198
as consideration 	 0.003484320557491289
tagging , 	 0.08
in simple 	 0.0018726591760299626
often inaccurate 	 0.022727272727272728
disassembling and 	 1.0
view of 	 0.3333333333333333
denote an 	 0.5
often a 	 0.022727272727272728
would consist 	 0.018867924528301886
for top-down 	 0.0036101083032490976
assessing whether 	 1.0
some rules 	 0.012048192771084338
, grammar 	 0.0005614823133071309
five years 	 0.4
hand or 	 0.14285714285714285
To overcome 	 0.1111111111111111
Subjectivity '' 	 1.0
ways to 	 0.125
a better 	 0.00245398773006135
between an 	 0.02564102564102564
Italy , 	 1.0
popular is 	 0.1111111111111111
off as 	 0.5
which includes 	 0.014492753623188406
handwritten cursive 	 0.5
democratizing publishing 	 0.5
measured in 	 0.16666666666666666
not context-free 	 0.008928571428571428
our life 	 0.2
absolutely necessary 	 1.0
methods work 	 0.045454545454545456
now looking 	 0.07692307692307693
speech recogniton 	 0.006578947368421052
wrote that 	 0.16666666666666666
went to 	 0.4
discourse studies 	 0.027777777777777776
's that 	 0.0196078431372549
movement to 	 1.0
Lifeline as 	 1.0
plural common 	 0.2
most practical 	 0.017241379310344827
University , 	 0.1111111111111111
document summarization 	 0.1388888888888889
Using almost 	 0.5
summarization in 	 0.04
annotated -LRB- 	 0.5
content question 	 0.08333333333333333
text in 	 0.050314465408805034
out '' 	 0.07142857142857142
off on 	 0.5
and recursive-descent 	 0.001445086705202312
original source 	 0.07692307692307693
and counter 	 0.001445086705202312
is subjectivity\/objectivity 	 0.0020325203252032522
CLAWS -LRB- 	 0.25
resources are 	 0.16666666666666666
textual summaries 	 0.2
paradigm calls 	 0.3333333333333333
`` shallow 	 0.005291005291005291
includes a 	 0.14285714285714285
scripts -LRB- 	 0.6666666666666666
, POS 	 0.0005614823133071309
modeling of 	 0.14285714285714285
Intelligence and 	 0.3333333333333333
specific context 	 0.047619047619047616
approach for 	 0.02857142857142857
is still 	 0.008130081300813009
on pattern 	 0.0047169811320754715
the article 	 0.0006920415224913495
often required 	 0.022727272727272728
the invention 	 0.0006920415224913495
20th-century newspaper 	 1.0
Jurafsky and 	 1.0
many cases 	 0.038461538461538464
, various 	 0.0005614823133071309
A post 	 0.02
, Alessandro 	 0.0005614823133071309
forecasts from 	 0.2
researchers found 	 0.1
adjacent and 	 0.16666666666666666
to coherent 	 0.0013280212483399733
predict what 	 0.16666666666666666
lower than 	 0.2
translation systems 	 0.02702702702702703
may fail 	 0.019230769230769232
errors per 	 0.2
of numbers 	 0.0017825311942959
obtained . 	 0.14285714285714285
Brazil , 	 1.0
the relative 	 0.0006920415224913495
defined as 	 0.16666666666666666
Around the 	 1.0
extraction task 	 0.03225806451612903
<s> Narrow 	 0.0007686395080707148
Sentences ; 	 1.0
discriminant analysis 	 1.0
a corpus 	 0.0036809815950920245
famous early 	 0.3333333333333333
in common 	 0.003745318352059925
had a 	 0.2857142857142857
, Strzalkowski 	 0.0005614823133071309
period of 	 0.5
Drum printer 	 1.0
except some 	 1.0
-LRB- ARRA 	 0.0027100271002710027
feature\/aspect is 	 1.0
Corpus -LRB- 	 0.0625
to evaluate 	 0.005312084993359893
if a 	 0.03571428571428571
ISO sub-committee 	 0.5
vibration , 	 1.0
statistical machine 	 0.09090909090909091
2006 and 	 0.3333333333333333
often continues 	 0.022727272727272728
semi-supervised learning 	 0.5
many examples 	 0.019230769230769232
problem in 	 0.09090909090909091
National Federation 	 0.3333333333333333
a `` 	 0.007361963190184049
he had 	 0.14285714285714285
, LexRank 	 0.0005614823133071309
other approaches 	 0.014285714285714285
tags used 	 0.3333333333333333
only a 	 0.05263157894736842
are directly 	 0.004149377593360996
TextRank and 	 0.14285714285714285
bigram , 	 1.0
, what 	 0.0011229646266142617
procedure lies 	 0.3333333333333333
ambiguous words 	 0.16666666666666666
positive , 	 0.14285714285714285
deteriorated with 	 1.0
regard . 	 0.2
now done 	 0.07692307692307693
<s> Recall 	 0.0023059185242121443
like HTML 	 0.03571428571428571
the sizes 	 0.0006920415224913495
corporation does 	 1.0
<s> Markov 	 0.0007686395080707148
and going 	 0.001445086705202312
and Weizenbaum 	 0.001445086705202312
correct according 	 0.06666666666666667
revolution in 	 1.0
between words 	 0.05128205128205128
real-world examples 	 0.16666666666666666
programming language 	 0.2
length of 	 0.25
the steady 	 0.001384083044982699
in universities 	 0.0018726591760299626
by people 	 0.011428571428571429
OnStar , 	 1.0
based approach 	 0.018518518518518517
processing problems 	 0.018518518518518517
where at 	 0.02857142857142857
and some 	 0.002890173410404624
lower level 	 0.4
with little 	 0.00546448087431694
and broadband 	 0.001445086705202312
both `` 	 0.06451612903225806
without limits 	 0.07692307692307693
currently . 	 0.14285714285714285
cause much 	 0.5
sciences concurrently 	 0.5
or `` 	 0.018018018018018018
by limiting 	 0.005714285714285714
page count 	 0.14285714285714285
Short history 	 1.0
newspaper pages 	 0.3333333333333333
number -RRB- 	 0.046511627906976744
-RRB- tagger 	 0.0027100271002710027
produced , 	 0.1111111111111111
with references 	 0.00546448087431694
the notable 	 0.0006920415224913495
is sometimes 	 0.006097560975609756
volume of 	 0.5
each template 	 0.022222222222222223
which sentences 	 0.007246376811594203
take as 	 0.1
example where 	 0.012345679012345678
, room 	 0.0005614823133071309
and DCD 	 0.001445086705202312
can start 	 0.0055248618784530384
be sufficient 	 0.004219409282700422
Ann Arbor 	 1.0
two waves 	 0.034482758620689655
mark was 	 0.3333333333333333
, domotic 	 0.0005614823133071309
Each level 	 0.16666666666666666
.5 decision 	 1.0
paraphrasing sections 	 1.0
not spend 	 0.008928571428571428
pattern . 	 0.16666666666666666
discriminate keyphrases 	 0.3333333333333333
includes both 	 0.14285714285714285
answer type 	 0.06666666666666667
deterministic rule 	 0.25
1991 A 	 0.3333333333333333
learn from 	 0.07692307692307693
-LRB- HMMs 	 0.005420054200542005
are particularly 	 0.004149377593360996
as subject 	 0.003484320557491289
Text-to-speech Text-proofing 	 1.0
Performance The 	 1.0
in translating 	 0.0018726591760299626
summarization -LRB- 	 0.04
and answers 	 0.001445086705202312
released `` 	 0.5
similarity or 	 0.1
, George 	 0.0005614823133071309
are being 	 0.008298755186721992
the recognizer 	 0.0006920415224913495
never went 	 0.2
it might 	 0.008547008547008548
in testing 	 0.0018726591760299626
this application 	 0.01098901098901099
the mid-90s 	 0.0006920415224913495
in these 	 0.0056179775280898875
study language 	 0.25
of extracting 	 0.00089126559714795
1987 -LRB- 	 0.3333333333333333
rates of 	 0.375
Work on 	 0.5
set , 	 0.05128205128205128
in Jones 	 0.0018726591760299626
<s> Optical 	 0.0015372790161414297
might vary 	 0.038461538461538464
the figure 	 0.0006920415224913495
children 's 	 0.5
the next 	 0.004152249134948097
Weaver wrote 	 1.0
his PhD 	 0.08333333333333333
as sentence 	 0.006968641114982578
<s> When 	 0.004611837048424289
by taking 	 0.005714285714285714
programs that 	 0.09090909090909091
and Chinese 	 0.001445086705202312
up in 	 0.09090909090909091
other native 	 0.014285714285714285
explore critical 	 0.25
movie review 	 0.3333333333333333
approximation thereof 	 0.16666666666666666
slowly and 	 0.5
to find 	 0.010624169986719787
of syntactic 	 0.0017825311942959
and eigenvector 	 0.001445086705202312
approaches designed 	 0.03571428571428571
term meaning 	 0.05555555555555555
; this 	 0.06382978723404255
Corpus was 	 0.125
terms to 	 0.07692307692307693
other NLP 	 0.014285714285714285
common plural 	 0.04
basic techniques 	 0.07692307692307693
by IMR 	 0.005714285714285714
is about 	 0.0020325203252032522
therefore to 	 0.2
by expanding 	 0.005714285714285714
an Electronic 	 0.007575757575757576
usage is 	 1.0
data used 	 0.025974025974025976
typically uses 	 0.05555555555555555
to all 	 0.00398406374501992
e.g. `` 	 0.017857142857142856
repeatedly reviewed 	 1.0
, leading 	 0.0005614823133071309
positive -RRB- 	 0.14285714285714285
This rubric 	 0.015873015873015872
depended on 	 1.0
process include 	 0.027777777777777776
`` natural 	 0.015873015873015872
or formulaic 	 0.0045045045045045045
or '' 	 0.0045045045045045045
a Japanese 	 0.001226993865030675
47 % 	 1.0
as ` 	 0.003484320557491289
computer speech 	 0.022727272727272728
ranking -LRB- 	 0.14285714285714285
Other issues 	 0.14285714285714285
despite the 	 0.3333333333333333
then direct 	 0.02857142857142857
as closed 	 0.003484320557491289
algorithms one 	 0.02857142857142857
looking wave 	 0.2
, ELIZA 	 0.0011229646266142617
necessary for 	 0.3
QA More 	 0.047619047619047616
Adriana Bolivar 	 1.0
Each sample 	 0.16666666666666666
which will 	 0.021739130434782608
digitize the 	 1.0
flight '' 	 0.5
, perhaps 	 0.0016844469399213925
of organized 	 0.00089126559714795
or sometimes 	 0.0045045045045045045
by computer 	 0.017142857142857144
have corpus 	 0.009615384615384616
market to 	 0.3333333333333333
looking at 	 0.2
to automate 	 0.0026560424966799467
1966 , 	 0.3333333333333333
wrong fairly 	 1.0
, keyphrases 	 0.0005614823133071309
by Makoto 	 0.005714285714285714
text itself 	 0.006289308176100629
`` defective 	 0.005291005291005291
-RRB- has 	 0.008130081300813009
is going 	 0.0020325203252032522
moderate with 	 0.2
NLP as 	 0.02127659574468085
each character 	 0.022222222222222223
input and\/or 	 0.024390243902439025
evaluation systems 	 0.018518518518518517
invented Penicillin 	 0.5
using digital 	 0.01694915254237288
achieves 98.5 	 0.5
verifiability . 	 1.0
types of 	 0.8571428571428571
questions -LRB- 	 0.038461538461538464
one meaning 	 0.03076923076923077
-LRB- digital 	 0.0027100271002710027
noun in 	 0.07142857142857142
sentence or 	 0.041666666666666664
more than 	 0.042105263157894736
English of 	 0.02702702702702703
: Content 	 0.00980392156862745
a subtopic 	 0.001226993865030675
than other 	 0.022222222222222223
system in 	 0.021505376344086023
and alignment 	 0.001445086705202312
to robots 	 0.0013280212483399733
assume there 	 0.5
Hindle D. 	 1.0
both in 	 0.03225806451612903
same problem 	 0.04
development has 	 0.08333333333333333
-LRB- MPE 	 0.0027100271002710027
many written 	 0.019230769230769232
retrieval results 	 0.14285714285714285
align recorded 	 1.0
possibilities but 	 0.2
generated from 	 0.13333333333333333
language learning 	 0.006756756756756757
summaries is 	 0.06976744186046512
singular forms 	 0.25
average distance 	 0.5
a window 	 0.001226993865030675
usually called 	 0.03125
the needs 	 0.0006920415224913495
perhaps , 	 0.16666666666666666
many NLP 	 0.019230769230769232
the latter 	 0.0006920415224913495
a prior 	 0.001226993865030675
mentioned in 	 0.16666666666666666
method . 	 0.125
the feature 	 0.0006920415224913495
structures , 	 0.2
unreferenced section 	 1.0
<s> Matches 	 0.0007686395080707148
Carston , 	 1.0
pilot effectiveness 	 0.2
world -LRB- 	 0.06666666666666667
and laughter 	 0.001445086705202312
both linguistic 	 0.03225806451612903
based recognition 	 0.018518518518518517
campaigns were 	 0.5
voice dialog 	 0.07692307692307693
experiment which 	 0.2
next stage 	 0.2857142857142857
corpus -LRB- 	 0.03225806451612903
categories , 	 0.1111111111111111
ensure verifiability 	 1.0
, Robert 	 0.0016844469399213925
recognition we 	 0.008264462809917356
, not 	 0.0039303761931499155
an Inuit 	 0.007575757575757576
and both 	 0.001445086705202312
Speech-to-text reporter 	 1.0
set and 	 0.02564102564102564
Heritage , 	 1.0
a bilingual 	 0.001226993865030675
extended in 	 1.0
in conference 	 0.0018726591760299626
an intermediary 	 0.007575757575757576
where only 	 0.02857142857142857
mail since 	 0.5
comparison , 	 0.3333333333333333
Command -RRB- 	 0.5
2500 articles 	 1.0
largely because 	 0.2
with speech 	 0.01092896174863388
dependency grammar 	 0.2
context -- 	 0.030303030303030304
speech With 	 0.006578947368421052
to news-gathering 	 0.0013280212483399733
which many 	 0.007246376811594203
when describing 	 0.05714285714285714
use considers 	 0.013888888888888888
tense of 	 0.5
Robyn Carston 	 1.0
fairly trivial 	 0.25
theory Conversation 	 0.07692307692307693
National Corpus 	 0.3333333333333333
in France 	 0.003745318352059925
entries , 	 0.5
position in 	 0.5
`` diverse 	 0.005291005291005291
or , 	 0.0045045045045045045
Research , 	 0.125
reported was 	 0.2
blend smoothly 	 0.6666666666666666
a campaign 	 0.001226993865030675
in pattern 	 0.0018726591760299626
together with 	 0.125
<s> To 	 0.006149116064565719
using statistical 	 0.03389830508474576
in any 	 0.0018726591760299626
perform data 	 0.09090909090909091
distinctions are 	 0.5
are often 	 0.016597510373443983
could search 	 0.0625
translation studies 	 0.013513513513513514
Approaches Bernard 	 0.3333333333333333
record of 	 1.0
alternative right-hand-sides 	 0.3333333333333333
On-line systems 	 0.3333333333333333
of language-processing 	 0.00089126559714795
asked within 	 0.3333333333333333
value is 	 0.3333333333333333
FoG , 	 0.5
definite on 	 1.0
US Navy 	 0.14285714285714285
-- discourse 	 0.04
, made 	 0.0011229646266142617
with payments 	 0.00546448087431694
and ` 	 0.001445086705202312
In 1971 	 0.009523809523809525
is constructed 	 0.0040650406504065045
be effective 	 0.008438818565400843
author wishes 	 0.3333333333333333
for multiple 	 0.0036101083032490976
98 % 	 1.0
resolve ambiguities 	 0.5
segment and 	 0.1111111111111111
for translation 	 0.0036101083032490976
P. , 	 1.0
, news 	 0.0005614823133071309
formalisms such 	 0.5
musical notations 	 1.0
models can 	 0.038461538461538464
that read 	 0.0035460992907801418
Italian -RRB- 	 0.5
president of 	 1.0
Foucault became 	 0.3333333333333333
various ways 	 0.1111111111111111
, Jef 	 0.0005614823133071309
e.g. stating 	 0.017857142857142856
phonetically different 	 1.0
many -LRB- 	 0.019230769230769232
and above 	 0.001445086705202312
Human sentences 	 0.2
the measurement 	 0.0006920415224913495
JAS-39 Gripen 	 1.0
of criteria 	 0.00089126559714795
the term 	 0.0034602076124567475
these terms 	 0.023809523809523808
an integrated 	 0.007575757575757576
normally requires 	 0.5
paraphrase . 	 1.0
confused with 	 1.0
approach using 	 0.02857142857142857
= Machine 	 0.1111111111111111
computer processing 	 0.022727272727272728
term artificial 	 0.05555555555555555
very attractive 	 0.024390243902439025
assist human 	 1.0
this time 	 0.03296703296703297
despite warnings 	 0.3333333333333333
Supervised text 	 1.0
assess the 	 0.3333333333333333
complicated statistical 	 0.3333333333333333
judgement , 	 0.3333333333333333
cutoff to 	 1.0
cognitive operation 	 0.5
matter how 	 0.3333333333333333
were found 	 0.04878048780487805
significant -RRB- 	 0.1111111111111111
language output 	 0.006756756756756757
printed pages 	 0.08333333333333333
the interlingua 	 0.0006920415224913495
ways . 	 0.125
images , 	 0.3333333333333333
<s> While 	 0.0023059185242121443
approximation to 	 0.3333333333333333
use `` 	 0.013888888888888888
`` Gismo 	 0.010582010582010581
<s> Brain 	 0.0007686395080707148
the overall 	 0.0020761245674740486
1990s , 	 0.3333333333333333
<s> metrics 	 0.0007686395080707148
<s> Advanced 	 0.0023059185242121443
represent varies 	 0.1111111111111111
became less 	 0.2
extrinsic evaluation 	 0.5
see Handwriting 	 0.05
analysis Applied 	 0.015384615384615385
the process 	 0.007612456747404845
years , 	 0.23809523809523808
David H. 	 0.25
developing Q&A 	 0.25
very distant 	 0.024390243902439025
also refer 	 0.014492753623188406
more formal 	 0.010526315789473684
, followed 	 0.0005614823133071309
appear near 	 0.0625
sailor → 	 0.2
in contrast 	 0.0018726591760299626
Piron mentions 	 0.3333333333333333
formal language 	 0.2222222222222222
each choice 	 0.022222222222222223
requires the 	 0.1875
Machinery and 	 1.0
be useful 	 0.012658227848101266
checked after 	 0.5
symbols defined 	 0.3333333333333333
significantly . 	 1.0
carefully design 	 1.0
we hear 	 0.022222222222222223
what type 	 0.03125
after going 	 0.08333333333333333
approximation was 	 0.16666666666666666
those running 	 0.045454545454545456
Booth and 	 1.0
are interpreted 	 0.004149377593360996
causes a 	 1.0
change focus 	 1.0
might shed 	 0.038461538461538464
grammar-based methods 	 1.0
knowledge . 	 0.037037037037037035
business data 	 0.25
included : 	 0.125
over 1,000 	 0.08333333333333333
a difficult 	 0.001226993865030675
classifiers make 	 0.5
entirely and 	 0.5
fields of 	 0.3333333333333333
currently in 	 0.14285714285714285
NLG the 	 0.047619047619047616
topic or 	 0.125
exploring the 	 1.0
was dramatically 	 0.012987012987012988
for natural 	 0.01444043321299639
-RRB- while 	 0.0027100271002710027
all official 	 0.023255813953488372
required to 	 0.14285714285714285
The technique 	 0.005208333333333333
the fact 	 0.002768166089965398
example of 	 0.08641975308641975
segmentation systems 	 0.030303030303030304
other potential 	 0.014285714285714285
entering a 	 0.5
; while 	 0.02127659574468085
Text simplification 	 0.16666666666666666
effect of 	 0.5
accuracy include 	 0.03225806451612903
, Ruth 	 0.0005614823133071309
, computational 	 0.0005614823133071309
is recall-based 	 0.0020325203252032522
'' <s/> 	 0.041237113402061855
four words 	 0.14285714285714285
these algorithms 	 0.023809523809523808
events . 	 1.0
that ten 	 0.0035460992907801418
and practical 	 0.001445086705202312
the human-readable 	 0.0006920415224913495
and simpler 	 0.001445086705202312
researchers to 	 0.1
merely copy 	 0.5
worked out 	 0.2
state transducers 	 0.07142857142857142
action . 	 0.2
, pronunciation 	 0.0005614823133071309
phone call 	 0.25
that statistical 	 0.0035460992907801418
Canada and 	 0.16666666666666666
with fixed 	 0.00546448087431694
used as 	 0.04424778761061947
at Brown 	 0.029411764705882353
of N 	 0.00089126559714795
document browsing 	 0.027777777777777776
wear a 	 1.0
two include 	 0.034482758620689655
exception , 	 1.0
summarization , 	 0.08
, sociology 	 0.0005614823133071309
considers the 	 0.5
triggered other 	 1.0
the conversations 	 0.0006920415224913495
conversion of 	 0.6666666666666666
and translation 	 0.004335260115606936
project , 	 0.38461538461538464
it was 	 0.02564102564102564
newspaper articles 	 0.3333333333333333
problem , 	 0.09090909090909091
state . 	 0.07142857142857142
the state 	 0.001384083044982699
Digest , 	 0.3333333333333333
sense a 	 0.125
commands . 	 0.4
text used 	 0.006289308176100629
HLT , 	 1.0
perform by 	 0.09090909090909091
Page\/Lex\/TextRank that 	 1.0
During the 	 0.5
- for 	 0.0625
translated it 	 0.25
Text Retrieval 	 0.16666666666666666
training step 	 0.07142857142857142
and report 	 0.001445086705202312
similar methods 	 0.037037037037037035
reported for 	 0.2
Phrases , 	 1.0
readily produces 	 0.3333333333333333
Microsoft Corporation 	 0.5
sub-field of 	 1.0
world and 	 0.06666666666666667
imaging is 	 1.0
The SATZ 	 0.005208333333333333
ratings for 	 0.1111111111111111
VISTA -RRB- 	 1.0
certainty of 	 1.0
that of 	 0.028368794326241134
object , 	 0.5
accurately if 	 0.5
this would 	 0.01098901098901099
ASR '' 	 0.16666666666666666
Street Journal 	 0.6666666666666666
isolated word 	 0.2
simply do 	 0.08333333333333333
algorithms , 	 0.14285714285714285
initial capital 	 0.3333333333333333
lowest level 	 1.0
forward than 	 1.0
very likely 	 0.024390243902439025
routed through 	 0.5
grammatical information 	 0.09090909090909091
the Vulcan 	 0.0006920415224913495
expensive and 	 0.14285714285714285
Analysis and 	 0.2
breaks exist 	 0.5
the implied 	 0.0006920415224913495
scoring Truecasing 	 0.5
tag `` 	 0.0625
though the 	 0.1
must produce 	 0.07142857142857142
much additional 	 0.045454545454545456
incorrectly causing 	 1.0
disambiguate sentence 	 0.3333333333333333
because there 	 0.06666666666666667
reverse -RRB- 	 0.5
a trivial 	 0.001226993865030675
spoken , 	 0.14285714285714285
overfitting the 	 0.5
This criterion 	 0.015873015873015872
large set 	 0.043478260869565216
sentences with 	 0.013157894736842105
healthcare is 	 1.0
Critical Genre 	 0.5
performance on 	 0.05555555555555555
a member 	 0.001226993865030675
may be 	 0.40384615384615385
mental processes 	 0.6666666666666666
translate more 	 0.16666666666666666
popular `` 	 0.1111111111111111
Web 2.0 	 0.1111111111111111
trade offs 	 0.5
sequences . 	 0.2222222222222222
shared concepts 	 0.5
and of 	 0.001445086705202312
grammars -RRB- 	 0.14285714285714285
entities -LRB- 	 0.14285714285714285
early 20th-century 	 0.1
accommodate direct 	 0.2
a categorical 	 0.001226993865030675
semantic similarity 	 0.047619047619047616
only automate 	 0.02631578947368421
as artificial 	 0.003484320557491289
control , 	 0.2
, no 	 0.0016844469399213925
voice than 	 0.07692307692307693
speakers . 	 0.25
a diverse 	 0.001226993865030675
organised by 	 1.0
effective . 	 0.3333333333333333
that in 	 0.0070921985815602835
-LRB- free 	 0.0027100271002710027
false starts 	 0.5
methods of 	 0.045454545454545456
, time 	 0.0005614823133071309
to this 	 0.00796812749003984
70 's 	 0.25
used a 	 0.02654867256637168
phonetic-based categories 	 1.0
abbreviations . 	 0.2
tasks include 	 0.03125
'' -RRB- 	 0.09278350515463918
-LRB- , 	 0.0027100271002710027
: control 	 0.00980392156862745
pilot to 	 0.4
of such 	 0.004456327985739751
converted the 	 0.3333333333333333
segments at 	 0.2
hand coding 	 0.07142857142857142
machine-learning paradigm 	 0.25
also known 	 0.08695652173913043
one alternative 	 0.015384615384615385
2 -RRB- 	 0.2
In one 	 0.009523809523809525
paragraph boundaries 	 0.3333333333333333
looking waves 	 0.2
may blend 	 0.019230769230769232
The fact 	 0.005208333333333333
quite similar 	 0.125
by statistics 	 0.005714285714285714
larger context 	 0.0625
-RRB- are 	 0.008130081300813009
generation . 	 0.2222222222222222
algorithms currently 	 0.02857142857142857
naturally spoken 	 0.5
transcription . 	 0.5
allows users 	 0.25
a particular 	 0.0049079754601227
90 % 	 1.0
harder to 	 0.2857142857142857
Single Word 	 1.0
are pre-determined 	 0.004149377593360996
An intrinsic 	 0.125
draft is 	 0.5
independently developed 	 1.0
on what 	 0.009433962264150943
generation of 	 0.1111111111111111
function as 	 0.125
<s> Task 	 0.0007686395080707148
and phonemes 	 0.001445086705202312
clear why 	 0.25
approach of 	 0.02857142857142857
the advantage 	 0.0006920415224913495
in similar 	 0.0018726591760299626
a different 	 0.0036809815950920245
Speech Technology 	 0.03225806451612903
deal with 	 0.5
keyboard and 	 0.3333333333333333
<s> Improved 	 0.0007686395080707148
variously defined 	 1.0
<s> Dragon 	 0.0007686395080707148
many words 	 0.038461538461538464
research . 	 0.09523809523809523
The process 	 0.015625
English-like syntax 	 0.3333333333333333
progress in 	 0.2857142857142857
<s> Additional 	 0.0007686395080707148
defined in 	 0.16666666666666666
not well 	 0.008928571428571428
grammatical gender 	 0.09090909090909091
terms in 	 0.07692307692307693
that humans 	 0.0070921985815602835
method that 	 0.0625
sections of 	 1.0
, internet 	 0.0005614823133071309
no distinction 	 0.07692307692307693
like sentence 	 0.03571428571428571
computer read 	 0.022727272727272728
-LRB- DeRose 	 0.0027100271002710027
best -RRB- 	 0.05555555555555555
with DTW 	 0.00546448087431694
inference algorithms 	 0.25
Error Rates 	 0.5
Civil Aviation 	 1.0
The model 	 0.005208333333333333
those meanings 	 0.045454545454545456
plural noun 	 0.4
non - 	 1.0
standard -LRB- 	 0.14285714285714285
of itself 	 0.00089126559714795
Keyphrase extraction 	 0.5
naval battle 	 0.3333333333333333
sentence boundary 	 0.0625
of syntax 	 0.0017825311942959
Koine Greek 	 1.0
The successful 	 0.005208333333333333
Web or 	 0.1111111111111111
applies those 	 0.14285714285714285
message boards 	 0.5
roughness , 	 1.0
new trend 	 0.041666666666666664
accuracy reported 	 0.03225806451612903
an 11 	 0.007575757575757576
, our 	 0.0005614823133071309
and LexRank 	 0.004335260115606936
<s> Mention 	 0.0007686395080707148
speech full 	 0.006578947368421052
processing Statistical 	 0.018518518518518517
mild repetitive 	 1.0
Real progress 	 0.5
knowledge or 	 0.037037037037037035
includes Turney 	 0.14285714285714285
A procedure 	 0.02
between automatically 	 0.02564102564102564
own ; 	 0.16666666666666666
help prevent 	 0.1111111111111111
helicopters , 	 0.5
been learned 	 0.029411764705882353
-RRB- languages 	 0.0027100271002710027
is need 	 0.0020325203252032522
that appear 	 0.014184397163120567
about speech 	 0.05
simple as 	 0.07692307692307693
'' approaches 	 0.010309278350515464
the chosen 	 0.0006920415224913495
requires six 	 0.0625
web may 	 0.125
The program 	 0.005208333333333333
, EMNLP 	 0.0005614823133071309
-RRB- Court 	 0.0027100271002710027
had failed 	 0.14285714285714285
Japanese . 	 0.125
research efforts 	 0.023809523809523808
Center , 	 1.0
have in 	 0.019230769230769232
outputting language 	 0.5
the wife 	 0.0006920415224913495
-RRB- by 	 0.0027100271002710027
does a 	 0.2
on work 	 0.0047169811320754715
gaming and 	 1.0
the ALPAC 	 0.001384083044982699
than the 	 0.08888888888888889
Online , 	 0.5
the harder 	 0.0020761245674740486
substantial resources 	 0.2
are at 	 0.008298755186721992
as of 	 0.006968641114982578
angry . 	 0.5
's quality 	 0.0196078431372549
and inspired 	 0.001445086705202312
together into 	 0.125
are extractive 	 0.004149377593360996
the built 	 0.0006920415224913495
-LRB- Speereo 	 0.0027100271002710027
which range 	 0.007246376811594203
In 1629 	 0.009523809523809525
noise pertain 	 0.125
applied different 	 0.06666666666666667
, in 	 0.01909039865244245
be able 	 0.02109704641350211
scanned page 	 0.3333333333333333
of Peru 	 0.00089126559714795
representation can 	 0.05263157894736842
<s> Paul 	 0.0007686395080707148
Text segmentation 	 0.16666666666666666
; A 	 0.02127659574468085
to solve 	 0.005312084993359893
This section 	 0.031746031746031744
simulated a 	 0.5
flying in 	 1.0
search Sentence 	 0.09090909090909091
Virtually any 	 1.0
Mariani J. 	 1.0
ambiguity than 	 0.125
makes tagging 	 0.125
statistical and 	 0.030303030303030304
to disambiguate 	 0.00398406374501992
, Michel 	 0.0011229646266142617
ParaEval -RRB- 	 1.0
roadmap of 	 1.0
sets ; 	 0.09090909090909091
processing in 	 0.037037037037037035
both summarization 	 0.03225806451612903
also manual 	 0.014492753623188406
Major tasks 	 0.5
identified is 	 0.2
assign positive 	 0.2
Alessandro Duranti 	 1.0
the equivalent 	 0.0006920415224913495
SVM , 	 1.0
languages The 	 0.02
Automatic translation 	 0.1111111111111111
methods are 	 0.045454545454545456
functions . 	 0.5
form letters 	 0.05
UC -RRB- 	 0.5
<s> Document 	 0.0015372790161414297
unigrams in 	 0.25
for Greek 	 0.0036101083032490976
sentences from 	 0.02631578947368421
handwriting . 	 0.5
read characters 	 0.14285714285714285
present in 	 0.8333333333333334
glossary words 	 0.5
turns -RRB- 	 0.3333333333333333
appear often 	 0.0625
1970s and 	 0.6666666666666666
may not 	 0.09615384615384616
computer gaming 	 0.022727272727272728
context -LRB- 	 0.06060606060606061
Descartes proposed 	 1.0
trying to 	 1.0
different methods 	 0.02040816326530612
problems of 	 0.058823529411764705
in Liu 	 0.0018726591760299626
It thus 	 0.02631578947368421
reuse , 	 1.0
parses the 	 0.5
individual lines 	 0.08333333333333333
count for 	 0.2
<s> Furthermore 	 0.004611837048424289
or structures 	 0.0045045045045045045
's promise 	 0.0196078431372549
, discontinuous 	 0.0005614823133071309
and generic 	 0.001445086705202312
contexts make 	 0.14285714285714285
2 -- 	 0.2
with hand-written 	 0.00546448087431694
rules and 	 0.023255813953488372
, since 	 0.002807411566535654
, styles 	 0.0005614823133071309
to predicting 	 0.0013280212483399733
the domain 	 0.001384083044982699
be expressed 	 0.012658227848101266
most summarization 	 0.017241379310344827
constructed by 	 0.5
research and 	 0.11904761904761904
-LRB- semi 	 0.0027100271002710027
meaning but 	 0.043478260869565216
real world 	 0.3333333333333333
generation technology 	 0.1111111111111111
on any 	 0.018867924528301886
In 1978 	 0.009523809523809525
F35 Lightning 	 1.0
text . 	 0.16981132075471697
Read vs. 	 1.0
their part 	 0.029411764705882353
, 1975 	 0.0005614823133071309
Klavans J. 	 1.0
be unrealistically 	 0.004219409282700422
, mathematical 	 0.0005614823133071309
debates , 	 1.0
advertisements . 	 1.0
Approaches One 	 0.3333333333333333
the envelope 	 0.0006920415224913495
If the 	 0.4
considered good 	 0.1111111111111111
noise , 	 0.125
names . 	 0.2857142857142857
<s> Harris 	 0.0007686395080707148
notably successful 	 0.3333333333333333
documents were 	 0.02631578947368421
, Petrov 	 0.0005614823133071309
naturalness . 	 1.0
word , 	 0.03333333333333333
significant effort 	 0.1111111111111111
of producing 	 0.00089126559714795
Bush 's 	 0.5
on technology 	 0.0047169811320754715
be written 	 0.004219409282700422
proper declaration 	 0.14285714285714285
knowledge comes 	 0.037037037037037035
language is 	 0.033783783783783786
correct '' 	 0.06666666666666667
difficult for 	 0.03571428571428571
sometimes suggest 	 0.07692307692307693
HMMs -RRB- 	 0.25
interest Topics 	 0.09090909090909091
relevant entities 	 0.14285714285714285
Computationally , 	 1.0
But recognition 	 0.16666666666666666
A parser 	 0.02
, rarity 	 0.0005614823133071309
right -LRB- 	 0.1
which draws 	 0.007246376811594203
in Italy 	 0.0018726591760299626
with maximal 	 0.00546448087431694
Hollenbach 1970 	 1.0
high levels 	 0.16666666666666666
`` nine 	 0.005291005291005291
specific right 	 0.047619047619047616
people to 	 0.125
Santoni B. 	 1.0
very specific 	 0.024390243902439025
assessments of 	 1.0
below 50 	 0.2
, Gender 	 0.0005614823133071309
available resources 	 0.058823529411764705
relevant to 	 0.14285714285714285
thesis at 	 1.0
the research 	 0.0020761245674740486
discourse -LRB- 	 0.05555555555555555
as BLEU 	 0.003484320557491289
in October 	 0.0018726591760299626
Unix operating 	 0.5
under a 	 0.2
closely related 	 0.4
`` Naturally 	 0.005291005291005291
John Du 	 0.125
a letter 	 0.001226993865030675
away -LRB- 	 0.5
or generators 	 0.0045045045045045045
article then 	 0.034482758620689655
distorted , 	 0.5
FoG triggered 	 0.5
, hopefully 	 0.0005614823133071309
information are 	 0.021739130434782608
in 1965 	 0.0018726591760299626
used being 	 0.008849557522123894
may all 	 0.019230769230769232
to acquire 	 0.0013280212483399733
At this 	 0.3333333333333333
the biomedical 	 0.0006920415224913495
employs a 	 0.5
multiple approaches 	 0.07692307692307693
choice in 	 0.125
therefore help 	 0.2
Evaluation An 	 0.1111111111111111
and Development 	 0.001445086705202312
shift positions 	 1.0
Potentially , 	 1.0
1980s . 	 0.2222222222222222
to serve 	 0.0013280212483399733
use hidden 	 0.013888888888888888
nonsensical to 	 1.0
without confusions 	 0.07692307692307693
of complex 	 0.00089126559714795
data to 	 0.012987012987012988
LOB Corpus 	 1.0
<s> English 	 0.0007686395080707148
human user 	 0.043478260869565216
printed texts 	 0.08333333333333333
multiple possible 	 0.15384615384615385
In terms 	 0.009523809523809525
Language Constraints 	 0.08333333333333333
part-of-speech categories 	 0.06666666666666667
himself translated 	 0.5
and human 	 0.001445086705202312
are fields 	 0.004149377593360996
decide : 	 0.25
mentioned earlier 	 0.3333333333333333
system will 	 0.010752688172043012
overall topics 	 0.16666666666666666
coefficients to 	 0.25
ending at 	 1.0
leaders of 	 1.0
science that 	 0.1
Penn Treebank 	 0.6666666666666666
comparing the 	 0.5
a table 	 0.0036809815950920245
Desktop & 	 1.0
impact on 	 0.5
can add 	 0.0055248618784530384
enough information 	 0.2
by selecting 	 0.011428571428571429
be based 	 0.004219409282700422
conducted the 	 0.2
\/ -LRB- 	 0.3333333333333333
speech recognizers 	 0.006578947368421052
Thus , 	 0.9166666666666666
which sounds 	 0.007246376811594203
proposed as 	 0.1111111111111111
languages using 	 0.02
the overriding 	 0.0006920415224913495
intervention : 	 1.0
checking that 	 1.0
effect the 	 0.5
current commercial 	 0.14285714285714285
person . 	 0.05263157894736842
selects the 	 0.5
into phonetic-based 	 0.01282051282051282
nonexistent in 	 1.0
nice ' 	 0.25
ATN -RRB- 	 1.0
matter of 	 0.3333333333333333
and\/or religious 	 0.3333333333333333
morphological analysis 	 0.3333333333333333
a function 	 0.001226993865030675
; Analysis 	 0.02127659574468085
for billing 	 0.0036101083032490976
Automatic summarization 	 0.2222222222222222
it that 	 0.008547008547008548
common , 	 0.08
further subdivided 	 0.125
hand-crafted knowledge 	 0.5
<s> svg 	 0.0007686395080707148
of deciding 	 0.00089126559714795
contained in 	 1.0
produce interpretable 	 0.045454545454545456
and power 	 0.001445086705202312
context-free grammar 	 0.45454545454545453
set of 	 0.717948717948718
general cursive 	 0.045454545454545456
consist of 	 1.0
pronouns and 	 0.5
summarization . 	 0.12
the definition 	 0.0020761245674740486
called a 	 0.05555555555555555
proposal for 	 1.0
the tagging 	 0.001384083044982699
formulaic language 	 1.0
, creating 	 0.0011229646266142617
sentence such 	 0.020833333333333332
readable human 	 0.3333333333333333
Recognition of 	 0.25
lists that 	 1.0
dedicated OCR 	 0.3333333333333333
expansion . 	 0.3333333333333333
, probabilities 	 0.0005614823133071309
its performance 	 0.02857142857142857
<s> Discursive 	 0.0007686395080707148
new cross-discipline 	 0.041666666666666664
we would 	 0.06666666666666667
rated with 	 1.0
this type 	 0.03296703296703297
a number 	 0.024539877300613498
At the 	 0.3333333333333333
<s> Rule-based 	 0.0007686395080707148
to convey 	 0.00398406374501992
answers from 	 0.16666666666666666
its users 	 0.02857142857142857
much larger 	 0.09090909090909091
far more 	 0.25
suitability for 	 0.5
is commonly 	 0.0040650406504065045
However , 	 0.8648648648648649
of reasons 	 0.00089126559714795
-LRB- Sonic 	 0.0027100271002710027
One key 	 0.07692307692307693
goal is 	 0.42857142857142855
language models 	 0.013513513513513514
language '' 	 0.013513513513513514
Hansard corpus 	 1.0
tasks ; 	 0.03125
at varying 	 0.014705882352941176
handling differences 	 0.5
While this 	 0.2
computationally ; 	 0.5
-LRB- -LRB- 	 0.0027100271002710027
bites dog 	 0.3333333333333333
decomposition into 	 1.0
initial sounds 	 0.3333333333333333
verbs are 	 0.2
visible under 	 0.3333333333333333
without intervening 	 0.07692307692307693
derivation -LRB- 	 0.5
whole words 	 0.1111111111111111
if uttered 	 0.03571428571428571
which use 	 0.007246376811594203
the Wordnet 	 0.0006920415224913495
<s> Accuracy 	 0.003843197540353574
decided a 	 0.3333333333333333
tree -LRB- 	 1.0
highly-specialized natural 	 1.0
for avoiding 	 0.0036101083032490976
used about 	 0.008849557522123894
do research 	 0.038461538461538464
reference that 	 0.125
memorandum `` 	 1.0
of adaptive 	 0.00089126559714795
only specific 	 0.02631578947368421
, Oklahoma 	 0.0005614823133071309
since 1965 	 0.1
MMR -RRB- 	 1.0
summarization on 	 0.02
. . 	 0.00078003120124805
not necessary 	 0.008928571428571428
programs , 	 0.18181818181818182
Hard of 	 0.5
vision . 	 1.0
similar ideas 	 0.037037037037037035
des parties 	 1.0
same meaning 	 0.04
tasks like 	 0.0625
<s> Prominent 	 0.0007686395080707148
a unit 	 0.001226993865030675
lead-in fighter 	 1.0
place to 	 0.25
<s> Profile 	 0.0007686395080707148
than only 	 0.044444444444444446
PageRank on 	 0.16666666666666666
within their 	 0.05555555555555555
probabilistic context-free 	 0.14285714285714285
syntax and 	 0.09090909090909091
hand-printed text 	 0.5
requires one 	 0.0625
, articulation 	 0.0005614823133071309
Some ISO 	 0.047619047619047616
studies , 	 0.5
automatically answer 	 0.047619047619047616
'' from 	 0.005154639175257732
to such 	 0.0026560424966799467
and trigrams 	 0.002890173410404624
book does 	 0.125
bootstrap using 	 1.0
<s> Such 	 0.006149116064565719
prefer to 	 0.5
a time 	 0.00245398773006135
result will 	 0.09090909090909091
possible improvement 	 0.041666666666666664
number 20 	 0.023255813953488372
Algorithms which 	 0.5
end up 	 0.25
vocabulary of 	 0.125
-RRB- evaluation 	 0.0027100271002710027
be trained 	 0.004219409282700422
<s> Before 	 0.0007686395080707148
keep a 	 0.3333333333333333
Analysis & 	 0.2
A.C. Nielsen 	 1.0
propositions , 	 1.0
triples that 	 0.3333333333333333
Cloud Computing 	 1.0
was connected 	 0.012987012987012988
expensive to 	 0.14285714285714285
and found 	 0.001445086705202312
information that 	 0.021739130434782608
answered , 	 0.2
for triples 	 0.0036101083032490976
industry . 	 0.3333333333333333
: fact 	 0.00980392156862745
4 letters 	 0.2
, rhetoric 	 0.0005614823133071309
<s> Whether 	 0.0015372790161414297
flexibility and 	 1.0
, spacecraft 	 0.0005614823133071309
data maintained 	 0.012987012987012988
Truecasing Statistical 	 1.0
Friday have 	 1.0
the interim 	 0.0006920415224913495
think that 	 0.3333333333333333
use training 	 0.027777777777777776
the whole 	 0.0006920415224913495
as from 	 0.003484320557491289
numbers , 	 0.2857142857142857
parametric values 	 1.0
the Viterbi 	 0.002768166089965398
for Italian 	 0.0036101083032490976
section called 	 0.16666666666666666
some statistical 	 0.012048192771084338
Dynamic Programming 	 0.2
, resulting 	 0.0005614823133071309
informativeness of 	 0.6666666666666666
: Stemming 	 0.00980392156862745
, James 	 0.0022459292532285235
Internet and 	 0.5
given NLP 	 0.041666666666666664
DARPA Speech 	 0.25
entirety , 	 1.0
, Norman 	 0.0005614823133071309
Commercial research 	 0.5
EMNLP , 	 1.0
two simple 	 0.034482758620689655
security process 	 1.0
Wordnet lexicon 	 1.0
is sufficient 	 0.0040650406504065045
at IBM 	 0.014705882352941176
each segment 	 0.044444444444444446
even `` 	 0.037037037037037035
order for 	 0.07142857142857142
on lexicon 	 0.0047169811320754715
use neural 	 0.013888888888888888
's EndWar 	 0.0196078431372549
sentence that 	 0.041666666666666664
SVOX . 	 1.0
that adjectives 	 0.0035460992907801418
computer . 	 0.06818181818181818
specific person 	 0.047619047619047616
provided a 	 0.2
as weighted 	 0.003484320557491289
The success 	 0.005208333333333333
financial section 	 0.25
personal computer 	 0.25
received a 	 0.5
tasks that 	 0.03125
Senseval and 	 1.0
-LRB- SWER 	 0.0027100271002710027
the news 	 0.001384083044982699
That is 	 1.0
<s> All 	 0.0007686395080707148
Pennsylvania in 	 1.0
multiply . 	 1.0
, signed 	 0.0005614823133071309
<s> Similarly 	 0.0007686395080707148
Parsers may 	 0.5
web blogs 	 0.125
practice of 	 0.5
machine at 	 0.012658227848101266
reveal that 	 1.0
hands , 	 1.0
this regard 	 0.01098901098901099
small integer 	 0.1111111111111111
no pauses 	 0.07692307692307693
1989 -RRB- 	 0.5
that , 	 0.0035460992907801418
: Automatically 	 0.00980392156862745
Web as 	 0.1111111111111111
stochastic . 	 0.125
of man-hours 	 0.00089126559714795
theory -RRB- 	 0.07692307692307693
<s> Speaker 	 0.0015372790161414297
and etc. 	 0.001445086705202312
were workshops 	 0.024390243902439025
workshops dedicated 	 0.5
on neat 	 0.0047169811320754715
on his 	 0.0047169811320754715
discriminative training 	 1.0
's work 	 0.0196078431372549
-RRB- can 	 0.008130081300813009
, science 	 0.0005614823133071309
were compared 	 0.04878048780487805
Formal equivalence 	 1.0
Whether NLP 	 0.5
Further applications 	 0.3333333333333333
simplified the 	 0.5
general format 	 0.045454545454545456
, a 	 0.02695115103874228
rarity and 	 1.0
resolve . 	 0.25
conversations , 	 0.3333333333333333
aspect , 	 0.5
corpus with 	 0.03225806451612903
; rejecting 	 0.0425531914893617
In corpus 	 0.009523809523809525
check how 	 0.5
ICASSP , 	 1.0
rather than 	 0.875
University by 	 0.1111111111111111
Newton pioneered 	 1.0
, complicating 	 0.0005614823133071309
other disciplines 	 0.014285714285714285
code , 	 0.14285714285714285
is transformed 	 0.0020325203252032522
, Jelinek 	 0.0005614823133071309
notations , 	 0.5
the words 	 0.004152249134948097
walk is 	 0.2
The Army 	 0.005208333333333333
central '' 	 0.6666666666666666
second system 	 0.1
include the 	 0.18518518518518517
error -LRB- 	 0.16666666666666666
`` Natural 	 0.005291005291005291
Japanese and 	 0.25
, text-to-speech 	 0.0011229646266142617
from script 	 0.009615384615384616
rescoring -RRB- 	 1.0
using machine 	 0.01694915254237288
scientific and 	 0.5
Jelinek F. 	 0.5
mainly with 	 0.16666666666666666
fulfill expectations 	 0.5
which he 	 0.007246376811594203
It sometimes 	 0.02631578947368421
recommending '' 	 1.0
<s> Tokens 	 0.0007686395080707148
the postal 	 0.0006920415224913495
larger corpus 	 0.125
Text grammar 	 0.16666666666666666
nautical term 	 0.5
and actioning 	 0.001445086705202312
affine and 	 1.0
be ranked 	 0.004219409282700422
sound on 	 0.05
There are 	 0.5454545454545454
ranked adjacent 	 0.2
especially because 	 0.06666666666666667
this makes 	 0.01098901098901099
categories in 	 0.1111111111111111
form words 	 0.05
sometimes had 	 0.07692307692307693
as multiple 	 0.003484320557491289
and movie 	 0.001445086705202312
adjectives and 	 0.3333333333333333
intuitive sense 	 1.0
era of 	 1.0
smart keyboard 	 1.0
our alphabetic 	 0.2
as input 	 0.006968641114982578
this document 	 0.01098901098901099
history . 	 0.25
as questions 	 0.003484320557491289
of several 	 0.00267379679144385
subjectivity\/objectivity identification 	 1.0
words -RRB- 	 0.027522935779816515
platform to 	 0.5
content of 	 0.08333333333333333
set a 	 0.05128205128205128
<s> By 	 0.0007686395080707148
an important 	 0.015151515151515152
, V.J. 	 0.0005614823133071309
'' corpora 	 0.005154639175257732
as often 	 0.003484320557491289
One could 	 0.07692307692307693
determining whether 	 0.16666666666666666
on Semantic 	 0.0047169811320754715
be approximated 	 0.004219409282700422
stop character 	 1.0
word at 	 0.016666666666666666
synthesis techniques 	 1.0
two classes 	 0.034482758620689655
be linked 	 0.008438818565400843
a lot 	 0.0036809815950920245
Parliament of 	 0.5
that will 	 0.0070921985815602835
want to 	 0.8333333333333334
possible answers 	 0.041666666666666664
that these 	 0.010638297872340425
cepstral normalization 	 0.5
years . 	 0.19047619047619047
search . 	 0.09090909090909091
Reinvestment Act 	 1.0
, real-time 	 0.0005614823133071309
reduced amount 	 0.25
right '' 	 0.1
, dimensionality 	 0.0005614823133071309
controller would 	 0.25
into two 	 0.01282051282051282
with high 	 0.01092896174863388
make a 	 0.2
its utility 	 0.02857142857142857
would have 	 0.05660377358490566
<s> Deferred 	 0.0007686395080707148
compare their 	 0.14285714285714285
table is 	 0.14285714285714285
Language Processor 	 0.08333333333333333
hand-produced rules 	 1.0
with rules 	 0.00546448087431694
In fact 	 0.0380952380952381
hidden Markov 	 0.875
longer sentences 	 1.0
OCR '' 	 0.04081632653061224
pitch , 	 1.0
harmonic mean 	 1.0
<s> ... 	 0.0007686395080707148
different realizations 	 0.02040816326530612
-LRB- some 	 0.0027100271002710027
morphological distinctions 	 0.3333333333333333
and SVOX 	 0.001445086705202312
short-time units 	 0.5
algorithm -RRB- 	 0.03571428571428571
, word 	 0.0005614823133071309
<s> Hybrid 	 0.0007686395080707148
the Vocabulary 	 0.0006920415224913495
languages or 	 0.02
Machine translation 	 0.5555555555555556
entirely committed 	 0.5
photos against 	 1.0
computer programming 	 0.022727272727272728
rules can 	 0.06976744186046512
would check 	 0.018867924528301886
look to 	 0.2
<s> On 	 0.003843197540353574
selected as 	 0.5
process of 	 0.3333333333333333
it requires 	 0.017094017094017096
seek to 	 1.0
algorithms used 	 0.02857142857142857
cepstral coefficients 	 0.5
words , 	 0.13761467889908258
to guide 	 0.0013280212483399733
connected by 	 0.2
Gary Hendrix 	 1.0
needed . 	 0.09523809523809523
` the 	 0.0625
demonstration in 	 0.2
degradation in 	 1.0
contain multiple 	 0.08333333333333333
be presented 	 0.004219409282700422
MLLT -RRB- 	 1.0
may contain 	 0.038461538461538464
recognition for 	 0.008264462809917356
Many documents 	 0.08333333333333333
abstract synopsis 	 1.0
the tagset 	 0.0006920415224913495
of Hearing 	 0.00089126559714795
up with 	 0.13636363636363635
corpora that 	 0.09090909090909091
level , 	 0.2
to multi-document 	 0.0013280212483399733
of implementing 	 0.00089126559714795
possible semantics 	 0.041666666666666664
analyses to 	 0.2
have on 	 0.009615384615384616
TWA where 	 1.0
relate to 	 1.0
Patent 2,026,329 	 0.3333333333333333
lowering of 	 1.0
including web 	 0.07142857142857142
ask the 	 0.5
to discrete 	 0.0013280212483399733
fashion , 	 1.0
written including 	 0.038461538461538464
measure of 	 0.18181818181818182
up differently 	 0.045454545454545456
Army Avionics 	 0.25
sometimes provided 	 0.07692307692307693
programs have 	 0.09090909090909091
other commercial 	 0.014285714285714285
% range 	 0.02564102564102564
techniques fall 	 0.043478260869565216
dBase system 	 1.0
, reasoning 	 0.0005614823133071309
of computational 	 0.00267379679144385
rephrase sentences 	 1.0
general use 	 0.045454545454545456
the English 	 0.0020761245674740486
door '' 	 0.5
'' summary 	 0.005154639175257732
relationship extraction 	 0.3333333333333333
ATNs used 	 0.3333333333333333
, potentially 	 0.0005614823133071309
needed -RRB- 	 0.6190476190476191
is that 	 0.024390243902439025
Much remains 	 0.3333333333333333
lexical segmentation 	 0.07692307692307693
generating natural 	 0.2
Topics of 	 1.0
applications Aerospace 	 0.04
contribute to 	 1.0
% accuracy 	 0.10256410256410256
dog bites 	 0.3333333333333333
using algorithms 	 0.01694915254237288
of unlabeled 	 0.00089126559714795
linear representation 	 0.14285714285714285
or 100000 	 0.0045045045045045045
templates may 	 1.0
summaries automatically 	 0.023255813953488372
many artificial 	 0.019230769230769232
simply guessed 	 0.08333333333333333
a societal 	 0.001226993865030675
humans possess 	 0.08333333333333333
system can 	 0.010752688172043012
starts with 	 0.5
embedded system 	 0.25
inflection is 	 1.0
mentions -LRB- 	 0.3333333333333333
dog to 	 0.3333333333333333
minimum classification 	 0.5
in multiple 	 0.0018726591760299626
job ; 	 0.5
representation , 	 0.15789473684210525
given loss 	 0.041666666666666664
extract sentences 	 0.25
different left 	 0.02040816326530612
letters : 	 0.1
often . 	 0.022727272727272728
lines segments 	 0.3333333333333333
Language modeling 	 0.08333333333333333
new approaches 	 0.041666666666666664
voice-activation , 	 1.0
generated readable 	 0.06666666666666667
studies and 	 0.25
processing of 	 0.037037037037037035
though this 	 0.1
pre-marked . 	 1.0
been tried 	 0.029411764705882353
have also 	 0.009615384615384616
to encode 	 0.0013280212483399733
levels in 	 0.045454545454545456
for creating 	 0.0036101083032490976
merged with 	 1.0
1957 and 	 1.0
usually are 	 0.03125
← barmaid 	 1.0
existing words 	 0.2
mail , 	 0.5
directly . 	 0.2
sales reports 	 0.3333333333333333
adaptive summarization 	 0.6666666666666666
produce more 	 0.09090909090909091
-LRB- EMR 	 0.0027100271002710027
its answer 	 0.02857142857142857
translation process 	 0.02702702702702703
mean word 	 0.5
within computer 	 0.05555555555555555
, Adam 	 0.0005614823133071309
for input 	 0.0036101083032490976
computer in 	 0.045454545454545456
1998 Language 	 0.25
reliability -RRB- 	 0.5
\/ target-language-independent 	 0.3333333333333333
Parliament . 	 0.5
English prose 	 0.02702702702702703
exclusively to 	 1.0
the basic 	 0.001384083044982699
In information 	 0.009523809523809525
levels are 	 0.045454545454545456
and instead 	 0.001445086705202312
just keeping 	 0.1111111111111111
`` Translation 	 0.005291005291005291
already discussed 	 0.2
work derived 	 0.041666666666666664
less than 	 0.25
programs to 	 0.09090909090909091
objectives : 	 0.5
customers , 	 0.5
Documents '' 	 1.0
answers . 	 0.08333333333333333
unlimited range 	 1.0
any feedback 	 0.03225806451612903
then end 	 0.02857142857142857
evaluating the 	 0.2
`` New 	 0.005291005291005291
of listening 	 0.00089126559714795
becomes . 	 0.25
and aircraft 	 0.001445086705202312
may pick 	 0.019230769230769232
Ethnomethodology . 	 1.0
morphosyntactic descriptor 	 1.0
it offered 	 0.008547008547008548
later became 	 0.1
the set 	 0.001384083044982699
<s> Therein 	 0.0007686395080707148
action applied 	 0.2
count . 	 0.2
, individual 	 0.0005614823133071309
factors which 	 0.3333333333333333
with larger 	 0.00546448087431694
logical deduction 	 0.16666666666666666
dynamics of 	 0.5
medical informatics 	 0.16666666666666666
, read 	 0.0011229646266142617
components can 	 0.2
called Cross-Sentence 	 0.05555555555555555
to human-written 	 0.0013280212483399733
class of 	 0.75
two senses 	 0.034482758620689655
that assigns 	 0.0035460992907801418
Foucault himself 	 0.3333333333333333
, columns 	 0.0005614823133071309
are systems 	 0.012448132780082987
, topics 	 0.0005614823133071309
alignment software 	 0.5
speaker as 	 0.05555555555555555
, modern 	 0.0005614823133071309
a piece 	 0.00245398773006135
<s> Extracted 	 0.0007686395080707148
or dictionary 	 0.0045045045045045045
We have 	 0.14285714285714285
or speech 	 0.0045045045045045045
Association for 	 1.0
, comprising 	 0.0005614823133071309
by Frost 	 0.005714285714285714
as text 	 0.003484320557491289
, communication 	 0.0005614823133071309
parser proposes 	 0.0625
good source 	 0.07692307692307693
modules , 	 0.5
processing , 	 0.16666666666666666
Pierce wrote 	 1.0
making them 	 0.14285714285714285
warping is 	 0.5
Dog bites 	 1.0
Street . 	 0.3333333333333333
converted them 	 0.3333333333333333
general term 	 0.045454545454545456
unit vertices 	 0.3333333333333333
Speereo Voice 	 0.5
best , 	 0.05555555555555555
selecting a 	 0.4
additional costs 	 0.16666666666666666
table '' 	 0.14285714285714285
event , 	 0.6666666666666666
are unambiguous 	 0.004149377593360996
has increased 	 0.011904761904761904
long input 	 0.5
first . 	 0.030303030303030304
He decided 	 0.125
-LRB- with 	 0.008130081300813009
objective document 	 0.2
temporal dependencies 	 0.5
, matching 	 0.0005614823133071309
, location 	 0.0005614823133071309
seem to 	 0.5
-LRB- correct 	 0.0027100271002710027
parser that 	 0.0625
written conversation 	 0.038461538461538464
of recognition 	 0.0017825311942959
much better 	 0.045454545454545456
Air controller 	 0.3333333333333333
less time 	 0.08333333333333333
pollen example 	 0.07692307692307693
words coming 	 0.009174311926605505
type is 	 0.14285714285714285
<s> Solving 	 0.0007686395080707148
: Given 	 0.09803921568627451
think of 	 0.3333333333333333
different relationships 	 0.02040816326530612
been superseded 	 0.014705882352941176
to decide 	 0.0026560424966799467
with either 	 0.00546448087431694
to lower 	 0.0013280212483399733
from , 	 0.009615384615384616
any number 	 0.03225806451612903
than polarity 	 0.022222222222222223
received considerable 	 0.5
judgments . 	 1.0
typically evaluated 	 0.05555555555555555
similar clues 	 0.037037037037037035
The ability 	 0.005208333333333333
and not 	 0.011560693641618497
global semitied 	 0.3333333333333333
but this 	 0.058823529411764705
still disagree 	 0.06666666666666667
for some 	 0.018050541516245487
of finite 	 0.00089126559714795
to Recognize 	 0.0013280212483399733
vastly less 	 1.0
gracefully with 	 1.0
to backup 	 0.0013280212483399733
using unweighted 	 0.01694915254237288
there would 	 0.025
were walking 	 0.024390243902439025
one that 	 0.015384615384615385
99 % 	 1.0
rates even 	 0.125
-LRB- 3rd 	 0.0027100271002710027
sold to 	 0.3333333333333333
dependencies . 	 1.0
Video games 	 1.0
tasks returning 	 0.03125
to 150 	 0.0013280212483399733
languages concepts 	 0.02
map one 	 0.5
the microphone 	 0.0006920415224913495
use in 	 0.013888888888888888
`` book 	 0.005291005291005291
Mobile devices 	 0.3333333333333333
both speech 	 0.03225806451612903
Voice commands 	 0.2
event . 	 0.3333333333333333
process that 	 0.05555555555555555
elements of 	 0.25
More powerful 	 0.1111111111111111
involve learning 	 0.16666666666666666
Two of 	 0.2857142857142857
Broadly , 	 1.0
polarity helped 	 0.125
the factors 	 0.0006920415224913495
on ; 	 0.0047169811320754715
HMT -RRB- 	 1.0
was proposed 	 0.025974025974025976
Keyphrase extractors 	 0.25
but only 	 0.014705882352941176
tools for 	 0.16666666666666666
with a 	 0.1092896174863388
% of 	 0.20512820512820512
Mr. is 	 0.5
camera . 	 0.5
and 2007 	 0.001445086705202312
in taxonomies 	 0.0018726591760299626
Penicillin '' 	 1.0
a readable 	 0.001226993865030675
Research Institute 	 0.125
In 1987 	 0.009523809523809525
day did 	 1.0
but such 	 0.014705882352941176
ever appear 	 1.0
features are 	 0.11538461538461539
is copied 	 0.0020325203252032522
any font 	 0.03225806451612903
one to 	 0.03076923076923077
Loriot & 	 1.0
machine representation 	 0.02531645569620253
those it 	 0.045454545454545456
clues are 	 0.3333333333333333
is to 	 0.03861788617886179
definitional questions 	 1.0
was painstakingly 	 0.012987012987012988
to treat 	 0.0013280212483399733
algorithms -- 	 0.02857142857142857
Canada Post 	 0.16666666666666666
being used 	 0.05555555555555555
turns . 	 0.3333333333333333
forms can 	 0.16666666666666666
commonly taught 	 0.125
probably use 	 0.25
intent . 	 1.0
The product 	 0.010416666666666666
the development 	 0.0034602076124567475
increased so 	 0.2
from MUC 	 0.009615384615384616
roughly , 	 0.6666666666666666
an entity 	 0.007575757575757576
Michel Foucault 	 1.0
counter examples 	 1.0
is precision 	 0.0020325203252032522
issue is 	 0.125
way sentiment 	 0.041666666666666664
infer where 	 1.0
will usually 	 0.02857142857142857
digitalized : 	 1.0
customisation by 	 1.0
theory and 	 0.07692307692307693
inaccurate or 	 1.0
contractions , 	 0.5
decode the 	 1.0
with reference 	 0.00546448087431694
input sales 	 0.024390243902439025
Computer Speech 	 0.3333333333333333
comes from 	 0.4
to multi-platforms 	 0.0013280212483399733
processes of 	 0.2
capabilities of 	 0.2
grammars of 	 0.07142857142857142
, relay 	 0.0005614823133071309
disambiguation Word-sense 	 0.1
, Brenton 	 0.0005614823133071309
Christmas fall 	 1.0
emotional state 	 0.25
recognize equivalent 	 0.1111111111111111
is quite 	 0.0020325203252032522
displays . 	 1.0
generated summary 	 0.06666666666666667
even more 	 0.037037037037037035
trivial word 	 0.25
<s> During 	 0.0030745580322828594
and probably 	 0.001445086705202312
confusability Speaker 	 1.0
rarely the 	 0.3333333333333333
Kurzweil sold 	 0.14285714285714285
to resolve 	 0.00398406374501992
also attempt 	 0.014492753623188406
underlying formal 	 0.3333333333333333
most other 	 0.017241379310344827
heuristics with 	 0.5
segment . 	 0.1111111111111111
was applied 	 0.012987012987012988
portable . 	 0.3333333333333333
lattice -RRB- 	 1.0
a closed-captioning 	 0.001226993865030675
broadcast news 	 1.0
predicted value 	 0.5
by voice 	 0.011428571428571429
, find 	 0.0011229646266142617
subsystem for 	 1.0
20 % 	 1.0
combined in 	 0.5
fragments that 	 1.0
exploits the 	 1.0
semantic theories 	 0.047619047619047616
After 30 	 0.3333333333333333
over sixty 	 0.08333333333333333
either manually 	 0.1
semantic ; 	 0.047619047619047616
as debates 	 0.003484320557491289
chatterbots were 	 0.5
should not 	 0.05263157894736842
the availability 	 0.0006920415224913495
far , 	 0.125
typically agree 	 0.05555555555555555
article and 	 0.06896551724137931
language generation 	 0.033783783783783786
cultural factors 	 1.0
higher order 	 0.14285714285714285
achieving fully 	 0.5
together in 	 0.125
major limitation 	 0.08333333333333333
play in 	 1.0
and business 	 0.001445086705202312
The difficulty 	 0.015625
linguistics . 	 0.05
the `` 	 0.0034602076124567475
be given 	 0.004219409282700422
web pages 	 0.125
Tagset '' 	 1.0
sentence transformations 	 0.020833333333333332
reranking in 	 1.0
significant momentum 	 0.1111111111111111
with machine 	 0.00546448087431694
a whole 	 0.00245398773006135
dependency parsers 	 0.2
approach in 	 0.02857142857142857
data category 	 0.012987012987012988
do you 	 0.038461538461538464
's annual 	 0.0196078431372549
Guidelines for 	 0.5
being referred 	 0.05555555555555555
'' continued 	 0.005154639175257732
feasible to 	 0.5
verb or 	 0.23076923076923078
the magazine 	 0.0006920415224913495
In 1996 	 0.009523809523809525
action should 	 0.2
gives examples 	 0.5
Regardless of 	 1.0
meteorologist -RRB- 	 1.0
a field 	 0.0036809815950920245
2009 -RRB- 	 0.3333333333333333
summarization hopes 	 0.02
forms of 	 0.3333333333333333
1982 , 	 0.3333333333333333
way , 	 0.041666666666666664
For some 	 0.01639344262295082
above . 	 0.07692307692307693
Plot Units 	 1.0
being scanned 	 0.05555555555555555
Halliday , 	 1.0
on casual 	 0.0047169811320754715
related to 	 0.26666666666666666
, Naive 	 0.0005614823133071309
and identify 	 0.002890173410404624
time on 	 0.030303030303030304
space . 	 0.2
avoiding linguistic 	 0.5
not sound 	 0.008928571428571428
& Server 	 0.125
Das , 	 1.0
-LRB- DARPA 	 0.0027100271002710027
all about 	 0.023255813953488372
delimiter . 	 1.0
source code 	 0.041666666666666664
illustrates some 	 0.5
4 , 	 0.2
by adding 	 0.011428571428571429
Speech and 	 0.16129032258064516
IE additionally 	 0.3333333333333333
Patent 1,915,993 	 0.3333333333333333
text printed 	 0.006289308176100629
<s> Last 	 0.0007686395080707148
sound signal 	 0.05
\* '' 	 0.25
ISO standards 	 0.5
's voices 	 0.0196078431372549
better QA 	 0.1111111111111111
section requires 	 0.3333333333333333
beer , 	 1.0
the attitude 	 0.0006920415224913495
fully solved 	 0.16666666666666666
The user 	 0.005208333333333333
Technologies that 	 1.0
by trying 	 0.005714285714285714
Language , 	 0.08333333333333333
database of 	 0.2
to threshold 	 0.0013280212483399733
environment . 	 0.3333333333333333
to submit 	 0.0013280212483399733
'' that 	 0.020618556701030927
been made 	 0.014705882352941176
grammar which 	 0.05405405405405406
Graesser , 	 1.0
train their 	 1.0
<s> Early 	 0.0015372790161414297
only succeeding 	 0.02631578947368421
John Pierce 	 0.125
and Japanese 	 0.001445086705202312
not typically 	 0.017857142857142856
to artificial 	 0.0026560424966799467
classification . 	 0.11764705882352941
disagree that 	 0.3333333333333333
a consideration 	 0.001226993865030675
For more 	 0.03278688524590164
the vowel 	 0.001384083044982699
<s> Real 	 0.0015372790161414297
graph small 	 0.07692307692307693
the technology 	 0.0006920415224913495
have produced 	 0.009615384615384616
Progress mainly 	 1.0
the last 	 0.0020761245674740486
syntax effectively 	 0.09090909090909091
in of 	 0.0018726591760299626
basically a 	 1.0
can often 	 0.0055248618784530384
combining the 	 0.25
Automotive speech 	 1.0
approximation of 	 0.16666666666666666
years long 	 0.047619047619047616
and extract 	 0.001445086705202312
by domain 	 0.005714285714285714
progressed over 	 1.0
tagging could 	 0.04
constraints . 	 0.25
major design 	 0.08333333333333333
improve information 	 0.07692307692307693
electronically searched 	 1.0
or adjective 	 0.0045045045045045045
starts , 	 0.5
Processing -LRB- 	 0.75
developed transformational 	 0.038461538461538464
IE -RRB- 	 0.6666666666666666
vulnerable to 	 1.0
the string 	 0.0006920415224913495
Eugene Charniak 	 1.0
UMLS -RRB- 	 1.0
such name 	 0.008130081300813009
discourse , 	 0.08333333333333333
of two 	 0.0017825311942959
various term 	 0.05555555555555555
as division 	 0.003484320557491289
of online 	 0.00089126559714795
decomposing it 	 1.0
Steven DeRose 	 1.0
-RRB- . 	 0.27371273712737126
meaningful portions 	 0.125
implemented using 	 0.2
of how 	 0.00089126559714795
intelligence -LRB- 	 0.125
those patterns 	 0.045454545454545456
tagger on 	 0.1111111111111111
Orleans by 	 0.5
number of 	 0.8372093023255814
description and 	 1.0
be formally 	 0.004219409282700422
and early 	 0.001445086705202312
high at 	 0.05555555555555555
value . 	 0.3333333333333333
dialogue in 	 0.5
first statistical 	 0.06060606060606061
NLP techniques 	 0.0425531914893617
categories ; 	 0.1111111111111111
whose easy-to-use 	 0.3333333333333333
evidence for 	 0.5
of their 	 0.0017825311942959
, thereby 	 0.0005614823133071309
significant taggers 	 0.1111111111111111
in those 	 0.0018726591760299626
speech dynamics 	 0.006578947368421052
For other 	 0.01639344262295082
strategy for 	 0.2
and voice 	 0.001445086705202312
and generating 	 0.001445086705202312
the Turney 	 0.0006920415224913495
word British 	 0.016666666666666666
Overall organization 	 1.0
intermediary , 	 0.3333333333333333
program and 	 0.045454545454545456
the ones 	 0.001384083044982699
an in-depth 	 0.007575757575757576
approach applies 	 0.02857142857142857
instructions . 	 1.0
human -- 	 0.021739130434782608
segmentation will 	 0.030303030303030304
past thirty 	 0.3333333333333333
successful NLG 	 0.1111111111111111
level we 	 0.05
an enormous 	 0.007575757575757576
uncertainties at 	 1.0
<s> Read 	 0.0007686395080707148
study of 	 0.25
The interpretation 	 0.005208333333333333
has to 	 0.05952380952380952
`` have 	 0.005291005291005291
Wikipedia 's 	 0.5
political discourse 	 0.3333333333333333
Independent '' 	 1.0
, Ernesto 	 0.0005614823133071309
order in 	 0.07142857142857142
using photocells 	 0.01694915254237288
analyses . 	 0.4
Federation of 	 1.0
precise function 	 0.3333333333333333
critical tasks 	 0.25
for help 	 0.0036101083032490976
speech is 	 0.006578947368421052
automatic translation 	 0.08695652173913043
others were 	 0.08333333333333333
textual summary 	 0.2
divided into 	 0.6666666666666666
usefully be 	 1.0
3 , 	 0.2
, any 	 0.0005614823133071309
is limited 	 0.0020325203252032522
sound to 	 0.05
difference was 	 0.25
interpreter , 	 0.5
representations . 	 0.25
meaningful units 	 0.125
`` open 	 0.005291005291005291
human-made summaries 	 0.5
Emanuel Goldberg 	 0.5
sense in 	 0.125
: Top-down 	 0.00980392156862745
working for 	 0.14285714285714285
is leading 	 0.0020325203252032522
human judge 	 0.021739130434782608
easy to 	 1.0
as normalization 	 0.003484320557491289
the unigrams 	 0.001384083044982699
-RRB- do 	 0.0027100271002710027
part-of-speech assignment 	 0.06666666666666667
are written 	 0.004149377593360996
gold standards 	 0.16666666666666666
simple bar 	 0.038461538461538464
people would 	 0.0625
objects -LRB- 	 0.2
understanding the 	 0.12121212121212122
analyzed for 	 0.2
a short 	 0.006134969325153374
allowable expression 	 0.5
the labor 	 0.0006920415224913495
model summaries 	 0.06666666666666667
performance had 	 0.05555555555555555
1993 there 	 0.3333333333333333
50 to 	 0.3333333333333333
Acoustical distortions 	 0.5
vertices ? 	 0.1111111111111111
collaborated to 	 1.0
Large-scale evaluation 	 1.0
are MARGIE 	 0.004149377593360996
extraction can 	 0.03225806451612903
e.g. with 	 0.017857142857142856
parsing concatenated 	 0.03571428571428571
adjective 40 	 0.14285714285714285
language such 	 0.006756756756756757
To decode 	 0.1111111111111111
Paul Drew 	 0.2
are highly 	 0.004149377593360996
of what 	 0.0035650623885918
documents and 	 0.02631578947368421
formats like 	 1.0
Document Understanding 	 0.25
is better 	 0.0020325203252032522
This parses 	 0.015873015873015872
area -RRB- 	 0.09090909090909091
submit them 	 0.5
improve this 	 0.15384615384615385
Other areas 	 0.14285714285714285
1980s saw 	 0.1111111111111111
several methods 	 0.045454545454545456
rules by 	 0.023255813953488372
When punctuation 	 0.14285714285714285
and recording 	 0.001445086705202312
role as 	 0.25
the ARCHILES 	 0.0006920415224913495
sentences so 	 0.013157894736842105
and when 	 0.001445086705202312
<s> TextRank 	 0.0023059185242121443
about as 	 0.025
The various 	 0.005208333333333333
vocabularies , 	 1.0
: Natural 	 0.00980392156862745
and usefulness 	 0.001445086705202312
nascent online 	 1.0
is phonetically 	 0.0020325203252032522
difficult task 	 0.03571428571428571
kinds of 	 1.0
a comprehensive 	 0.0036809815950920245
`` ask 	 0.005291005291005291
limited type 	 0.1
simple morphology 	 0.038461538461538464
Typhoon currently 	 1.0
design the 	 0.25
UK RAF 	 0.25
where much 	 0.02857142857142857
keyphrases for 	 0.05714285714285714
students , 	 0.3333333333333333
publications . 	 1.0
: Example-based 	 0.00980392156862745
over most 	 0.08333333333333333
that overlap 	 0.0035460992907801418
a five-star 	 0.001226993865030675
uses cosine 	 0.07142857142857142
technology are 	 0.045454545454545456
Also , 	 1.0
Wallace Chafe 	 1.0
least one 	 0.2
only in 	 0.05263157894736842
severe in 	 1.0
question classification 	 0.023809523809523808
parsers can 	 0.07692307692307693
in English 	 0.009363295880149813
question understanding 	 0.023809523809523808
work . 	 0.08333333333333333
help automatic 	 0.1111111111111111
discontinuous , 	 0.3333333333333333
how many 	 0.10344827586206896
Please help 	 0.6666666666666666
favor a 	 0.5
any markup 	 0.03225806451612903
linguistics -RRB- 	 0.1
available soon 	 0.058823529411764705
questions asking 	 0.038461538461538464
phones . 	 0.5
runs PageRank 	 1.0
Orleans '' 	 0.5
Intelligent Machines 	 0.3333333333333333
, Stephen 	 0.0005614823133071309
trend to 	 1.0
version of 	 0.6666666666666666
chance of 	 1.0
HMMs learn 	 0.125
`` to 	 0.010582010582010581
dynamic motion 	 0.2
using some 	 0.03389830508474576
year . 	 0.5
Chinese have 	 0.14285714285714285
be turned 	 0.004219409282700422
networks as 	 0.07142857142857142
as adjectives 	 0.003484320557491289
table of 	 0.42857142857142855
country . 	 0.5
On what 	 0.16666666666666666
For this 	 0.04918032786885246
words accidentally 	 0.009174311926605505
same as 	 0.08
algorithm known 	 0.03571428571428571
U.S. Army 	 0.14285714285714285
Why , 	 0.14285714285714285
expected answer 	 0.14285714285714285
of 2009 	 0.00089126559714795
morphological , 	 0.3333333333333333
even human 	 0.037037037037037035
one detail 	 0.015384615384615385
as SVM 	 0.003484320557491289
Rogerian psychotherapist 	 1.0
and paragraphs 	 0.001445086705202312
separate it 	 0.2
stopwords . 	 1.0
levels for 	 0.13636363636363635
and singular 	 0.001445086705202312
be modified 	 0.004219409282700422
solely on 	 1.0
appear as 	 0.0625
necessary subtask 	 0.1
judge fluency 	 0.25
information and 	 0.021739130434782608
expensive , 	 0.42857142857142855
much slower 	 0.09090909090909091
taking the 	 0.6
difficulties in 	 0.5
reporter -LRB- 	 1.0
an average 	 0.007575757575757576
E , 	 1.0
a method 	 0.0049079754601227
the reference 	 0.0006920415224913495
idea , 	 0.14285714285714285
recognition refers 	 0.008264462809917356
momentum for 	 1.0
models -LRB- 	 0.11538461538461539
turn requires 	 0.16666666666666666
-- only 	 0.04
not lead 	 0.008928571428571428
Tagger , 	 1.0
resolution : 	 0.25
, definition 	 0.0005614823133071309
same input 	 0.08
framework based 	 0.25
returned by 	 0.25
perspective so 	 0.25
English on 	 0.02702702702702703
less likely 	 0.16666666666666666
involves deciding 	 0.1
will have 	 0.05714285714285714
per second 	 0.5
order logic 	 0.07142857142857142
The ideal 	 0.005208333333333333
receipts , 	 1.0
researched tasks 	 1.0
an earlier 	 0.007575757575757576
libraries GRM 	 0.5
by ears 	 0.005714285714285714
This phenomenon 	 0.031746031746031744
wingmen with 	 1.0
while others 	 0.2
standard written 	 0.07142857142857142
simple demonstrations 	 0.038461538461538464
accuracies in 	 1.0
getting published 	 0.25
generate weather 	 0.05555555555555555
all rules 	 0.023255813953488372
well human-ratings 	 0.03571428571428571
call routing 	 0.3333333333333333
<s> Intra-texual 	 0.0007686395080707148
containing examples 	 0.125
logic , 	 0.25
-- is 	 0.04
the size 	 0.001384083044982699
would never 	 0.018867924528301886
<s> Constraints 	 0.0007686395080707148
<s> Every 	 0.0007686395080707148
The phenomenon 	 0.005208333333333333
uses several 	 0.07142857142857142
still quite 	 0.06666666666666667
expansion of 	 0.3333333333333333
to analyze 	 0.0013280212483399733
operate on 	 1.0
less accurate 	 0.08333333333333333
<s> Language 	 0.0007686395080707148
handmade list 	 1.0
output of 	 0.15384615384615385
be solved 	 0.004219409282700422
card imprints 	 0.25
Piron , 	 0.3333333333333333
a sophisticated 	 0.001226993865030675
Helicopters The 	 1.0
<s> Trained 	 0.0007686395080707148
libraries for 	 0.5
instance , 	 0.6428571428571429
identified the 	 0.2
keep track 	 0.3333333333333333
was delayed 	 0.012987012987012988
e.g. SCU 	 0.017857142857142856
correct -RRB- 	 0.06666666666666667
non-textual components 	 1.0
opposite of 	 1.0
syntactic relations 	 0.07692307692307693
: `` 	 0.0196078431372549
benefit from 	 1.0
role the 	 0.25
substantial amount 	 0.2
this work 	 0.01098901098901099
Battle management 	 0.5
asking for 	 0.5
, Cleave 	 0.0005614823133071309
Last level 	 1.0
-LRB- 2008 	 0.0027100271002710027
system whose 	 0.010752688172043012
corpus of 	 0.22580645161290322
to different 	 0.0013280212483399733
in order 	 0.013108614232209739
use is 	 0.013888888888888888
but IR 	 0.014705882352941176
assertive -LRB- 	 1.0
his work 	 0.08333333333333333
Duranti , 	 1.0
are given 	 0.012448132780082987
decorrelating the 	 1.0
Writing -RRB- 	 1.0
very large 	 0.024390243902439025
cases make 	 0.05555555555555555
Adda G. 	 0.5
's dissertation 	 0.0196078431372549
-LRB- arguably 	 0.0027100271002710027
these same 	 0.023809523809523808
why certain 	 0.14285714285714285
by grid 	 0.005714285714285714
, Englund 	 0.0005614823133071309
get this 	 0.14285714285714285
, simulated 	 0.0005614823133071309
computer applications 	 0.022727272727272728
methods is 	 0.022727272727272728
start of 	 0.2857142857142857
-LRB- 99 	 0.0027100271002710027
introduction of 	 1.0
radios , 	 1.0
including recognition 	 0.07142857142857142
analysis , 	 0.1076923076923077
dozens of 	 1.0
that now 	 0.0035460992907801418
NNS for 	 1.0
conference headed 	 0.5
Rules post-processed 	 0.3333333333333333
phone error 	 0.25
looking to 	 0.4
cases -LRB- 	 0.05555555555555555
in spite 	 0.0018726591760299626
hopes to 	 1.0
If web 	 0.1
restaurant reviews 	 0.5
10,000 to 	 1.0
Research on 	 0.125
different speaking 	 0.02040816326530612
areas with 	 0.3333333333333333
Kucera and 	 1.0
orthography . 	 0.5
ushered in 	 1.0
marking abbreviations 	 0.5
include voice 	 0.037037037037037035
case that 	 0.058823529411764705
are not 	 0.02074688796680498
times they 	 0.2
time-consuming and 	 0.3333333333333333
Office -LRB- 	 1.0
besides words 	 1.0
, sentiment 	 0.0005614823133071309
more effective 	 0.010526315789473684
classification , 	 0.058823529411764705
an ellipsis 	 0.007575757575757576
or noun 	 0.0045045045045045045
computer -LRB- 	 0.022727272727272728
in domains 	 0.0018726591760299626
from closely 	 0.009615384615384616
word boundaries 	 0.016666666666666666
these represent 	 0.023809523809523808
meaning from 	 0.043478260869565216
% accurate 	 0.05128205128205128
discriminate between 	 0.3333333333333333
or verb 	 0.0045045045045045045
linguistic discourse 	 0.0625
of identifying 	 0.00089126559714795
more principled 	 0.010526315789473684
of telephony 	 0.00089126559714795
techniques similar 	 0.043478260869565216
Overview of 	 1.0
at all 	 0.07352941176470588
cohesion '' 	 1.0
a 70 	 0.001226993865030675
succession of 	 1.0
1970s , 	 0.3333333333333333
English alphabet 	 0.05405405405405406
Chinese or 	 0.14285714285714285
section of 	 0.16666666666666666
language that 	 0.006756756756756757
web page 	 0.125
; or 	 0.02127659574468085
the textual 	 0.0006920415224913495
'' as 	 0.02577319587628866
about 25 	 0.025
methods were 	 0.045454545454545456
and volume 	 0.001445086705202312
Lamb , 	 1.0
any case 	 0.0967741935483871
vowel in 	 1.0
<s> Although 	 0.005380476556495004
methodologies . 	 1.0
qualities of 	 0.5
be possible 	 0.008438818565400843
and lexical 	 0.001445086705202312
or verify 	 0.0045045045045045045
methodology to 	 0.5
approach which 	 0.05714285714285714
, identifying 	 0.0016844469399213925
any learning 	 0.03225806451612903
-RRB- Transcription 	 0.0027100271002710027
ratings produced 	 0.1111111111111111
questioner might 	 0.25
a limit 	 0.001226993865030675
a reasonable 	 0.00245398773006135
would look 	 0.03773584905660377
be roughly 	 0.004219409282700422
<s> Formal 	 0.0007686395080707148
acoustics -RRB- 	 1.0
parsers in 	 0.07692307692307693
people for 	 0.0625
Iraq in 	 0.5
on-line recognition 	 0.3333333333333333
simple implementations 	 0.038461538461538464
-RRB- HMMs 	 0.0027100271002710027
lexer would 	 1.0
of them 	 0.0017825311942959
Verschueren , 	 1.0
jet fighter 	 1.0
redundant sentences 	 1.0
suitable translation 	 0.25
contain subjective 	 0.08333333333333333
combined with 	 0.5
lexical exigencies 	 0.07692307692307693
machines or 	 0.25
of low 	 0.00089126559714795
context that 	 0.030303030303030304
'' systems 	 0.015463917525773196
first such 	 0.030303030303030304
for determining 	 0.007220216606498195
pre-process data 	 1.0
sentences for 	 0.013157894736842105
Applied Intelligence 	 0.5
of FoG 	 0.00089126559714795
Major issues 	 0.5
Language Input 	 0.08333333333333333
ontologies ' 	 0.16666666666666666
independent system 	 0.5
1956 and 	 1.0
parser are 	 0.0625
words appear 	 0.009174311926605505
, speech-to-text 	 0.0005614823133071309
sentences instead 	 0.013157894736842105
100 % 	 0.6666666666666666
<s> Keyphrase 	 0.0023059185242121443
but weaker 	 0.014705882352941176
appropriate number 	 0.25
's seminal 	 0.0196078431372549
history -RRB- 	 0.25
and text 	 0.005780346820809248
have all 	 0.009615384615384616
verbal unit 	 1.0
led by 	 0.3333333333333333
valuable detailed 	 0.5
An example 	 0.1875
to pauses 	 0.0013280212483399733
or as 	 0.009009009009009009
edition published 	 1.0
human translators 	 0.021739130434782608
Many systems 	 0.08333333333333333
own right 	 0.16666666666666666
or movies 	 0.0045045045045045045
five-star scale 	 1.0
specify precisely 	 1.0
and question 	 0.001445086705202312
cope with 	 1.0
matching -RRB- 	 0.2
you meant 	 0.07692307692307693
less standardised 	 0.08333333333333333
required , 	 0.14285714285714285
speaking computer 	 0.125
and search 	 0.001445086705202312
interactive use 	 0.25
is clear 	 0.0020325203252032522
taking a 	 0.2
or poetry 	 0.0045045045045045045
involve counting 	 0.16666666666666666
is little 	 0.0020325203252032522
relationship mentions 	 0.16666666666666666
their systems 	 0.058823529411764705
, Rajman 	 0.0005614823133071309
centrality . 	 0.5
given approach 	 0.08333333333333333
began offering 	 0.14285714285714285
Behind this 	 1.0
be achieved 	 0.02109704641350211
out in 	 0.14285714285714285
Accuracy is 	 0.14285714285714285
others . 	 0.25
can still 	 0.0055248618784530384
defined only 	 0.16666666666666666
the supervised 	 0.0006920415224913495
on linguistic 	 0.0047169811320754715
but evaluation 	 0.014705882352941176
an input-stream 	 0.007575757575757576
are : 	 0.008298755186721992
, making 	 0.0005614823133071309
of automatic 	 0.0017825311942959
different angle 	 0.02040816326530612
frequencies , 	 0.5
meaningful information 	 0.125
, employs 	 0.0005614823133071309
glass-box evaluation 	 1.0
program . 	 0.13636363636363635
, syntax 	 0.0011229646266142617
What should 	 0.09090909090909091
automatic machine 	 0.043478260869565216
tagging is 	 0.08
a hidden 	 0.001226993865030675
often be 	 0.022727272727272728
to How 	 0.0013280212483399733
two benefits 	 0.034482758620689655
into account 	 0.038461538461538464
, Richard 	 0.0005614823133071309
corpora such 	 0.09090909090909091
written languages 	 0.19230769230769232
eigenvector centrality 	 0.5
other scientific 	 0.014285714285714285
but that 	 0.04411764705882353
the English-French 	 0.0006920415224913495
Ruth Wodak 	 1.0
maximum likelihood 	 0.3333333333333333
produces usable 	 0.25
signal . 	 0.16666666666666666
between posts 	 0.02564102564102564
online news 	 0.125
used is 	 0.008849557522123894
extractive -LRB- 	 0.14285714285714285
there 's 	 0.025
other . 	 0.02857142857142857
made with 	 0.0625
be described 	 0.004219409282700422
linguistic rules 	 0.0625
this technology 	 0.01098901098901099
be -LRB- 	 0.004219409282700422
recognition and 	 0.05785123966942149
<s> Thus 	 0.009223674096848577
both LexRank 	 0.03225806451612903
document 7110.65 	 0.027777777777777776
geological analysis 	 1.0
and direction 	 0.001445086705202312
general speaker 	 0.045454545454545456
rapid access 	 1.0
or places 	 0.0045045045045045045
to post-process 	 0.0013280212483399733
adjust\/correct the 	 1.0
ATC simulators 	 0.2
or match 	 0.0045045045045045045
also an 	 0.014492753623188406
containing four 	 0.125
extent with 	 0.25
proper names 	 0.14285714285714285
Processing -RRB- 	 0.25
into `` 	 0.01282051282051282
of DA 	 0.00089126559714795
arbitrarily long 	 1.0
authors claimed 	 0.4
machine reading 	 0.012658227848101266
accuracy for 	 0.06451612903225806
their ratings 	 0.029411764705882353
edges ? 	 0.2857142857142857
of hand-written 	 0.00267379679144385
to minimize 	 0.0013280212483399733
Many of 	 0.16666666666666666
as there 	 0.003484320557491289
representation language 	 0.10526315789473684
book Language 	 0.125
of patterns 	 0.00089126559714795
too expensive 	 0.3333333333333333
-LRB- See 	 0.01084010840108401
actually playing 	 0.3333333333333333
been extended 	 0.014705882352941176
or emails 	 0.0045045045045045045
, Politics 	 0.0005614823133071309
applies directly 	 0.14285714285714285
, audio 	 0.0011229646266142617
Recovery and 	 1.0
is ambiguous 	 0.0020325203252032522
publication devoted 	 0.3333333333333333
Polar Lander 	 1.0
translator need 	 0.14285714285714285
time series 	 0.030303030303030304
issues of 	 0.2
achieved translating 	 0.1
to very 	 0.0013280212483399733
been debated 	 0.014705882352941176
computer input 	 0.022727272727272728
telephone speech 	 0.5
perhaps by 	 0.16666666666666666
chosen publications 	 0.2
-RRB- provides 	 0.0027100271002710027
parse computationally 	 0.1111111111111111
e.g. from 	 0.017857142857142856
procedure for 	 0.3333333333333333
known key 	 0.038461538461538464
and Command 	 0.001445086705202312
be satisfactory 	 0.004219409282700422
le français 	 1.0
general ontologies 	 0.045454545454545456
typical parser 	 0.1111111111111111
at MIT 	 0.029411764705882353
easier on 	 0.125
script will 	 0.25
or knowledge 	 0.0045045045045045045
acts or 	 0.3333333333333333
power increased 	 0.25
have the 	 0.009615384615384616
boundaries and 	 0.09090909090909091
rate -LRB- 	 0.09090909090909091
robustness of 	 0.25
identification . 	 0.2
entering the 	 0.5
is different 	 0.0020325203252032522
explicit features 	 0.2
unusual in 	 1.0
be also 	 0.004219409282700422
forecast -LRB- 	 1.0
Vocabulary size 	 0.3333333333333333
& Hollenbach 	 0.125
and often 	 0.004335260115606936
the direct 	 0.0006920415224913495
task-based evaluations 	 0.75
and most 	 0.001445086705202312
doctors -RRB- 	 0.3333333333333333
started the 	 0.25
important topics 	 0.0625
anything , 	 1.0
was tested 	 0.012987012987012988
both very 	 0.03225806451612903
recognizing and 	 0.2
SR systems 	 0.3333333333333333
position and 	 0.25
to upload 	 0.0013280212483399733
was FoG 	 0.012987012987012988
invented examples 	 0.5
include automatic 	 0.037037037037037035
in 1993 	 0.0018726591760299626
approaches can 	 0.03571428571428571
current major 	 0.14285714285714285
with only 	 0.00546448087431694
grammar : 	 0.02702702702702703
tasks in 	 0.09375
models are 	 0.038461538461538464
essentially two 	 0.125
usable output 	 1.0
One of 	 0.15384615384615385
left to 	 0.16666666666666666
basic knowledge 	 0.07692307692307693
illustrates how 	 0.5
can serve 	 0.011049723756906077
1994 , 	 1.0
people speaking 	 0.125
addition , 	 0.3333333333333333
these environments 	 0.023809523809523808
like that 	 0.03571428571428571
when processed 	 0.02857142857142857
in use 	 0.003745318352059925
opposed to 	 1.0
Markov Models 	 0.16666666666666666
British National 	 0.3333333333333333
Junqua and 	 1.0
other in 	 0.014285714285714285
, we 	 0.009545199326221224
or in 	 0.0045045045045045045
Like keyphrase 	 0.5
the pollen 	 0.0006920415224913495
start symbol 	 0.2857142857142857
evaluate the 	 0.5
, etc. 	 0.011229646266142616
OCR Since 	 0.02040816326530612
you 've 	 0.15384615384615385
'' to 	 0.015463917525773196
so accurate 	 0.03333333333333333
functions such 	 0.5
sets -LRB- 	 0.09090909090909091
accurate simply 	 0.14285714285714285
star ratings 	 0.5
'' http:\/\/arxiv.org\/abs\/1104.2086 	 0.005154639175257732
headed by 	 1.0
closely associate 	 0.2
the removal 	 0.0006920415224913495
of omni-font 	 0.00089126559714795
translation would 	 0.02702702702702703
un-supervised '' 	 1.0
Scansoft , 	 1.0
in agglutinative 	 0.0018726591760299626
searching and 	 0.3333333333333333
it may 	 0.017094017094017096
a sentence 	 0.01717791411042945
the topic 	 0.001384083044982699
the translator 	 0.0006920415224913495
information to 	 0.08695652173913043
end , 	 0.125
and developed 	 0.002890173410404624
interface Home 	 0.25
interest , 	 0.09090909090909091
, The 	 0.0005614823133071309
has 2 	 0.011904761904761904
text as 	 0.006289308176100629
to much 	 0.0013280212483399733
types , 	 0.07142857142857142
for the 	 0.11191335740072202
a worldwide 	 0.001226993865030675
increasing the 	 0.3333333333333333
<s> Yet 	 0.0007686395080707148
the Standard 	 0.0006920415224913495
Markov chain 	 0.05555555555555555
<s> Extrinsic 	 0.0015372790161414297
to : 	 0.0013280212483399733
the waves 	 0.0006920415224913495
with . 	 0.00546448087431694
Company for 	 0.5
be poorly 	 0.004219409282700422
also Machine 	 0.014492753623188406
it generalizes 	 0.008547008547008548
`` foreign 	 0.005291005291005291
, training 	 0.0005614823133071309
prior attempts 	 0.3333333333333333
generating the 	 0.2
the UC 	 0.0006920415224913495
To upgrade 	 0.1111111111111111
is apple 	 0.0020325203252032522
`` fastens 	 0.005291005291005291
Translator -RRB- 	 1.0
-RRB- words 	 0.0027100271002710027
being conducted 	 0.05555555555555555
Given an 	 0.07142857142857142
, opening 	 0.0005614823133071309
voice and 	 0.07692307692307693
This is 	 0.2698412698412698
trees using 	 0.16666666666666666
Fournier d'Albe 	 1.0
Speaker Dependent 	 0.16666666666666666
captioning , 	 1.0
<s> Information 	 0.0007686395080707148
data needed 	 0.012987012987012988
into several 	 0.01282051282051282
tasks are 	 0.125
only Wikipedia 	 0.02631578947368421
simply apply 	 0.08333333333333333
is difficult 	 0.008130081300813009
punched cards 	 1.0
for acquiring 	 0.0036101083032490976
Barbara Johnstone 	 1.0
is essentially 	 0.006097560975609756
papers by 	 0.3333333333333333
sublanguage of 	 0.3333333333333333
required for 	 0.14285714285714285
and semantic 	 0.004335260115606936
BLEU measure 	 0.3333333333333333
approximates the 	 0.5
all perform 	 0.023255813953488372
can aid 	 0.0055248618784530384
sailor '' 	 0.4
bi-directional inference 	 1.0
answered questions 	 0.6
basis of 	 0.6666666666666666
Back-End or 	 1.0
of TF-IDF 	 0.00089126559714795
the model 	 0.0020761245674740486
machine-generated summaries 	 1.0
system usability 	 0.010752688172043012
referring expression 	 0.5
terms that 	 0.07692307692307693
that output 	 0.0035460992907801418
between the 	 0.1794871794871795
tagging has 	 0.04
machine for 	 0.012658227848101266
from single 	 0.009615384615384616
software produces 	 0.037037037037037035
sentiment is 	 0.04
a learner 	 0.001226993865030675
, but 	 0.02695115103874228
limited in 	 0.1
real estate 	 0.1111111111111111
information is 	 0.043478260869565216
speaker recognition 	 0.05555555555555555
be needed 	 0.004219409282700422
of reasoned 	 0.00089126559714795
About 90 	 0.5
<s> Scope 	 0.0007686395080707148
virtually any 	 0.5
a digital 	 0.00245398773006135
the correct 	 0.004152249134948097
Mutual Information 	 1.0
in which 	 0.0149812734082397
on Shepard 	 0.0047169811320754715
'' - 	 0.010309278350515464
translation Statistical 	 0.013513513513513514
of inflected 	 0.00089126559714795
seems to 	 1.0
about the 	 0.2
vs. preposition 	 0.08333333333333333
but most 	 0.029411764705882353
Web -RRB- 	 0.1111111111111111
related questions 	 0.06666666666666667
representations are 	 0.25
have taken 	 0.009615384615384616
already published 	 0.2
prefer the 	 0.5
Language as 	 0.08333333333333333
deterministic rules 	 0.25
+ , 	 0.3333333333333333
, Michael 	 0.0022459292532285235
text illustrates 	 0.006289308176100629
This was 	 0.015873015873015872
or dimensions 	 0.0045045045045045045
their context 	 0.029411764705882353
occur together 	 0.2
restricted-domain QA 	 1.0
see below 	 0.05
-LRB- For 	 0.005420054200542005
a canonical 	 0.001226993865030675
some parsing 	 0.012048192771084338
large amounts 	 0.043478260869565216
-LRB- rather 	 0.0027100271002710027
multiple times 	 0.07692307692307693
respect to 	 1.0
applies to 	 0.14285714285714285
forms , 	 0.16666666666666666
Some critics 	 0.047619047619047616
SAM -LRB- 	 1.0
semantic interpretation 	 0.047619047619047616
metrics . 	 0.1111111111111111
tried , 	 0.3333333333333333
as business 	 0.003484320557491289
products ' 	 0.25
coarse-grained relations 	 1.0
multi-document extractive 	 0.25
cursive script 	 0.4
part-of-speech tagger 	 0.06666666666666667
social work 	 0.07142857142857142
that scans 	 0.0035460992907801418
answers might 	 0.08333333333333333
medium -RRB- 	 0.3333333333333333
most difficult 	 0.017241379310344827
an email 	 0.007575757575757576
important subproblem 	 0.0625
two possibilities 	 0.034482758620689655
1914 , 	 1.0
businesses look 	 0.5
American prisoners 	 0.2
superseded by 	 1.0
feature segment 	 0.07692307692307693
the sense 	 0.0006920415224913495
, linear-time 	 0.0005614823133071309
to doctors 	 0.0013280212483399733
they refer 	 0.05
n't vice 	 0.25
probably important 	 0.25
act theory 	 0.25
believed that 	 1.0
of outputs 	 0.00089126559714795
that has 	 0.02127659574468085
arranged hierarchically 	 1.0
question is 	 0.09523809523809523
, at 	 0.0016844469399213925
his method 	 0.08333333333333333
formal modeling 	 0.1111111111111111
Discursive psychology 	 1.0
certain restrictions 	 0.14285714285714285
Harris at 	 0.1111111111111111
designed grammars 	 0.14285714285714285
editor , 	 1.0
on large 	 0.0047169811320754715
meaning of 	 0.30434782608695654
left-recursion and 	 1.0
tested in 	 0.5
as Penn 	 0.003484320557491289
World Health 	 0.14285714285714285
important sentences 	 0.125
's intent 	 0.0196078431372549
-LRB- that 	 0.01084010840108401
condition that 	 1.0
calculates n-gram 	 1.0
a robot 	 0.001226993865030675
Afghanistan or 	 1.0
complex cognitive 	 0.041666666666666664
-LRB- DOE 	 0.0027100271002710027
suggest a 	 0.3333333333333333
to these 	 0.0026560424966799467
List of 	 1.0
basic sound 	 0.15384615384615385
and SpeechTEK 	 0.001445086705202312
a global 	 0.001226993865030675
the system 	 0.01384083044982699
others they 	 0.08333333333333333
for descriptive 	 0.0036101083032490976
speech a 	 0.006578947368421052
pertain strongly 	 1.0
<s> Google 	 0.0007686395080707148
brought together 	 1.0
and relevant 	 0.001445086705202312
But unfortunately 	 0.16666666666666666
derived from 	 0.5
following : 	 0.13333333333333333
processing the 	 0.018518518518518517
earlier Brown 	 0.25
convinced many 	 1.0
human in 	 0.021739130434782608
. -RRB- 	 0.0062402496099844
Latin . 	 0.25
engine , 	 0.16666666666666666
and non-annotated 	 0.001445086705202312
the American 	 0.001384083044982699
another -RRB- 	 0.07692307692307693
recognition scores 	 0.008264462809917356
Computed every 	 1.0
form a 	 0.15
each example 	 0.044444444444444446
is again 	 0.0020325203252032522
set on 	 0.02564102564102564
in 1971 	 0.0018726591760299626
an infinitive 	 0.007575757575757576
: Rules 	 0.0196078431372549
constructs -RRB- 	 0.3333333333333333
, online 	 0.0016844469399213925
no knowledge 	 0.07692307692307693
its output 	 0.08571428571428572
candidacies and 	 1.0
during verbalization 	 0.1
foster the 	 1.0
into linguistically 	 0.01282051282051282
signal or 	 0.16666666666666666
Nagao in 	 1.0
late 1980s 	 0.4444444444444444
constituents , 	 0.5
at language 	 0.014705882352941176
to recognize 	 0.00796812749003984
Main article 	 1.0
DeRose and 	 0.2
as doctors 	 0.003484320557491289
the higher 	 0.0006920415224913495
DOE -RRB- 	 1.0
indiscriminate . 	 1.0
, '' 	 0.0016844469399213925
where each 	 0.02857142857142857
evaluation with 	 0.018518518518518517
studies of 	 0.25
a system 	 0.012269938650306749
empirical solutions 	 1.0
involves doing 	 0.1
seen an 	 0.2
summarization and 	 0.02
methods parse 	 0.022727272727272728
Intelligent Character 	 0.3333333333333333
data will 	 0.012987012987012988
below -RRB- 	 0.4
making decisions 	 0.14285714285714285
containing several 	 0.125
information request 	 0.021739130434782608
the U.S. 	 0.002768166089965398
differ , 	 0.3333333333333333
Methods for 	 0.5
interlingual machine 	 0.75
a year 	 0.001226993865030675
-RRB- applications 	 0.0027100271002710027
Brain has 	 1.0
a word 	 0.013496932515337423
the document 	 0.004152249134948097
or larger 	 0.0045045045045045045
speech there 	 0.006578947368421052
an investigation 	 0.007575757575757576
and possibly 	 0.001445086705202312
disruptive to 	 1.0
the financial 	 0.0006920415224913495
, Emanuel 	 0.0011229646266142617
attention , 	 0.5
chapter , 	 1.0
Political discourse 	 1.0
formal rules 	 0.1111111111111111
to match 	 0.0026560424966799467
10 % 	 0.25
1952 and 	 0.5
learning disabilities 	 0.023255813953488372
could imagine 	 0.0625
parsed efficiently 	 0.25
derivation and 	 0.25
be phrased 	 0.004219409282700422
-LRB- UC 	 0.0027100271002710027
select whole 	 0.16666666666666666
the phrases 	 0.0006920415224913495
available for 	 0.11764705882352941
Similarly , 	 1.0
field within 	 0.037037037037037035
stands for 	 1.0
dimensions For 	 0.3333333333333333
learner , 	 0.5
simulation of 	 0.3333333333333333
can identify 	 0.0055248618784530384
tend to 	 1.0
that handles 	 0.0035460992907801418
extracting meaningful 	 0.2
success and 	 0.2
whereas when 	 0.3333333333333333
hence need 	 0.5
a pollen 	 0.001226993865030675
language are 	 0.006756756756756757
in many 	 0.0149812734082397
Words , 	 0.25
what we 	 0.09375
definition , 	 0.4
last night 	 0.2
simple pro 	 0.038461538461538464
to 98 	 0.0013280212483399733
The first 	 0.036458333333333336
robust against 	 0.25
<s> Machine 	 0.0023059185242121443
as nouns 	 0.003484320557491289
expand our 	 1.0
weather forecasts 	 0.5714285714285714
of great 	 0.00089126559714795
each input 	 0.022222222222222223
A. Woods 	 0.2
the trainee 	 0.0006920415224913495
The European 	 0.005208333333333333
, emoticons 	 0.0005614823133071309
might want 	 0.038461538461538464
completely nonsensical 	 1.0
i.e. requiring 	 0.05263157894736842
linguistic typology 	 0.0625
theory in 	 0.07692307692307693
content and 	 0.16666666666666666
is no 	 0.0020325203252032522
has given 	 0.011904761904761904
complex spoken 	 0.041666666666666664
by Yehoshua 	 0.005714285714285714
Hearing , 	 1.0
excess of 	 1.0
ends a 	 0.5
accurate by 	 0.14285714285714285
the south 	 0.001384083044982699
D. Faber 	 0.2
it seems 	 0.008547008547008548
Paroubek P. 	 1.0
patterns would 	 0.2
based representation 	 0.018518518518518517
but these 	 0.014705882352941176
More up 	 0.1111111111111111
soon developed 	 0.3333333333333333
answers that 	 0.08333333333333333
that making 	 0.0035460992907801418
consideration the 	 0.3333333333333333
Some of 	 0.19047619047619047
other similar 	 0.014285714285714285
protection from 	 1.0
coefficients . 	 0.25
or it 	 0.0045045045045045045
often span 	 0.022727272727272728
Optophone , 	 1.0
or left-to-right 	 0.0045045045045045045
what sense 	 0.03125
a statistical 	 0.0036809815950920245
be parsed 	 0.004219409282700422
internalize the 	 1.0
for both 	 0.0036101083032490976
summary used 	 0.023809523809523808
hard if-then 	 0.3333333333333333
as overall 	 0.003484320557491289
sub-committee is 	 1.0
grouped with 	 0.5
the Austrian 	 0.0006920415224913495
Languages like 	 0.3333333333333333
a multileveled 	 0.001226993865030675
bridge the 	 1.0
terms of 	 0.5384615384615384
function words 	 0.125
comparison of 	 0.3333333333333333
structure rules 	 0.08333333333333333
language use 	 0.02702702702702703
the '' 	 0.0006920415224913495
grammar having 	 0.02702702702702703
space exploration 	 0.2
on training 	 0.0047169811320754715
from this 	 0.009615384615384616
charge services 	 1.0
farther forward 	 1.0
media has 	 0.16666666666666666
other non-textual 	 0.014285714285714285
of unigrams 	 0.0017825311942959
commercial system 	 0.09090909090909091
the informativeness 	 0.0006920415224913495
a radiology 	 0.001226993865030675
need a 	 0.19047619047619047
<s> DTW 	 0.0007686395080707148
, including 	 0.004491858506457047
individual sentences 	 0.08333333333333333
nor even 	 1.0
at revealing 	 0.014705882352941176
in essentially 	 0.0018726591760299626
paper legal 	 0.09090909090909091
progress - 	 0.14285714285714285
defined to 	 0.16666666666666666
concentrates on 	 1.0
human-made model 	 0.5
Extrinsic evaluations 	 0.5
Competing semantic 	 1.0
sentence extraction 	 0.020833333333333332
depths of 	 1.0
, plural 	 0.0005614823133071309
platform for 	 0.5
working to 	 0.14285714285714285
answer . 	 0.23333333333333334
automatic speech 	 0.13043478260869565
overall contextual 	 0.16666666666666666
of cases 	 0.00089126559714795
sounds on 	 0.06666666666666667
is some 	 0.0020325203252032522
nice beach 	 0.25
related languages 	 0.06666666666666667
into computer-understandable 	 0.01282051282051282
for single 	 0.0036101083032490976
Ge'ez script 	 1.0
turn simplified 	 0.16666666666666666
them into 	 0.05263157894736842
ATC situation 	 0.2
publish a 	 1.0
the language 	 0.005536332179930796
Cook , 	 1.0
choose different 	 0.5
normalize for 	 1.0
solved first 	 0.2
Speech segmentation 	 0.0967741935483871
discrete terms 	 0.3333333333333333
extremely expensive 	 0.25
corpora and 	 0.09090909090909091
difficulty using 	 0.14285714285714285
Shift-Reduce parsing 	 1.0
of users 	 0.0017825311942959
The Association 	 0.005208333333333333
speeds made 	 0.5
transfer-based , 	 0.3333333333333333
of special 	 0.00089126559714795
current state 	 0.14285714285714285
text corpora 	 0.006289308176100629
<s> Summarization 	 0.0015372790161414297
alignment method 	 0.5
But from 	 0.16666666666666666
errata , 	 1.0
may appear 	 0.019230769230769232
and Markov 	 0.001445086705202312
industry currently 	 0.3333333333333333
correct summary 	 0.06666666666666667
feature is 	 0.07692307692307693
rule-based algorithms 	 0.14285714285714285
goal of 	 0.2857142857142857
algorithm , 	 0.10714285714285714
These systems 	 0.23529411764705882
picture distortion 	 0.25
apply the 	 0.2
Input for 	 0.5
Issues While 	 0.5
consisted of 	 1.0
Interactional sociolinguistics 	 1.0
USAF , 	 1.0
syntax to 	 0.18181818181818182
to customize 	 0.0026560424966799467
many types 	 0.019230769230769232
or positive 	 0.0045045045045045045
words just 	 0.009174311926605505
then combining 	 0.02857142857142857
and does 	 0.001445086705202312
of regular 	 0.00089126559714795
Category = 	 0.5
for abbreviations 	 0.0036101083032490976
of Canada 	 0.0017825311942959
translation methodologies 	 0.013513513513513514
States and 	 0.14285714285714285
Yet ELIZA 	 1.0
of four 	 0.00089126559714795
<s> Psycholinguists 	 0.0007686395080707148
these databases 	 0.023809523809523808
derivation or 	 0.25
on simple 	 0.0047169811320754715
F-score , 	 1.0
a smaller 	 0.001226993865030675
Dictionary-based Main 	 0.5
a collect 	 0.001226993865030675
2000 -RRB- 	 0.3333333333333333
-RRB- vs. 	 0.0027100271002710027
to progress 	 0.0013280212483399733
accuracy substantially 	 0.03225806451612903
similarity of 	 0.1
an adjective 	 0.03787878787878788
difficult problem 	 0.03571428571428571
e.g. WordNet 	 0.017857142857142856
particular NLP 	 0.07692307692307693
Grammar , 	 1.0
labor intensive 	 0.5
-- thus 	 0.04
, definitional 	 0.0005614823133071309
provides additional 	 0.5
classifying its 	 0.2
higher error 	 0.14285714285714285
centers of 	 1.0
negative to 	 0.125
they create 	 0.025
sets in 	 0.09090909090909091
longest running 	 1.0
, allowing 	 0.0005614823133071309
also cut 	 0.014492753623188406
error analysis 	 0.08333333333333333
anywhere on 	 1.0
attention to 	 0.5
speakers might 	 0.25
, psycholinguistics 	 0.0005614823133071309
prolific inventor 	 1.0
is edited 	 0.0020325203252032522
wife of 	 1.0
`` ASR 	 0.005291005291005291
most widely 	 0.017241379310344827
Cary Grant 	 1.0
detailed background 	 0.5
difficult words 	 0.03571428571428571
a growing 	 0.001226993865030675
reads it 	 0.5
a context-free 	 0.0036809815950920245
estimate the 	 0.5
some written 	 0.024096385542168676
of supervised 	 0.00089126559714795
person reads 	 0.05263157894736842
out a 	 0.07142857142857142
and in 	 0.010115606936416185
Shared tasks 	 1.0
mid-90s . 	 1.0
exactly this 	 0.3333333333333333
features indicating 	 0.038461538461538464
, article 	 0.0016844469399213925
from false 	 0.009615384615384616
machine recognition 	 0.012658227848101266
human kind 	 0.021739130434782608
larger tasks 	 0.0625
the vertices 	 0.0006920415224913495
of incorrect 	 0.00089126559714795
's opinions 	 0.0196078431372549
are exceptions 	 0.004149377593360996
surrounding the 	 0.2
, written 	 0.0005614823133071309
of Ethnomethodology 	 0.00089126559714795
proved negligibly 	 0.3333333333333333
performed using 	 0.1
which its 	 0.007246376811594203
Text-proofing Natural 	 1.0
special fonts 	 0.2
year later 	 0.16666666666666666
analysis Sublanguage 	 0.015384615384615385
which class 	 0.007246376811594203
recursive productions 	 1.0
search engines 	 0.18181818181818182
<s> Dynamic 	 0.0023059185242121443
1949 . 	 0.5
7 across 	 0.42857142857142855
original paper 	 0.07692307692307693
'' . 	 0.06701030927835051
texts to 	 0.11764705882352941
grow without 	 1.0
sponsored by 	 0.5
polynomial time 	 1.0
since 1971 	 0.1
of fusion 	 0.00089126559714795
the area 	 0.001384083044982699
Since this 	 0.2
a full-text 	 0.001226993865030675
input are 	 0.024390243902439025
are performed 	 0.004149377593360996
as to 	 0.013937282229965157
match up 	 0.16666666666666666
1 % 	 0.5
fast-evolving field 	 1.0
discuss the 	 1.0
transformational grammar 	 1.0
to humans 	 0.0013280212483399733
of characters 	 0.0017825311942959
by whom 	 0.005714285714285714
from randomly 	 0.009615384615384616
, organization 	 0.0011229646266142617
transmitting by 	 1.0
with thought-to-paper 	 0.00546448087431694
marked for 	 0.6666666666666666
specific tasks 	 0.047619047619047616
Therefore , 	 1.0
The result 	 0.005208333333333333
notable early 	 1.0
tests , 	 0.25
with different 	 0.01092896174863388
the documents 	 0.0020761245674740486
it off 	 0.008547008547008548
are already 	 0.004149377593360996
suffer . 	 1.0
Software -LRB- 	 0.5
useful summary 	 0.07142857142857142
scores that 	 0.2
from mild 	 0.009615384615384616
<s> Each 	 0.003843197540353574
low agreement 	 0.3333333333333333
sometimes , 	 0.07692307692307693
those utility 	 0.045454545454545456
; e.g. 	 0.0425531914893617
-LRB- 1995 	 0.0027100271002710027
evaluation metrics 	 0.018518518518518517
slower , 	 1.0
scores are 	 0.2
which entertaining 	 0.007246376811594203
humans as 	 0.08333333333333333
, it 	 0.01347557551937114
article `` 	 0.034482758620689655
A shallow 	 0.04
including sentiment 	 0.07142857142857142
Scope and 	 1.0
statistical NLP 	 0.06060606060606061
are capable 	 0.008298755186721992
correlation is 	 0.5
be declared 	 0.004219409282700422
2006 , 	 0.3333333333333333
simply ranks 	 0.08333333333333333
up the 	 0.045454545454545456
but has 	 0.014705882352941176
Unsupervised Morpheme 	 0.16666666666666666
assessment , 	 1.0
used an 	 0.008849557522123894
from its 	 0.009615384615384616
waves . 	 0.14285714285714285
-RRB- leverages 	 0.0027100271002710027
typically how 	 0.05555555555555555
from text 	 0.019230769230769232
so forth 	 0.03333333333333333
words -LRB- 	 0.027522935779816515
resulting from 	 0.25
into sentences 	 0.01282051282051282
understand that 	 0.14285714285714285
paper explored 	 0.09090909090909091
demonstrations , 	 1.0
ARNS system 	 1.0
is necessary 	 0.0040650406504065045
Environmental noise 	 1.0
of 200 	 0.00089126559714795
and easily 	 0.001445086705202312
recently identified 	 0.3333333333333333
task is 	 0.14285714285714285
-LRB- Wilensky 	 0.0027100271002710027
a paper 	 0.001226993865030675
they can 	 0.15
Scotland , 	 0.2
sets to 	 0.09090909090909091
through sentiment 	 0.125
these approaches 	 0.047619047619047616
-LRB- k 	 0.0027100271002710027
ontologies and 	 0.16666666666666666
moderate should 	 0.2
setting , 	 0.4
science and 	 0.1
department , 	 0.5
method is 	 0.0625
The accuracy 	 0.010416666666666666
or phonemes 	 0.009009009009009009
program with 	 0.045454545454545456
since ROUGE-1 	 0.1
sentence . 	 0.14583333333333334
The `` 	 0.010416666666666666
generate some 	 0.05555555555555555
short paragraph 	 0.125
Grammatical dependency 	 1.0
decision-making , 	 1.0
of printed 	 0.0017825311942959
'' all 	 0.005154639175257732
is now 	 0.006097560975609756
as the 	 0.09407665505226481
the left 	 0.001384083044982699
potential redundancy 	 0.14285714285714285
key clauses 	 0.16666666666666666
from our 	 0.009615384615384616
n-gram ROUGE 	 0.5
Wetherell , 	 1.0
a classifier 	 0.001226993865030675
machine learning 	 0.24050632911392406
any of 	 0.06451612903225806
Such inflection 	 0.125
Computing + 	 0.5
mining . 	 0.2
uninterrupted and 	 1.0
information need 	 0.021739130434782608
Increase as 	 1.0
graph will 	 0.15384615384615385
Human judgement 	 0.2
SHRDLU simulated 	 0.16666666666666666
extrinsic , 	 0.16666666666666666
to estimate 	 0.00398406374501992
based on 	 0.8333333333333334
knowledge system 	 0.037037037037037035
sentences `` 	 0.02631578947368421
or evaluation 	 0.0045045045045045045
, ^ 	 0.0011229646266142617
This technique 	 0.015873015873015872
-LRB- though 	 0.0027100271002710027
Grows : 	 1.0
no information 	 0.07692307692307693
length cutoff 	 0.125
manner . 	 0.75
code of 	 0.14285714285714285
started trying 	 0.25
context The 	 0.06060606060606061
database , 	 0.1
`` centrality 	 0.005291005291005291
5000 or 	 1.0
tuned weights 	 1.0
automatically , 	 0.047619047619047616
an abbreviation 	 0.007575757575757576
Shipibo . 	 0.5
by creating 	 0.005714285714285714
of semantic 	 0.004456327985739751
what happens 	 0.03125
but not 	 0.058823529411764705
systems typically 	 0.008928571428571428
Further information 	 0.3333333333333333
quantitative one 	 0.25
that part 	 0.0035460992907801418
one according 	 0.015384615384615385
After the 	 0.3333333333333333
that underlies 	 0.0035460992907801418
`` Man 	 0.010582010582010581
measure that 	 0.09090909090909091
free as 	 0.25
are currently 	 0.008298755186721992
term voice 	 0.05555555555555555
parsing . 	 0.10714285714285714
<s> Consider 	 0.0015372790161414297
that about 	 0.0035460992907801418
Character Recognition 	 1.0
least historically 	 0.2
sorting center 	 1.0
phonetic segments 	 0.5
networks emerged 	 0.07142857142857142
data analysis 	 0.012987012987012988
news articles 	 0.23076923076923078
lessened . 	 1.0
d'Albe developed 	 1.0
theories on 	 0.2
<s> Intrinsic 	 0.0015372790161414297
evaluation Intrinsic 	 0.018518518518518517
have unambiguous 	 0.009615384615384616
rare or 	 0.25
for terms 	 0.0036101083032490976
recogniton by 	 0.5
supported by 	 1.0
representation -LRB- 	 0.10526315789473684
and create 	 0.002890173410404624
accomplished in 	 1.0
recognizing entire 	 0.2
in CSR 	 0.0018726591760299626
assumptions . 	 0.2
, distinct 	 0.0005614823133071309
per minute 	 0.25
intelligent character 	 1.0
`` zero 	 0.005291005291005291
whole phrases 	 0.1111111111111111
sentence-ending markers 	 1.0
summary covers 	 0.023809523809523808
, produced 	 0.0016844469399213925
matching up 	 0.2
design feature 	 0.25
computers and 	 0.1111111111111111
achieve accuracy 	 0.5
shallow '' 	 0.16666666666666666
it statically 	 0.008547008547008548
In Europe 	 0.01904761904761905
for keyphrase 	 0.0036101083032490976
These results 	 0.058823529411764705
users sent 	 0.1111111111111111
information of 	 0.021739130434782608
, Flickinger 	 0.0005614823133071309
Another possible 	 0.07692307692307693
discover these 	 1.0
Grass pollen 	 1.0
topic by 	 0.125
by parser 	 0.005714285714285714
detected important 	 0.5
and analytical 	 0.001445086705202312
the Mars 	 0.0006920415224913495
scanned can 	 0.3333333333333333
Significant advances 	 1.0
for people 	 0.007220216606498195
nautical context 	 0.5
into French 	 0.02564102564102564
is needed 	 0.0020325203252032522
precision because 	 0.2
the undercarriage 	 0.0006920415224913495
methods require 	 0.022727272727272728
the Brown 	 0.005536332179930796
<s> Decoding 	 0.0007686395080707148
Lee Pike 	 1.0
are important 	 0.004149377593360996
, graphic 	 0.0005614823133071309
properly . 	 0.5
tag to 	 0.0625
fast and 	 1.0
produced like 	 0.1111111111111111
explore and 	 0.25
edge between 	 0.3333333333333333
then with 	 0.02857142857142857
in fact 	 0.0018726591760299626
Speech recogniton 	 0.03225806451612903
appear `` 	 0.0625
or phrases 	 0.009009009009009009
considered as 	 0.1111111111111111
discussions . 	 0.3333333333333333
domains such 	 0.125
instance some 	 0.07142857142857142
-LRB- among 	 0.005420054200542005
by keyphrase 	 0.005714285714285714
of heuristics 	 0.00089126559714795
voice response 	 0.07692307692307693
to discriminate 	 0.0026560424966799467
evaluations . 	 0.16666666666666666
Conferences , 	 0.5
'' , 	 0.15463917525773196
similar application 	 0.037037037037037035
cases on 	 0.05555555555555555
creating a 	 0.2857142857142857
as that 	 0.003484320557491289
Question processing 	 0.14285714285714285
dependence vs. 	 1.0
sources , 	 0.16666666666666666
and retrieving 	 0.001445086705202312
the periods 	 0.0006920415224913495
undertaken to 	 0.5
of being 	 0.00089126559714795
surprisingly , 	 0.3333333333333333
or lexical 	 0.009009009009009009
rates on 	 0.125
reverse process 	 0.5
duplicate or 	 0.5
translation by 	 0.013513513513513514
directly to 	 0.4
word divider 	 0.016666666666666666
Verbyx VRX 	 1.0
spelled ` 	 1.0
, Perceptron 	 0.0005614823133071309
colloquially termed 	 1.0
Europe was 	 0.2
are pre-marked 	 0.004149377593360996
for cartoon 	 0.0036101083032490976
generation : 	 0.2222222222222222
terminology , 	 1.0
and weapons 	 0.001445086705202312
retrieved . 	 1.0
are variously 	 0.004149377593360996
right , 	 0.1
-- i.e. 	 0.04
much smaller 	 0.045454545454545456
Harris beginning 	 0.1111111111111111
has little 	 0.011904761904761904
northeast of 	 1.0
a case 	 0.001226993865030675
Unicode Consortium 	 1.0
, list 	 0.0005614823133071309
single words 	 0.07142857142857142
In short 	 0.009523809523809525
of spoken 	 0.0017825311942959
at level 	 0.014705882352941176
their translation 	 0.029411764705882353
keyphrases from 	 0.02857142857142857
turn , 	 0.16666666666666666
: To 	 0.00980392156862745
`` STT 	 0.005291005291005291
are explicitly 	 0.004149377593360996
telephony , 	 0.6666666666666666
simple terms 	 0.038461538461538464
qualitative manner 	 0.5
hardly any 	 1.0
so far 	 0.03333333333333333
many in 	 0.019230769230769232
which can 	 0.036231884057971016
to various 	 0.0013280212483399733
pilot , 	 0.2
full stop 	 0.4
summaries must 	 0.023255813953488372
but vocabulary 	 0.014705882352941176
by increasing 	 0.005714285714285714
printed text 	 0.25
probabilistically at 	 1.0
Statistical models 	 0.1111111111111111
Flow of 	 1.0
what a 	 0.09375
counselling -RRB- 	 1.0
-RRB- output 	 0.0027100271002710027
areas -- 	 0.16666666666666666
digital assistants 	 0.14285714285714285
and parsing 	 0.001445086705202312
was sold 	 0.012987012987012988
field since 	 0.037037037037037035
've seen 	 0.5
the entire 	 0.0006920415224913495
vs. `` 	 0.16666666666666666
patterns in 	 0.2
above to 	 0.07692307692307693
Among these 	 1.0
stability in 	 1.0
blocks to 	 0.25
results of 	 0.047619047619047616
, vehicle 	 0.0005614823133071309
that character-by-character 	 0.0035460992907801418
innovative Web-based 	 1.0
about 12 	 0.025
interim year 	 1.0
home '' 	 1.0
are the 	 0.04564315352697095
sailor dogs 	 0.2
group at 	 0.25
: setting 	 0.00980392156862745
<s> There 	 0.006917755572636433
meaningless tokens 	 1.0
different aspects 	 0.02040816326530612
about 70 	 0.05
performance continued 	 0.05555555555555555
and Thai 	 0.001445086705202312
President Obama 	 0.25
in automatic 	 0.003745318352059925
summarization often 	 0.02
processed documents 	 0.16666666666666666
or millions 	 0.0045045045045045045
the tag 	 0.001384083044982699
translation software 	 0.04054054054054054
other areas 	 0.02857142857142857
that act 	 0.0035460992907801418
but a 	 0.014705882352941176
the stationary 	 0.0006920415224913495
optimization methods 	 1.0
AT&T libraries 	 1.0
the objectives 	 0.0006920415224913495
occur in 	 0.4
or form 	 0.0045045045045045045
correct . 	 0.2
to mental 	 0.0013280212483399733
expressed by 	 0.16666666666666666
ICR software 	 0.3333333333333333
is strong 	 0.0020325203252032522
e.g. Phonemes 	 0.017857142857142856
vertices is 	 0.1111111111111111
capitalized . 	 0.3333333333333333
Evaluation -RRB- 	 0.2222222222222222
<s> LL 	 0.0015372790161414297
because the 	 0.13333333333333333
evaluation . 	 0.05555555555555555
becomes harder 	 0.25
ratings , 	 0.1111111111111111
to choose 	 0.0013280212483399733
the need 	 0.0020761245674740486
use by 	 0.027777777777777776
A machine 	 0.02
tasks -LRB- 	 0.03125
more informative 	 0.010526315789473684
rejecting those 	 0.3333333333333333
to narrative 	 0.0013280212483399733
order , 	 0.14285714285714285
earlier some 	 0.25
person was 	 0.05263157894736842
European Union 	 0.3333333333333333
information display 	 0.021739130434782608
requires an 	 0.0625
specific task 	 0.047619047619047616
A more 	 0.02
a widely-reported 	 0.001226993865030675
sophisticated measures 	 0.14285714285714285
an excellent 	 0.007575757575757576
interpretation capabilities 	 0.5
results reported 	 0.047619047619047616
Performing grammatical 	 1.0
meanings of 	 0.25
of internal 	 0.00089126559714795
verifying certain 	 1.0
parsing can 	 0.07142857142857142
in processing 	 0.0018726591760299626
substantial funding 	 0.2
capture the 	 0.5
systems such 	 0.017857142857142856
-LRB- SBD 	 0.0027100271002710027
the Alenia 	 0.0006920415224913495
resource consumption 	 0.2
is too 	 0.0020325203252032522
with computer-aided 	 0.00546448087431694
minimum phone 	 0.5
and HLT 	 0.001445086705202312
great importance 	 0.3333333333333333
other aspects 	 0.014285714285714285
comprehension . 	 0.42857142857142855
`` look 	 0.005291005291005291
contains only 	 0.1
and cross-lingual 	 0.001445086705202312
<s> According 	 0.0007686395080707148
by PageRank 	 0.005714285714285714
of connected 	 0.00089126559714795
for several 	 0.007220216606498195
This includes 	 0.015873015873015872
neighbors , 	 0.3333333333333333
Software OCR 	 0.5
helps doctors 	 0.5
been operated 	 0.014705882352941176
Frederick Jelinek 	 1.0
under the 	 0.2
spectral-domain of 	 1.0
by numbers 	 0.005714285714285714
created . 	 0.14285714285714285
Tags usually 	 1.0
upgrade a 	 1.0
template-matching OCR 	 1.0
error types 	 0.08333333333333333
retrieving information 	 1.0
an easier 	 0.007575757575757576
of front-end 	 0.00089126559714795
parse reranking 	 0.1111111111111111
our learned 	 0.2
and Jabberwacky 	 0.001445086705202312
-LRB- usually 	 0.005420054200542005
trivial -RRB- 	 0.25
linguistic and 	 0.0625
building a 	 1.0
the interactions 	 0.0006920415224913495
should represent 	 0.10526315789473684
way they 	 0.041666666666666664
but BLEU 	 0.014705882352941176
Slembrouck , 	 1.0
votes from 	 1.0
lightweight ontologies 	 1.0
faces a 	 1.0
atmosphere -LRB- 	 1.0
a restricted 	 0.001226993865030675
natural summaries 	 0.013333333333333334
of trying 	 0.00089126559714795
<s> Much 	 0.0023059185242121443
methods and 	 0.022727272727272728
now common 	 0.07692307692307693
-LRB- often 	 0.005420054200542005
on both 	 0.0047169811320754715
of intelligence 	 0.00089126559714795
a criterion 	 0.001226993865030675
Frost , 	 1.0
models were 	 0.038461538461538464
initial , 	 0.3333333333333333
Noise in 	 1.0
ones . 	 0.2
reader processed 	 0.1
multiple topics 	 0.07692307692307693
the United 	 0.004844290657439446
the creation 	 0.0006920415224913495
example demonstrates 	 0.012345679012345678
from it 	 0.009615384615384616
reviews respectively 	 0.16666666666666666
<s> First 	 0.0007686395080707148
`` sounds 	 0.005291005291005291
technique chosen 	 0.14285714285714285
express all 	 0.2
requires its 	 0.0625
this refined 	 0.01098901098901099
Server OCR 	 1.0
playing in 	 1.0
other academic 	 0.014285714285714285
Campaigns -RRB- 	 1.0
-RRB- of 	 0.018970189701897018
an interlingual 	 0.007575757575757576
<s> For 	 0.043812451960030745
English , 	 0.16216216216216217
suffix '' 	 1.0
is doing 	 0.0020325203252032522
In all 	 0.009523809523809525
data be 	 0.012987012987012988
by inputting 	 0.005714285714285714
article by 	 0.034482758620689655
spoken sentence 	 0.07142857142857142
evaluation , 	 0.05555555555555555
Robotics Speech-to-text 	 1.0
Carnegie Mellon 	 1.0
, support 	 0.0005614823133071309
an equivalent 	 0.007575757575757576
Current research 	 0.2
research attempts 	 0.047619047619047616
Grant ever 	 1.0
as candidates 	 0.003484320557491289
years in 	 0.047619047619047616
noun than 	 0.07142857142857142
Variation analysis 	 1.0
sound creates 	 0.05
of around 	 0.0017825311942959
dividing speech 	 0.3333333333333333
consumer , 	 1.0
algorithms requires 	 0.02857142857142857
Intrinsic evaluations 	 0.3333333333333333
from United 	 0.009615384615384616
the mechanism 	 0.0006920415224913495
various boolean 	 0.05555555555555555
is search 	 0.0020325203252032522
1987 , 	 0.6666666666666666
by relying 	 0.005714285714285714
years development 	 0.09523809523809523
for test 	 0.0036101083032490976
certain e-communities 	 0.14285714285714285
distinct from 	 0.42857142857142855
some of 	 0.1566265060240964
watertight barmaid 	 1.0
a movie 	 0.00245398773006135
<s> `` 	 0.003843197540353574
Automatic tagging 	 0.1111111111111111
process `` 	 0.027777777777777776
of parameters 	 0.00089126559714795
10 digits 	 0.125
Intuitively , 	 1.0
sentences but 	 0.02631578947368421
Extrinsic evaluation 	 0.5
than speech 	 0.022222222222222223
: whereas 	 0.00980392156862745
project ongoing 	 0.07692307692307693
science , 	 0.4
<s> Modern 	 0.0007686395080707148
tables with 	 0.3333333333333333
1971 and 	 0.3333333333333333
research were 	 0.023809523809523808
conversations such 	 0.3333333333333333
we want 	 0.044444444444444446
algebra word 	 0.5
` beyond 	 0.0625
from improved 	 0.009615384615384616
generally achieved 	 0.09090909090909091
A. Lauriault\/Loriot 	 0.4
set appends 	 0.02564102564102564
details of 	 0.5
high recognition 	 0.05555555555555555
a security 	 0.001226993865030675
the improvement 	 0.0006920415224913495
not included 	 0.008928571428571428
toy world 	 0.5
Process . 	 1.0
not new 	 0.008928571428571428
a person 	 0.013496932515337423
power The 	 0.25
questions from 	 0.038461538461538464
Language Generation 	 0.08333333333333333
part may 	 0.037037037037037035
changed from 	 0.5
<s> That 	 0.0023059185242121443
French were 	 0.25
's gonna 	 0.0196078431372549
It had 	 0.02631578947368421
online resource 	 0.125
and Janet 	 0.001445086705202312
software resources 	 0.037037037037037035
Sparkle campaign 	 1.0
in-depth knowledge 	 0.6666666666666666
HMMs are 	 0.25
<s> Context 	 0.0007686395080707148
Makoto Nagao 	 1.0
ones are 	 0.1
heuristic post-processing 	 0.3333333333333333
of many 	 0.0017825311942959
... . 	 0.5
positions as 	 1.0
computer code 	 0.022727272727272728
determine both 	 0.043478260869565216
machine-learning-based implementation 	 1.0
The lowest 	 0.005208333333333333
into general 	 0.01282051282051282
languages such 	 0.1
pilots flying 	 0.5
'' or 	 0.02577319587628866
parser for 	 0.0625
the programs 	 0.0006920415224913495
sub-titling , 	 1.0
documents , 	 0.23684210526315788
be either 	 0.004219409282700422
been trained 	 0.014705882352941176
1985 , 	 1.0
We apply 	 0.14285714285714285
Edmund Fournier 	 1.0
induction . 	 1.0
intended semantic 	 0.2
underlying idea 	 0.3333333333333333
1-July-2005 , 	 1.0
vary considerably 	 0.16666666666666666
beach , 	 1.0
is performed 	 0.0040650406504065045
designed for 	 0.14285714285714285
to infer 	 0.0013280212483399733
commanding an 	 1.0
e.g. transformational 	 0.017857142857142856
keyphrases of 	 0.02857142857142857
identifying the 	 0.6666666666666666
negligibly rare 	 1.0
and neural 	 0.002890173410404624
is another 	 0.0040650406504065045
Issues In 	 0.5
typically finds 	 0.05555555555555555
should we 	 0.05263157894736842
strength score 	 0.2
and W. 	 0.001445086705202312
rank , 	 0.16666666666666666
parsers were 	 0.07692307692307693
life ? 	 0.25
further information 	 0.125
computers to 	 0.1111111111111111
extraction is 	 0.06451612903225806
result -LRB- 	 0.09090909090909091
and rich 	 0.001445086705202312
were identified 	 0.024390243902439025
of whom 	 0.00089126559714795
`` eat 	 0.005291005291005291
require exponential 	 0.045454545454545456
a specific 	 0.006134969325153374
made of 	 0.1875
meaningful way 	 0.125
best path 	 0.05555555555555555
Bottom-up parsing 	 1.0
or automotive 	 0.0045045045045045045
NLG output 	 0.047619047619047616
may or 	 0.019230769230769232
automates the 	 1.0
examples of 	 0.20833333333333334
its grammatical 	 0.02857142857142857
simply requires 	 0.08333333333333333
NLG systems 	 0.23809523809523808
and Civil 	 0.001445086705202312
: Produce 	 0.00980392156862745
QA -RRB- 	 0.047619047619047616
consumed from 	 1.0
2010 ? 	 0.3333333333333333
put together 	 0.25
Many real 	 0.08333333333333333
up an 	 0.045454545454545456
vectors would 	 0.3333333333333333
or MLLT 	 0.0045045045045045045
, Gdaniec 	 0.0005614823133071309
can simplify 	 0.0055248618784530384
Man dog 	 0.5
module that 	 0.3333333333333333
syntax are 	 0.09090909090909091
generate form 	 0.05555555555555555
question answering 	 0.21428571428571427
Norman Fairclough 	 0.5
result , 	 0.2727272727272727
is embedded 	 0.0020325203252032522
in abstractive 	 0.0018726591760299626
developed into 	 0.038461538461538464
computer language 	 0.022727272727272728
preselects small 	 1.0
somehow internalize 	 1.0
n Computer 	 0.5
the similarity 	 0.0006920415224913495
all of 	 0.09302325581395349
computers have 	 0.1111111111111111
' , 	 0.3157894736842105
parser The 	 0.125
the dominance 	 0.0006920415224913495
caused problems 	 1.0
tasks implemented 	 0.03125
assignment . 	 0.5
<s> BASEBALL 	 0.0007686395080707148
ATC -RRB- 	 0.2
domain is 	 0.05
Microphone on 	 1.0
natural languages 	 0.12
vertices be 	 0.1111111111111111
offer the 	 1.0
usually measured 	 0.03125
the Amount 	 0.0006920415224913495
the first 	 0.010380622837370242
best word 	 0.05555555555555555
breadth and 	 0.5
of Energy 	 0.00089126559714795
dependent system 	 0.3333333333333333
other word 	 0.014285714285714285
being able 	 0.05555555555555555
general software 	 0.045454545454545456
many speech 	 0.019230769230769232
several quality 	 0.045454545454545456
sentence in 	 0.041666666666666664
even articles 	 0.037037037037037035
produce a 	 0.13636363636363635
the translation 	 0.004152249134948097
simple parsing 	 0.038461538461538464
modal . 	 1.0
In 1969 	 0.01904761904761905
` kick 	 0.0625
of this 	 0.00980392156862745
93-95 % 	 1.0
approach involves 	 0.02857142857142857
keeping the 	 0.5
RCA product 	 0.2
some time 	 0.024096385542168676
had been 	 0.07142857142857142
as needed 	 0.003484320557491289
maintained within 	 0.5
-LRB- U.S. 	 0.005420054200542005
SIGGEN portion 	 1.0
broken in 	 0.2
are efficient 	 0.004149377593360996
texts so 	 0.058823529411764705
replace the 	 1.0
levels or 	 0.045454545454545456
translation Transfer-based 	 0.013513513513513514
see that 	 0.05
parsing ambiguous 	 0.03571428571428571
an action 	 0.007575757575757576
-RRB- examples 	 0.0027100271002710027
evidence of 	 0.5
of Question 	 0.00089126559714795
closed-captioning of 	 1.0
from multiple 	 0.009615384615384616
uttered ; 	 0.3333333333333333
, Verbyx 	 0.0005614823133071309
all alternative 	 0.023255813953488372
the cosine 	 0.0006920415224913495
unfortunately , 	 1.0
systems were 	 0.05357142857142857
that for 	 0.0035460992907801418
varied from 	 1.0
on an 	 0.014150943396226415
an LDA-based 	 0.007575757575757576
the BLEU 	 0.0006920415224913495
filtering out 	 1.0
Our evaluation 	 0.3333333333333333
data within 	 0.012987012987012988
CLAWS , 	 0.5
speech Task 	 0.006578947368421052
, parsing 	 0.0016844469399213925
Hands-free computing 	 1.0
then generated 	 0.02857142857142857
teletype typewritten 	 1.0
Harris et 	 0.1111111111111111
This reader 	 0.015873015873015872
use or 	 0.027777777777777776
most research 	 0.017241379310344827
involved , 	 0.16666666666666666
much easier 	 0.045454545454545456
a watertight 	 0.001226993865030675
the course 	 0.0006920415224913495
<s> An 	 0.008455034588777863
sublanguage analysis 	 0.3333333333333333
can then 	 0.0055248618784530384
up to 	 0.22727272727272727
Google . 	 0.5
Subsumption -LRB- 	 1.0
graph can 	 0.07692307692307693
solutions . 	 0.5
that specific 	 0.0035460992907801418
orthogonal to 	 1.0
, proper 	 0.0005614823133071309
stationary probability 	 0.14285714285714285
to protect 	 0.0013280212483399733
for Computational 	 0.0036101083032490976
lectures , 	 1.0
the semantic 	 0.001384083044982699
of government 	 0.0017825311942959
, machine 	 0.0016844469399213925
brain recognizes 	 0.3333333333333333
late Claude 	 0.1111111111111111
ambiguity because 	 0.125
levels , 	 0.045454545454545456
not easily 	 0.026785714285714284
, Malcolm 	 0.0005614823133071309
for singular 	 0.007220216606498195
the inter-texual 	 0.0006920415224913495
parsed by 	 0.75
and it 	 0.0072254335260115606
= 2PR 	 0.1111111111111111
<s> Generally 	 0.003843197540353574
difference is 	 0.25
audio , 	 1.0
computer-aided language 	 0.3333333333333333
are further 	 0.004149377593360996
Biden visited 	 0.3333333333333333
the issue 	 0.002768166089965398
was first 	 0.012987012987012988
concatenated text 	 1.0
Mobile Smartphones 	 0.3333333333333333
published in 	 0.14285714285714285
widely used 	 0.875
be produced 	 0.004219409282700422
which could 	 0.007246376811594203
display for 	 0.5
of sentiment 	 0.004456327985739751
be located 	 0.004219409282700422
essentially to 	 0.125
Northern areas 	 0.3333333333333333
, C 	 0.0005614823133071309
with Japanese 	 0.00546448087431694
on speech 	 0.0047169811320754715
articles , 	 0.125
making up 	 0.14285714285714285
`` Statistical 	 0.005291005291005291
But the 	 0.16666666666666666
then spoke 	 0.02857142857142857
Germany . 	 0.5
them which 	 0.05263157894736842
because while 	 0.03333333333333333
either user-specified 	 0.1
contexts in 	 0.14285714285714285
identification is 	 0.2
of anomalies 	 0.00089126559714795
decelerations during 	 1.0
of very 	 0.0017825311942959
use either 	 0.013888888888888888
epidemic which 	 1.0
-LRB- including 	 0.0027100271002710027
of keywords 	 0.00089126559714795
with American 	 0.00546448087431694
a complex 	 0.006134969325153374
EARS project 	 1.0
of interest 	 0.00267379679144385
nodes to 	 0.14285714285714285
outside world 	 0.5
as natural 	 0.003484320557491289
used the 	 0.008849557522123894
to proper 	 0.0013280212483399733
`` centroid 	 0.005291005291005291
increase in 	 0.75
Ken Church 	 1.0
polarity classification 	 0.125
, 10 	 0.0011229646266142617
1950s by 	 0.25
odd looking 	 1.0
keyphrase . 	 0.05263157894736842
a list 	 0.008588957055214725
For example 	 0.6229508196721312
thresholded to 	 1.0
nodes that 	 0.14285714285714285
words commonly 	 0.009174311926605505
resolution remains 	 0.25
would allow 	 0.018867924528301886
useful review 	 0.07142857142857142
some languages 	 0.024096385542168676
could therefore 	 0.0625
innumerable studies 	 1.0
best guesses 	 0.05555555555555555
professionals . 	 1.0
- \/ 	 0.0625
on-line , 	 0.3333333333333333
Optical Character 	 0.3333333333333333
accent , 	 1.0
logical representation 	 0.16666666666666666
where one 	 0.02857142857142857
preceding token 	 1.0
more compactly 	 0.010526315789473684
for grammars 	 0.0036101083032490976
because it 	 0.1
Orientation -- 	 1.0
Sentence boundaries 	 0.2
The parser 	 0.005208333333333333
be derived 	 0.008438818565400843
of planning 	 0.00089126559714795
2.0 The 	 0.5
on context 	 0.0047169811320754715
the Ge'ez 	 0.0006920415224913495
a degree 	 0.001226993865030675
and legal 	 0.001445086705202312
segmentation is 	 0.2727272727272727
In 2004 	 0.009523809523809525
blend into 	 0.3333333333333333
and Vietnamese 	 0.001445086705202312
involved in 	 0.16666666666666666
maintain tractability 	 1.0
not necessarily 	 0.017857142857142856
location , 	 1.0
in such 	 0.0056179775280898875
require advanced 	 0.045454545454545456
more generally 	 0.010526315789473684
world assumption 	 0.13333333333333333
, TaleSpin 	 0.0005614823133071309
with many 	 0.00546448087431694
<s> Military 	 0.0007686395080707148
short-time stationary 	 0.5
warping -LRB- 	 0.25
are vulnerable 	 0.004149377593360996
proper lexical 	 0.14285714285714285
the perception 	 0.0006920415224913495
Gustav Tauschek 	 1.0
explored . 	 0.5
equipment was 	 0.3333333333333333
of as 	 0.0017825311942959
called query-biased 	 0.05555555555555555
-LRB- Journal 	 0.0027100271002710027
of parse 	 0.00089126559714795
is seen 	 0.0020325203252032522
% still 	 0.02564102564102564
and methodologies 	 0.001445086705202312
the speakers 	 0.0006920415224913495
in ontologies 	 0.0018726591760299626
Royal Australian 	 0.5
required translation 	 0.14285714285714285
be separated 	 0.004219409282700422
rise of 	 0.5
beyond simple 	 0.16666666666666666
which items 	 0.007246376811594203
's results 	 0.0196078431372549
These are 	 0.11764705882352941
not the 	 0.044642857142857144
barmaid . 	 0.16666666666666666
differ in 	 0.3333333333333333
Eastern Peru 	 1.0
measures can 	 0.3333333333333333
In practice 	 0.009523809523809525
one can 	 0.015384615384615385
mark word 	 0.3333333333333333
is generated 	 0.006097560975609756
Although Harris 	 0.125
problems and 	 0.11764705882352941
guessed at 	 1.0
than by 	 0.022222222222222223
and cognition 	 0.001445086705202312
require rapid 	 0.045454545454545456
The combination 	 0.005208333333333333
Advanced , 	 0.2
of conversations 	 0.00089126559714795
, mail 	 0.0005614823133071309
subdivided into 	 1.0
season , 	 1.0
problem yet 	 0.022727272727272728
deep systems 	 0.14285714285714285
incorporates a 	 1.0
a wide 	 0.00245398773006135
-LRB- MEAD 	 0.0027100271002710027
this example 	 0.01098901098901099
produced tones 	 0.1111111111111111
, an 	 0.005614823133071308
clean it 	 0.5
While supervised 	 0.2
an untagged 	 0.007575757575757576
it conducted 	 0.008547008547008548
short time 	 0.125
and obtained 	 0.001445086705202312
translation method 	 0.013513513513513514
Stanford University 	 0.5
some large 	 0.012048192771084338
2 '' 	 0.2
the accuracy 	 0.0006920415224913495
run the 	 0.4
diverse '' 	 0.5
, cognitive 	 0.0005614823133071309
trees . 	 0.3333333333333333
envelope based 	 1.0
isolated words 	 0.2
flight displays 	 0.5
reasoning components 	 0.14285714285714285
is technology 	 0.0020325203252032522
translation paradigms 	 0.013513513513513514
simply by 	 0.08333333333333333
methods In 	 0.022727272727272728
-RRB- recognize 	 0.0027100271002710027
<s> Political 	 0.0007686395080707148
distinct sentences 	 0.14285714285714285
rely on 	 0.8571428571428571
<s> Future 	 0.0007686395080707148
scientists , 	 1.0
words are 	 0.09174311926605505
subjectivity of 	 0.5
of key 	 0.00089126559714795
accuracy was 	 0.03225806451612903
undertake harder 	 1.0
changed direction 	 0.5
be repeated 	 0.004219409282700422
before classifying 	 0.16666666666666666
A typical 	 0.04
, sufficiently 	 0.0005614823133071309
, natural 	 0.0005614823133071309
-LRB- disambiguation 	 0.0027100271002710027
large-scale content-analysis 	 1.0
has meant 	 0.011904761904761904
nodes based 	 0.14285714285714285
many stochastic 	 0.019230769230769232
examples where 	 0.041666666666666664
classifying short-time 	 0.2
computer-type OCR 	 1.0
across their 	 0.2
the true 	 0.0006920415224913495
results , 	 0.09523809523809523
Systems that 	 0.3333333333333333
automatically and 	 0.09523809523809523
Statistics guided 	 0.3333333333333333
differences It 	 0.3333333333333333
computers . 	 0.2222222222222222
GenEx algorithm 	 1.0
learned is 	 0.2
The genetic 	 0.005208333333333333
site . 	 1.0
referred to 	 1.0
continued , 	 0.1111111111111111
by periods 	 0.005714285714285714
So , 	 0.3333333333333333
a hierarchy 	 0.001226993865030675
a phone 	 0.001226993865030675
an Australian 	 0.007575757575757576
supervised machine 	 0.0625
for American 	 0.0036101083032490976
, Reader 	 0.0005614823133071309
of traditional 	 0.00089126559714795
summary by 	 0.023809523809523808
statistical techniques 	 0.06060606060606061
heavily inflected 	 1.0
powerful grammars 	 1.0
increasing number 	 0.3333333333333333
human summaries 	 0.021739130434782608
programming algorithms 	 0.2
the state-of-the-art 	 0.0006920415224913495
than conversation 	 0.022222222222222223
Creating the 	 0.5
when outputting 	 0.02857142857142857
dynamically creating 	 0.5
of segmentation 	 0.00089126559714795
efficient however 	 0.3333333333333333
near each 	 1.0
deciding on 	 0.16666666666666666
be faster 	 0.004219409282700422
are examples 	 0.012448132780082987
acquire basic 	 1.0
some extent 	 0.012048192771084338
simulated the 	 0.5
are probably 	 0.004149377593360996
'' would 	 0.010309278350515464
processing plain 	 0.018518518518518517
to produce 	 0.013280212483399735
would , 	 0.018867924528301886
an experiment 	 0.007575757575757576
in texts 	 0.0018726591760299626
were later 	 0.024390243902439025
system answers 	 0.010752688172043012
of descriptive 	 0.00089126559714795
graph would 	 0.07692307692307693
`` can 	 0.005291005291005291
Sensory , 	 1.0
like summarization 	 0.03571428571428571
<s> QA 	 0.0007686395080707148
because recognition 	 0.03333333333333333
presented with 	 0.16666666666666666
switched to 	 1.0
from IBM 	 0.009615384615384616
documents -LRB- 	 0.13157894736842105
we will 	 0.08888888888888889
, NLG 	 0.0005614823133071309
way and 	 0.041666666666666664
'' can 	 0.02577319587628866
UPV -RRB- 	 1.0
and methods 	 0.001445086705202312
accuracy rate 	 0.06451612903225806
-RRB- amongst 	 0.0027100271002710027
translation Interlingual 	 0.02702702702702703
data rather 	 0.012987012987012988
supervised classification 	 0.125
would difficult 	 0.018867924528301886
that consider 	 0.0035460992907801418
an entire 	 0.007575757575757576
a group 	 0.001226993865030675
have become 	 0.009615384615384616
summaries for 	 0.023255813953488372
tagger to 	 0.1111111111111111
<s> Results 	 0.0007686395080707148
-LRB- counselling 	 0.0027100271002710027
Recognition or 	 0.125
template containing 	 0.25
alone usually 	 0.25
-LRB- titled 	 0.0027100271002710027
<s> Just 	 0.0007686395080707148
and other 	 0.01300578034682081
and visible 	 0.001445086705202312
used similar 	 0.008849557522123894
ACL Anthology 	 0.5
at binary 	 0.014705882352941176
preposition , 	 1.0
a team 	 0.001226993865030675
system operators 	 0.010752688172043012
to data 	 0.0026560424966799467
the UK 	 0.002768166089965398
of disambiguation 	 0.00089126559714795
items . 	 0.5
each document 	 0.022222222222222223
, yielding 	 0.0005614823133071309
: What 	 0.00980392156862745
page scanner 	 0.14285714285714285
pre - 	 1.0
, recognizing 	 0.0005614823133071309
Artificial Intelligence 	 0.5
recursively . 	 0.5
Digital Syphon 	 1.0
linguistics Cognitive 	 0.05
was of 	 0.012987012987012988
computers -RRB- 	 0.1111111111111111
phrase `` 	 0.1
and speech 	 0.001445086705202312
convey . 	 0.3333333333333333
increased and 	 0.2
Note also 	 0.1111111111111111
pauses . 	 0.25
together to 	 0.125
recognize speech 	 0.1111111111111111
ambiguities in 	 0.25
Poncini , 	 1.0
Lehrberger 1982 	 1.0
frequencies -LRB- 	 0.5
an excerpt 	 0.007575757575757576
most fundamental 	 0.017241379310344827
are some 	 0.004149377593360996
so on 	 0.16666666666666666
Inuit virtually 	 1.0
-LRB- citation 	 0.03523035230352303
for any 	 0.0036101083032490976
, Grishman 	 0.0005614823133071309
as hidden 	 0.003484320557491289
Hirschman 1998 	 0.5
semantic relationship 	 0.047619047619047616
measuring similarity 	 1.0
is challenging 	 0.0020325203252032522
increased from 	 0.6
theorists of 	 1.0
, either 	 0.0011229646266142617
common when 	 0.04
life scientists 	 0.25
other we 	 0.014285714285714285
no evident 	 0.07692307692307693
be kept 	 0.004219409282700422
of accuracy 	 0.0017825311942959
going thus 	 0.25
answer corpus 	 0.03333333333333333
a necessary 	 0.001226993865030675
be very 	 0.012658227848101266
abbreviation , 	 0.5
, also 	 0.002807411566535654
be processed 	 0.004219409282700422
than 150 	 0.022222222222222223
different types 	 0.04081632653061224
the adjacent 	 0.0006920415224913495
waves describe 	 0.14285714285714285
it about 	 0.008547008547008548
distance to 	 0.3333333333333333
a strength 	 0.001226993865030675
and Lifeline 	 0.001445086705202312
rank order 	 0.16666666666666666
automatically as 	 0.047619047619047616
emails and 	 0.5
ability to 	 0.75
for evaluation 	 0.0036101083032490976
summarise electronic 	 0.3333333333333333
human -LRB- 	 0.043478260869565216
recognition -LRB- 	 0.049586776859504134
Front-End speech 	 1.0
need is 	 0.047619047619047616
best candidate 	 0.05555555555555555
EHR -RRB- 	 0.3333333333333333
productions . 	 1.0
that simultaneously 	 0.0035460992907801418
given sequences 	 0.041666666666666664
software and 	 0.037037037037037035
simply verbs 	 0.08333333333333333
a dialogue 	 0.00245398773006135
browsing by 	 1.0
Nations and 	 0.5
-LRB- transcription 	 0.0027100271002710027
History Some 	 0.5
of Latin-script 	 0.00089126559714795
-RRB- had 	 0.0027100271002710027
the Northern 	 0.001384083044982699
modeling has 	 0.14285714285714285
statistical language 	 0.030303030303030304
Importance of 	 1.0
all been 	 0.023255813953488372
significant complexity 	 0.1111111111111111
software user 	 0.037037037037037035
a demonstration 	 0.0036809815950920245
uh '' 	 1.0
that OCR 	 0.0035460992907801418
in different 	 0.0056179775280898875
, studying 	 0.0005614823133071309
, UPV 	 0.0005614823133071309
or her 	 0.009009009009009009
two steps 	 0.034482758620689655
voice commands 	 0.07692307692307693
as maximum 	 0.003484320557491289
other hand 	 0.07142857142857142
developed at 	 0.07692307692307693
or answers 	 0.0045045045045045045
working out 	 0.14285714285714285
library are 	 0.5
special challenges 	 0.2
Associated Press 	 1.0
system involves 	 0.010752688172043012
mean of 	 0.5
training document 	 0.03571428571428571
ISO\/TC37\/SC4 . 	 1.0
interfaces Symantec 	 0.5
news stories 	 0.07692307692307693
use text 	 0.013888888888888888
<s> Translation 	 0.0007686395080707148
characterizes its 	 1.0
writing -RRB- 	 0.1111111111111111
How are 	 0.2857142857142857
the Defense 	 0.0006920415224913495
two methods 	 0.034482758620689655
after testing 	 0.08333333333333333
first layer 	 0.030303030303030304
As the 	 0.05555555555555555
wave and 	 0.1111111111111111
to . 	 0.00398406374501992
be apparent 	 0.004219409282700422
termed Direct 	 0.25
of Engineers 	 0.0017825311942959
up a 	 0.09090909090909091
discussed involve 	 0.14285714285714285
provide any 	 0.16666666666666666
meet Wikipedia 	 0.25
He taught 	 0.125
oral talk-in-interaction 	 1.0
characters and 	 0.0625
simple procedure 	 0.038461538461538464
a first 	 0.00245398773006135
and time-consuming 	 0.001445086705202312
however empirical 	 0.07692307692307693
Aided summarization 	 0.3333333333333333
included as 	 0.125
approach . 	 0.05714285714285714
to program 	 0.0013280212483399733
local ' 	 0.3333333333333333
to HMM 	 0.0013280212483399733
handover system 	 1.0
the person 	 0.002768166089965398
To avoid 	 0.1111111111111111
typewritten messages 	 0.2
, people 	 0.0005614823133071309
greatly affect 	 0.14285714285714285
inter-texual and 	 0.5
toolkit is 	 0.5
<s> See 	 0.0007686395080707148
for comparison 	 0.0036101083032490976
2009 -LRB- 	 0.3333333333333333
EHR will 	 0.3333333333333333
specific voice 	 0.047619047619047616
structured resources 	 0.16666666666666666
to maintain 	 0.0013280212483399733
are Deaf 	 0.004149377593360996
removed . 	 1.0
routing -LRB- 	 0.3333333333333333
course of 	 0.3333333333333333
prevent incorrect 	 1.0
token generation 	 0.25
and built 	 0.001445086705202312
multilingual corpus 	 0.3333333333333333
another he 	 0.07692307692307693
segmentation Sentence 	 0.030303030303030304
blind people 	 0.75
Context-free grammars 	 1.0
may explore 	 0.019230769230769232
the human 	 0.0020761245674740486
Treebank project 	 0.16666666666666666
are reported 	 0.004149377593360996
of years 	 0.00089126559714795
generated text 	 0.13333333333333333
their answers 	 0.029411764705882353
the early 	 0.0020761245674740486
paragraphs , 	 0.25
is generally 	 0.0020325203252032522
book on 	 0.125
not functioning 	 0.008928571428571428
untagged corpus 	 1.0
the HTK 	 0.0006920415224913495
any kind 	 0.03225806451612903
mentioned the 	 0.16666666666666666
that much 	 0.0035460992907801418
as objective 	 0.003484320557491289
with collecting 	 0.00546448087431694
has increasingly 	 0.011904761904761904
yet been 	 0.5
On others 	 0.16666666666666666
to limit 	 0.0026560424966799467
as discussed 	 0.003484320557491289
storm , 	 1.0
simulates the 	 1.0
in spirit 	 0.0018726591760299626
had mentioned 	 0.07142857142857142
, task-based 	 0.0005614823133071309
endeavors such 	 1.0
people untrained 	 0.0625
is devoted 	 0.0020325203252032522
Some systems 	 0.09523809523809523
never been 	 0.4
grammars , 	 0.14285714285714285
<s> Sentence 	 0.0023059185242121443
payment systems 	 1.0
automate sentiment 	 0.3333333333333333
these tools 	 0.023809523809523808
the primary 	 0.001384083044982699
corpus . 	 0.03225806451612903
IT technology 	 1.0
position of 	 0.25
May 2012 	 0.5
was reading 	 0.012987012987012988
Vulcan program 	 0.5
and researchers 	 0.001445086705202312
, USMC 	 0.0005614823133071309
1,000,000 words 	 1.0
United States 	 0.7777777777777778
use natural 	 0.013888888888888888
obtained a 	 0.2857142857142857
producing the 	 0.3333333333333333
evaluated in 	 0.14285714285714285
segmentation that 	 0.030303030303030304
-LRB- this 	 0.0027100271002710027
, how 	 0.0005614823133071309
very difficult 	 0.04878048780487805
extraction Task 	 0.03225806451612903
spelling . 	 1.0
chatterbots such 	 0.5
When the 	 0.14285714285714285
available and 	 0.058823529411764705
routing to 	 0.3333333333333333
that we 	 0.010638297872340425
made indifferent 	 0.0625
document before 	 0.027777777777777776
features , 	 0.038461538461538464
? <s/> 	 0.5
programming languages 	 0.6
given an 	 0.041666666666666664
' could 	 0.05263157894736842
be computed 	 0.004219409282700422
language from 	 0.006756756756756757
and correctly-developed 	 0.001445086705202312
to a 	 0.03718459495351926
On a 	 0.16666666666666666
works These 	 0.5
2001 and 	 0.5
manner rather 	 0.25
<s> Due 	 0.0007686395080707148
the University 	 0.0006920415224913495
that combine 	 0.0035460992907801418
to distinguish 	 0.006640106241699867
speech designed 	 0.006578947368421052
easier task 	 0.125
and produce 	 0.002890173410404624
the above 	 0.001384083044982699
evaluation purposes 	 0.018518518518518517
unclear whether 	 1.0
Projects Agency 	 1.0
G-loads . 	 1.0
direction of 	 0.3333333333333333
where semantics 	 0.02857142857142857
initially clear 	 1.0
texts written 	 0.058823529411764705
question or 	 0.023809523809523808
issue for 	 0.125
disseminate it 	 1.0
speech tagging 	 0.013157894736842105
Scotland . 	 0.4
of various 	 0.00089126559714795
summarization is 	 0.12
it could 	 0.008547008547008548
Black 1991 	 0.5
usually have 	 0.03125
larger volume 	 0.0625
early systems 	 0.1
work is 	 0.125
Lehnert 1981 	 0.3333333333333333
several seconds 	 0.045454545454545456
speakers to 	 0.25
the European 	 0.001384083044982699
as keyphrases 	 0.003484320557491289
umbrella term 	 1.0
-LRB- Microsoft 	 0.0027100271002710027
characters . 	 0.125
fixed schemata 	 0.5
POS-tagging algorithms 	 1.0
function -LRB- 	 0.125
a person\/persons 	 0.001226993865030675
here , 	 0.5
are consumed 	 0.004149377593360996
, queries 	 0.0005614823133071309
translated , 	 0.25
should correspond 	 0.05263157894736842
of just 	 0.00089126559714795
word . 	 0.13333333333333333
% correct 	 0.02564102564102564
entry . 	 0.25
general personal 	 0.045454545454545456
public opinion 	 1.0
on computational 	 0.0047169811320754715
any condition 	 0.03225806451612903
sense disambiguation 	 0.25
resorting to 	 1.0
-LRB- Schank 	 0.0027100271002710027
encoding world 	 1.0
systems use 	 0.05357142857142857
been closely 	 0.014705882352941176
all these 	 0.023255813953488372
, slowly 	 0.0005614823133071309
to perform 	 0.005312084993359893
fairly simple 	 0.25
or indiscriminate 	 0.0045045045045045045
turns-at-talk . 	 1.0
taggers for 	 0.14285714285714285
by applying 	 0.005714285714285714
of medical 	 0.0017825311942959
language and 	 0.013513513513513514
with questions 	 0.01092896174863388
materials to 	 0.5
processing ; 	 0.018518518518518517
last decade 	 0.4
engineers worked 	 1.0
Interlingual machine 	 0.6666666666666666
appear within 	 0.0625
model of 	 0.03333333333333333
query-biased summaries 	 1.0
compared German 	 0.14285714285714285
transformed into 	 1.0
programs often 	 0.09090909090909091
Analysis of 	 0.2
target-language-independent representation 	 1.0
time window 	 0.030303030303030304
to determine 	 0.014608233731739707
be available 	 0.004219409282700422
Analysis Standardization 	 0.2
typical features 	 0.1111111111111111
further discussed 	 0.125
field are 	 0.037037037037037035
or phrase 	 0.0045045045045045045
language document 	 0.006756756756756757
Summarization of 	 0.25
of retail 	 0.00089126559714795
stream of 	 0.5
multiscript texts 	 1.0
you see 	 0.07692307692307693
e.g. pictures 	 0.017857142857142856
cross-discipline of 	 1.0
and Ken 	 0.001445086705202312
been achieved 	 0.029411764705882353
the front 	 0.0020761245674740486
keywords , 	 0.5
of Chomskyan 	 0.00089126559714795
knowledge source 	 0.037037037037037035
understanding in 	 0.030303030303030304
method -LRB- 	 0.0625
annotated and 	 0.5
scale -RRB- 	 0.16666666666666666
their estimated 	 0.029411764705882353
a potentially 	 0.001226993865030675
not explicitly 	 0.008928571428571428
slow speech 	 0.5
generators . 	 0.5
-RRB- have 	 0.005420054200542005
, rather 	 0.0011229646266142617
importantly , 	 1.0
life experience 	 0.25
<s> Important 	 0.0007686395080707148
best algorithms 	 0.05555555555555555
levels even 	 0.045454545454545456
, speeches 	 0.0005614823133071309
limiting the 	 1.0
Engineers , 	 0.5
uses continuous 	 0.07142857142857142
to think 	 0.0013280212483399733
: Translation 	 0.00980392156862745
distinguish reliably 	 0.2
ends up 	 0.5
Communication . 	 1.0
original text 	 0.46153846153846156
vol-2 Black 	 1.0
key words 	 0.16666666666666666
previously unseen 	 0.5
essay scoring 	 1.0
similarity metrics 	 0.1
- not 	 0.0625
Work in 	 0.5
technologies to 	 0.25
con sentiment 	 1.0
best option 	 0.05555555555555555
Head-driven phrase 	 1.0
more power 	 0.010526315789473684
in terms 	 0.011235955056179775
nouns , 	 0.6666666666666666
time or 	 0.030303030303030304
the algorithms 	 0.001384083044982699
An 8 	 0.0625
vocabulary sizes 	 0.125
Given basic 	 0.07142857142857142
Such algorithms 	 0.125
deep approach 	 0.14285714285714285
is parsing 	 0.0020325203252032522
Prior implementations 	 1.0
1990s . 	 0.3333333333333333
stage using 	 0.2
source software 	 0.041666666666666664
at lower 	 0.014705882352941176
Gina Poncini 	 1.0
MT -LRB- 	 0.2
fly have 	 1.0
input into 	 0.024390243902439025
intelligence that 	 0.25
precision . 	 0.2
ignore this 	 1.0
First summarizes 	 1.0
focuses on 	 1.0
approaches presume 	 0.03571428571428571
the Technolangue\/Easy 	 0.0006920415224913495
particular dataset 	 0.07692307692307693
from some 	 0.009615384615384616
, Battle 	 0.0005614823133071309
input and 	 0.04878048780487805
discourse -RRB- 	 0.027777777777777776
a tonal 	 0.001226993865030675
-LRB- WER 	 0.0027100271002710027
even appears 	 0.037037037037037035
written scripts 	 0.038461538461538464
<s> Speech 	 0.011529592621060722
the ROUGE 	 0.0006920415224913495
as first 	 0.003484320557491289
the proposed 	 0.001384083044982699
for speech 	 0.01444043321299639
between two 	 0.05128205128205128
computerization of 	 1.0
was higher 	 0.012987012987012988
Evaluation of 	 0.1111111111111111
<s> Recently 	 0.0007686395080707148
the easier 	 0.0006920415224913495
page , 	 0.42857142857142855
statistical inference 	 0.06060606060606061
themselves as 	 0.25
respectively . 	 1.0
commercial products 	 0.09090909090909091
a language 	 0.007361963190184049
scanner to 	 0.3333333333333333
generated . 	 0.2
financial benefits 	 0.25
paradigm of 	 0.3333333333333333
Iraq and 	 0.5
which have 	 0.014492753623188406
-LRB- by 	 0.005420054200542005
fidelity of 	 1.0
for voice 	 0.0036101083032490976
a Fourier 	 0.001226993865030675
the Penpoint 	 0.0006920415224913495
summarizing multiple 	 1.0
British English 	 0.3333333333333333
possibilities must 	 0.2
opinions -RRB- 	 0.5
search engine 	 0.09090909090909091
, NNS 	 0.0005614823133071309
lexical analyser 	 0.07692307692307693
the identification 	 0.001384083044982699
consistently use 	 0.3333333333333333
he proposed 	 0.14285714285714285
systems -RRB- 	 0.008928571428571428
easily parsed 	 0.1111111111111111
Dec. 2011 	 1.0
a sub-field 	 0.001226993865030675
the Latin 	 0.0006920415224913495
TNO developed 	 1.0
successful for 	 0.1111111111111111
previously-written human 	 1.0
transformations to 	 0.5
the gap 	 0.0006920415224913495
consider a 	 0.25
the text 	 0.017993079584775088
various combinations 	 0.05555555555555555
combination of 	 0.2
i.e. source 	 0.05263157894736842
up pronouns 	 0.045454545454545456
to summarise 	 0.00398406374501992
gold-standard against 	 1.0
differ from 	 0.3333333333333333
schemes frequently 	 0.5
for identifying 	 0.0036101083032490976
effectively launched 	 0.3333333333333333
Other taggers 	 0.14285714285714285
Theo van 	 1.0
that simple 	 0.0035460992907801418
classes are 	 0.2
translation Automotive 	 0.013513513513513514
of proper 	 0.00089126559714795
which merged 	 0.007246376811594203
and get 	 0.001445086705202312
Then we 	 0.2
two ? 	 0.034482758620689655
terminate a 	 1.0
one year 	 0.015384615384615385
materials . 	 0.5
with word 	 0.01639344262295082
modified in 	 1.0
of analyzing 	 0.00089126559714795
letters or 	 0.1
This has 	 0.015873015873015872
a category 	 0.001226993865030675
tokens that 	 0.14285714285714285
systems since 	 0.008928571428571428
what kinds 	 0.03125
been explored 	 0.014705882352941176
significant semiotic 	 0.1111111111111111
human-like interaction 	 1.0
as PC 	 0.003484320557491289
typically from 	 0.05555555555555555
grammar ' 	 0.02702702702702703
one place 	 0.015384615384615385
considers an 	 0.5
the Sociologist 	 0.0006920415224913495
potential and 	 0.14285714285714285
like syntax 	 0.03571428571428571
more unmanageable 	 0.010526315789473684
further commercializing 	 0.125
automatically created 	 0.047619047619047616
time are 	 0.030303030303030304
the cost 	 0.0006920415224913495
saw the 	 1.0
should predict 	 0.05263157894736842
context dependency 	 0.030303030303030304
vertices should 	 0.1111111111111111
methods QA 	 0.022727272727272728
<s> SHRDLU 	 0.0015372790161414297
development in 	 0.08333333333333333
<s> Often 	 0.0023059185242121443
very complex 	 0.024390243902439025
also considerable 	 0.014492753623188406
and shorter 	 0.001445086705202312
is processed 	 0.0020325203252032522
have made 	 0.009615384615384616
a reliance 	 0.001226993865030675
understand simple 	 0.2857142857142857
distinct ideas 	 0.14285714285714285
one distinguishes 	 0.015384615384615385
provide summaries 	 0.16666666666666666
removing objective 	 0.5
bridging relationship 	 1.0
it . 	 0.042735042735042736
specific domain 	 0.14285714285714285
Vauquois ' 	 1.0
involved disabilities 	 0.16666666666666666
million word 	 0.3333333333333333
describe informal 	 0.16666666666666666
hard and 	 0.16666666666666666
still somewhat 	 0.06666666666666667
Dependent '' 	 1.0
solving larger 	 1.0
him or 	 0.5
fighter aircraft 	 0.5
its decomposition 	 0.02857142857142857
have complex 	 0.009615384615384616
typewritten pages 	 0.2
their lack 	 0.029411764705882353
of tokens 	 0.0017825311942959
service on 	 0.2
the judge 	 0.0006920415224913495
Wendy Lehnert 	 1.0
the part 	 0.001384083044982699
or legal 	 0.0045045045045045045
NLG may 	 0.047619047619047616
her to 	 0.5
the shipment 	 0.0006920415224913495
of which 	 0.008912655971479501
1965 based 	 0.25
or English 	 0.0045045045045045045
smaller more 	 0.14285714285714285
, communicative 	 0.0005614823133071309
voice recognition 	 0.07692307692307693
in principle 	 0.0018726591760299626
what Biden 	 0.03125
transformations . 	 0.5
to translate 	 0.00398406374501992
Error Rate 	 0.5
who is 	 0.2
Independence : 	 1.0
of logical 	 0.00089126559714795
words from 	 0.01834862385321101
1935 Tauschek 	 1.0
N-best list 	 1.0
first and 	 0.030303030303030304
greatly improved 	 0.14285714285714285
a verb 	 0.007361963190184049
into modern 	 0.01282051282051282
return a 	 0.5
associate discrete 	 0.5
to market 	 0.0013280212483399733
statistical natural 	 0.030303030303030304
which recognized 	 0.007246376811594203
the leaders 	 0.0006920415224913495
and pasted 	 0.001445086705202312
opinions or 	 0.5
Part-of-speech Tagging 	 0.5
text '' 	 0.006289308176100629
<s> Some 	 0.012298232129131437
document\/text genre 	 0.5
use only 	 0.027777777777777776
answers to 	 0.08333333333333333
also concluded 	 0.014492753623188406
even allows 	 0.037037037037037035
term for 	 0.16666666666666666
2004 . 	 0.3333333333333333
recognition because 	 0.008264462809917356
statistical engine 	 0.030303030303030304
ideas , 	 0.25
a valid 	 0.001226993865030675
10 . 	 0.125
Security Agency 	 1.0
helicopter environment 	 0.5
, maybe 	 0.0005614823133071309
, one 	 0.003368893879842785
the QA 	 0.0006920415224913495
manual evaluation 	 0.5
are saying 	 0.004149377593360996
the systems 	 0.0020761245674740486
grammatical contexts 	 0.09090909090909091
Language understanding 	 0.08333333333333333
standard expression 	 0.07142857142857142
post-secondary look 	 1.0
with such 	 0.00546448087431694
an '' 	 0.007575757575757576
difficulty of 	 0.42857142857142855
<s> Short 	 0.0007686395080707148
setting of 	 0.2
dictionary entries 	 0.14285714285714285
Many Electronic 	 0.08333333333333333
in 1933 	 0.0018726591760299626
explore the 	 0.25
's specific 	 0.0196078431372549
have direct 	 0.009615384615384616
Web pages 	 0.2222222222222222
-- if 	 0.08
or she 	 0.0045045045045045045
without understanding 	 0.07692307692307693
that appears 	 0.0035460992907801418
hand-annotated with 	 1.0
-LRB- ATC 	 0.0027100271002710027
same information 	 0.04
no spaces 	 0.07692307692307693
dynamics and 	 0.5
highly redundant 	 0.1111111111111111
the Optophone 	 0.0006920415224913495
consecutively and 	 1.0
social sciences 	 0.14285714285714285
Turn around 	 1.0
years various 	 0.047619047619047616
extract a 	 0.25
Extract subjective 	 1.0
T vertices\/unigrams 	 0.16666666666666666
parses -LRB- 	 0.5
<s> Adda 	 0.0007686395080707148
sentences to 	 0.07894736842105263
a count 	 0.001226993865030675
the world 	 0.0020761245674740486
discussing how 	 0.5
how air 	 0.034482758620689655
count T 	 0.2
and Windows 	 0.001445086705202312
is easy 	 0.0020325203252032522
thus it 	 0.1
since words 	 0.1
service with 	 0.2
a character 	 0.001226993865030675
decide to 	 0.25
proposes some 	 1.0
precision and 	 0.4
is less 	 0.0020325203252032522
often mentioned 	 0.022727272727272728
select `` 	 0.16666666666666666
parsing aims 	 0.03571428571428571
comprehensive survey 	 0.2
a substantial 	 0.001226993865030675
others -RRB- 	 0.08333333333333333
natural and 	 0.02666666666666667
searching -LRB- 	 0.3333333333333333
from small 	 0.019230769230769232
30 years 	 0.6666666666666666
English : 	 0.02702702702702703
desired -RRB- 	 0.2
is becoming 	 0.0020325203252032522
Gdaniec C. 	 1.0
Voice2Go -RRB- 	 1.0
tag for 	 0.0625
the real 	 0.0020761245674740486
report in 	 0.25
standardised text 	 1.0
'' tag 	 0.005154639175257732
to judge 	 0.0026560424966799467
communication -LRB- 	 0.4
nine '' 	 1.0
filtered out 	 0.3333333333333333
, Computer 	 0.0005614823133071309
they use 	 0.025
did Christmas 	 0.2
a news 	 0.00245398773006135
<s> and 	 0.0007686395080707148
Corps of 	 1.0
recognition equipment 	 0.008264462809917356
recognition of 	 0.0743801652892562
on increasingly 	 0.0047169811320754715
emoticons , 	 1.0
disease , 	 1.0
nodes in 	 0.14285714285714285
A possible 	 0.02
is part 	 0.0020325203252032522
<s> IBM 	 0.0007686395080707148
of coherent 	 0.00089126559714795
means the 	 0.16666666666666666
linguistics and 	 0.05
everyday life 	 1.0
by comparing 	 0.005714285714285714
first -LRB- 	 0.030303030303030304
inflectional morphology 	 1.0
yet to 	 0.5
boundaries of 	 0.09090909090909091
testing the 	 0.2
online reviews 	 0.25
3rd rev 	 1.0
Even though 	 1.0
it 's 	 0.008547008547008548
convey meaning 	 0.3333333333333333
conducted until 	 0.2
recognition in 	 0.024793388429752067
published a 	 0.2857142857142857
depth understanding 	 0.3333333333333333
T to 	 0.16666666666666666
evaluation might 	 0.018518518518518517
small , 	 0.2222222222222222
Facebook -RRB- 	 1.0
<s> Essentially 	 0.0007686395080707148
are spoken 	 0.004149377593360996
senses . 	 0.5
segmentation tools 	 0.06060606060606061
W. G. 	 0.5
and competitions 	 0.001445086705202312
equivalence relations 	 0.5
document production 	 0.027777777777777776
remains a 	 0.25
it affects 	 0.008547008547008548
statistically evaluated 	 1.0
more likely 	 0.010526315789473684
or neutral 	 0.0045045045045045045
like a 	 0.07142857142857142
domain -LRB- 	 0.05
many others 	 0.019230769230769232
`` out 	 0.005291005291005291
each pilot 	 0.022222222222222223
Due to 	 1.0
resolution , 	 0.25
many strokes 	 0.019230769230769232
comprehensive theories 	 0.2
speech -LRB- 	 0.02631578947368421
-RRB- as 	 0.008130081300813009
or people 	 0.009009009009009009
stationary probabilities 	 0.14285714285714285
from a 	 0.11538461538461539
developed a 	 0.11538461538461539
to consistently 	 0.0013280212483399733
Document summarization 	 0.25
tag probabilities 	 0.0625
restaurant , 	 0.5
create odd 	 0.058823529411764705
only cares 	 0.02631578947368421
subtypes of 	 1.0
to those 	 0.0026560424966799467
the dBase 	 0.0006920415224913495
uses only 	 0.07142857142857142
combination hidden 	 0.2
performance mainly 	 0.05555555555555555
need to 	 0.47619047619047616
made up 	 0.0625
indeed , 	 0.3333333333333333
within that 	 0.05555555555555555
, to 	 0.0072992700729927005
-RRB- a 	 0.005420054200542005
to prepare 	 0.0013280212483399733
1950s , 	 0.5
A corpus 	 0.02
making more 	 0.14285714285714285
such features 	 0.008130081300813009
characters for 	 0.0625
Part-of-speech tagging 	 0.5
similarity classes 	 0.1
most famous 	 0.034482758620689655
it agrees 	 0.008547008547008548
not only 	 0.0625
nearly perfect 	 0.5
-LRB- CWA 	 0.0027100271002710027
many similar 	 0.019230769230769232
wave from 	 0.1111111111111111
machine-learning algorithms 	 0.25
<s> Words 	 0.0015372790161414297
such rules 	 0.008130081300813009
heteroscedastic linear 	 1.0
counting cases 	 1.0
and architecture 	 0.001445086705202312
arguably -RRB- 	 0.5
often rely 	 0.022727272727272728
sound blocks 	 0.05
Michael Stubbs 	 0.25
approaches : 	 0.14285714285714285
-LRB- greater 	 0.0027100271002710027
ground established 	 1.0
Paragraph Structure 	 1.0
workload , 	 1.0
advanced scanning 	 0.4
accordingly . 	 1.0
, this 	 0.003368893879842785
A speaker 	 0.04
Electronic Health 	 0.5
phonemes is 	 0.16666666666666666
valid summary 	 1.0
or -LRB- 	 0.0045045045045045045
heuristic final 	 0.3333333333333333
noun 40 	 0.07142857142857142
-LRB- orally 	 0.0027100271002710027
-RRB- ^ 	 0.0027100271002710027
two levels 	 0.034482758620689655
Word sense 	 0.2857142857142857
understanding and 	 0.030303030303030304
Treebank Project 	 0.16666666666666666
naive Bayes 	 0.5
the conversion 	 0.0006920415224913495
harder and 	 0.14285714285714285
creating pre-defined 	 0.14285714285714285
Disambiguation Main 	 1.0
metrics , 	 0.1111111111111111
of User 	 0.00089126559714795
applications such 	 0.04
When a 	 0.14285714285714285
vs. Spontaneous 	 0.08333333333333333
Woods introduced 	 1.0
-LRB- Hirschman 	 0.0027100271002710027
, rapidly 	 0.0005614823133071309
world currently 	 0.06666666666666667
<s> Systems 	 0.006149116064565719
fall in 	 0.25
The relations 	 0.026041666666666668
In these 	 0.009523809523809525
a Markov 	 0.001226993865030675
but sometimes 	 0.014705882352941176
summaries to 	 0.046511627906976744
requires citations 	 0.0625
distinctions -LRB- 	 0.5
results demonstrate 	 0.047619047619047616
can select 	 0.0055248618784530384
made by 	 0.0625
analyses of 	 0.2
records . 	 0.5
the entities 	 0.0006920415224913495
aircraft -LRB- 	 0.2857142857142857
the part-of-speech 	 0.0006920415224913495
use software 	 0.013888888888888888
The rule-based 	 0.005208333333333333
while logic 	 0.05
Although these 	 0.125
other scripts 	 0.014285714285714285
analyzing human-written 	 0.2
develop dedicated 	 0.2
computed with 	 0.5
Inclusive choice 	 1.0
documents or 	 0.02631578947368421
flood-control pumps 	 1.0
not . 	 0.017857142857142856
<s> Types 	 0.0015372790161414297
For the 	 0.01639344262295082
production has 	 0.3333333333333333
predict -RRB- 	 0.16666666666666666
essentially calculates 	 0.125
discourses and 	 0.5
working examples 	 0.14285714285714285
a plural 	 0.001226993865030675
of the 	 0.17379679144385027
+ Mobile 	 0.16666666666666666
and funding 	 0.001445086705202312
contexts , 	 0.2857142857142857
Once performed 	 0.4
degrees of 	 0.5
representation . 	 0.21052631578947367
`` A 	 0.005291005291005291
<s> Extraction 	 0.0015372790161414297
group of 	 0.5
EVALITA web 	 0.5
breaks are 	 0.5
complex sound 	 0.125
Much of 	 0.3333333333333333
multi-word phrases 	 1.0
present special 	 0.16666666666666666
`` sad 	 0.005291005291005291
, cursive 	 0.0005614823133071309
an associated 	 0.007575757575757576
when such 	 0.05714285714285714
, responding 	 0.0005614823133071309
my hand-compiled 	 1.0
`` generalized 	 0.005291005291005291
convention in 	 1.0
computer-aided analysis 	 0.3333333333333333
a recent 	 0.001226993865030675
speed . 	 0.42857142857142855
elementary sounds 	 1.0
issue on 	 0.125
U.S. has 	 0.14285714285714285
complex NLG 	 0.041666666666666664
that funding 	 0.0035460992907801418
the best 	 0.008996539792387544
devoted exclusively 	 0.2
, electrical 	 0.0005614823133071309
process . 	 0.1388888888888889
combination and 	 0.2
automatically to 	 0.047619047619047616
ambiguous and 	 0.16666666666666666
been seen 	 0.04411764705882353
to expand 	 0.0013280212483399733
of navigation 	 0.00089126559714795
and worked 	 0.001445086705202312
dependency theory 	 0.2
by a 	 0.10285714285714286
popular evaluation 	 0.1111111111111111
, Dr. 	 0.0005614823133071309
Gender = 	 1.0
that affective 	 0.0035460992907801418
Ethnography of 	 1.0
entropy-based summarization 	 1.0
where an 	 0.02857142857142857
Brown Corpus 	 0.8571428571428571
`` universal 	 0.010582010582010581
first use 	 0.030303030303030304
in 1954 	 0.003745318352059925
and world 	 0.001445086705202312
lexical similarity 	 0.07692307692307693
printed messages 	 0.08333333333333333
most NLP 	 0.017241379310344827
of freely 	 0.00089126559714795
`` polarity 	 0.005291005291005291
term parsing 	 0.05555555555555555
phrases supported 	 0.0625
adjective , 	 0.14285714285714285
them with 	 0.05263157894736842
by part 	 0.005714285714285714
while Snyder 	 0.05
with no 	 0.01092896174863388
continued with 	 0.1111111111111111
Tannen , 	 1.0
intonation , 	 1.0
also growing 	 0.014492753623188406
taken place 	 0.3333333333333333
phonemes . 	 0.16666666666666666
keyphrase using 	 0.05263157894736842
R. Howarth 	 0.16666666666666666
and document 	 0.002890173410404624
a template 	 0.00245398773006135
Data sources 	 1.0
in polynomial 	 0.0018726591760299626
these natural 	 0.023809523809523808
another language 	 0.23076923076923078
that a 	 0.010638297872340425
Speech When 	 0.03225806451612903
including probabilistic 	 0.07142857142857142
Lakoff , 	 1.0
categories can 	 0.1111111111111111
are still 	 0.016597510373443983
summaries There 	 0.023255813953488372
remains another 	 0.25
, Gail 	 0.0005614823133071309
popular strategy 	 0.1111111111111111
determine what 	 0.043478260869565216
, error-prone 	 0.0005614823133071309
short , 	 0.125
nets . 	 1.0
attached to 	 0.5
-RRB- mark 	 0.0027100271002710027
Gee , 	 1.0
processing to 	 0.018518518518518517
neighbors are 	 0.3333333333333333
In speech 	 0.009523809523809525
in recognition 	 0.003745318352059925
funding for 	 0.25
driven , 	 1.0
areas of 	 0.3333333333333333
HMM states 	 0.3333333333333333
`` text 	 0.005291005291005291
beyond polarity 	 0.16666666666666666
consult information 	 1.0
style and 	 0.5
Jefferson , 	 1.0
for evaluating 	 0.010830324909747292
accomplish with 	 1.0
following the 	 0.06666666666666667
in general 	 0.009363295880149813
was able 	 0.05194805194805195
late 1950s 	 0.1111111111111111
they consider 	 0.025
morphologically rich 	 1.0
between discourse 	 0.1282051282051282
parsing a 	 0.03571428571428571
c -RRB- 	 1.0
summarise financial 	 0.3333333333333333
statistical translation 	 0.030303030303030304
speech recognition 	 0.42105263157894735
just a 	 0.2222222222222222
, contains 	 0.0005614823133071309
manually created 	 0.25
compute various 	 0.5
-LRB- MMR 	 0.0027100271002710027
an underlying 	 0.007575757575757576
other text 	 0.02857142857142857
services . 	 0.6666666666666666
levels is 	 0.045454545454545456
produced word 	 0.1111111111111111
human judgments 	 0.021739130434782608
known type 	 0.038461538461538464
, texts 	 0.0005614823133071309
their own 	 0.029411764705882353
`` patient 	 0.005291005291005291
dynamic study 	 0.2
spoken languages 	 0.14285714285714285
critical or 	 0.25
whole workday 	 0.1111111111111111
hits than 	 1.0
periods can 	 0.3333333333333333
rules : 	 0.046511627906976744
generally , 	 0.09090909090909091
manipulate . 	 0.3333333333333333
of general 	 0.00089126559714795
-RRB- task-based 	 0.005420054200542005
were based 	 0.024390243902439025
breaking -LRB- 	 0.5
comprehensive hand-crafted 	 0.2
who have 	 0.2
Battle Management 	 0.5
first solid 	 0.030303030303030304
Wilson , 	 1.0
an EHR 	 0.007575757575757576
LexRank was 	 0.08333333333333333
is more 	 0.006097560975609756
methods Some 	 0.022727272727272728
are different 	 0.004149377593360996
A series 	 0.02
kind , 	 0.09090909090909091
by rules 	 0.005714285714285714
answering The 	 0.08333333333333333
task depends 	 0.023809523809523808
e.g. person 	 0.017857142857142856
authors found 	 0.2
electronic medical 	 0.5
a kind 	 0.001226993865030675
distinguish from 	 0.2
etc. '' 	 0.045454545454545456
several ambiguous 	 0.045454545454545456
Lehnart . 	 1.0
1970 -RRB- 	 0.3333333333333333
LUNAR was 	 0.3333333333333333
-LRB- stationary 	 0.0027100271002710027
is orthogonal 	 0.0020325203252032522
`` warped 	 0.005291005291005291
broad agreement 	 0.25
creation of 	 1.0
included speech 	 0.125
of Natural 	 0.00089126559714795
inspired the 	 1.0
publishing , 	 1.0
generated -LRB- 	 0.06666666666666667
Real time 	 0.5
field because 	 0.037037037037037035
Summarization systems 	 0.25
two distinctive 	 0.034482758620689655
-RRB- at 	 0.0027100271002710027
L. 1998 	 1.0
'' of 	 0.005154639175257732
brain . 	 0.3333333333333333
were simply 	 0.024390243902439025
<s> Application-Oriented 	 0.0007686395080707148
called grammatical 	 0.05555555555555555
easier part 	 0.125
to focus 	 0.0013280212483399733
subjects with 	 1.0
given amount 	 0.041666666666666664
center , 	 1.0
NLP An 	 0.02127659574468085
and printed 	 0.001445086705202312
order '' 	 0.07142857142857142
summarise conditions 	 0.3333333333333333
mining , 	 0.2
a mixture 	 0.001226993865030675
the majority 	 0.0006920415224913495
of phrase 	 0.00089126559714795
characterized in 	 0.25
and perhaps 	 0.001445086705202312
identify and 	 0.08333333333333333
fundamentally different 	 1.0
to work 	 0.0026560424966799467
a long-time 	 0.001226993865030675
for quantitatively 	 0.0036101083032490976
, consider 	 0.0005614823133071309
the sounds 	 0.001384083044982699
on the 	 0.30660377358490565
methods build 	 0.022727272727272728
and computer 	 0.001445086705202312
an arithmetic 	 0.007575757575757576
's usually 	 0.0196078431372549
guessing wrong 	 1.0
Penn -RRB- 	 0.1111111111111111
January 2010 	 0.5
the concepts 	 0.0006920415224913495
component . 	 0.2
trivial , 	 0.25
Racter , 	 1.0
are words 	 0.004149377593360996
have several 	 0.009615384615384616
large dictionaries 	 0.043478260869565216
text classification 	 0.006289308176100629
actually correct 	 0.3333333333333333
, reliability 	 0.0005614823133071309
: Information 	 0.00980392156862745
For keyphrase 	 0.01639344262295082
and analyze 	 0.001445086705202312
Japanese -RRB- 	 0.125
a journal 	 0.001226993865030675
Cynthia Hardy 	 1.0
sets of 	 0.36363636363636365
microphone . 	 1.0
to himself 	 0.0013280212483399733
noise -LRB- 	 0.125
humans can 	 0.08333333333333333
standard result 	 0.07142857142857142
came into 	 0.5
would reduce 	 0.018867924528301886
feedback on 	 0.5
cards for 	 1.0
of understanding 	 0.00089126559714795
expressions that 	 0.3333333333333333
separately from 	 1.0
a fast-evolving 	 0.001226993865030675
if there 	 0.07142857142857142
adequately solved 	 1.0
'' -LRB- 	 0.04639175257731959
multitude of 	 1.0
draft document 	 0.5
-LRB- An 	 0.005420054200542005
general purpose 	 0.09090909090909091
either an 	 0.1
just seen 	 0.1111111111111111
might use 	 0.07692307692307693
of spectral-domain 	 0.00089126559714795
of stochastic 	 0.00089126559714795
have so-called 	 0.009615384615384616
untrained on 	 1.0
software to 	 0.07407407407407407
ME is 	 0.5
David 's 	 0.25
linguistic resources 	 0.0625
summarization It 	 0.02
follow-the-bouncing-ball video 	 1.0
needed for 	 0.09523809523809523
GPO -RRB- 	 1.0
would choose 	 0.018867924528301886
on a 	 0.10849056603773585
series of 	 0.875
some detail 	 0.012048192771084338
short textual 	 0.125
then chosen 	 0.02857142857142857
in context 	 0.00749063670411985
each word 	 0.1111111111111111
the nouns 	 0.0006920415224913495
walking slowly 	 0.3333333333333333
voice is 	 0.07692307692307693
be combined 	 0.004219409282700422
in the 	 0.2602996254681648
programs in 	 0.09090909090909091
about pronouns 	 0.025
systems is 	 0.026785714285714284
can determine 	 0.011049723756906077
The sub-committee 	 0.005208333333333333
on upper 	 0.0047169811320754715
human speech 	 0.021739130434782608
recognition programs 	 0.008264462809917356
machine to 	 0.012658227848101266
and placement 	 0.001445086705202312
The 26 	 0.005208333333333333
simply using 	 0.08333333333333333
what might 	 0.03125
; Given 	 0.02127659574468085
were published 	 0.024390243902439025
, affective 	 0.0005614823133071309
large quantities 	 0.08695652173913043
difference between 	 0.25
purpose when 	 0.2
as weights 	 0.003484320557491289
entropy model 	 0.2
words being 	 0.009174311926605505
-LRB- ISRI 	 0.0027100271002710027
language or 	 0.013513513513513514
smaller sub-sounds 	 0.14285714285714285
States Air 	 0.14285714285714285
M. De 	 0.25
release or 	 0.3333333333333333
editing and 	 0.5
structuring : 	 1.0
labeled keyphrases 	 0.3333333333333333
with misspelled 	 0.00546448087431694
<s> Because 	 0.0015372790161414297
take into 	 0.3
are summaries 	 0.004149377593360996
candidates , 	 0.2
Modern NLP 	 0.3333333333333333
, the 	 0.058394160583941604
words to 	 0.009174311926605505
hand-compiled list 	 1.0
volume and 	 0.25
include . 	 0.037037037037037035
Open-domain question 	 1.0
which means 	 0.028985507246376812
Context and 	 1.0
analysis -RRB- 	 0.03076923076923077
simple domains 	 0.038461538461538464
Pang who 	 0.3333333333333333
general speech 	 0.045454545454545456
issues are 	 0.2
that may 	 0.0070921985815602835
, Phrases 	 0.0005614823133071309
ANR-Passage project 	 1.0
Department of 	 1.0
test the 	 0.1
summarization faces 	 0.02
user can 	 0.07142857142857142
those influenced 	 0.045454545454545456
, containing 	 0.0011229646266142617
Tell me 	 1.0
emotion , 	 1.0
engine . 	 0.6666666666666666
best to 	 0.1111111111111111
by Xuedong 	 0.005714285714285714
= Human 	 0.1111111111111111
about basic 	 0.025
semiotic event 	 1.0
parsing and 	 0.03571428571428571
information deemed 	 0.021739130434782608
finite set 	 0.2
-LRB- Keyphrase 	 0.0027100271002710027
the management 	 0.0006920415224913495
' . 	 0.10526315789473684
pauses in 	 0.25
time factor 	 0.030303030303030304
question marks 	 0.023809523809523808
An extractive 	 0.0625
release parameters 	 0.3333333333333333
Phonemes , 	 1.0
on lower 	 0.0047169811320754715
garden-path sentences 	 1.0
create texts 	 0.058823529411764705
fluent native 	 1.0
any other 	 0.06451612903225806
both query 	 0.03225806451612903
scale rather 	 0.16666666666666666
This makes 	 0.015873015873015872
splicing and 	 1.0
Two particular 	 0.14285714285714285
words such 	 0.01834862385321101
to unigram 	 0.0013280212483399733
annotation and 	 0.25
substitutions . 	 1.0
paradigms . 	 1.0
learning model 	 0.023255813953488372
subjective sentences 	 0.16666666666666666
processing news 	 0.018518518518518517
queries on 	 0.3333333333333333
model , 	 0.1
appear consecutively 	 0.0625
Judith M. 	 1.0
, answer 	 0.0005614823133071309
adapted to 	 1.0
specification . 	 0.5
securely ; 	 1.0
and Plot 	 0.001445086705202312
Manual evaluation 	 0.6666666666666666
to segment 	 0.00398406374501992
B. , 	 1.0
phonetic segmentation 	 0.5
an optimal 	 0.007575757575757576
modifying words 	 1.0
a deep 	 0.001226993865030675
original voice 	 0.07692307692307693
of mobile 	 0.00089126559714795
reasoning mechanisms 	 0.14285714285714285
visual and\/or 	 0.5
, processing 	 0.0005614823133071309
later licensed 	 0.1
decade in 	 0.3333333333333333
this way 	 0.02197802197802198
Research and 	 0.125
that looks 	 0.0035460992907801418
government sponsored 	 0.3333333333333333
Arabic and 	 0.25
to performance 	 0.0013280212483399733
, Henry 	 0.0005614823133071309
thirty years 	 1.0
and multitude 	 0.001445086705202312
MAHS = 	 1.0
words will 	 0.01834862385321101
unigrams placed 	 0.08333333333333333
considerable attention 	 0.2
, adjective 	 0.0005614823133071309
in 1984 	 0.0018726591760299626
<s> Closed-domain 	 0.0007686395080707148
image consisting 	 0.3333333333333333
is hard 	 0.0020325203252032522
greatly on 	 0.14285714285714285
common term 	 0.04
of problems 	 0.0017825311942959
clearly not 	 0.3333333333333333
with one 	 0.00546448087431694
of valuable 	 0.00089126559714795
: GRASSHOPPER 	 0.00980392156862745
's many 	 0.0196078431372549
in isolation 	 0.0018726591760299626
and used 	 0.001445086705202312
applications there 	 0.04
discourse and 	 0.1111111111111111
the IEEE 	 0.001384083044982699
Commercial applications 	 0.5
above -- 	 0.07692307692307693
helped the 	 0.3333333333333333
Alan Turing 	 1.0
Both acoustic 	 0.3333333333333333
largest speech 	 1.0
adding sentences 	 0.5
shown to 	 0.4
campaign dedicated 	 0.2
systems do 	 0.008928571428571428
or paragraph 	 0.0045045045045045045
to blind 	 0.0013280212483399733
to video 	 0.0013280212483399733
is `` 	 0.0040650406504065045
from printed 	 0.009615384615384616
likely source 	 0.0625
limit is 	 0.25
languages of 	 0.02
last example 	 0.2
it must 	 0.008547008547008548
combining those 	 0.25
range of 	 0.5714285714285714
; in 	 0.02127659574468085
PC history 	 0.25
first primitive 	 0.030303030303030304
The CyberEmotions 	 0.005208333333333333
pronunciations or 	 1.0
2005 -RRB- 	 1.0
as Maximal 	 0.003484320557491289
or disease 	 0.0045045045045045045
corpora , 	 0.09090909090909091
standards and 	 0.2
settings . 	 1.0
it into 	 0.042735042735042736
pumps '' 	 0.5
The earliest 	 0.005208333333333333
the methods 	 0.0006920415224913495
why HMMs 	 0.14285714285714285
operation . 	 0.5
such an 	 0.016260162601626018
genetic algorithm 	 1.0
Dogged '' 	 1.0
different ways 	 0.02040816326530612
abbreviation MT 	 0.5
statistical decision-making 	 0.030303030303030304
in trying 	 0.0018726591760299626
or Web-based 	 0.0045045045045045045
textual corpora 	 0.2
characters that 	 0.0625
-RRB- classifier 	 0.0027100271002710027
spoke the 	 1.0
mechanisms , 	 0.5
forward-backward algorithm 	 1.0
Parseval\/GEIG project 	 1.0
decisions based 	 0.2
medicine or 	 1.0
Forces Security 	 1.0
of substantial 	 0.00089126559714795
original sound 	 0.07692307692307693
Lander used 	 0.5
question-answering abilities 	 0.5
tasks such 	 0.0625
rules -- 	 0.023255813953488372
the goal 	 0.002768166089965398
measured by 	 0.5
universities around 	 1.0
is for 	 0.0020325203252032522
Other measures 	 0.14285714285714285
to automated 	 0.0013280212483399733
and efficient 	 0.002890173410404624
by Pang 	 0.005714285714285714
evaluation -LRB- 	 0.018518518518518517
noise problem 	 0.125
with regard 	 0.02185792349726776
translation was 	 0.02702702702702703
font . 	 0.6666666666666666
of reproducing 	 0.00089126559714795
find that 	 0.07692307692307693
systems . 	 0.08928571428571429
and current 	 0.001445086705202312
normally do 	 0.5
<s> Data 	 0.0007686395080707148
but robustness 	 0.014705882352941176
in two 	 0.0018726591760299626
might provide 	 0.038461538461538464
evaluation comes 	 0.018518518518518517
acquiring coarse-grained 	 1.0
wrote ELIZA 	 0.16666666666666666
often argued 	 0.022727272727272728
languages are 	 0.02
very effective 	 0.024390243902439025
picture above 	 0.25
essentially a 	 0.25
automated sentiment 	 0.14285714285714285
After training 	 0.3333333333333333
graphics -- 	 1.0
system depend 	 0.010752688172043012
impressive results 	 0.5
are widely 	 0.004149377593360996
text segmentation 	 0.0440251572327044
non-linearly to 	 1.0
state -LRB- 	 0.07142857142857142
or con 	 0.0045045045045045045
Mars Microphone 	 0.5
applications , 	 0.16
Syntactic ; 	 1.0
superset of 	 1.0
Note that 	 0.7777777777777778
read the 	 0.14285714285714285
well to 	 0.03571428571428571
<s> human 	 0.0007686395080707148
simpler sounds 	 0.6666666666666666
Yes\/No vs. 	 1.0
patent for 	 0.25
-LRB- AFTI 	 0.0027100271002710027
PC platform 	 0.25
time -LRB- 	 0.06060606060606061
-LRB- one 	 0.0027100271002710027
their framework 	 0.029411764705882353
usually the 	 0.03125
bag of 	 1.0
<s> Use 	 0.0007686395080707148
raised in 	 1.0
currently focus 	 0.14285714285714285
<s> require 	 0.0007686395080707148
Recognition is 	 0.125
same string 	 0.04
process correctly 	 0.027777777777777776
WordNet . 	 0.5
in Scotland 	 0.0018726591760299626
statistics , 	 0.125
small local 	 0.1111111111111111
the AVRADA 	 0.0006920415224913495
'' implicate 	 0.005154639175257732
more fine-grained 	 0.010526315789473684
of 500,000 	 0.00089126559714795
`` um 	 0.005291005291005291
of simpler 	 0.0017825311942959
to start 	 0.0026560424966799467
Online software 	 0.5
both research 	 0.03225806451612903
T final 	 0.16666666666666666
became an 	 0.2
, Thai 	 0.0005614823133071309
The English 	 0.005208333333333333
allow a 	 0.2
would output 	 0.018867924528301886
global ` 	 0.3333333333333333
helped overall 	 0.3333333333333333
appropriately , 	 0.5
, Janet 	 0.0005614823133071309
gets around 	 0.5
of POS 	 0.0017825311942959
promoting diversity 	 1.0
a car 	 0.001226993865030675
Reader 's 	 1.0
, discourse 	 0.0005614823133071309
chunks of 	 1.0
complex sounds 	 0.041666666666666664
summary . 	 0.2619047619047619
NLP that 	 0.02127659574468085
Evaluation The 	 0.1111111111111111
`` Part-of-speech 	 0.005291005291005291
generate . 	 0.05555555555555555
high ranks 	 0.05555555555555555
canned text 	 0.5
records is 	 0.25
interpret and 	 1.0
-LRB- selecting 	 0.0027100271002710027
-RRB- represents 	 0.0027100271002710027
case-based reasoning 	 1.0
confusions with 	 1.0
text normalization 	 0.006289308176100629
not contain 	 0.008928571428571428
satisfactory in 	 1.0
a well-defined 	 0.001226993865030675
defines components 	 0.5
cause the 	 0.5
where using 	 0.02857142857142857
help in 	 0.1111111111111111
Realisation : 	 1.0
sufficient . 	 0.6
too many 	 0.3333333333333333
poor coverage 	 1.0
purposes , 	 0.25
, standard 	 0.0011229646266142617
full-text search 	 1.0
Anaphor resolution 	 1.0
their effectiveness 	 0.029411764705882353
also capitalized 	 0.014492753623188406
and occurs 	 0.001445086705202312
deciding to 	 0.3333333333333333
to re-encode 	 0.0013280212483399733
of overlap 	 0.00089126559714795
structures of 	 0.2
within different 	 0.05555555555555555
opportunity to 	 0.5
be specified 	 0.004219409282700422
robustness against 	 0.5
used to 	 0.19469026548672566
that alone 	 0.0035460992907801418
to five 	 0.0013280212483399733
problems . 	 0.17647058823529413
the mechanical 	 0.0006920415224913495
medical records 	 0.3333333333333333
kept either 	 1.0
obvious at 	 1.0
still very 	 0.06666666666666667
Many different 	 0.16666666666666666
compared a 	 0.14285714285714285
, like 	 0.0016844469399213925
variation across 	 1.0
redundancy in 	 0.6666666666666666
to rate 	 0.0026560424966799467
by spaces 	 0.005714285714285714
this right 	 0.01098901098901099
as language 	 0.003484320557491289
speech-recognition engine 	 0.6666666666666666
specific theoretical 	 0.047619047619047616
'' such 	 0.005154639175257732
camera , 	 0.5
R. Harris 	 0.16666666666666666
its suitability 	 0.02857142857142857
, Bobrow 	 0.0005614823133071309
of prisoner-of-war 	 0.00089126559714795
an arbitrarily 	 0.007575757575757576
provided within 	 0.2
patterns rather 	 0.2
addressed in 	 0.5
for air 	 0.0036101083032490976
out the 	 0.21428571428571427
1954 on 	 0.3333333333333333
<s> Inclusive 	 0.0007686395080707148
help understand 	 0.1111111111111111
is the 	 0.09146341463414634
section needs 	 0.16666666666666666
less -RRB- 	 0.08333333333333333
reliable sources 	 0.25
earliest-used machine 	 0.5
have used 	 0.019230769230769232
semantics is 	 0.07142857142857142
to eliminate 	 0.0026560424966799467
a subsystem 	 0.001226993865030675
summary ' 	 0.023809523809523808
negative labels 	 0.125
we produce 	 0.044444444444444446
with diversity 	 0.00546448087431694
2011 campaign 	 0.5
reached . 	 0.5
Reukos S. 	 1.0
Schober , 	 1.0
In this 	 0.047619047619047616
engines , 	 0.3333333333333333
rate these 	 0.09090909090909091
systems on 	 0.008928571428571428
recognition vary 	 0.008264462809917356
to merge 	 0.0013280212483399733
Methods Computers 	 0.25
realized on 	 1.0
9 parts 	 1.0
as high 	 0.003484320557491289
in using 	 0.0056179775280898875
rejecting `` 	 0.6666666666666666
texts from 	 0.058823529411764705
-RRB- Cohesion 	 0.0027100271002710027
MCE -RRB- 	 1.0
PageRank to 	 0.3333333333333333
formal representations 	 0.2222222222222222
textbook of 	 0.5
forecasts in 	 0.2
noise in 	 0.125
With IT 	 0.14285714285714285
to form 	 0.005312084993359893
English . 	 0.13513513513513514
One can 	 0.07692307692307693
Pike , 	 1.0
informal exchange 	 0.5
use , 	 0.05555555555555555
source and 	 0.041666666666666664
and LOB 	 0.001445086705202312
integrate reasoning 	 1.0
words not 	 0.009174311926605505
President Biden 	 0.25
from limited 	 0.009615384615384616
`` who 	 0.005291005291005291
for What 	 0.0036101083032490976
Politics -LRB- 	 1.0
clearly visible 	 0.3333333333333333
first-cut can 	 1.0
, conjunction 	 0.0005614823133071309
applied the 	 0.06666666666666667
sub-problems , 	 1.0
Sync -RRB- 	 1.0
problems colloquially 	 0.11764705882352941
predicate logic 	 1.0
<s> Attribute 	 0.0007686395080707148
as its 	 0.010452961672473868
learning the 	 0.023255813953488372
a professor 	 0.001226993865030675
essence of 	 1.0
classification indicates 	 0.058823529411764705
<s> Both 	 0.0015372790161414297
of subjectivity 	 0.00089126559714795
vocabulary , 	 0.125
the 93-95 	 0.0006920415224913495
it can 	 0.05128205128205128
that nuggets 	 0.0035460992907801418
boundary ' 	 0.16666666666666666
-LRB- QA 	 0.0027100271002710027
this system 	 0.01098901098901099
CoNLL shared 	 1.0
provides for 	 0.5
a keyboard 	 0.001226993865030675
a correct 	 0.00245398773006135
-LRB- at 	 0.0027100271002710027
further research 	 0.125
that carried 	 0.0035460992907801418
late 1930s 	 0.1111111111111111
human thought 	 0.021739130434782608
human meteorologist 	 0.021739130434782608
30 % 	 0.3333333333333333
and Roger 	 0.001445086705202312
management of 	 0.2857142857142857
software Desktop 	 0.037037037037037035
The translator 	 0.005208333333333333
, favor 	 0.0005614823133071309
the type 	 0.002768166089965398
The examples 	 0.005208333333333333
The United 	 0.005208333333333333
is RDF 	 0.0020325203252032522
only on 	 0.05263157894736842
: Translations 	 0.00980392156862745
card number 	 0.25
systems to 	 0.008928571428571428
list of 	 0.7272727272727273
User Interface 	 0.5
has already 	 0.011904761904761904
Products began 	 0.5
probabilities to 	 0.09090909090909091
expected for 	 0.14285714285714285
and this 	 0.001445086705202312
, Aletta 	 0.0005614823133071309
the challenge 	 0.0006920415224913495
tasks due 	 0.03125
with storing 	 0.00546448087431694
or otherwise 	 0.0045045045045045045
resources , 	 0.3333333333333333
in its 	 0.003745318352059925
presents the 	 1.0
meaning which 	 0.043478260869565216
to take 	 0.006640106241699867
copying important 	 1.0
pour le 	 1.0
behavior even 	 0.5
If probabilities 	 0.1
Symantec Corporation 	 0.5
teach that 	 1.0
first few 	 0.030303030303030304
metrics are 	 0.1111111111111111
with Optical 	 0.00546448087431694
, Harvey 	 0.0005614823133071309
political negligence 	 0.3333333333333333
good candidates 	 0.23076923076923078
be `` 	 0.004219409282700422
, highly-specialized 	 0.0005614823133071309
DARPA -LRB- 	 0.25
network -LRB- 	 0.16666666666666666
, due 	 0.0005614823133071309
domains where 	 0.125
very recent 	 0.024390243902439025
their closest 	 0.029411764705882353
question . 	 0.047619047619047616
provide a 	 0.3333333333333333
be divided 	 0.004219409282700422
neat , 	 1.0
in embedded 	 0.0018726591760299626
other pieces 	 0.014285714285714285
used include 	 0.008849557522123894
partly statistical 	 1.0
smaller dictionary 	 0.14285714285714285
other modifying 	 0.014285714285714285
than optimizing 	 0.022222222222222223
, Paul 	 0.0016844469399213925
when given 	 0.05714285714285714
strategies , 	 0.5
: Putting 	 0.00980392156862745
word separators 	 0.016666666666666666
to be 	 0.057104913678618856
, medicine 	 0.0005614823133071309
at run-time 	 0.014705882352941176
's software 	 0.0196078431372549
a Standard 	 0.001226993865030675
are names 	 0.004149377593360996
the program 	 0.0020761245674740486
the generation 	 0.0006920415224913495
relevance theory 	 0.3333333333333333
in smaller 	 0.0018726591760299626
TextRank While 	 0.07142857142857142
Top-down parsing 	 1.0
primitive computer-type 	 1.0
Was he 	 1.0
have a 	 0.125
walking patterns 	 0.3333333333333333
to describe 	 0.0026560424966799467
were realized 	 0.024390243902439025
W. Handel 	 0.5
NLG is 	 0.09523809523809523
learning applications 	 0.023255813953488372
<s> Incorporating 	 0.0007686395080707148
simpler questions 	 0.3333333333333333
Big wave 	 1.0
takes the 	 0.3333333333333333
picture on 	 0.25
1976 the 	 0.5
reader was 	 0.2
learn a 	 0.23076923076923078
automatic lip-synch 	 0.043478260869565216
feature or 	 0.07692307692307693
test documents 	 0.2
summary sentences 	 0.023809523809523808
in NIST 	 0.0018726591760299626
type , 	 0.07142857142857142
global ' 	 0.3333333333333333
OCR patents 	 0.02040816326530612
, such 	 0.019651880965749578
and controlling 	 0.001445086705202312
segments besides 	 0.2
, contrast 	 0.0005614823133071309
custom software 	 0.5
complexity . 	 0.08333333333333333
this particular 	 0.01098901098901099
referring expressions 	 0.5
Contrary to 	 1.0
frame has 	 0.5
, moves 	 0.0005614823133071309
shown that 	 0.2
concluded that 	 1.0
Speaker Independent 	 0.16666666666666666
Thus the 	 0.08333333333333333
for innumerable 	 0.0036101083032490976
the features 	 0.0034602076124567475
reasonable chance 	 0.5
weather data 	 0.14285714285714285
extensive research 	 0.3333333333333333
and creation 	 0.001445086705202312
sociology , 	 1.0
Winograd finished 	 0.3333333333333333
for developing 	 0.007220216606498195
the learning 	 0.0006920415224913495
extractive summarization 	 0.42857142857142855
that by 	 0.0035460992907801418
state transducer 	 0.14285714285714285
comprehension and 	 0.14285714285714285
discards any 	 1.0
been done 	 0.029411764705882353
ratings . 	 0.1111111111111111
In part-of-speech 	 0.009523809523809525
learning from 	 0.046511627906976744
, Alan 	 0.0005614823133071309
a dissertation 	 0.001226993865030675
only the 	 0.10526315789473684
Matches between 	 1.0
tool . 	 0.5
sets , 	 0.09090909090909091
getting into 	 0.25
of personalised 	 0.00089126559714795
deaf telephony 	 1.0
the machine 	 0.0006920415224913495
conducted with 	 0.2
unknowns , 	 1.0
significant increase 	 0.1111111111111111
break sentences 	 0.5
learning algorithm 	 0.11627906976744186
produces all 	 0.25
should vertices 	 0.05263157894736842
of questions 	 0.004456327985739751
further speaker 	 0.125
translation : 	 0.02702702702702703
IBM Research 	 0.3333333333333333
we register 	 0.022222222222222223
perfectly , 	 1.0
frequently used 	 0.5
high , 	 0.05555555555555555
quality can 	 0.1
step that 	 0.13333333333333333
some methods 	 0.024096385542168676
many as 	 0.019230769230769232
the social 	 0.0020761245674740486
of life 	 0.00089126559714795
following is 	 0.06666666666666667
retrieval module 	 0.14285714285714285
comes mainly 	 0.2
be programmed 	 0.008438818565400843
recall-based to 	 0.5
parsing community 	 0.03571428571428571
on it 	 0.0047169811320754715
Telephone Company 	 1.0
Computer Problem 	 0.16666666666666666
of triples 	 0.00089126559714795
failed to 	 1.0
two enabling 	 0.034482758620689655
commands or 	 0.2
usefulness of 	 1.0
Digest coupons 	 0.3333333333333333
tonal language 	 1.0
describing a 	 0.25
probable answer 	 1.0
processed incorrectly 	 0.16666666666666666
equivalent ideas 	 0.2
to benefit 	 0.0013280212483399733
entropy -LRB- 	 0.2
that cause 	 0.0035460992907801418
Walter Kintsch 	 1.0
the mid 	 0.0006920415224913495
and derive 	 0.001445086705202312
enough to 	 0.2
evaluation approach 	 0.018518518518518517
simulators with 	 1.0
when translating 	 0.02857142857142857
system that 	 0.03225806451612903
Speech processing 	 0.03225806451612903
relations -LRB- 	 0.08333333333333333
parser is 	 0.1875
licensed on 	 1.0
algorithm exploits 	 0.03571428571428571
The progress 	 0.005208333333333333
Solutions have 	 1.0
text corresponds 	 0.006289308176100629
necessary to 	 0.2
needed , 	 0.047619047619047616
1999 -RRB- 	 0.5
those used 	 0.13636363636363635
project . 	 0.07692307692307693
Nuance Voice 	 0.3333333333333333
has been 	 0.3333333333333333
parsing or 	 0.07142857142857142
goals of 	 1.0
would generate 	 0.018867924528301886
Artificial neural 	 0.5
size and 	 0.3333333333333333
the times 	 0.0006920415224913495
left-most and 	 0.5
Unsourced material 	 1.0
often uses 	 0.022727272727272728
termed coarticulation 	 0.25
hierarchically in 	 1.0
accuracy using 	 0.03225806451612903
exponential time 	 0.5
trigrams , 	 0.5
far from 	 0.125
may use 	 0.019230769230769232
other cases 	 0.02857142857142857
like Japanese 	 0.03571428571428571
tasks defined 	 0.03125
may also 	 0.019230769230769232
Parsers are 	 0.5
listening to 	 1.0
by Lawrence 	 0.005714285714285714
analyzed and 	 0.2
! <s/> 	 1.0
only relief 	 0.02631578947368421
MARGIE -LRB- 	 1.0
, translation 	 0.0011229646266142617
what they 	 0.03125
phonemes of 	 0.16666666666666666
same column 	 0.04
% -RRB- 	 0.02564102564102564
rubric includes 	 1.0
better than 	 0.1111111111111111
the simple 	 0.0006920415224913495
The profile 	 0.005208333333333333
become more 	 0.25
of tags 	 0.00089126559714795
is always 	 0.0040650406504065045
system which 	 0.010752688172043012
be further 	 0.004219409282700422
values of 	 0.5
even disappear 	 0.037037037037037035
With isolated 	 0.14285714285714285
closed-domain might 	 1.0
rendered view 	 1.0
is shown 	 0.0020325203252032522
included analyses 	 0.125
and perspective 	 0.001445086705202312
language interface 	 0.006756756756756757
enormous amount 	 1.0
<s> Other 	 0.005380476556495004
the stock 	 0.0006920415224913495
noun phrases 	 0.07142857142857142
might be 	 0.23076923076923078
drawn right-to-left 	 1.0
by question 	 0.005714285714285714
learn explicit 	 0.07692307692307693
-LRB- DA 	 0.005420054200542005
polarity of 	 0.25
right context 	 0.1
Their algorithm 	 0.5
speech data 	 0.006578947368421052
word -RRB- 	 0.016666666666666666
Decoding of 	 0.5
character for 	 0.045454545454545456
then given 	 0.02857142857142857
disciplines , 	 1.0
also called 	 0.043478260869565216
on to 	 0.009433962264150943
word stems 	 0.016666666666666666
coming between 	 1.0
being asked 	 0.05555555555555555
, G 	 0.0005614823133071309
in Norman 	 0.0018726591760299626
this . 	 0.01098901098901099
Pallet D.S. 	 0.5
we might 	 0.022222222222222223
parsing written 	 0.03571428571428571
learn to 	 0.07692307692307693
and assess 	 0.001445086705202312
level . 	 0.05
<s> Perspectives 	 0.0007686395080707148
With sufficient 	 0.14285714285714285
important Web 	 0.0625
linguistics concerned 	 0.05
abstraction involves 	 0.25
determine keyphrases 	 0.043478260869565216
analytical approaches 	 0.5
Machine Summarization 	 0.1111111111111111
many of 	 0.038461538461538464
RCA Drum 	 0.2
Accuracy for 	 0.14285714285714285
was declared 	 0.012987012987012988
`` recommending 	 0.005291005291005291
not all 	 0.017857142857142856
by receivers 	 0.005714285714285714
documents more 	 0.02631578947368421
Speech Recognition 	 0.0967741935483871
task-based -LRB- 	 0.25
training a 	 0.03571428571428571
very similar 	 0.0975609756097561
brain is 	 0.3333333333333333
Chinese , 	 0.2857142857142857
incomplete sentences 	 1.0
system -LRB- 	 0.021505376344086023
a certain 	 0.001226993865030675
8 % 	 1.0
approach described 	 0.02857142857142857
of part 	 0.00089126559714795
here there 	 0.5
, separate 	 0.0011229646266142617
is analyzed 	 0.006097560975609756
, meanings 	 0.0005614823133071309
of OCR 	 0.0035650623885918
sentences -LRB- 	 0.02631578947368421
research also 	 0.023809523809523808
database industry 	 0.1
of systems 	 0.0017825311942959
of diagonal 	 0.00089126559714795
tourism information 	 1.0
large multilingual 	 0.043478260869565216
of perspective 	 0.00089126559714795
Bible Society 	 1.0
, maximum 	 0.0005614823133071309
that preclude 	 0.0035460992907801418
Jelinek and 	 0.5
including the 	 0.07142857142857142
once you 	 1.0
ICR make 	 0.3333333333333333
mathematical framework 	 0.5
, research 	 0.0011229646266142617
from several 	 0.009615384615384616
a real-time 	 0.001226993865030675
program by 	 0.045454545454545456
-LRB- essentially 	 0.0027100271002710027
shallowest , 	 1.0
ambiguities or 	 0.25
and copying 	 0.001445086705202312
worked on 	 0.4
earliest-used algorithms 	 0.5
segmentation In 	 0.030303030303030304
, consisting 	 0.0005614823133071309
also many 	 0.014492753623188406
relevant information 	 0.14285714285714285
`` depth 	 0.005291005291005291
The software 	 0.005208333333333333
includes mainly 	 0.14285714285714285
general principles 	 0.045454545454545456
nice side 	 0.25
manually designed 	 0.25
recognized draft 	 0.16666666666666666
languages which 	 0.02
fueled interest 	 1.0
corpus , 	 0.06451612903225806
parsers will 	 0.15384615384615385
Individuals with 	 1.0
and domain 	 0.001445086705202312
level features 	 0.05
state-of-the-art in 	 0.5
computer user 	 0.022727272727272728
includes making 	 0.14285714285714285
sound should 	 0.1
claiming to 	 1.0
, MySpace 	 0.0005614823133071309
automating abstractive 	 1.0
isolation . 	 0.5
most significant 	 0.017241379310344827
<s> Unsourced 	 0.0007686395080707148
being expended 	 0.05555555555555555
IEEE Transactions 	 0.6666666666666666
sometimes ambiguous 	 0.07692307692307693
large variety 	 0.043478260869565216
general with 	 0.045454545454545456
Did Marilyn 	 1.0
of nodes 	 0.0035650623885918
have questioned 	 0.009615384615384616
human raters 	 0.021739130434782608
and can 	 0.011560693641618497
to measure 	 0.005312084993359893
a top-down 	 0.001226993865030675
Retrieval Conference 	 1.0
informal behavior 	 0.5
understanding , 	 0.09090909090909091
performed by 	 0.2
multimedia documents 	 0.5
, aspect 	 0.0005614823133071309
would only 	 0.018867924528301886
man '' 	 1.0
reports -RRB- 	 0.4
Research in 	 0.125
retrieval -- 	 0.14285714285714285
desired structure 	 0.2
have approached 	 0.009615384615384616
indirect left-recursion 	 1.0
taken up 	 0.3333333333333333
occurring ' 	 1.0
at processing 	 0.014705882352941176
are much 	 0.008298755186721992
second edition 	 0.1
many researchers 	 0.019230769230769232
classifier is 	 0.14285714285714285
addressee at 	 1.0
grammatical and 	 0.09090909090909091
<s> Potentially 	 0.0007686395080707148
as could 	 0.003484320557491289
distinctive initial 	 0.5
, associating 	 0.0005614823133071309
agglutinative languages 	 1.0
when multiple 	 0.02857142857142857
still to 	 0.06666666666666667
understand the 	 0.2857142857142857
that words 	 0.0070921985815602835
would identify 	 0.018867924528301886
the predicted 	 0.0006920415224913495
from context 	 0.009615384615384616
sponsored evaluations 	 0.5
by humans 	 0.011428571428571429
with corpus 	 0.00546448087431694
for their 	 0.007220216606498195
allow blind 	 0.2
nodes for 	 0.14285714285714285
text databases 	 0.006289308176100629
analysis -LRB- 	 0.06153846153846154
in evaluation 	 0.0018726591760299626
LexRank The 	 0.08333333333333333
D.S. 1998 	 1.0
can '' 	 0.011049723756906077
by Su 	 0.005714285714285714
substitution of 	 1.0
a camera 	 0.001226993865030675
some major 	 0.012048192771084338
Confusable Words 	 1.0
computer programs 	 0.045454545454545456
geography , 	 1.0
Measuring progress 	 1.0
though such 	 0.1
cases -RRB- 	 0.05555555555555555
analysis of 	 0.18461538461538463
glossary or 	 0.5
Grace project 	 1.0
the implications 	 0.0006920415224913495
accepts some 	 0.5
system . 	 0.11827956989247312
Ochs , 	 1.0
create a 	 0.4117647058823529
, he 	 0.0011229646266142617
95 % 	 1.0
guesses ' 	 1.0
Goldberg developed 	 0.5
displaced by 	 1.0
remarkably similar 	 1.0
his company 	 0.08333333333333333
a highly 	 0.001226993865030675
movie reviews 	 0.3333333333333333
we rank 	 0.022222222222222223
the intermediary 	 0.0006920415224913495
been annotated 	 0.014705882352941176
make the 	 0.05
span several 	 1.0
it Contains 	 0.008547008547008548
it runs 	 0.008547008547008548
<s> But 	 0.004611837048424289
a logical 	 0.001226993865030675
discrete phonetic 	 0.3333333333333333
those human 	 0.045454545454545456
multiple source 	 0.07692307692307693
word of 	 0.016666666666666666
was most 	 0.012987012987012988
makes intuitive 	 0.125
Lichtenstein ? 	 1.0
The algorithm 	 0.005208333333333333
showed how 	 0.25
a real 	 0.00245398773006135
automatically answering 	 0.047619047619047616
and unexpected 	 0.001445086705202312
looks at 	 0.25
this graph 	 0.01098901098901099
Vulcan later 	 0.5
use Machine 	 0.013888888888888888
still used 	 0.06666666666666667
imagine the 	 1.0
ambiguous English 	 0.08333333333333333
final keyphrase 	 0.1111111111111111
main '' 	 0.125
nested one 	 1.0
E. Brill 	 0.25
tested the 	 0.5
semantic lexicons 	 0.047619047619047616
program would 	 0.045454545454545456
regions for 	 0.5
communication Pragmatics 	 0.2
more , 	 0.010526315789473684
or fade 	 0.0045045045045045045
Blind In 	 0.5
A Universal 	 0.02
cosine transform 	 0.3333333333333333
when the 	 0.11428571428571428
applied , 	 0.06666666666666667
whereas speed 	 0.3333333333333333
, How 	 0.0005614823133071309
of other 	 0.0035650623885918
now largely 	 0.07692307692307693
Puma helicopter 	 1.0
effective in 	 0.3333333333333333
FAA document 	 0.5
text based 	 0.006289308176100629
specific objects 	 0.047619047619047616
now rely 	 0.07692307692307693
so most 	 0.03333333333333333
importance . 	 0.16666666666666666
a separate 	 0.00245398773006135
of different 	 0.0017825311942959
the British 	 0.0006920415224913495
of Pennsylvania 	 0.00089126559714795
and compare 	 0.002890173410404624
some local 	 0.012048192771084338
perform an 	 0.09090909090909091
of databases 	 0.00089126559714795
been successfully 	 0.014705882352941176
Stubbs , 	 1.0
language system 	 0.006756756756756757
such corpora 	 0.016260162601626018
2009 to 	 0.3333333333333333
contrastive analysis 	 1.0
grammar rules 	 0.13513513513513514
sold his 	 0.3333333333333333
Automatically translate 	 1.0
mission to 	 1.0
find only 	 0.07692307692307693
depending what 	 0.25
A similar 	 0.02
script used 	 0.25
a problem 	 0.0036809815950920245
desired answers 	 0.2
of corpus 	 0.00089126559714795
attractive acoustic 	 0.3333333333333333
a scale 	 0.001226993865030675
generated by 	 0.06666666666666667
recursively defines 	 0.5
very simple 	 0.04878048780487805
e.g. Known 	 0.017857142857142856
the position 	 0.0006920415224913495
Another key 	 0.07692307692307693
word was 	 0.016666666666666666
<s> Initial 	 0.0007686395080707148
or some 	 0.013513513513513514
<s> Langues 	 0.0007686395080707148
Of particular 	 1.0
all natural 	 0.023255813953488372
exhibited good 	 1.0
`` Intelligent 	 0.005291005291005291
to contain 	 0.0013280212483399733
edited and 	 1.0
training ; 	 0.03571428571428571
program , 	 0.045454545454545456
Gripen cockpit 	 1.0
have to 	 0.019230769230769232
vary in 	 0.5
immunology , 	 1.0
those which 	 0.045454545454545456
of discrete 	 0.00089126559714795
also being 	 0.014492753623188406
, syllables 	 0.0005614823133071309
that within 	 0.0070921985815602835
are made 	 0.012448132780082987
source materials 	 0.041666666666666664
often work 	 0.022727272727272728
Natural language 	 0.6666666666666666
grammar . 	 0.10810810810810811
battle management 	 1.0
have focused 	 0.019230769230769232
products . 	 0.25
and strength 	 0.001445086705202312
own it 	 0.16666666666666666
that accuracy 	 0.0035460992907801418
History The 	 0.5
Yale University 	 0.5
Our brain 	 0.6666666666666666
machines , 	 0.25
been opinionated 	 0.014705882352941176
this step 	 0.01098901098901099
top of 	 0.2
limit to 	 0.25
necessarily portable 	 0.5
world knowledge 	 0.13333333333333333
walk to 	 0.2
remembering , 	 1.0
-LRB- IR 	 0.0027100271002710027
sentiment . 	 0.04
combine the 	 0.3333333333333333
Systems with 	 0.08333333333333333
reduction , 	 0.5
NLP comes 	 0.02127659574468085
Knowing this 	 1.0
are rarely 	 0.004149377593360996
fuse with 	 1.0
Harris , 	 0.1111111111111111
<s> Aggregation 	 0.0007686395080707148
to speech 	 0.00398406374501992
gestures in 	 0.5
its understanding 	 0.02857142857142857
while parsing 	 0.05
articulated theory 	 1.0
if word 	 0.03571428571428571
<s> Part-of-speech 	 0.0007686395080707148
and extrinsic 	 0.001445086705202312
The goal 	 0.005208333333333333
customers . 	 0.5
tasks has 	 0.03125
cryptanalyst at 	 1.0
references -RRB- 	 0.5
lines , 	 0.3333333333333333
when deployed 	 0.02857142857142857
those East 	 0.045454545454545456
and model 	 0.001445086705202312
how well 	 0.20689655172413793
-LRB- RAE 	 0.0027100271002710027
Both methods 	 0.3333333333333333
early text-to-speech 	 0.1
annotation or 	 0.25
phonemes with 	 0.16666666666666666
recognizing named 	 0.2
helicopter . 	 0.25
are also 	 0.03319502074688797
held each 	 1.0
generalized ATNs 	 1.0
light . 	 0.3333333333333333
Haton -LRB- 	 1.0
' -RRB- 	 0.05263157894736842
a class 	 0.001226993865030675
manipulate the 	 0.3333333333333333
the company 	 0.0006920415224913495
understanding system 	 0.030303030303030304
<s> Correct 	 0.0007686395080707148
A simplified 	 0.02
uses stochastic 	 0.07142857142857142
models Main 	 0.038461538461538464
In about 	 0.009523809523809525
Examples include 	 0.3333333333333333
and context 	 0.004335260115606936
word processors 	 0.016666666666666666
by one 	 0.005714285714285714
SemEval -RRB- 	 1.0
more words 	 0.010526315789473684
summary is 	 0.047619047619047616
1952 . 	 0.5
various constructions 	 0.05555555555555555
of n-dimensional 	 0.00089126559714795
and R. 	 0.001445086705202312
considerable interest 	 0.2
controller , 	 0.5
two summaries 	 0.034482758620689655
SCU in 	 1.0
reporting -LRB- 	 0.3333333333333333
typical sentence 	 0.1111111111111111
a second 	 0.0036809815950920245
begin with 	 0.6666666666666666
online opinion 	 0.125
naval resource 	 0.6666666666666666
the use 	 0.010380622837370242
lot and 	 0.3333333333333333
POS tags 	 0.15384615384615385
extracting answers 	 0.2
in statistical 	 0.003745318352059925
unsupervised summarization 	 0.25
between dynamically 	 0.02564102564102564
, Systran 	 0.0005614823133071309
ten-year-long research 	 1.0
being psychotherapy 	 0.05555555555555555
specific contexts 	 0.047619047619047616
different co-occurring 	 0.02040816326530612
represented using 	 0.16666666666666666
the statistics 	 0.0006920415224913495
techniques on 	 0.043478260869565216
Mouffe , 	 1.0
be filtered 	 0.012658227848101266
<s> Relationship 	 0.0007686395080707148
producing natural 	 0.3333333333333333
often has 	 0.045454545454545456
known keyphrases 	 0.15384615384615385
Claude Piron 	 1.0
simple rules 	 0.038461538461538464
to names 	 0.0013280212483399733
asking why 	 0.5
a central 	 0.001226993865030675
will seem 	 0.02857142857142857
five commands 	 0.2
into consideration 	 0.01282051282051282
even level 	 0.037037037037037035
, they 	 0.004491858506457047
e.g. feature 	 0.017857142857142856
resources it 	 0.16666666666666666
Their methods 	 0.5
Terry Winograd 	 1.0
model temporal 	 0.03333333333333333
steadily , 	 1.0
speech naturally 	 0.006578947368421052
listed on 	 1.0
1928 the 	 1.0
complicated because 	 0.3333333333333333
are three 	 0.004149377593360996
conveniently as 	 1.0
product . 	 0.14285714285714285
based only 	 0.018518518518518517
account context 	 0.3333333333333333
direct comparison 	 0.16666666666666666
the shift-reduce 	 0.0006920415224913495
quality of 	 0.5
one more 	 0.015384615384615385
unambiguous sentence-ending 	 0.5
below . 	 0.4
've just 	 0.5
meeting summarization 	 1.0
English this 	 0.02702702702702703
distribution ; 	 0.25
, picture 	 0.0005614823133071309
rule should 	 0.3333333333333333
as either 	 0.003484320557491289
precisely an 	 1.0
some fundamental 	 0.012048192771084338
stored more 	 1.0
and is 	 0.008670520231213872
offered the 	 1.0
investigation performed 	 1.0
Meehan , 	 1.0
answers , 	 0.08333333333333333
single word 	 0.07142857142857142
for personal 	 0.0036101083032490976
is produced 	 0.0040650406504065045
<verb> ← 	 1.0
analyzing a 	 0.2
rather can 	 0.0625
can help 	 0.0055248618784530384
characters using 	 0.0625
horoscope machines 	 1.0
1990 -RRB- 	 0.6666666666666666
ambiguous word 	 0.08333333333333333
possible transcriptions 	 0.08333333333333333
it word 	 0.008547008547008548
Other systems 	 0.14285714285714285
concepts between 	 0.2
<s> Unfortunately 	 0.0007686395080707148
identify . 	 0.08333333333333333
or using 	 0.009009009009009009
utterance can 	 0.3333333333333333
deep parsing 	 0.14285714285714285
or nonexistent 	 0.0045045045045045045
real-time character 	 0.5
match the 	 0.3333333333333333
languages have 	 0.04
um '' 	 1.0
`` happy 	 0.005291005291005291
document gives 	 0.027777777777777776
We also 	 0.14285714285714285
are limited 	 0.004149377593360996
the case 	 0.005536332179930796
<s> Introduction 	 0.0007686395080707148
networks are 	 0.07142857142857142
scaling system 	 1.0
<s> It 	 0.026133743274404306
the following 	 0.004844290657439446
difficult process 	 0.03571428571428571
Recognition '' 	 0.375
system also 	 0.010752688172043012
the photos 	 0.0006920415224913495
main verb 	 0.125
translation can 	 0.02702702702702703
or EHR 	 0.0045045045045045045
26 letters 	 1.0
the open-access 	 0.0006920415224913495
2004 , 	 0.3333333333333333
manage their 	 1.0
generation , 	 0.1111111111111111
quantitatively comparing 	 1.0
Fully Automated 	 1.0
to dozens 	 0.0013280212483399733
mention that 	 0.3333333333333333
the diagramming 	 0.001384083044982699
: Transfer-based 	 0.00980392156862745
`` Computational 	 0.005291005291005291
six numbers 	 0.5
intelligence and 	 0.125
known summaries 	 0.038461538461538464
recognizers have 	 0.5
the sentence 	 0.004152249134948097
some other 	 0.08433734939759036
Reading the 	 0.5
just as 	 0.2222222222222222
voice has 	 0.07692307692307693
gets about 	 0.5
perception that 	 0.5
the production 	 0.0006920415224913495
the learner 	 0.0006920415224913495
Some classifiers 	 0.047619047619047616
recognition research 	 0.008264462809917356
encouraged researchers 	 1.0
of case-based 	 0.00089126559714795
pick the 	 1.0
the reader 	 0.002768166089965398
easily as 	 0.1111111111111111
to do 	 0.00398406374501992
features would 	 0.038461538461538464
information . 	 0.08695652173913043
understanding evaluation 	 0.030303030303030304
programmed with 	 0.5
three basic 	 0.3333333333333333
Stemming Text 	 1.0
<s> Parsing 	 0.0030745580322828594
` local 	 0.0625
markers , 	 0.3333333333333333
but have 	 0.029411764705882353
the co-occurrence 	 0.0006920415224913495
and recognition 	 0.001445086705202312
QA system 	 0.23809523809523808
as an 	 0.04529616724738676
are now 	 0.016597510373443983
or Continuous 	 0.0045045045045045045
, semantically 	 0.0005614823133071309
from has 	 0.009615384615384616
expectancy of 	 1.0
computer to 	 0.045454545454545456
automata that 	 1.0
knowledge of 	 0.14814814814814814
the JAS-39 	 0.0006920415224913495
words of 	 0.027522935779816515
takes into 	 0.3333333333333333
Black-box vs. 	 0.5
Bayes , 	 0.3333333333333333
Described above 	 1.0
than in 	 0.022222222222222223
founder of 	 1.0
The performance 	 0.005208333333333333
Before a 	 0.5
the past 	 0.001384083044982699
<s> Components 	 0.0007686395080707148
the equipment 	 0.0006920415224913495
outputs of 	 1.0
stochastic matrix 	 0.125
`` Conversational 	 0.005291005291005291
About 47 	 0.5
mainland Scotland 	 1.0
: navigation 	 0.00980392156862745
vertices . 	 0.2222222222222222
language will 	 0.006756756756756757
sophisticated NLG 	 0.14285714285714285
-RRB- Around 	 0.0027100271002710027
newspaper . 	 0.3333333333333333
future developments 	 0.3333333333333333
for naval 	 0.0036101083032490976
1970 -LRB- 	 0.3333333333333333
Science Research 	 0.5
with Tom 	 0.00546448087431694
developed by 	 0.038461538461538464
is easier 	 0.0040650406504065045
walking more 	 0.3333333333333333
semantics or 	 0.14285714285714285
4-gram matching 	 1.0
Tipster project 	 1.0
-LRB- sailor 	 0.005420054200542005
, most 	 0.004491858506457047
is made 	 0.0040650406504065045
function of 	 0.125
CSR -RRB- 	 0.6666666666666666
Xerox , 	 0.5
linked with 	 0.3333333333333333
weapons release 	 1.0
processing : 	 0.018518518518518517
cross-lingual -RRB- 	 0.5
its vocabulary 	 0.02857142857142857
because analyzing 	 0.03333333333333333
: The 	 0.0392156862745098
, OnStar 	 0.0005614823133071309
linguists can 	 0.3333333333333333
related tasks 	 0.2
deal primarily 	 0.25
-LRB- SemEval 	 0.0027100271002710027
is in 	 0.006097560975609756
grammatical constituents 	 0.09090909090909091
attitude may 	 0.5
any topic 	 0.06451612903225806
of part-of-speech 	 0.0017825311942959
is expected 	 0.0020325203252032522
get some 	 0.14285714285714285
signal is 	 0.16666666666666666
1998 -RRB- 	 0.5
essential difference 	 1.0
`` Japanese 	 0.005291005291005291
is exactly 	 0.0020325203252032522
<s> Like 	 0.0007686395080707148
all quantitative 	 0.023255813953488372
urgent early 	 1.0
integer , 	 1.0
<s> Coreference 	 0.0007686395080707148
designers , 	 1.0
Invoice OCR 	 1.0
on finding 	 0.0047169811320754715
<s> Methods 	 0.0023059185242121443
discussed above 	 0.14285714285714285
working from 	 0.14285714285714285
comprehend Morse 	 1.0
keyphrases are 	 0.05714285714285714
testing . 	 0.2
ears , 	 1.0
real human 	 0.1111111111111111
updated textbook 	 1.0
think about 	 0.3333333333333333
paying attention 	 1.0
In 1935 	 0.009523809523809525
in solving 	 0.0018726591760299626
network to 	 0.16666666666666666
of domains 	 0.00089126559714795
key area 	 0.3333333333333333
into canned 	 0.01282051282051282
semantic parsing 	 0.047619047619047616
enumerate every 	 1.0
popular media 	 0.1111111111111111
later part-of-speech 	 0.1
lexicons with 	 0.5
we ultimately 	 0.022222222222222223
both time 	 0.03225806451612903
; and 	 0.0851063829787234
, Carnegie 	 0.0005614823133071309
language expression 	 0.006756756756756757
many years 	 0.019230769230769232
common strategy 	 0.04
MT companies 	 0.2
words relate 	 0.009174311926605505
available from 	 0.058823529411764705
Englund -LRB- 	 1.0
an instance 	 0.007575757575757576
<s> Sometimes 	 0.0007686395080707148
documents is 	 0.02631578947368421
The next 	 0.005208333333333333
affected by 	 1.0
requires expansion 	 0.0625
content -LRB- 	 0.08333333333333333
If a 	 0.1
German taggers 	 0.25
with C4 	 0.00546448087431694
production when 	 0.3333333333333333
if it 	 0.07142857142857142
MUC and 	 1.0
and negative 	 0.002890173410404624
English sentences 	 0.02702702702702703
with language 	 0.00546448087431694
into separate 	 0.02564102564102564
System -RRB- 	 1.0
subject , 	 0.25
the broadcast 	 0.0006920415224913495
happy . 	 1.0
a rich 	 0.0036809815950920245
to put 	 0.0026560424966799467
used more 	 0.008849557522123894
Finally , 	 1.0
reliably -- 	 1.0
systems take 	 0.008928571428571428
different way 	 0.02040816326530612
opening '' 	 1.0
image representing 	 0.3333333333333333
read text 	 0.14285714285714285
text may 	 0.006289308176100629
inflected languages 	 1.0
appears several 	 0.2
several ways 	 0.045454545454545456
Efficient algorithms 	 1.0
i.e. , 	 0.3684210526315789
answer may 	 0.03333333333333333
<s> NLG 	 0.0015372790161414297
of robustness 	 0.00089126559714795
's OCR 	 0.0196078431372549
Cognitive Systems 	 0.3333333333333333
, UMLS 	 0.0005614823133071309
actual data 	 0.2
NLG to 	 0.14285714285714285
Its main 	 0.5
Incorporating diversity 	 1.0
an OCR 	 0.007575757575757576
2,026,329 -RRB- 	 1.0
Romance languages 	 1.0
grammar -LRB- 	 0.02702702702702703
neutral . 	 0.5
affect OCR 	 0.3333333333333333
common components 	 0.04
Hence the 	 0.5
-LRB- roughly 	 0.005420054200542005
categories themselves 	 0.1111111111111111
apple the 	 0.3333333333333333
politics , 	 1.0
this problem 	 0.07692307692307693
, OCR 	 0.0011229646266142617
measurement is 	 0.5
document collections 	 0.027777777777777776
field that 	 0.07407407407407407
authors provide 	 0.2
environment as 	 0.16666666666666666
more readily 	 0.010526315789473684
understanding is 	 0.06060606060606061
pass . 	 1.0
preliminary approach 	 0.3333333333333333
is measured 	 0.006097560975609756
, -RRB- 	 0.0005614823133071309
this area 	 0.03296703296703297
to internal 	 0.0013280212483399733
Marilyn Monroe 	 1.0
development cost 	 0.08333333333333333
correlate best 	 0.3333333333333333
, Joseph 	 0.0005614823133071309
application . 	 0.07142857142857142
whole . 	 0.1111111111111111
One such 	 0.07692307692307693
template , 	 0.25
the techniques 	 0.0006920415224913495
communication studies 	 0.2
set to 	 0.02564102564102564
dissertation . 	 0.3333333333333333
perception of 	 0.5
Another important 	 0.07692307692307693
rare -- 	 0.25
posed by 	 0.3333333333333333
for measuring 	 0.0036101083032490976
language , 	 0.04054054054054054
weaknesses . 	 1.0
accuracy -RRB- 	 0.06451612903225806
recognition rates 	 0.01652892561983471
all where 	 0.023255813953488372
consistently achieve 	 0.3333333333333333
document level 	 0.027777777777777776
Given a 	 0.7142857142857143
automatic methodology 	 0.043478260869565216
Hopper , 	 1.0
what features 	 0.03125
Lancaster-Oslo-Bergen Corpus 	 1.0
will likely 	 0.05714285714285714
has focused 	 0.047619047619047616
used during 	 0.008849557522123894
is typically 	 0.006097560975609756
without a 	 0.07692307692307693
documents as 	 0.02631578947368421
length normalization 	 0.125
G. Lehnart 	 0.5
segmentation and 	 0.06060606060606061
as Turkish 	 0.003484320557491289
enhance accessibility 	 1.0
College at 	 0.5
more qualities 	 0.010526315789473684
that involves 	 0.0035460992907801418
phrase , 	 0.1
and labor 	 0.001445086705202312
controversial . 	 1.0
-RRB- automatically 	 0.0027100271002710027
at short 	 0.014705882352941176
recognition As 	 0.008264462809917356
the longest 	 0.0006920415224913495
often under 	 0.022727272727272728
model for 	 0.06666666666666667
printed by 	 0.08333333333333333
phrases to 	 0.0625
simple queries 	 0.038461538461538464
to locate 	 0.0013280212483399733
word to 	 0.016666666666666666
restricted vocabularies 	 0.25
not just 	 0.008928571428571428
coverage . 	 0.3333333333333333
of text 	 0.0213903743315508
cursive text 	 0.2
factory -RRB- 	 1.0
who applied 	 0.1
describe words 	 0.16666666666666666
-RRB- '' 	 0.005420054200542005
well-defined problem 	 1.0
a likelihood 	 0.001226993865030675
and RCA 	 0.001445086705202312
of Generation 	 0.00089126559714795
on machine-learning 	 0.0047169811320754715
automatic is 	 0.043478260869565216
output a 	 0.07692307692307693
sentences -RRB- 	 0.02631578947368421
what day 	 0.03125
some improvement 	 0.012048192771084338
the time 	 0.004152249134948097
that identify 	 0.0035460992907801418
De Guzman 	 1.0
reporting on 	 0.3333333333333333
inventor Jacob 	 1.0
into standard 	 0.01282051282051282
hear this 	 0.5
learns a 	 1.0
texts and 	 0.058823529411764705
which we 	 0.007246376811594203
Statistical NLP 	 0.2222222222222222
converted it 	 0.3333333333333333
the Unix 	 0.001384083044982699
is rarely 	 0.0020325203252032522
Document reader 	 0.25
queries to 	 0.3333333333333333
Future of 	 0.5
would expect 	 0.018867924528301886
people or 	 0.0625
divided up 	 0.3333333333333333
was demonstrated 	 0.012987012987012988
Vocalizations vary 	 1.0
, quite 	 0.0005614823133071309
bottom-up parsers 	 1.0
wave is 	 0.2222222222222222
and frequency 	 0.001445086705202312
humans deemed 	 0.08333333333333333
processed , 	 0.16666666666666666
<s> vs. 	 0.0015372790161414297
^ , 	 0.6666666666666666
during the 	 0.4
has used 	 0.011904761904761904
last year 	 0.2
various genres 	 0.05555555555555555
at by 	 0.014705882352941176
modern statistical 	 0.2
speaker adaptation 	 0.1111111111111111
NASA 's 	 1.0
J. Phillips 	 0.3333333333333333
-- computer 	 0.04
is non-trivial 	 0.0020325203252032522
the representation 	 0.0006920415224913495
<s> Aided 	 0.0007686395080707148
correctly . 	 1.0
by larger 	 0.005714285714285714
D , 	 1.0
because longer 	 0.03333333333333333
short summary 	 0.125
about following 	 0.025
: Neural 	 0.00980392156862745
what information 	 0.03125
coherence and 	 0.6666666666666666
language modeling 	 0.006756756756756757
a greater 	 0.001226993865030675
, more 	 0.0022459292532285235
Amount line 	 1.0
themselves . 	 0.25
aspects of 	 0.8571428571428571
of processing 	 0.00089126559714795
and potential 	 0.001445086705202312
upper level 	 1.0
based engine 	 0.018518518518518517
overcome this 	 0.5
Yale which 	 0.5
two extremes 	 0.034482758620689655
Translations are 	 1.0
job , 	 0.5
the entity 	 0.0006920415224913495
such accuracy 	 0.008130081300813009
Foucault , 	 0.3333333333333333
Weizenbaum between 	 0.3333333333333333
Mandarin and 	 1.0
was not 	 0.025974025974025976
was LILOG 	 0.012987012987012988
performance improvements 	 0.05555555555555555
, generating 	 0.0005614823133071309
include Chinese 	 0.037037037037037035
whether a 	 0.15384615384615385
are BASEBALL 	 0.004149377593360996
also possible 	 0.043478260869565216
approaches to 	 0.17857142857142858
person does 	 0.05263157894736842
ōrātiōnis -RRB- 	 1.0
James A. 	 0.5
community , 	 1.0
involves both 	 0.1
also classify 	 0.014492753623188406
general , 	 0.2727272727272727
final language 	 0.1111111111111111
some form 	 0.04819277108433735
several phases 	 0.045454545454545456
`` learning 	 0.021164021164021163
Solving this 	 0.5
the lexicon 	 0.0006920415224913495
probabilities , 	 0.09090909090909091
ellipsis , 	 1.0
learning '' 	 0.11627906976744186
Blind -LRB- 	 0.5
door of 	 0.25
textual representation 	 0.2
A somewhat 	 0.02
how useful 	 0.034482758620689655
the top 	 0.002768166089965398
devised to 	 0.5
typically around 	 0.05555555555555555
'' aimed 	 0.005154639175257732
foreign '' 	 0.5
technology has 	 0.045454545454545456
second -RRB- 	 0.1
naturally occurring 	 0.5
HMMs underlie 	 0.125
fastens -LRB- 	 1.0
models of 	 0.038461538461538464
deterministic decisions 	 0.25
ones in 	 0.1
professional translator 	 1.0
ROUGE-1 values 	 0.2
will `` 	 0.05714285714285714
a technique 	 0.001226993865030675
process to 	 0.1111111111111111
some assertive 	 0.012048192771084338
of texts 	 0.0017825311942959
data which 	 0.012987012987012988
commercial perspective 	 0.09090909090909091
Genres of 	 1.0
input that 	 0.04878048780487805
window of 	 1.0
: Creating 	 0.0196078431372549
relevant text 	 0.14285714285714285
would contribute 	 0.018867924528301886
certain cases 	 0.14285714285714285
the 1990s 	 0.001384083044982699
, Pang 	 0.0005614823133071309
to get 	 0.005312084993359893
games , 	 1.0
to test 	 0.0026560424966799467
stationary process 	 0.14285714285714285
is obtained 	 0.0020325203252032522
the success 	 0.001384083044982699
<s> Precision 	 0.0007686395080707148
states that 	 0.25
: David 	 0.00980392156862745
The Unicode 	 0.005208333333333333
linear-time versions 	 1.0
by finding 	 0.005714285714285714
highlighting candidate 	 1.0
making the 	 0.2857142857142857
understanding of 	 0.15151515151515152
pursued after 	 1.0
English-like command 	 0.3333333333333333
of newspaper 	 0.00089126559714795
implemented , 	 0.2
1965 it 	 0.25
techniques could 	 0.043478260869565216
choice of 	 0.25
performance of 	 0.1111111111111111
parliament and 	 1.0
Sound Graph 	 0.3333333333333333
translation and 	 0.04054054054054054
MAHT and 	 1.0
of HMM-based 	 0.00089126559714795
decisions -- 	 0.1
automatic procedures 	 0.043478260869565216
human -RRB- 	 0.043478260869565216
complete sentences 	 1.0
likely following 	 0.0625
: This 	 0.0392156862745098
`` still 	 0.005291005291005291
left recursive 	 0.16666666666666666
scores significantly 	 0.2
tagging program 	 0.04
and graphics 	 0.001445086705202312
oriented systems 	 1.0
process Main 	 0.027777777777777776
reader to 	 0.1
successful systems 	 0.1111111111111111
base , 	 0.5
our knowledge 	 0.2
year despite 	 0.16666666666666666
readable summary 	 0.3333333333333333
give predicted 	 0.25
as 10 	 0.003484320557491289
-- or 	 0.04
went past 	 0.2
the coherence 	 0.0006920415224913495
to ensure 	 0.0013280212483399733
keyphrases your 	 0.02857142857142857
conversations . 	 0.3333333333333333
of meaningful 	 0.00089126559714795
and 1980s 	 0.002890173410404624
lessening of 	 1.0
summaries using 	 0.023255813953488372
Xuedong Huang 	 1.0
4 star 	 0.2
requires a 	 0.0625
a part-of-speech 	 0.001226993865030675
G. , 	 0.5
that represents 	 0.0035460992907801418
automated target 	 0.14285714285714285
early market 	 0.1
done with 	 0.18181818181818182
results for 	 0.047619047619047616
-RRB- Bhatia 	 0.0027100271002710027
of Business-card 	 0.00089126559714795
edges build 	 0.14285714285714285
neural-network output 	 1.0
It approaches 	 0.02631578947368421
to construct 	 0.0013280212483399733
degree . 	 0.16666666666666666
portions . 	 1.0
et al. 	 1.0
stems from 	 0.5
To find 	 0.1111111111111111
E. Longacre 	 0.5
Engineers '' 	 0.5
including linguistics 	 0.14285714285714285
adjective ; 	 0.14285714285714285
-LRB- ōrātiōnis 	 0.0027100271002710027
<s> Text 	 0.0015372790161414297
and converted 	 0.001445086705202312
to correlate 	 0.0013280212483399733
the emergence 	 0.0006920415224913495
the summarization 	 0.0020761245674740486
component sentences 	 0.2
trivial due 	 0.25
some invalid 	 0.012048192771084338
General Post 	 1.0
the Senseval 	 0.0006920415224913495
produce keyphrases 	 0.045454545454545456
the question 	 0.011072664359861591
constructions occur 	 1.0
characters from 	 0.0625
a piecewise 	 0.001226993865030675
the vocabulary 	 0.0006920415224913495
It 's 	 0.05263157894736842
answer -LRB- 	 0.03333333333333333
such systems 	 0.008130081300813009
are very 	 0.016597510373443983
declaration of 	 1.0
unwanted constructs 	 1.0
tools deploy 	 0.16666666666666666
negative sentiment 	 0.125
thus has 	 0.1
one way 	 0.015384615384615385
the average 	 0.0006920415224913495
Japanese camp 	 0.125
Audio Processing 	 0.5
C4 .5 	 1.0
Howarth , 	 1.0
development , 	 0.16666666666666666
a sublanguage 	 0.001226993865030675
the character 	 0.0006920415224913495
an attempt 	 0.022727272727272728
word segmentation 	 0.05
Standard Oil 	 0.5
or names 	 0.0045045045045045045
français . 	 1.0
and applications 	 0.001445086705202312
more corpus 	 0.010526315789473684
hand-printed characters 	 0.25
II -LRB- 	 0.5
heritage -LRB- 	 1.0
be known 	 0.004219409282700422
tasks . 	 0.125
often difficult 	 0.022727272727272728
vagueness of 	 1.0
children , 	 0.5
broad and 	 0.25
trigrams without 	 0.5
other possible 	 0.014285714285714285
will never 	 0.02857142857142857
to government 	 0.0013280212483399733
to suggest 	 0.0013280212483399733
these devices 	 0.023809523809523808
Home automation 	 1.0
the morphology 	 0.0006920415224913495
document . 	 0.1388888888888889
boundary information 	 0.16666666666666666
scans paper 	 1.0
multi-way scale 	 1.0
company for 	 0.3333333333333333
the earliest-used 	 0.001384083044982699
were printed 	 0.024390243902439025
many are 	 0.019230769230769232
sort of 	 0.6666666666666666
both algorithms 	 0.03225806451612903
as latent 	 0.003484320557491289
analysis -- 	 0.015384615384615385
, HMM-based 	 0.0005614823133071309
goal -RRB- 	 0.14285714285714285
are pulled 	 0.004149377593360996
so for 	 0.03333333333333333
U.S. Patent 	 0.42857142857142855
human translation 	 0.043478260869565216
the publication 	 0.001384083044982699
be fully 	 0.008438818565400843
of one 	 0.0035650623885918
learned model 	 0.2
tries to 	 1.0
by Piron 	 0.005714285714285714
documents where 	 0.02631578947368421
17 ambiguous 	 1.0
phenomenon is 	 0.2
transform , 	 0.4
more software 	 0.010526315789473684
Separate a 	 0.5
<s> Informally 	 0.0007686395080707148
involved fully 	 0.3333333333333333
to fine 	 0.0013280212483399733
, within 	 0.0005614823133071309
are claiming 	 0.004149377593360996
which give 	 0.007246376811594203
work by 	 0.08333333333333333
Defense Advanced 	 1.0
Human Summarization 	 0.2
want not 	 0.16666666666666666
also terminate 	 0.014492753623188406
use directly 	 0.013888888888888888
Michael Halliday 	 0.25
cursive characters 	 0.2
on post-processing 	 0.0047169811320754715
= accusative 	 0.1111111111111111
allow for 	 0.2
of ability 	 0.00089126559714795
typically involved 	 0.05555555555555555
an objective 	 0.007575757575757576
languages , 	 0.22
summaries with 	 0.046511627906976744
<s> Neural 	 0.0015372790161414297
, content 	 0.0005614823133071309
investigates the 	 1.0
real-world knowledge 	 0.16666666666666666
useful work 	 0.07142857142857142
a writer 	 0.001226993865030675
the areas 	 0.001384083044982699
reasoned views 	 1.0
the CCD 	 0.0006920415224913495
<s> Back-End 	 0.0007686395080707148
multiple part-of-speech 	 0.07692307692307693
read not 	 0.14285714285714285
Research Corporation 	 0.125
-LRB- based 	 0.005420054200542005
most sentiment 	 0.017241379310344827
compiler , 	 0.3333333333333333
classification decisions 	 0.058823529411764705
unit , 	 0.3333333333333333
of similarity 	 0.00089126559714795
serve other 	 0.2
VOLSUNGA . 	 1.0
basics and 	 1.0
than T 	 0.022222222222222223
produce both 	 0.045454545454545456
the NLP 	 0.0006920415224913495
approach allows 	 0.02857142857142857
the conversational 	 0.0006920415224913495
becoming more 	 1.0
reducing the 	 0.5
have many 	 0.04807692307692308
right kind 	 0.1
model mechanisms 	 0.03333333333333333
what extent 	 0.03125
one . 	 0.03076923076923077
are focused 	 0.004149377593360996
deals with 	 1.0
finds many 	 1.0
be retrained 	 0.004219409282700422
as one 	 0.006968641114982578
tokens , 	 0.14285714285714285
, dynamic 	 0.0005614823133071309
knowledge sources 	 0.037037037037037035
citation needed 	 1.0
, selecting 	 0.0005614823133071309
portable to 	 0.3333333333333333
recent years 	 0.5
character , 	 0.09090909090909091
Statistical techniques 	 0.1111111111111111
strongly than 	 0.5
in rank 	 0.0018726591760299626
just robustness 	 0.1111111111111111
commercial efforts 	 0.18181818181818182
While some 	 0.2
A very 	 0.02
custom speech 	 0.5
which they 	 0.014492753623188406
Weizenbaum at 	 0.3333333333333333
world applications 	 0.06666666666666667
text-to-speech technology 	 0.25
the concept 	 0.001384083044982699
derived meaning 	 0.16666666666666666
ultraviolet light 	 1.0
are free 	 0.004149377593360996
'' tagging 	 0.005154639175257732
; Speech 	 0.02127659574468085
small incremental 	 0.1111111111111111
War II 	 1.0
important part 	 0.0625
as multi-document 	 0.003484320557491289
which used 	 0.007246376811594203
Realtime Speech 	 1.0
automatically focus 	 0.047619047619047616
by an 	 0.011428571428571429
possibilities , 	 0.2
approaches Supervised 	 0.03571428571428571
recognition-related project 	 1.0
debated much 	 1.0
rules ATNs 	 0.023255813953488372
, typical 	 0.0005614823133071309
pre-existing keyphrases 	 0.5
, Google 	 0.0005614823133071309
Japanese , 	 0.125
Category : 	 0.5
characters rather 	 0.0625
Models . 	 0.3333333333333333
NLP with 	 0.02127659574468085
the Sparkle 	 0.0006920415224913495
grammar that 	 0.02702702702702703
schematic organization 	 1.0
; but 	 0.0425531914893617
important example 	 0.0625
both individual 	 0.03225806451612903
polarity on 	 0.125
holder of 	 1.0
binary classification 	 0.5
and techniques 	 0.001445086705202312
HMM based 	 0.3333333333333333
of efforts 	 0.00089126559714795
Yehoshua Bar-Hillel 	 1.0
articles in 	 0.25
interface commercially 	 0.25
funding has 	 0.125
language being 	 0.013513513513513514
only by 	 0.05263157894736842
translations , 	 0.5
name of 	 0.2
is intended 	 0.0040650406504065045
an automated 	 0.007575757575757576
a relative 	 0.001226993865030675
see wide 	 0.05
concept , 	 0.25
basic sub-signals 	 0.07692307692307693
operated on 	 0.5
Corpus Research 	 0.0625
`` tag 	 0.010582010582010581
Systran , 	 1.0
speaker or 	 0.05555555555555555
solution can 	 1.0
instance of 	 0.14285714285714285
, developed 	 0.0005614823133071309
under construction 	 0.2
extraction and 	 0.06451612903225806
on understanding 	 0.0047169811320754715
common feature 	 0.04
machine-learning systems 	 0.25
is , 	 0.018292682926829267
the pilot 	 0.0006920415224913495
or component 	 0.0045045045045045045
separated out 	 0.3333333333333333
in recognizing 	 0.003745318352059925
different speaker 	 0.02040816326530612
specialised expertise 	 0.5
, notably 	 0.0005614823133071309
demonstrate . 	 1.0
from systems 	 0.009615384615384616
<s> Edges 	 0.0015372790161414297
as possible 	 0.017421602787456445
tagging system 	 0.04
well it 	 0.07142857142857142
consideration of 	 0.3333333333333333
have different 	 0.019230769230769232
or insufficient 	 0.0045045045045045045
not using 	 0.008928571428571428
As an 	 0.1111111111111111
, improving 	 0.0005614823133071309
to understand 	 0.00398406374501992
capitalization can 	 0.3333333333333333
would then 	 0.018867924528301886
parser attempts 	 0.0625
with part-of-speech 	 0.00546448087431694
by giving 	 0.005714285714285714
Faber , 	 1.0
clear imaging 	 0.25
keyphrases , 	 0.05714285714285714
theories of 	 0.6
Acoustical signals 	 0.5
needs additional 	 0.1
central `` 	 0.3333333333333333
clarification . 	 0.3333333333333333
coefficients and 	 0.25
Automatic learning 	 0.1111111111111111
following `` 	 0.06666666666666667
, typewritten 	 0.0011229646266142617
now named 	 0.15384615384615385
of rules 	 0.00267379679144385
reputations . 	 1.0
and checked 	 0.001445086705202312
and which 	 0.001445086705202312
text analytics 	 0.006289308176100629
text on 	 0.006289308176100629
approaches differ 	 0.03571428571428571
room acoustics 	 1.0
some machine 	 0.012048192771084338
four different 	 0.2857142857142857
database available 	 0.1
, trigram 	 0.0011229646266142617
written-out number 	 1.0
character recognition 	 0.5
unlabeled data 	 1.0
both its 	 0.03225806451612903
why applying 	 0.14285714285714285
controllers Training 	 0.3333333333333333
`` angry 	 0.005291005291005291
how big 	 0.034482758620689655
or Latin 	 0.0045045045045045045
involve working 	 0.16666666666666666
languages in 	 0.02
language understanding 	 0.0945945945945946
to rescore 	 0.0013280212483399733
, sets 	 0.0005614823133071309
documents have 	 0.02631578947368421
was followed 	 0.012987012987012988
far too 	 0.125
Comparing these 	 1.0
To perform 	 0.1111111111111111
for continued 	 0.0036101083032490976
the more 	 0.0034602076124567475
edges with 	 0.14285714285714285
, isolated 	 0.0005614823133071309
LDA-based projection 	 1.0
Evaluation As 	 0.1111111111111111
generally evaluated 	 0.09090909090909091
meet President 	 0.25
sounds . 	 0.06666666666666667
chosen is 	 0.2
evaluated , 	 0.14285714285714285
Johnstone , 	 1.0
successful NLP 	 0.1111111111111111
likely uttered 	 0.0625
easy-to-use syntax 	 1.0
groups , 	 0.2
or 45 	 0.0045045045045045045
seconds , 	 1.0
the label 	 0.0006920415224913495
Svenka Savic 	 1.0
classifier for 	 0.14285714285714285
the different 	 0.0006920415224913495
EUROPARL , 	 1.0
sequences that 	 0.1111111111111111
, PangeaMT 	 0.0005614823133071309
and QA 	 0.001445086705202312
then include 	 0.02857142857142857
case , 	 0.17647058823529413
unsupervised and 	 0.125
Corpus of 	 0.0625
in our 	 0.0018726591760299626
understanding systems 	 0.030303030303030304
Open source 	 1.0
assumptions on 	 0.2
article accuracy 	 0.034482758620689655
corpus denote 	 0.03225806451612903
characterized by 	 0.5
entities in 	 0.14285714285714285
unrealistically high 	 1.0
other places 	 0.014285714285714285
for summarization 	 0.0036101083032490976
current text 	 0.14285714285714285
criteria is 	 0.25
<s> Semantic 	 0.0007686395080707148
; Michel 	 0.02127659574468085
, incomplete 	 0.0005614823133071309
Xerox eventually 	 0.5
although not 	 0.16666666666666666
knowledge frequently 	 0.037037037037037035
the finite 	 0.0006920415224913495
even where 	 0.037037037037037035
21 taggers 	 1.0
sounds -LRB- 	 0.13333333333333333
technology also 	 0.045454545454545456
were repeatedly 	 0.024390243902439025
output simply 	 0.038461538461538464
hand -RRB- 	 0.07142857142857142
Eagles Guidelines 	 1.0
, René 	 0.0005614823133071309
and slang 	 0.001445086705202312
interaction , 	 0.125
; Amplitude 	 0.02127659574468085
speech can 	 0.013157894736842105
specific letter 	 0.047619047619047616
common cases 	 0.04
and shallowest 	 0.001445086705202312
in Japanese 	 0.0018726591760299626
methods . 	 0.022727272727272728
World Wide 	 0.5714285714285714
F. , 	 1.0
names , 	 0.2857142857142857
criterion of 	 0.5
power , 	 0.25
submit their 	 0.5
spaces or 	 0.2
corpora specifically 	 0.09090909090909091
division rather 	 0.5
we do 	 0.022222222222222223
overt morphological 	 1.0
slide represent 	 1.0
example , 	 0.6666666666666666
, of 	 0.0022459292532285235
exploration , 	 1.0
single binary 	 0.07142857142857142
ontologies . 	 0.5
feeling that 	 1.0
see Tablet 	 0.05
automatically tuned 	 0.047619047619047616
, abstraction 	 0.0005614823133071309
and so 	 0.008670520231213872
a process 	 0.0049079754601227
instance when 	 0.07142857142857142
on which 	 0.014150943396226415
about NLP 	 0.025
Speaker Dependence 	 0.16666666666666666
of sequential 	 0.00089126559714795
word in 	 0.06666666666666667
informativeness . 	 0.3333333333333333
parsers is 	 0.07692307692307693
research groups 	 0.023809523809523808
of immunology 	 0.00089126559714795
indicate a 	 0.3333333333333333
Speaker Recognition 	 0.16666666666666666
a consumer 	 0.001226993865030675
not 100 	 0.008928571428571428
has never 	 0.023809523809523808
For nouns 	 0.01639344262295082
data where 	 0.012987012987012988
the ten-year-long 	 0.0006920415224913495
Apart from 	 1.0
algorithm As 	 0.03571428571428571
mainly on 	 0.16666666666666666
= singular 	 0.1111111111111111
grammars that 	 0.07142857142857142
top ranking 	 0.2
conceptual ontologies 	 0.5
have helped 	 0.009615384615384616
early AI 	 0.2
not require 	 0.008928571428571428
Deborah Schiffrin 	 0.5
on programer 	 0.0047169811320754715
to enable 	 0.0013280212483399733
Cleave and 	 1.0
John McCarthy 	 0.125
coverage of 	 0.3333333333333333
fact ambiguous 	 0.09090909090909091
that do 	 0.0035460992907801418
open , 	 0.25
their stationary 	 0.029411764705882353
the years 	 0.0006920415224913495
sense , 	 0.125
reproducing formatted 	 1.0
analyzing it 	 0.2
1989 ? 	 0.5
interact with 	 1.0
domain tends 	 0.05
is required 	 0.0040650406504065045
of theories 	 0.00089126559714795
comprising multiple 	 0.5
program a 	 0.09090909090909091
and differing 	 0.001445086705202312
in ASR 	 0.003745318352059925
information -LRB- 	 0.021739130434782608
put the 	 0.25
is focused 	 0.0020325203252032522
features might 	 0.038461538461538464
contextual or 	 0.5
especially statistical 	 0.06666666666666667
are under 	 0.004149377593360996
of shared 	 0.00089126559714795
each sentence 	 0.022222222222222223
European group 	 0.3333333333333333
was hand-written 	 0.012987012987012988
were surprisingly 	 0.024390243902439025
upon which 	 1.0
automate the 	 0.3333333333333333
one feasibility 	 0.015384615384615385
-LRB- Adda 	 0.0027100271002710027
characters themselves 	 0.0625
ambiguity . 	 0.125
are put 	 0.004149377593360996
threshold is 	 0.25
output that 	 0.07692307692307693
pilot workload 	 0.2
a period 	 0.00245398773006135
is provided 	 0.0020325203252032522
particularly effective 	 0.2
the ambiguity 	 0.0006920415224913495
be referenced 	 0.004219409282700422
Institute -LRB- 	 1.0
been created 	 0.029411764705882353
Standard Annex 	 0.5
or quantities 	 0.0045045045045045045
<s> Training 	 0.0007686395080707148
continued to 	 0.3333333333333333
<s> Interactive 	 0.0007686395080707148
contain enough 	 0.08333333333333333
rules to 	 0.06976744186046512
be . 	 0.004219409282700422
some kind 	 0.04819277108433735
having considerable 	 0.2
rate . 	 0.09090909090909091
marking up 	 0.5
rule-based methods 	 0.14285714285714285
descriptive tags 	 0.3333333333333333
automatic metric 	 0.043478260869565216
formal or 	 0.1111111111111111
internal semantic 	 0.2
expensive since 	 0.14285714285714285
collection -RRB- 	 0.2
has received 	 0.011904761904761904
cell phone 	 1.0
run each 	 0.2
like relevance 	 0.03571428571428571
example above 	 0.012345679012345678
no means 	 0.07692307692307693
In 1993 	 0.009523809523809525
associating a 	 1.0
in-principle obstacles 	 1.0
assumptions about 	 0.2
R. Schroeder 	 0.16666666666666666
by Turney 	 0.005714285714285714
as references 	 0.003484320557491289
enough data 	 0.4
incorrect letters 	 0.3333333333333333
-LRB- ParaEval 	 0.0027100271002710027
English-French record 	 1.0
most important 	 0.034482758620689655
that larger 	 0.0035460992907801418
approximately divided 	 0.5
by air 	 0.011428571428571429
or Auto 	 0.0045045045045045045
full sentenced 	 0.2
on broad 	 0.0047169811320754715
and space 	 0.001445086705202312
entertaining last 	 0.5
merges highly 	 1.0
Most of 	 0.5
<s> Then 	 0.003843197540353574
of natural 	 0.0196078431372549
a lexicon 	 0.00245398773006135
the space 	 0.0020761245674740486
yesterday and 	 0.3333333333333333
at Stanford 	 0.014705882352941176
entity , 	 0.4
Network classifies 	 1.0
possible analyses 	 0.08333333333333333
PageRank . 	 0.16666666666666666
to bridge 	 0.0013280212483399733
Greek and 	 0.3333333333333333
challenges . 	 0.5
chain random 	 1.0
Aviation Authorities 	 1.0
occurrence , 	 0.5
path sentences 	 0.5
loss function 	 1.0
Attribute grammars 	 1.0
performance and 	 0.1111111111111111
be checked 	 0.004219409282700422
Dr. Kenneth 	 1.0
incorporate logical 	 1.0
results suggest 	 0.047619047619047616
arbitrary piece 	 0.3333333333333333
unseen data 	 1.0
Wayne Ratliff 	 1.0
to seize 	 0.0013280212483399733
is contrast 	 0.0020325203252032522
used in 	 0.20353982300884957
domains ASR 	 0.125
Widdowson , 	 1.0
Sociologist Harold 	 1.0
an array 	 0.007575757575757576
interpreted as 	 1.0
Brill tagger 	 0.3333333333333333
text , 	 0.18867924528301888
Larry R. 	 0.5
not wear 	 0.008928571428571428
`` in 	 0.010582010582010581
recognition using 	 0.008264462809917356
Apparatus for 	 1.0
a cell 	 0.00245398773006135
parsers for 	 0.15384615384615385
they relate 	 0.025
computer OCR 	 0.022727272727272728
into words 	 0.038461538461538464
a user-specified 	 0.001226993865030675
a patent 	 0.00245398773006135
Italian ; 	 0.5
analysis . 	 0.09230769230769231
generates a 	 0.6666666666666666
Zellig Harris 	 1.0
differences themselves 	 0.3333333333333333
tagging techniques 	 0.04
to new 	 0.005312084993359893
Recall-Oriented Understudy 	 1.0
for Speech 	 0.0036101083032490976
analysis can 	 0.03076923076923077
which do 	 0.007246376811594203
is like 	 0.0040650406504065045
create summaries 	 0.058823529411764705
achieved by 	 0.2
DARPA -RRB- 	 0.25
Interactive voice 	 0.5
consider the 	 0.5
equivalent questions 	 0.2
a stream 	 0.001226993865030675
Hendrix formed 	 1.0
, social 	 0.0011229646266142617
special image 	 0.2
also been 	 0.057971014492753624
water . 	 1.0
<s> Shared 	 0.0007686395080707148
Constraints e.g. 	 0.3333333333333333
jokes -LRB- 	 1.0
article quoting 	 0.034482758620689655
make soft 	 0.2
it vibrates 	 0.008547008547008548
and Martin 	 0.001445086705202312
More sophisticated 	 0.3333333333333333
Peter Turney 	 1.0
information theory 	 0.021739130434782608
always the 	 0.3333333333333333
which focused 	 0.007246376811594203
they still 	 0.025
, have 	 0.0011229646266142617
ROUGE-1 is 	 0.2
these words 	 0.047619047619047616
the impact 	 0.001384083044982699
They can 	 0.3333333333333333
corpus - 	 0.03225806451612903
an answer 	 0.015151515151515152
evaluating summaries 	 0.4
: complicated 	 0.00980392156862745
is broad 	 0.0020325203252032522
of keyphrases 	 0.00267379679144385
R -RRB- 	 1.0
Subsequently a 	 1.0
defining programming 	 1.0
the characters 	 0.0006920415224913495
and memory 	 0.001445086705202312
the corresponding 	 0.001384083044982699
150 examples 	 0.5
more interest 	 0.010526315789473684
the orthography 	 0.0006920415224913495
in each 	 0.0018726591760299626
-LRB- discourse 	 0.0027100271002710027
task usually 	 0.023809523809523808
of concern 	 0.00089126559714795
for editing 	 0.0036101083032490976
keyword matching 	 1.0
to accomplish 	 0.0013280212483399733
sequential lines 	 1.0
invention of 	 1.0
topics . 	 0.14285714285714285
but also 	 0.07352941176470588
high rank 	 0.05555555555555555
understanding approximates 	 0.030303030303030304
<s> Ensemble 	 0.0007686395080707148
this method 	 0.01098901098901099
metrics like 	 0.1111111111111111
universal language 	 0.3333333333333333
fluency to 	 1.0
, verb 	 0.0011229646266142617
conversation with 	 0.5
way is 	 0.041666666666666664
as focusing 	 0.003484320557491289
academic research 	 1.0
the bridging 	 0.0006920415224913495
usually creating 	 0.03125
large quantity 	 0.043478260869565216
was Pollen 	 0.012987012987012988
during a 	 0.2
throughout a 	 1.0
copied despite 	 0.5
GRACE d'évaluation 	 1.0
the talk 	 0.0006920415224913495
, machine-aided 	 0.0005614823133071309
disabilities that 	 0.25
broadband technologies 	 1.0
Conference Technolangue\/Easy 	 0.5
OCR software 	 0.08163265306122448
can perform 	 0.0055248618784530384
surfer model 	 1.0
Wodak , 	 1.0
sentiment classification 	 0.04
of emails 	 0.00089126559714795
find answers 	 0.07692307692307693
special ink 	 0.2
basic OCR 	 0.07692307692307693
parts of 	 1.0
were called 	 0.024390243902439025
automatically evaluating 	 0.047619047619047616
Despite the 	 1.0
topics discussed 	 0.14285714285714285
Reading '' 	 0.5
be keyphrases 	 0.004219409282700422
1979 -RRB- 	 1.0
systems have 	 0.044642857142857144
vs. extrinsic 	 0.08333333333333333
as part 	 0.006968641114982578
values to 	 0.125
sentence breaking 	 0.020833333333333332
Conversational analysis 	 1.0
writing , 	 0.2222222222222222
answering . 	 0.16666666666666666
this paper 	 0.01098901098901099
The recognition 	 0.005208333333333333
of linguistic 	 0.0017825311942959
multiple subtasks 	 0.07692307692307693
words emerge 	 0.009174311926605505
a bill 	 0.001226993865030675
decide when 	 0.25
others more 	 0.08333333333333333
every combination 	 0.3333333333333333
array . 	 1.0
undertook recognition 	 1.0
they superimpose 	 0.025
segmentation in 	 0.030303030303030304
sufficiently well 	 1.0
abstraction can 	 0.25
on Hidden 	 0.0047169811320754715
be maintained 	 0.004219409282700422
More complex 	 0.1111111111111111
has gone 	 0.011904761904761904
sentiment strength 	 0.04
Whether a 	 0.5
range . 	 0.14285714285714285
protect New 	 1.0
for most 	 0.007220216606498195
quite distinct 	 0.125
ACL , 	 0.5
we need 	 0.13333333333333333
a product 	 0.001226993865030675
separate field 	 0.1
term applies 	 0.1111111111111111
rise to 	 0.5
assigning the 	 1.0
example generic 	 0.012345679012345678
include a 	 0.1111111111111111
different groups 	 0.02040816326530612
of original 	 0.00089126559714795
dismiss the 	 1.0
, once 	 0.0005614823133071309
his visit 	 0.08333333333333333
apply increasingly 	 0.2
grammars alone 	 0.07142857142857142
parameters , 	 0.25
Harris The 	 0.1111111111111111
genre . 	 0.5
lexicon with 	 0.1111111111111111
correctly-developed summaries 	 1.0
and Zacharov 	 0.001445086705202312
sophisticated algorithms 	 0.2857142857142857
customer service 	 1.0
-LRB- F-16 	 0.0027100271002710027
personalised business 	 1.0
speech-recognition machine 	 0.3333333333333333
in several 	 0.003745318352059925
will contain 	 0.02857142857142857
Mars Polar 	 0.5
after 30 	 0.08333333333333333
some labeled 	 0.012048192771084338
LexisNexis was 	 1.0
symbol in 	 0.25
useful only 	 0.07142857142857142
tag sets 	 0.25
Extraction and 	 0.3333333333333333
used by 	 0.07964601769911504
can sometimes 	 0.0055248618784530384
be thousands 	 0.004219409282700422
recognition applications 	 0.008264462809917356
mostly work 	 0.5
-RRB- The 	 0.005420054200542005
sounds very 	 0.06666666666666667
telephony is 	 0.3333333333333333
user needs 	 0.07142857142857142
simple tasks 	 0.038461538461538464
about unigrams 	 0.025
the parts 	 0.0006920415224913495
, annotation 	 0.0005614823133071309
most likely 	 0.05172413793103448
Constraints are 	 0.3333333333333333
it builds 	 0.008547008547008548
of 1928 	 0.00089126559714795
and geospatial 	 0.001445086705202312
aloud from 	 1.0
though automating 	 0.1
that you 	 0.0035460992907801418
free beer 	 0.25
to fulfill 	 0.0026560424966799467
on different 	 0.0047169811320754715
also pioneered 	 0.014492753623188406
processes such 	 0.2
, explanation 	 0.0005614823133071309
, Reukos 	 0.0005614823133071309
input which 	 0.024390243902439025
of semantics 	 0.00089126559714795
influenced by 	 1.0
filtered by 	 0.3333333333333333
Computing Machinery 	 0.5
usually rated 	 0.03125
humans when 	 0.08333333333333333
consideration neural 	 0.3333333333333333
, Theo 	 0.0005614823133071309
gather information 	 1.0
in US 	 0.0018726591760299626
architecture uses 	 0.5
the 2011 	 0.0006920415224913495
required many 	 0.14285714285714285
broad tags 	 0.25
-RRB- an 	 0.0027100271002710027
mid 1980s 	 1.0
which structured 	 0.007246376811594203
the random 	 0.0006920415224913495
document set 	 0.027777777777777776
, preparation 	 0.0005614823133071309
`` inadequate 	 0.005291005291005291
a pre-structured 	 0.001226993865030675
quotations , 	 1.0
like being 	 0.03571428571428571
Agency -LRB- 	 0.5
payments . 	 1.0
This approach 	 0.031746031746031744
Leo Spitzer 	 1.0
acts , 	 0.3333333333333333
through its 	 0.125
relative certainty 	 0.3333333333333333
simply based 	 0.08333333333333333
question into 	 0.023809523809523808
be identified 	 0.008438818565400843
war camp 	 1.0
canned phrases 	 0.5
single `` 	 0.07142857142857142
questions or 	 0.038461538461538464
k -RRB- 	 1.0
, artificial 	 0.0011229646266142617
<s> Sound 	 0.0015372790161414297
semantic disambiguation 	 0.047619047619047616
distortion , 	 1.0
signal to 	 0.16666666666666666
car or 	 1.0
systems based 	 0.017857142857142856
be quite 	 0.004219409282700422
human-language question 	 1.0
representations of 	 0.25
for English 	 0.0036101083032490976
extrinsic performance 	 0.16666666666666666
artifacts , 	 1.0
parsing accuracy 	 0.03571428571428571
has included 	 0.011904761904761904
completion of 	 1.0
greater accuracy 	 0.3333333333333333
granted a 	 1.0
isolated speech 	 0.4
-RRB- to 	 0.01084010840108401
a higher 	 0.00245398773006135
have already 	 0.009615384615384616
software , 	 0.037037037037037035
of size 	 0.00089126559714795
causing it 	 1.0
constraints are 	 0.25
domain , 	 0.15
that domain 	 0.0035460992907801418
to detect 	 0.0013280212483399733
, verbs 	 0.0011229646266142617
strong and 	 0.25
simple conditions 	 0.038461538461538464
report finalized 	 0.25
a great 	 0.00245398773006135
dependency relations 	 0.2
computer-aided translation 	 0.3333333333333333
though other 	 0.1
quality criteria 	 0.1
larger source 	 0.0625
future tense 	 0.3333333333333333
sensible manner 	 1.0
then , 	 0.08571428571428572
forms . 	 0.16666666666666666
project -LRB- 	 0.07692307692307693
<s> Prior 	 0.0007686395080707148
through the 	 0.5
do something 	 0.038461538461538464
in most 	 0.00749063670411985
analog signal 	 0.5
the president 	 0.001384083044982699
triple probabilities 	 1.0
Aermacchi M-346 	 1.0
in Europe 	 0.003745318352059925
informative with 	 0.5
speech into 	 0.013157894736842105
<s> Human-machine 	 0.0007686395080707148
solve the 	 0.25
have difficulty 	 0.009615384615384616
printing , 	 1.0
precision - 	 0.2
translation is 	 0.013513513513513514
1999 L'action 	 0.5
the walk 	 0.0006920415224913495
way we 	 0.08333333333333333
vital . 	 1.0
resolution is 	 0.25
the inferior 	 0.0006920415224913495
Medical Language 	 0.5
summarization have 	 0.02
the use\/mention 	 0.0006920415224913495
the scope 	 0.001384083044982699
data sources 	 0.025974025974025976
The Georgetown 	 0.015625
by these 	 0.005714285714285714
systems : 	 0.008928571428571428
and statistics 	 0.002890173410404624
paper data 	 0.09090909090909091
instances , 	 0.3333333333333333
is red 	 0.0020325203252032522
A useful 	 0.02
is used 	 0.026422764227642278
software was 	 0.037037037037037035
complex sets 	 0.041666666666666664
<s> Vocabulary 	 0.0007686395080707148
or meets 	 0.0045045045045045045
of parser 	 0.0017825311942959
general approaches 	 0.045454545454545456
<s> Virtually 	 0.0007686395080707148
one used 	 0.015384615384615385
an AI-complete 	 0.007575757575757576
<s> High-order 	 0.0007686395080707148
ISO\/TC37 and 	 1.0
`` language 	 0.010582010582010581
a storm 	 0.001226993865030675
to arrive 	 0.0013280212483399733
concerning coherence 	 1.0
, producing 	 0.0005614823133071309
DUC 2001 	 1.0
preclude using 	 1.0
a purely 	 0.001226993865030675
Advanced applications 	 0.2
in four 	 0.003745318352059925
<s> Basic 	 0.0007686395080707148
evaluation Black-box 	 0.018518518518518517
OCR machines 	 0.02040816326530612
the major 	 0.001384083044982699
Sacks , 	 1.0
distribution that 	 0.5
lacks pre-existing 	 1.0
We can 	 0.2857142857142857
nodes should 	 0.14285714285714285
been previously 	 0.014705882352941176
entry -LRB- 	 0.25
is concerned 	 0.0040650406504065045
, June 	 0.0005614823133071309
ID card 	 1.0
and coverage 	 0.001445086705202312
, only 	 0.0011229646266142617
candidate , 	 0.3333333333333333
, grammatical 	 0.0005614823133071309
expect ; 	 0.3333333333333333
a coherent 	 0.00245398773006135
to 150,000 	 0.0013280212483399733
determines how 	 0.3333333333333333
These edges 	 0.058823529411764705
`` Turn 	 0.005291005291005291
isolated-word recognizers 	 1.0
simply model 	 0.08333333333333333
to use 	 0.013280212483399735
topics , 	 0.14285714285714285
can assign 	 0.0055248618784530384
of our 	 0.00089126559714795
studying the 	 1.0
read musical 	 0.14285714285714285
, Chantal 	 0.0005614823133071309
surrounding consonants 	 0.2
written without 	 0.038461538461538464
produce the 	 0.13636363636363635
to compiled 	 0.0013280212483399733
use context-free 	 0.013888888888888888
fine degrees 	 0.5
speech tools 	 0.006578947368421052
level provides 	 0.05
Tigrinya among 	 1.0
expression or 	 0.1
tools require 	 0.16666666666666666
<s> LREC 	 0.0015372790161414297
assigned , 	 0.5
, Inc. 	 0.0011229646266142617
criteria . 	 0.25
and ontology 	 0.001445086705202312
's blocks 	 0.0196078431372549
previous Section 	 0.3333333333333333
different sentences 	 0.061224489795918366
at A.C. 	 0.014705882352941176
followed by 	 0.5
linguists would 	 0.3333333333333333
applies both 	 0.2857142857142857
and large-scale 	 0.001445086705202312
to allow 	 0.00398406374501992
Little further 	 1.0
allowing us 	 0.3333333333333333
1983 , 	 1.0
depending on 	 0.75
training . 	 0.03571428571428571
a much 	 0.0036809815950920245
only into 	 0.02631578947368421
HMM-based part 	 0.3333333333333333
particularly prone 	 0.2
providing a 	 0.5
but could 	 0.014705882352941176
by paying 	 0.005714285714285714
combines the 	 1.0
skip the 	 1.0
Morphological segmentation 	 1.0
seen the 	 0.1
, automates 	 0.0005614823133071309
Biden was 	 0.3333333333333333
deemed the 	 0.5
complex than 	 0.08333333333333333
at different 	 0.014705882352941176
So an 	 0.3333333333333333
-5 to 	 1.0
architecture Regardless 	 0.5
such capabilities 	 0.008130081300813009
answering systems 	 0.08333333333333333
amenable to 	 1.0
different approaches 	 0.02040816326530612
of bottom-up 	 0.00089126559714795
keyphrase to 	 0.05263157894736842
paragraph summary 	 0.3333333333333333
program focuses 	 0.045454545454545456
containing words 	 0.125
same general 	 0.04
and actual 	 0.001445086705202312
subjectivity used 	 0.5
source can 	 0.041666666666666664
researchers undertake 	 0.1
particular case 	 0.07692307692307693
Pallet 1998 	 0.5
software . 	 0.037037037037037035
summarization Like 	 0.02
efforts were 	 0.14285714285714285
psychotherapy . 	 1.0
RAE -RRB- 	 1.0
work progressed 	 0.041666666666666664
machine that 	 0.012658227848101266
recognized or 	 0.16666666666666666
surprising popularity 	 1.0
- EVALITA 	 0.0625
-RRB- Telematics 	 0.0027100271002710027
imprints for 	 1.0
needs of 	 0.1
represent only 	 0.1111111111111111
statistical quantity 	 0.030303030303030304
different issue 	 0.02040816326530612
house -LRB- 	 0.5
not aid 	 0.008928571428571428
an era 	 0.007575757575757576
perhaps the 	 0.16666666666666666
or worse 	 0.0045045045045045045
in political 	 0.0018726591760299626
a fully 	 0.001226993865030675
on Text 	 0.0047169811320754715
Birkbeck College 	 1.0
substantial ambiguity 	 0.2
that it 	 0.010638297872340425
Amplitude -LRB- 	 1.0
be unique 	 0.004219409282700422
<s> Increasingly 	 0.0015372790161414297
of summaries 	 0.0035650623885918
up for 	 0.09090909090909091
successful demonstration 	 0.1111111111111111
-RRB- is 	 0.02981029810298103
PARRY , 	 1.0
see List 	 0.05
, Maximum 	 0.0005614823133071309
only as 	 0.02631578947368421
user interfaces 	 0.14285714285714285
Canadian parliament 	 0.5
the POS 	 0.0020761245674740486
of related 	 0.00267379679144385
extractor follows 	 0.5
may require 	 0.038461538461538464
creating systems 	 0.14285714285714285
the phrase 	 0.002768166089965398
idea is 	 0.14285714285714285
particularly difficult 	 0.2
-RRB- found 	 0.0027100271002710027
-LRB- Pallet 	 0.0027100271002710027
its definition 	 0.02857142857142857
loud . 	 1.0
<s> Internet 	 0.0007686395080707148
used SYSTRAN 	 0.008849557522123894
vertex would 	 0.3333333333333333
methods can 	 0.022727272727272728
considerable variation 	 0.2
dogs -RRB- 	 0.14285714285714285
Shipibo Paragraph 	 0.5
<s> Rescoring 	 0.0007686395080707148
could recognize 	 0.0625
parser can 	 0.0625
no matter 	 0.07692307692307693
team led 	 1.0
defines the 	 0.5
the DUC 	 0.0006920415224913495
installing speech 	 1.0
algorithms work 	 0.02857142857142857
descriptive rather 	 0.3333333333333333
into battle 	 0.01282051282051282
generates summaries 	 0.3333333333333333
system whereby 	 0.010752688172043012
-LRB- Black 	 0.0027100271002710027
<s> Algorithms 	 0.0015372790161414297
that determines 	 0.0070921985815602835
and achieves 	 0.001445086705202312
technology would 	 0.045454545454545456
Drew , 	 1.0
or may 	 0.009009009009009009
word is 	 0.06666666666666667
prisoner of 	 1.0
learn it 	 0.07692307692307693
importance would 	 0.16666666666666666
speech and 	 0.013157894736842105
pragmatics of 	 0.3333333333333333
a rudimentary 	 0.001226993865030675
top-down parsers 	 0.25
single speaker 	 0.07142857142857142
conditions in 	 0.2
<s> Two 	 0.005380476556495004
syntactic coverage 	 0.07692307692307693
Speech recognition 	 0.2903225806451613
seen -RRB- 	 0.1
-LRB- P 	 0.0027100271002710027
used on 	 0.008849557522123894
allows the 	 0.375
logical form 	 0.16666666666666666
processed with 	 0.3333333333333333
on discourse 	 0.0047169811320754715
IEEE ASRU 	 0.3333333333333333
new POS 	 0.041666666666666664
human intervention 	 0.021739130434782608
fire '' 	 0.5
candidates instead 	 0.2
words must 	 0.009174311926605505
implications of 	 1.0
word with 	 0.016666666666666666
with any 	 0.00546448087431694
phrases with 	 0.0625
each phoneme 	 0.022222222222222223
relationship with 	 0.16666666666666666
interface . 	 0.25
Generally , 	 0.6
function is 	 0.125
subproblem of 	 1.0
from Fully 	 0.009615384615384616
running publication 	 0.3333333333333333
that removing 	 0.0035460992907801418
memory of 	 0.5
when it 	 0.02857142857142857
analysis or 	 0.046153846153846156
pages . 	 0.2857142857142857
a hard 	 0.00245398773006135
currently used 	 0.14285714285714285
logical assertions 	 0.16666666666666666
with sentences 	 0.00546448087431694
tasks have 	 0.03125
speed , 	 0.2857142857142857
of children 	 0.00089126559714795
code . 	 0.42857142857142855
to high 	 0.00398406374501992
a positive 	 0.001226993865030675
proper syntax 	 0.14285714285714285
other task 	 0.014285714285714285
of 98 	 0.00089126559714795
, combined 	 0.0005614823133071309
e.g. Chinese 	 0.017857142857142856
properly the 	 0.5
transcriptions is 	 0.5
language comprehension 	 0.006756756756756757
optimistic about 	 1.0
are called 	 0.008298755186721992
standard metric 	 0.07142857142857142
shallow . 	 0.16666666666666666
does the 	 0.1
words occur 	 0.009174311926605505
of discourse 	 0.00980392156862745
It consists 	 0.02631578947368421
that answered 	 0.0035460992907801418
dialog with 	 0.5
between lexical 	 0.05128205128205128
who maintains 	 0.1
often considered 	 0.022727272727272728
than instances 	 0.022222222222222223
constrained , 	 1.0
been using 	 0.029411764705882353
adapt to 	 1.0
Schegloff , 	 1.0
a discussion 	 0.001226993865030675
fact that 	 0.45454545454545453
to two 	 0.0013280212483399733
summaries , 	 0.06976744186046512
delayed until 	 1.0
some sort 	 0.012048192771084338
the Information 	 0.0006920415224913495
, -LRB- 	 0.0016844469399213925
reason why 	 0.25
processed Airline 	 0.16666666666666666
mimic the 	 1.0
and their 	 0.008670520231213872
approaches emphasize 	 0.03571428571428571
his wingmen 	 0.08333333333333333
which usually 	 0.007246376811594203
databases into 	 0.125
Robert Wilensky 	 0.25
100 million 	 0.3333333333333333
- processing 	 0.0625
AI than 	 0.3333333333333333
mostly as 	 0.5
Later , 	 1.0
succeeding on 	 1.0
Driver-license OCR 	 1.0
which would 	 0.014492753623188406
all handwritten 	 0.023255813953488372
Clancy 's 	 1.0
wanted to 	 1.0
and have 	 0.001445086705202312
pre-defined by 	 0.5
, Mariani 	 0.0005614823133071309
so-called discriminative 	 0.3333333333333333
`` Mr. 	 0.005291005291005291
were very 	 0.04878048780487805
transducers with 	 1.0
of similar 	 0.0017825311942959
computer based 	 0.022727272727272728
Tagging Guidelines 	 1.0
, unverified 	 0.0005614823133071309
his students 	 0.16666666666666666
reduction of 	 0.5
and that 	 0.002890173410404624
machine printed 	 0.012658227848101266
determines the 	 0.6666666666666666
to school-age 	 0.0013280212483399733
extracting sentences 	 0.2
various features 	 0.05555555555555555
use simple 	 0.013888888888888888
the target 	 0.006228373702422145
made between 	 0.0625
caught ' 	 1.0
aircraft , 	 0.2857142857142857
is relevant 	 0.0020325203252032522
contain densely 	 0.08333333333333333
to computers 	 0.0013280212483399733
of disparate 	 0.00089126559714795
sentiment of 	 0.04
transducer verifying 	 0.5
allows movement 	 0.125
, while 	 0.007860752386299831
individual trained 	 0.08333333333333333
research has 	 0.14285714285714285
resulted in 	 1.0
N is 	 0.3333333333333333
is importance 	 0.0020325203252032522
make no 	 0.05
He then 	 0.125
boards and 	 1.0
Turing published 	 0.5
extraction algorithm 	 0.03225806451612903
usually in 	 0.09375
backup methods 	 1.0
still be 	 0.06666666666666667
major degradation 	 0.08333333333333333
The Brown 	 0.015625
NLP ranking 	 0.02127659574468085
covariance Gaussians 	 0.5
in speech 	 0.013108614232209739
collections vary 	 0.25
only complete 	 0.02631578947368421
automated online 	 0.14285714285714285
which consisted 	 0.007246376811594203
we wanted 	 0.022222222222222223
length . 	 0.125
segmentation task 	 0.030303030303030304
word can 	 0.03333333333333333
consecutive years 	 0.5
roughly proportional 	 0.3333333333333333
expression which 	 0.1
HMMs , 	 0.125
have proposed 	 0.009615384615384616
the EARS 	 0.0006920415224913495
an approach 	 0.030303030303030304
and require 	 0.004335260115606936
Corporation and 	 0.25
shows , 	 1.0
given restaurant 	 0.041666666666666664
Edges are 	 1.0
and practice 	 0.001445086705202312
the output 	 0.0020761245674740486
approach is 	 0.14285714285714285
as within 	 0.003484320557491289
in very 	 0.0056179775280898875
possible to 	 0.125
a reduced 	 0.001226993865030675
large percentage 	 0.043478260869565216
promise to 	 1.0
right-to-left , 	 1.0
tackles each 	 1.0
capabilities , 	 0.2
measure than 	 0.09090909090909091
be compared 	 0.004219409282700422
presented to 	 0.16666666666666666
a written-out 	 0.001226993865030675
questions about 	 0.15384615384615385
a training 	 0.001226993865030675
printed records 	 0.08333333333333333
disparate fields 	 1.0
but switched 	 0.014705882352941176
Each concept 	 0.16666666666666666
is unable 	 0.0020325203252032522
found most 	 0.07142857142857142
segments and 	 0.2
the multiple 	 0.0006920415224913495
programmers began 	 1.0
objects listed 	 0.2
attribute or 	 0.5
<s> Was 	 0.0007686395080707148
: TextRank 	 0.0196078431372549
ways : 	 0.25
type provided 	 0.07142857142857142
and enterprise 	 0.001445086705202312
finding the 	 0.4
documents . 	 0.13157894736842105
speaker and 	 0.05555555555555555
`` On 	 0.005291005291005291
retrieval and 	 0.2857142857142857
statement , 	 1.0
and performance 	 0.001445086705202312
target language 	 0.7272727272727273
task of 	 0.21428571428571427
stochastic methods 	 0.125
What distinguishes 	 0.09090909090909091
test document 	 0.1
findings were 	 1.0
proven useful 	 1.0
on whether 	 0.0047169811320754715
for customisation 	 0.0036101083032490976
emails -RRB- 	 0.5
original . 	 0.07692307692307693
would recognize 	 0.018867924528301886
correlation between 	 0.5
applying PageRank 	 0.5
a way 	 0.006134969325153374
effectiveness of 	 0.3333333333333333
there general 	 0.025
identity of 	 1.0
representations such 	 0.25
mental representations 	 0.3333333333333333
Latin pars 	 0.25
and processing 	 0.001445086705202312
→ <verb> 	 0.3333333333333333
analytics to 	 1.0
achieved accuracy 	 0.2
AI-complete '' 	 0.6666666666666666
multimedia -LRB- 	 0.5
systems explore 	 0.008928571428571428
splitting may 	 0.5
how strong 	 0.034482758620689655
thought-to-paper communication 	 1.0
portable , 	 0.3333333333333333
developed CLAWS 	 0.038461538461538464
a genetic 	 0.001226993865030675
forecasts to 	 0.2
or two 	 0.009009009009009009
device required 	 0.5
English verbs 	 0.02702702702702703
system at 	 0.010752688172043012
Technology -LRB- 	 0.3333333333333333
Law and 	 1.0
sentences when 	 0.013157894736842105
Kenneth Lee 	 1.0
may then 	 0.019230769230769232
a final 	 0.001226993865030675
approaches the 	 0.03571428571428571
on written 	 0.0047169811320754715
letters are 	 0.1
but other 	 0.014705882352941176
The Duchess 	 0.005208333333333333
and removed 	 0.001445086705202312
unit block 	 0.3333333333333333
into segments 	 0.02564102564102564
along with 	 1.0
traditional linguistics 	 1.0
meaningful symbols 	 0.125
whether the 	 0.15384615384615385
as words 	 0.003484320557491289
and persuasion 	 0.001445086705202312
generate examples 	 0.05555555555555555
lexical and 	 0.15384615384615385
-LRB- natural 	 0.0027100271002710027
dictation system 	 1.0
as voicemail 	 0.003484320557491289
text mining 	 0.012578616352201259
disambiguation . 	 0.1
phones and 	 0.5
dictionary and 	 0.14285714285714285
some summarization 	 0.012048192771084338
with human 	 0.01092896174863388
contains embedded 	 0.1
lies the 	 0.5
challenged and 	 1.0
at Cognitive 	 0.014705882352941176
morphology , 	 0.7142857142857143
, Discontinuous 	 0.0005614823133071309
and Italian 	 0.001445086705202312
be -RRB- 	 0.004219409282700422
bank . 	 1.0
agreement among 	 0.3333333333333333
quite high 	 0.125
translators and 	 1.0
maximum entropy 	 0.5
mechanized sorting 	 1.0
OCR is 	 0.061224489795918366
function either 	 0.125
was due 	 0.012987012987012988
articles or 	 0.25
the Wall 	 0.001384083044982699
and by 	 0.001445086705202312
-RRB- may 	 0.0027100271002710027
keyphrase extraction 	 0.631578947368421
relations are 	 0.08333333333333333
cluster of 	 1.0
and include 	 0.002890173410404624
Ask.com . 	 1.0
placed in 	 1.0
high as 	 0.05555555555555555
But also 	 0.16666666666666666
identifiers . 	 1.0
linguistic research 	 0.0625
Sample a 	 1.0
Corporation -LRB- 	 0.5
researchers in 	 0.1
However the 	 0.02702702702702703
steer-point coordinates 	 1.0
'' with 	 0.020618556701030927
Discourse analysis 	 1.0
If it 	 0.1
strategy gets 	 0.2
risk -LRB- 	 0.5
be asked 	 0.004219409282700422
semantics formalization 	 0.07142857142857142
processing The 	 0.018518518518518517
keyphrases by 	 0.02857142857142857
containing the 	 0.375
way to 	 0.4166666666666667
gold standard 	 0.8333333333333334
the title 	 0.0006920415224913495
a distinction 	 0.001226993865030675
the future 	 0.001384083044982699
characteristics . 	 0.5
lexicon and 	 0.1111111111111111
without documents 	 0.07692307692307693
of English 	 0.00267379679144385
places and 	 0.5
derive meaning 	 0.5
task should 	 0.023809523809523808
readers . 	 0.5
over a 	 0.08333333333333333
classification of 	 0.058823529411764705
, simple 	 0.0011229646266142617
analysis include 	 0.015384615384615385
vocabulary size 	 0.125
This sequence 	 0.015873015873015872
we can 	 0.06666666666666667
little any 	 0.3333333333333333
in multiscript 	 0.0018726591760299626
built in 	 0.3333333333333333
using online 	 0.01694915254237288
profile may 	 0.3333333333333333
short commands 	 0.125
tagging by 	 0.04
marks may 	 0.25
domains and 	 0.25
QA The 	 0.047619047619047616
relationships in 	 0.16666666666666666
SPOTLIGHT system 	 1.0
exceeded the 	 1.0
citations for 	 0.3333333333333333
absorbing Markov 	 0.3333333333333333
the segmentation 	 0.0006920415224913495
units , 	 0.14285714285714285
system by 	 0.010752688172043012
refer to 	 1.0
situation . 	 0.5
rule-based machine-translation 	 0.14285714285714285
poorly defined 	 1.0
IBM and 	 0.3333333333333333
A comprehensive 	 0.02
production and 	 0.3333333333333333
, correlation 	 0.0005614823133071309
include versions 	 0.037037037037037035
conferences held 	 1.0
the 100 	 0.0006920415224913495
tasks , 	 0.125
% . 	 0.23076923076923078
known . 	 0.038461538461538464
an F-score 	 0.007575757575757576
it learns 	 0.008547008547008548
implementing a 	 1.0
had similar 	 0.07142857142857142
realm of 	 1.0
the . 	 0.0006920415224913495
and speed 	 0.002890173410404624
Sentence boundary 	 0.2
for specific 	 0.0036101083032490976
the collection 	 0.0006920415224913495
achieved only 	 0.1
adaptation greatly 	 0.3333333333333333
stock . 	 0.6666666666666666
solved . 	 0.2
In any 	 0.01904761904761905
arrive at 	 1.0
of applications 	 0.00267379679144385
summary of 	 0.07142857142857142
evaluation can 	 0.037037037037037035
video -RRB- 	 0.2
started around 	 0.25
more forms 	 0.010526315789473684
ATIS . 	 1.0
underlying knowledge 	 0.3333333333333333
used when 	 0.017699115044247787
are known 	 0.012448132780082987
using ATC 	 0.01694915254237288
incremental improvements 	 1.0
pollen level 	 0.15384615384615385
those organised 	 0.045454545454545456
interested in 	 1.0
means Category 	 0.16666666666666666
technique in 	 0.14285714285714285
ROUGE measures 	 0.2
especially useful 	 0.06666666666666667
uses -LRB- 	 0.07142857142857142
Weizenbaum sidestepped 	 0.3333333333333333
that involve 	 0.0035460992907801418
Animate = 	 1.0
: Emergent 	 0.00980392156862745
-LRB- Style 	 0.0027100271002710027
Laclau , 	 1.0
closest counterparts 	 0.5
Goldberg continued 	 0.5
for computers 	 0.0036101083032490976
typical large-vocabulary 	 0.1111111111111111
produce , 	 0.045454545454545456
the nature 	 0.0034602076124567475
are instructed 	 0.004149377593360996
delimited . 	 0.25
using CSIS 	 0.01694915254237288
error rate 	 0.4166666666666667
word vector 	 0.016666666666666666
their device 	 0.029411764705882353
systems become 	 0.008928571428571428
whether an 	 0.07692307692307693
NLP evaluation 	 0.06382978723404255
questioned the 	 1.0
's house 	 0.0392156862745098
objective evaluation 	 0.2
People with 	 1.0
is usually 	 0.016260162601626018
a new 	 0.007361963190184049
`` features 	 0.005291005291005291
co-occur at 	 0.5
conditions ; 	 0.2
might co-occur 	 0.038461538461538464
are connected 	 0.004149377593360996
evaluation methods 	 0.018518518518518517
paper skew 	 0.09090909090909091
<s> Digitized 	 0.0007686395080707148
`` training 	 0.005291005291005291
and 2009 	 0.001445086705202312
operations . 	 1.0
was . 	 0.012987012987012988
a linear 	 0.00245398773006135
evaluation considers 	 0.018518518518518517
encouraging , 	 1.0
be difficult 	 0.004219409282700422
for further 	 0.010830324909747292
, Japanese 	 0.0011229646266142617
<s> So 	 0.0023059185242121443
processing part 	 0.018518518518518517
the addressee 	 0.0006920415224913495
by Schank 	 0.005714285714285714
a novel 	 0.001226993865030675
fall between 	 0.25
what the 	 0.125
Using the 	 0.5
NLG summaries 	 0.047619047619047616
semantics of 	 0.07142857142857142
retail sales 	 1.0
Additional aspects 	 1.0
in sentiment 	 0.0018726591760299626
phrases . 	 0.3125
following . 	 0.13333333333333333
explore what 	 0.25
see references 	 0.05
Learning Some 	 1.0
and `` 	 0.028901734104046242
contractions like 	 0.5
with restricted 	 0.00546448087431694
distinguishes these 	 0.5
syntactic structure 	 0.07692307692307693
23 letters 	 1.0
Deciding what 	 1.0
steps of 	 0.5
also considered 	 0.014492753623188406
in all 	 0.0056179775280898875
does , 	 0.1
expected to 	 0.2857142857142857
tagset by 	 1.0
for Friday 	 0.010830324909747292
using shallow 	 0.01694915254237288
with learning 	 0.00546448087431694
are germane 	 0.004149377593360996
Aided Machine 	 0.3333333333333333
perform as 	 0.09090909090909091
as is 	 0.013937282229965157
adjectives . 	 0.3333333333333333
and abstraction 	 0.002890173410404624
, neural 	 0.0022459292532285235
separated by 	 0.6666666666666666
the choice 	 0.0006920415224913495
higher than 	 0.14285714285714285
negative , 	 0.125
online expression 	 0.125
and Speech 	 0.001445086705202312
of costly 	 0.00089126559714795
M-346 Master 	 1.0
<s> Intuitively 	 0.0007686395080707148
analog wave 	 0.5
summarization exactly 	 0.02
vehicle Navigation 	 1.0
software started 	 0.037037037037037035
`` breadth 	 0.005291005291005291
OCR vendors 	 0.02040816326530612
large sets 	 0.08695652173913043
that used 	 0.0035460992907801418
began planning 	 0.14285714285714285
PageRank selects 	 0.16666666666666666
word `` 	 0.016666666666666666
as conveniently 	 0.003484320557491289
objects of 	 0.2
for programming 	 0.0036101083032490976
both appear 	 0.03225806451612903
in linguistic 	 0.003745318352059925
to structured 	 0.0013280212483399733
testing is 	 0.2
articulation , 	 1.0
<s> Later 	 0.0007686395080707148
is described 	 0.0020325203252032522
Recent applications 	 0.3333333333333333
allow discriminative 	 0.2
state a 	 0.07142857142857142
Several papers 	 0.3333333333333333
only of 	 0.02631578947368421
Voice Input 	 0.2
AFTI -RRB- 	 1.0
do this 	 0.07692307692307693
P + 	 0.5
of approaches 	 0.00089126559714795
, `` 	 0.01403705783267827
A promising 	 0.02
term is 	 0.05555555555555555
standard table 	 0.07142857142857142
`` computer 	 0.005291005291005291
handwriting , 	 0.5
as HMM 	 0.003484320557491289
open-ended questions 	 1.0
provider dictates 	 1.0
inseparable part 	 1.0
cartoon animation 	 1.0
topic boundaries 	 0.125
issues were 	 0.2
had not 	 0.07142857142857142
Automatic vs. 	 0.1111111111111111
the Gene 	 0.0006920415224913495
processing tasks 	 0.018518518518518517
tagging for 	 0.04
and paragraph 	 0.001445086705202312
NAACL , 	 1.0
robot questions 	 0.5
the authors 	 0.0006920415224913495
spaces used 	 0.2
place in 	 0.5
's job 	 0.0392156862745098
their corresponding 	 0.029411764705882353
as simple 	 0.006968641114982578
annual Loebner 	 0.5
it will 	 0.017094017094017096
time step 	 0.030303030303030304
also stems 	 0.014492753623188406
Harvey Sacks 	 1.0
data -RRB- 	 0.03896103896103896
voice applications 	 0.07692307692307693
broad , 	 0.25
system developed 	 0.010752688172043012
points out 	 0.5
on sentence 	 0.0047169811320754715
, medial 	 0.0005614823133071309
a scaling 	 0.001226993865030675
the corpus 	 0.0006920415224913495
assign a 	 0.4
the burden 	 0.0006920415224913495
SWER -RRB- 	 1.0
of artificial 	 0.00089126559714795
you want 	 0.07692307692307693
spectrum using 	 1.0
strength of 	 0.6
milliseconds . 	 0.5
which led 	 0.007246376811594203
covers the 	 0.5
symbols or 	 0.3333333333333333
potential to 	 0.2857142857142857
<s> Besides 	 0.0007686395080707148
to Australia 	 0.0013280212483399733
processing tools 	 0.018518518518518517
and ask 	 0.001445086705202312
video sub-titling 	 0.2
To translate 	 0.1111111111111111
-LRB- so 	 0.005420054200542005
and life 	 0.001445086705202312
profile feature 	 0.3333333333333333
assertions , 	 0.5
's input 	 0.0196078431372549
view language 	 0.3333333333333333
on functional 	 0.0047169811320754715
the 1960s 	 0.001384083044982699
The fonts 	 0.005208333333333333
methods -LRB- 	 0.045454545454545456
be robust 	 0.004219409282700422
of 3 	 0.00089126559714795
an hour 	 0.007575757575757576
tagging -LRB- 	 0.04
A precise 	 0.02
now absorbing 	 0.07692307692307693
possibilities multiply 	 0.2
that machine 	 0.010638297872340425
when integrated 	 0.02857142857142857
-LRB- as 	 0.018970189701897018
, intelligent 	 0.0005614823133071309
later , 	 0.5
or grammatical 	 0.0045045045045045045
equivalent information 	 0.2
can only 	 0.011049723756906077
by Jurafsky 	 0.005714285714285714
graph -RRB- 	 0.07692307692307693
judgement often 	 0.3333333333333333
opinion expressed 	 0.2
- and 	 0.125
decision trees 	 1.0
any pauses 	 0.03225806451612903
has yet 	 0.011904761904761904
<s> The 	 0.11222136817832437
become clear 	 0.25
systems existing 	 0.008928571428571428
statistics readily 	 0.125
threshold or 	 0.5
very broad 	 0.04878048780487805
are numerous 	 0.004149377593360996
networks has 	 0.07142857142857142
, Liberman 	 0.0005614823133071309
unigram matching 	 0.2
formed the 	 0.2
`` blue 	 0.010582010582010581
most parts 	 0.05172413793103448
large data 	 0.043478260869565216
are in 	 0.012448132780082987
certain assumptions 	 0.14285714285714285
are maximum 	 0.004149377593360996
it has 	 0.03418803418803419
research to 	 0.023809523809523808
formalisms\/languages . 	 1.0
the personal 	 0.0006920415224913495
10msec , 	 0.5
decisions are 	 0.1
the Canadian 	 0.001384083044982699
news domain 	 0.23076923076923078
for French 	 0.010830324909747292
opinionated , 	 1.0
domain-specific keyphrase 	 0.5
is split 	 0.0040650406504065045
LexRank paper 	 0.08333333333333333
for example 	 0.06498194945848375
part-of-speech tags 	 0.06666666666666667
available isolated-word 	 0.058823529411764705
of 1 	 0.00089126559714795
The task 	 0.020833333333333332
ideas -RRB- 	 0.25
the GALE 	 0.001384083044982699
main drawback 	 0.125
reader designed 	 0.2
WYSIWYM framework 	 1.0
computerized text 	 0.5
input sentence 	 0.024390243902439025
larger corpora 	 0.0625
keyphrases to 	 0.02857142857142857
-LRB- b 	 0.0027100271002710027
Conversation analysis 	 1.0
subscription department 	 1.0
Unsupervised keyphrase 	 0.3333333333333333
machines by 	 0.25
LMF -RRB- 	 1.0
assumption -LRB- 	 0.5
has not 	 0.023809523809523808
article verb 	 0.034482758620689655
main difficulty 	 0.25
`` Fundamentals 	 0.010582010582010581
occur , 	 0.2
ink -LRB- 	 1.0
but much 	 0.014705882352941176
time-consuming . 	 0.3333333333333333
the comprehension 	 0.0006920415224913495
Bhatia , 	 1.0
predict keyphrases 	 0.16666666666666666
existing multilingual 	 0.2
especially those 	 0.2
to dry 	 0.0013280212483399733
include : 	 0.1111111111111111
nouns were 	 0.1111111111111111
the Lander 	 0.0006920415224913495
explicitly present 	 0.25
the media 	 0.0006920415224913495
automatic summaries 	 0.13043478260869565
, factors 	 0.0005614823133071309
shortened version 	 1.0
up-to-date research 	 1.0
recommendation '' 	 1.0
of language 	 0.004456327985739751
sophisticated methods 	 0.14285714285714285
Corpus and 	 0.125
base or 	 0.25
several -RRB- 	 0.045454545454545456
dependent on 	 0.6666666666666666
addition might 	 0.16666666666666666
The Associated 	 0.005208333333333333
and dictionary-based 	 0.001445086705202312
they improved 	 0.025
of data 	 0.006238859180035651
would fail 	 0.018867924528301886
Pointwise Mutual 	 1.0
second step 	 0.2
make ; 	 0.05
This unreferenced 	 0.015873015873015872
, contractions 	 0.0005614823133071309
of word-frequency 	 0.00089126559714795
deciding where 	 0.16666666666666666
these are 	 0.047619047619047616
more commonly 	 0.021052631578947368
especially common 	 0.13333333333333333
These findings 	 0.058823529411764705
Naive Bayes 	 1.0
them are 	 0.10526315789473684
computerized language 	 0.5
Swales , 	 1.0
use techniques 	 0.013888888888888888
-LRB- basically 	 0.0027100271002710027
a cheque 	 0.001226993865030675
a cluster 	 0.00245398773006135
contrast to 	 0.25
laws calling 	 1.0
possible . 	 0.125
a deterministic 	 0.00245398773006135
and sub-categories 	 0.001445086705202312
co-articulation of 	 1.0
speech acts 	 0.019736842105263157
for medical 	 0.0036101083032490976
a general 	 0.0036809815950920245
contain strings 	 0.08333333333333333
This allows 	 0.031746031746031744
keyphrases attached 	 0.02857142857142857
of symbols 	 0.00089126559714795
are SHRDLU 	 0.004149377593360996
of NLG 	 0.00089126559714795
of researchers 	 0.00089126559714795
as Eugene 	 0.003484320557491289
: task-based 	 0.00980392156862745
the meantime 	 0.0006920415224913495
The system 	 0.03125
operation of 	 0.5
the blind 	 0.0006920415224913495
CLAWS pioneered 	 0.25
reflect a 	 1.0
<s> Front-End 	 0.0007686395080707148
to ask 	 0.0013280212483399733
data of 	 0.012987012987012988
that can 	 0.04609929078014184
or FST 	 0.0045045045045045045
15-20 million 	 1.0
shallow approach 	 0.3333333333333333
row , 	 1.0
-LRB- December 	 0.0027100271002710027
references . 	 0.25
considered for 	 0.1111111111111111
and rule-based 	 0.001445086705202312
Nunan , 	 1.0
, usually 	 0.002807411566535654
What you 	 0.09090909090909091
be found 	 0.012658227848101266
<s> because 	 0.0007686395080707148
of top-down 	 0.0017825311942959
is angry 	 0.0020325203252032522
of that 	 0.0017825311942959
German and 	 0.25
are capitalized 	 0.004149377593360996
quality standards 	 0.1
text comprehension 	 0.006289308176100629
they are 	 0.175
spending limit 	 1.0
mainly the 	 0.16666666666666666
rates greatly 	 0.125
whole sentences 	 0.2222222222222222
to `` 	 0.005312084993359893
knowledge bases 	 0.037037037037037035
summaries do 	 0.023255813953488372
this procedure 	 0.01098901098901099
sound in 	 0.05
area of 	 0.45454545454545453
also needs 	 0.014492753623188406
other words 	 0.02857142857142857
systems for 	 0.017857142857142856
easily portable 	 0.2222222222222222
Cohesion and 	 1.0
fonts are 	 0.3333333333333333
times throughout 	 0.2
researchers need 	 0.1
often allows 	 0.022727272727272728
that says 	 0.0035460992907801418
features making 	 0.038461538461538464
anomalies . 	 1.0
any QA 	 0.03225806451612903
entities , 	 0.2857142857142857
non-Western scripts 	 1.0
command interpreters 	 0.5
Encouraging results 	 1.0
classes : 	 0.2
the tokens 	 0.001384083044982699
limited application 	 0.1
<s> Adverse 	 0.0007686395080707148
random walk 	 0.5714285714285714
is lessened 	 0.0020325203252032522
for special 	 0.0036101083032490976
1966 -RRB- 	 0.3333333333333333
rhetoric , 	 1.0
development of 	 0.5833333333333334
look-up tables 	 1.0
more data 	 0.021052631578947368
regardless of 	 1.0
threshold to 	 0.25
a reader 	 0.001226993865030675
appends the 	 1.0
the prolific 	 0.0006920415224913495
task remains 	 0.023809523809523808
on stochastic 	 0.0047169811320754715
the successful 	 0.0006920415224913495
e.g. Noise 	 0.017857142857142856
have tested 	 0.009615384615384616
the Penn 	 0.005536332179930796
information can 	 0.021739130434782608
results show 	 0.047619047619047616
separate tokens 	 0.1
Human Aided 	 0.2
Chinese is 	 0.14285714285714285
by mapping 	 0.005714285714285714
the goals 	 0.0006920415224913495
of glass-box 	 0.00089126559714795
sound that 	 0.05
would probably 	 0.018867924528301886
source for 	 0.041666666666666664
are traditionally 	 0.008298755186721992
construction , 	 0.3333333333333333
Since 2000 	 0.2
by multiplying 	 0.005714285714285714
Linguistics '' 	 0.3333333333333333
and more 	 0.0072254335260115606
deeply , 	 1.0
template slot 	 0.25
challenges -RRB- 	 0.5
very different 	 0.07317073170731707
the dynamics 	 0.0006920415224913495
regression -LRB- 	 1.0
builds up 	 0.5
popular being 	 0.1111111111111111
by human 	 0.017142857142857144
Jan Blommaert 	 1.0
two measures 	 0.034482758620689655
crossed below 	 1.0
phonemes in 	 0.16666666666666666
supervised '' 	 0.1875
's informativeness 	 0.0196078431372549
The Apple 	 0.005208333333333333
, NAACL 	 0.0005614823133071309
capital letters 	 0.3333333333333333
1974 Ray 	 1.0
simply too 	 0.08333333333333333
, Louise 	 0.0005614823133071309
distant from 	 1.0
relationship to 	 0.16666666666666666
- passage 	 0.0625
would thus 	 0.018867924528301886
businesses looking 	 0.5
more consistent 	 0.010526315789473684
wave . 	 0.2222222222222222
technology in 	 0.045454545454545456
of hand 	 0.00089126559714795
1,000 words 	 0.5
religious services 	 1.0
tools mostly 	 0.16666666666666666
two approaches 	 0.034482758620689655
The simplest 	 0.005208333333333333
and Web 	 0.001445086705202312
is responsible 	 0.0020325203252032522
and attempt 	 0.001445086705202312
and merging 	 0.001445086705202312
a startlingly 	 0.001226993865030675
with matching 	 0.00546448087431694
single language 	 0.07142857142857142
, exploring 	 0.0005614823133071309
move items 	 1.0
certain patterns 	 0.14285714285714285
next word 	 0.2857142857142857
states such 	 0.25
examples as 	 0.041666666666666664
preliminary recognition 	 0.3333333333333333
` hitcha 	 0.0625
that part-of-speech 	 0.0035460992907801418
computer interaction 	 0.022727272727272728
and TextRank 	 0.001445086705202312
mentions '' 	 0.3333333333333333
simple English 	 0.038461538461538464
applications of 	 0.04
TextRank uses 	 0.14285714285714285
is distorted 	 0.0040650406504065045
person who 	 0.05263157894736842
eventually spun 	 1.0
judgement or 	 0.3333333333333333
that had 	 0.0035460992907801418
normalized by 	 1.0
data set 	 0.012987012987012988
the lexical 	 0.0006920415224913495
distinction of 	 0.2
without reference 	 0.07692307692307693
waves can 	 0.14285714285714285
analysis depends 	 0.015384615384615385
for training 	 0.010830324909747292
build on 	 0.3333333333333333
depends on 	 0.875
words immediately 	 0.009174311926605505
grammar parsing 	 0.02702702702702703
sounds it 	 0.06666666666666667
Loebner prize 	 1.0
are generated 	 0.004149377593360996
occurs in 	 1.0
that builds 	 0.0035460992907801418
TextRank algorithm 	 0.07142857142857142
, Eurospeech\/ICSLP 	 0.0005614823133071309
statistical properties 	 0.030303030303030304
Sager at 	 0.5
attractive method 	 0.3333333333333333
<s> ATNs 	 0.0007686395080707148
since 2000 	 0.1
finalized . 	 1.0
and one 	 0.001445086705202312
first came 	 0.030303030303030304
sentences begin 	 0.013157894736842105
corpus to 	 0.03225806451612903
not consistently 	 0.017857142857142856
Hafiz , 	 1.0
amongst a 	 1.0
, degraded-images 	 0.0005614823133071309
intelligence , 	 0.125
book a 	 0.125
`` tagged 	 0.010582010582010581
elaboration , 	 1.0
Norval , 	 1.0
we take 	 0.022222222222222223
the jet 	 0.0006920415224913495
difficult . 	 0.07142857142857142
properties , 	 0.25
then be 	 0.02857142857142857
directed . 	 1.0
, multilingual 	 0.0005614823133071309
methods achieved 	 0.022727272727272728
radiology report 	 1.0
does not 	 0.5
: Separate 	 0.0196078431372549
between adjacent 	 0.05128205128205128
document 's 	 0.027777777777777776
processing and 	 0.018518518518518517
camp with 	 0.5
a neural 	 0.001226993865030675
MT Hybrid 	 0.2
but those 	 0.014705882352941176
e.g. yes-no 	 0.017857142857142856
medial and 	 1.0
<s> Typical 	 0.0015372790161414297
not appear 	 0.008928571428571428
POS -RRB- 	 0.07692307692307693
mutual information 	 1.0
for ASR 	 0.0036101083032490976
typewritten or 	 0.2
online assistant 	 0.125
-LRB- HMT 	 0.0027100271002710027
both isloated 	 0.03225806451612903
major issues 	 0.08333333333333333
targets to 	 1.0
techniques to 	 0.17391304347826086
's intrinsic 	 0.0196078431372549
Beigi covers 	 1.0
block of 	 1.0
include straightforward 	 0.037037037037037035
or moderate 	 0.0045045045045045045
model will 	 0.03333333333333333
shallow methods 	 0.16666666666666666
issues relating 	 0.2
fraction of 	 1.0
more recent 	 0.010526315789473684
i.e. the 	 0.2631578947368421
<s> Overview 	 0.0015372790161414297
-LRB- ME 	 0.0027100271002710027
fair gold-standard 	 1.0
use\/mention distinction 	 1.0
sometimes referred 	 0.23076923076923078
exercises on 	 1.0
Effective natural 	 1.0
color images 	 1.0
graphs and 	 1.0
above 95 	 0.07692307692307693
for them 	 0.007220216606498195
need for 	 0.14285714285714285
article , 	 0.10344827586206896
keyphrases as 	 0.02857142857142857
while LexRank 	 0.1
the role 	 0.001384083044982699
a tag 	 0.001226993865030675
the EMR 	 0.0006920415224913495
with applications 	 0.00546448087431694
their similarity 	 0.029411764705882353
claim that 	 1.0
better understanding 	 0.1111111111111111
An increasing 	 0.0625
marks , 	 0.25
, then 	 0.006176305446378439
document can 	 0.027777777777777776
2007 and 	 0.2
used mostly 	 0.008849557522123894
, even 	 0.0039303761931499155
be gained 	 0.004219409282700422
while capturing 	 0.05
stationary signal 	 0.2857142857142857
review of 	 0.3333333333333333
and combining 	 0.001445086705202312
segmentation problems 	 0.06060606060606061
Eurospeech\/ICSLP -LRB- 	 1.0
been popular 	 0.014705882352941176
automation Interactive 	 1.0
highly ranked 	 0.1111111111111111
<s> consider 	 0.0007686395080707148
limited amounts 	 0.1
<s> From 	 0.0007686395080707148
aid users 	 0.25
letter ? 	 0.16666666666666666
-LRB- allowing 	 0.0027100271002710027
text 's 	 0.006289308176100629
the bi-directional 	 0.0006920415224913495
redundancy . 	 0.3333333333333333
improve document 	 0.07692307692307693
their hands 	 0.029411764705882353
alphabet are 	 0.3333333333333333
could read 	 0.0625
a larger 	 0.0049079754601227
from first 	 0.009615384615384616
early as 	 0.1
formally , 	 0.5
e.g. marking 	 0.017857142857142856
do we 	 0.038461538461538464
, showing 	 0.0005614823133071309
to database 	 0.0013280212483399733
, other 	 0.0005614823133071309
input data 	 0.14634146341463414
whose usage 	 0.3333333333333333
SHRDLU could 	 0.16666666666666666
measure -LRB- 	 0.09090909090909091
about nearly 	 0.025
necessarily match 	 0.5
, so 	 0.00673778775968557
extraction or 	 0.03225806451612903
strong feeling 	 0.25
that produce 	 0.0035460992907801418
-RRB- NASA 	 0.0027100271002710027
algorithm optimizes 	 0.03571428571428571
terms do 	 0.07692307692307693
systems which 	 0.017857142857142856
several alternative 	 0.045454545454545456
strengths of 	 0.5
assumptions , 	 0.2
review . 	 0.3333333333333333
The results 	 0.005208333333333333
Both QA 	 0.3333333333333333
with an 	 0.0273224043715847
MIT . 	 0.5
then noun 	 0.02857142857142857
linear regression 	 0.14285714285714285
not used 	 0.017857142857142856
analysis and 	 0.03076923076923077
of vertices 	 0.00089126559714795
is broken 	 0.0020325203252032522
Canada . 	 0.16666666666666666
will cover 	 0.02857142857142857
left and 	 0.3333333333333333
groups submit 	 0.2
and check 	 0.001445086705202312
or service 	 0.0045045045045045045
GALE project 	 1.0
`` Spoken 	 0.005291005291005291
a commercial 	 0.00245398773006135
: Overall 	 0.00980392156862745
was later 	 0.012987012987012988
to digitize 	 0.0013280212483399733
popular in 	 0.1111111111111111
Moore 's 	 1.0
, PAM 	 0.0005614823133071309
about an 	 0.025
+ Web-based 	 0.16666666666666666
its own 	 0.14285714285714285
social contexts 	 0.07142857142857142
faster computers 	 0.3333333333333333
democracy . 	 1.0
<s> Leading 	 0.0007686395080707148
, real 	 0.0005614823133071309
meaning to 	 0.08695652173913043
domain of 	 0.1
possibility to 	 0.25
in picture 	 0.0018726591760299626
attempts to 	 0.5
Searches , 	 1.0
multiple documents 	 0.15384615384615385
fall into 	 0.5
reading and 	 0.25
' properties 	 0.05263157894736842
be evaluated 	 0.008438818565400843
and gets 	 0.001445086705202312
useful keyphrases 	 0.07142857142857142
etc. ; 	 0.045454545454545456
recent book 	 0.125
similar in 	 0.037037037037037035
on absorbing 	 0.0047169811320754715
, Larry 	 0.0005614823133071309
Vocabulary is 	 0.3333333333333333
Snyder performed 	 0.5
, Klavans 	 0.0005614823133071309
in Germany 	 0.003745318352059925
'' could 	 0.005154639175257732
Jef Verschueren 	 1.0
, computer 	 0.0011229646266142617
This model 	 0.031746031746031744
maintained by 	 0.5
<s> Despite 	 0.0007686395080707148
the intended 	 0.001384083044982699
devices take 	 0.25
specialized algorithms 	 0.5
own assumptions 	 0.16666666666666666
matter . 	 0.3333333333333333
-- 10 	 0.04
the fixed 	 0.0006920415224913495
Interspeech -RRB- 	 1.0
more strongly 	 0.010526315789473684
recent developments 	 0.125
could understand 	 0.0625
some perception 	 0.012048192771084338
for QA 	 0.010830324909747292
effort , 	 0.25
parse garden-path 	 0.1111111111111111
, bigram 	 0.0016844469399213925
political forums 	 0.3333333333333333
stating that 	 1.0
return ? 	 0.5
Deep approaches 	 1.0
technology providers 	 0.045454545454545456
Advanced Research 	 0.2
may denote 	 0.019230769230769232
, pruned 	 0.0005614823133071309
is largely 	 0.0020325203252032522
Universal Part-of-Speech 	 1.0
to mention 	 0.0013280212483399733
, pre-defined 	 0.0005614823133071309
MT has 	 0.2
on unsupervised 	 0.0047169811320754715
detection of 	 0.5
teams in 	 0.5
coherent discourse 	 0.2
shop or 	 1.0
develop as 	 0.2
on content 	 0.0047169811320754715
behavior of 	 0.5
sentences are 	 0.09210526315789473
a clarification 	 0.001226993865030675
, imagery 	 0.0005614823133071309
sentences flow 	 0.013157894736842105
% or 	 0.02564102564102564
a predefined 	 0.001226993865030675
but article 	 0.014705882352941176
technology , 	 0.13636363636363635
subject -RRB- 	 0.125
support vector 	 0.25
Force and 	 0.5
or produce 	 0.0045045045045045045
favor accuracy 	 0.5
corpus for 	 0.03225806451612903
one instance 	 0.015384615384615385
Language Processing 	 0.25
as it 	 0.003484320557491289
letters . 	 0.4
, statistics 	 0.0011229646266142617
extent -RRB- 	 0.25
or ` 	 0.0045045045045045045
appliance control 	 1.0
computer automated 	 0.022727272727272728
starting to 	 1.0
typology , 	 1.0
<s> Tags 	 0.0007686395080707148
in various 	 0.0056179775280898875
at SRI 	 0.014705882352941176
IMR -RRB- 	 0.5
the work 	 0.001384083044982699
personal computing 	 0.25
originally as 	 0.5
2.0 was 	 0.5
Evaluating summaries 	 1.0
→ dogs 	 0.3333333333333333
of democracy 	 0.00089126559714795
some tasks 	 0.012048192771084338
and classifying 	 0.001445086705202312
the earlier 	 0.0006920415224913495
the speech 	 0.006920415224913495
narrative text 	 1.0
of negative 	 0.00089126559714795
<s> Eight 	 0.0007686395080707148
that minimizes 	 0.0070921985815602835
often , 	 0.022727272727272728
are implemented 	 0.004149377593360996
will approach 	 0.02857142857142857
<s> Since 	 0.0030745580322828594
, you 	 0.0005614823133071309
singular common 	 0.25
an individual 	 0.007575757575757576
phrase structure 	 0.2
form . 	 0.1
implementations of 	 1.0
say whether 	 0.14285714285714285
C. , 	 1.0
marks are 	 0.25
to apply 	 0.0013280212483399733
created by 	 0.2857142857142857
2004 -RRB- 	 0.3333333333333333
implicit assumptions 	 1.0
not remember 	 0.008928571428571428
declared during 	 0.5
recognition software 	 0.024793388429752067
very common 	 0.04878048780487805
lexical statistics 	 0.07692307692307693
Just which 	 1.0
recently there 	 0.3333333333333333
a multi-way 	 0.001226993865030675
disambiguate parts 	 0.3333333333333333
well . 	 0.07142857142857142
make better 	 0.05
where word 	 0.02857142857142857
, Gina 	 0.0005614823133071309
check for 	 0.5
concept into 	 0.25
to error 	 0.0013280212483399733
are broken 	 0.004149377593360996
researchers have 	 0.3
CANDIDE from 	 1.0
closest the 	 0.5
Semantic Orientation 	 0.3333333333333333
Given enough 	 0.07142857142857142
which involves 	 0.007246376811594203
successful HMM-based 	 0.1111111111111111
not obvious 	 0.008928571428571428
principled way 	 1.0
included question-answering 	 0.125
key theorists 	 0.16666666666666666
a recall-based 	 0.001226993865030675
ranked with 	 0.2
possible word 	 0.041666666666666664
languages text 	 0.02
`` speech 	 0.005291005291005291
of pragmatics 	 0.00089126559714795
% -LRB- 	 0.07692307692307693
in NLG 	 0.0056179775280898875
applications have 	 0.08
<s> Natural 	 0.004611837048424289
known cases 	 0.038461538461538464
parsers are 	 0.15384615384615385
automated technologies 	 0.14285714285714285
's methodology 	 0.0196078431372549
such cases 	 0.016260162601626018
has unambiguously 	 0.011904761904761904
this field 	 0.02197802197802198
and metrics 	 0.001445086705202312
before -RRB- 	 0.3333333333333333
connected text 	 0.2
measure one 	 0.09090909090909091
NP-complete . 	 1.0
, volume 	 0.0005614823133071309
point scale 	 0.3333333333333333
separate out 	 0.1
have increased 	 0.028846153846153848
and FAA 	 0.001445086705202312
the choices 	 0.0006920415224913495
languages was 	 0.02
not sufficient 	 0.008928571428571428
compared - 	 0.14285714285714285
we have 	 0.06666666666666667
data on 	 0.012987012987012988
text is 	 0.025157232704402517
`` beyond 	 0.005291005291005291
Annual Test 	 1.0
devised primarily 	 0.5
parse trees 	 0.2222222222222222
A restricted 	 0.02
at helping 	 0.014705882352941176
relay services 	 1.0
a useful 	 0.001226993865030675
and Dale 	 0.001445086705202312
be to 	 0.008438818565400843
processing task 	 0.018518518518518517
constructs -LRB- 	 0.3333333333333333
have much 	 0.009615384615384616
, Ingria 	 0.0005614823133071309
a gold 	 0.00245398773006135
tag of 	 0.0625
standard techniques 	 0.07142857142857142
is coherent 	 0.0020325203252032522
be deployed 	 0.004219409282700422
In some 	 0.0380952380952381
missions . 	 1.0
navigation , 	 0.5
years -LRB- 	 0.047619047619047616
what you 	 0.03125
construct over 	 0.3333333333333333
silence are 	 1.0
pioneered the 	 0.3333333333333333
abstractive summarization 	 0.3333333333333333
centroid sentence 	 0.5
prisoners or 	 0.5
QA performance 	 0.047619047619047616
classification for 	 0.11764705882352941
they also 	 0.025
In 1950 	 0.01904761904761905
may generate 	 0.019230769230769232
` best 	 0.0625
signal can 	 0.3333333333333333
unigrams , 	 0.25
the end 	 0.001384083044982699
magazine 's 	 1.0
Based on 	 1.0
Graph This 	 1.0
entities employed 	 0.14285714285714285
translation MAHT 	 0.013513513513513514
Transactions on 	 1.0
in source 	 0.0018726591760299626
-RRB- approach 	 0.0027100271002710027
cognitive psychology 	 0.5
problem for 	 0.022727272727272728
categories -LRB- 	 0.1111111111111111
1997 , 	 0.5
steered toward 	 1.0
to adjust\/correct 	 0.0013280212483399733
for this 	 0.018050541516245487
typewritten text 	 0.2
, writing 	 0.0011229646266142617
accessibility , 	 1.0
or words 	 0.0045045045045045045
in logical 	 0.0018726591760299626
whereas in 	 0.3333333333333333
'' sentence 	 0.005154639175257732
as separate 	 0.003484320557491289
<s> Apart 	 0.0007686395080707148
has grown 	 0.011904761904761904
Service has 	 1.0
see what 	 0.05
Parsing algorithms 	 0.2
spoken text 	 0.07142857142857142
other structure 	 0.014285714285714285
courses of 	 1.0
, using 	 0.005614823133071308
automatizing the 	 1.0
map to 	 0.5
are parsed 	 0.004149377593360996
of Mandarin 	 0.00089126559714795
a finite 	 0.00245398773006135
trees , 	 0.5
to improve 	 0.01195219123505976
numbers on 	 0.14285714285714285
high pollen 	 0.05555555555555555
<s> Research 	 0.0015372790161414297
Then the 	 0.4
Recognize if 	 1.0
Some scholars 	 0.047619047619047616
most upper 	 0.017241379310344827
linear transform 	 0.14285714285714285
SHRDLU for 	 0.16666666666666666
was all 	 0.012987012987012988
seize the 	 1.0
influence in 	 1.0
sentences of 	 0.02631578947368421
aspect is 	 0.5
Closed-domain question 	 1.0
blogs and 	 0.5
in titles 	 0.0018726591760299626
, biographical 	 0.0005614823133071309
the result 	 0.002768166089965398
arbitrary length 	 0.3333333333333333
, David 	 0.0016844469399213925
nearly anything 	 0.5
up of 	 0.045454545454545456
index entries 	 1.0
potential of 	 0.2857142857142857
example the 	 0.012345679012345678
150 separate 	 0.5
scripts , 	 0.3333333333333333
information overload 	 0.021739130434782608
Mobile telephony 	 0.3333333333333333
recognizing hand-printed 	 0.2
surrounding words 	 0.4
and a 	 0.023121387283236993
stochastic semantic 	 0.125
knowledge specific 	 0.037037037037037035
sources or 	 0.16666666666666666
bunch of 	 1.0
, semantics 	 0.0016844469399213925
'' occur 	 0.005154639175257732
Arabic , 	 0.25
usually separated 	 0.03125
by highly 	 0.005714285714285714
of certain 	 0.00089126559714795
relies on 	 1.0
at Birkbeck 	 0.029411764705882353
NP for 	 1.0
a robotic 	 0.001226993865030675
others , 	 0.08333333333333333
or lowering 	 0.0045045045045045045
relic of 	 1.0
things -RRB- 	 0.3333333333333333
people create 	 0.0625
focus is 	 0.14285714285714285
software that 	 0.037037037037037035
beginning to 	 0.5
in January 	 0.003745318352059925
with values 	 0.01639344262295082
as mentioned 	 0.003484320557491289
involved the 	 0.16666666666666666
distribution of 	 0.25
represented as 	 0.3333333333333333
ambiguous . 	 0.25
sentences ' 	 0.013157894736842105
the screen 	 0.0006920415224913495
markers over 	 0.3333333333333333
methods need 	 0.045454545454545456
http:\/\/haydn.isi.edu\/ROUGE\/ -RRB- 	 1.0
a wave 	 0.0049079754601227
greatly reduced 	 0.14285714285714285
, see 	 0.0011229646266142617
This strategy 	 0.015873015873015872
evaluation requires 	 0.018518518518518517
application where 	 0.07142857142857142
extractive keyphrase 	 0.14285714285714285
evaluation of 	 0.07407407407407407
The more 	 0.005208333333333333
improvement of 	 0.5
helicopter pilot 	 0.25
discourse relationships 	 0.027777777777777776
do predict 	 0.038461538461538464
score based 	 0.16666666666666666
recognize the 	 0.4444444444444444
D. Das 	 0.2
function for 	 0.125
are commonly 	 0.004149377593360996
Sound waves 	 0.3333333333333333
only real 	 0.02631578947368421
Chilton , 	 1.0
a 4 	 0.001226993865030675
negative up 	 0.125
returns text 	 1.0
their spoken 	 0.029411764705882353
test how 	 0.1
emotional communication 	 0.25
linguistic term 	 0.0625
the unsupervised 	 0.0006920415224913495
learning problem 	 0.023255813953488372
the historical 	 0.0006920415224913495
OCR-A font 	 1.0
a slide 	 0.001226993865030675
and Haton 	 0.001445086705202312
name must 	 0.2
pages , 	 0.42857142857142855
such signals 	 0.008130081300813009
, several 	 0.0005614823133071309
constructs can 	 0.3333333333333333
1,500 documents 	 1.0
a preposition 	 0.001226993865030675
such representation 	 0.008130081300813009
getting enough 	 0.25
hypothesis `` 	 1.0
of features 	 0.00089126559714795
Lauriault\/Loriot , 	 1.0
usually abbreviated 	 0.03125
similarities to 	 0.5
A direct 	 0.02
recognition . 	 0.05785123966942149
recognition '' 	 0.01652892561983471
software -LRB- 	 0.037037037037037035
passage web 	 1.0
-RRB- measure 	 0.0027100271002710027
pattern recognition 	 0.6666666666666666
vertices , 	 0.1111111111111111
as Robert 	 0.003484320557491289
as corresponding 	 0.003484320557491289
the meaning 	 0.006920415224913495
Why did 	 0.14285714285714285
groups at 	 0.2
language like 	 0.006756756756756757
need at 	 0.047619047619047616
, both 	 0.0016844469399213925
to disseminate 	 0.0013280212483399733
the known 	 0.0034602076124567475
cosine values 	 0.3333333333333333
QA It 	 0.047619047619047616
technique uses 	 0.14285714285714285
of guessing 	 0.00089126559714795
segment the 	 0.1111111111111111
and echoes 	 0.001445086705202312
into smaller 	 0.01282051282051282
of computerized 	 0.0017825311942959
that generate 	 0.0035460992907801418
and noise 	 0.001445086705202312
A summary 	 0.02
it proved 	 0.008547008547008548
subjective information 	 0.3333333333333333
-LRB- i.e. 	 0.02981029810298103
an abstractive 	 0.015151515151515152
speaker normalization 	 0.05555555555555555
to manipulate 	 0.0026560424966799467
found recognition 	 0.07142857142857142
overlap , 	 0.25
expect answers 	 0.3333333333333333
of 500 	 0.0017825311942959
the extraction 	 0.002768166089965398
other fields 	 0.014285714285714285
modern systems 	 0.2
derive part-of-speech 	 0.5
, Talmy 	 0.0005614823133071309
prisoners ? 	 0.5
Rosa Caldas-Coulthard 	 1.0
Text linguistics 	 0.16666666666666666
might also 	 0.038461538461538464
particular words 	 0.07692307692307693
the moderate 	 0.0020761245674740486
boundaries in 	 0.09090909090909091
to learn 	 0.00796812749003984
quality . 	 0.1
art . 	 0.5
summarizes that 	 1.0
parser and 	 0.0625
stub reader 	 1.0
-RRB- , 	 0.21138211382113822
appear . 	 0.0625
In normal 	 0.009523809523809525
computer-understandable data 	 1.0
algorithms differ 	 0.02857142857142857
CCD flatbed 	 1.0
There is 	 0.2727272727272727
again statistically 	 1.0
the centers 	 0.0006920415224913495
the specification 	 0.001384083044982699
any supervised 	 0.03225806451612903
statistical approach 	 0.030303030303030304
Information extraction 	 0.2
sound impressive 	 0.05
used , 	 0.07079646017699115
in supervised 	 0.0018726591760299626
complexity , 	 0.16666666666666666
Spanish , 	 0.5
and atmosphere 	 0.001445086705202312
units as 	 0.14285714285714285
one may 	 0.015384615384615385
corresponding summaries 	 0.16666666666666666
also marked 	 0.014492753623188406
by The 	 0.005714285714285714
Generally speaking 	 0.4
in parametric 	 0.0018726591760299626
written for 	 0.038461538461538464
phase is 	 1.0
than what 	 0.022222222222222223
see is 	 0.05
of communication 	 0.0017825311942959
, Zellig 	 0.0011229646266142617
and align 	 0.001445086705202312
Knowledge of 	 0.5
, E 	 0.0005614823133071309
Human-machine interaction 	 1.0
quite expensive 	 0.125
to express 	 0.0013280212483399733
warped '' 	 1.0
such phrases 	 0.008130081300813009
Its results 	 0.5
`` higher 	 0.005291005291005291
the ARNS 	 0.0006920415224913495
-LRB- plural 	 0.0027100271002710027
techniques used 	 0.043478260869565216
morphemes and 	 0.3333333333333333
also because 	 0.014492753623188406
POS tagging 	 0.38461538461538464
In recent 	 0.01904761904761905
done in 	 0.45454545454545453
focus on 	 0.5714285714285714
'' 100 	 0.005154639175257732
machine-translation approaches 	 0.5
breathing was 	 1.0
arbitrary new 	 0.3333333333333333
US patent 	 0.2857142857142857
capabilities and 	 0.2
pars -LRB- 	 1.0
, resolve 	 0.0005614823133071309
a database 	 0.0036809815950920245
Translation process 	 0.6666666666666666
and also 	 0.001445086705202312
phrases that 	 0.0625
voice dialing 	 0.07692307692307693
define these 	 0.5
and entered 	 0.001445086705202312
usually can 	 0.03125
them but 	 0.05263157894736842
beforehand -LRB- 	 1.0
spontaneous speech 	 1.0
be distinguished 	 0.004219409282700422
structure . 	 0.16666666666666666
Medical Records 	 0.5
of chatterbots 	 0.00089126559714795
most text 	 0.017241379310344827
Paul W. 	 0.2
social networks 	 0.21428571428571427
two meanings 	 0.034482758620689655
Dragon Systems 	 1.0
with which 	 0.00546448087431694
U.S. program 	 0.14285714285714285
worked by 	 0.2
mining refers 	 0.2
pages getting 	 0.14285714285714285
`` Application-Oriented 	 0.005291005291005291
measures how 	 0.3333333333333333
to both 	 0.00398406374501992
understanding can 	 0.030303030303030304
robust to 	 0.25
and hearings 	 0.001445086705202312
often not 	 0.045454545454545456
the founder 	 0.0006920415224913495
as CLAWS 	 0.003484320557491289
answer reuse 	 0.03333333333333333
interaction with 	 0.125
opinion in 	 0.4
to highly 	 0.0013280212483399733
and on 	 0.002890173410404624
, TNO 	 0.0005614823133071309
conversation or 	 0.25
, has 	 0.0005614823133071309
role of 	 0.25
output -RRB- 	 0.038461538461538464
a toy 	 0.00245398773006135
has proven 	 0.011904761904761904
and final 	 0.001445086705202312
Swedish pilots 	 1.0
size of 	 0.16666666666666666
country into 	 0.25
proper noun 	 0.14285714285714285
there is 	 0.35
Question answering 	 0.2857142857142857
latent semantic 	 1.0
The cache 	 0.005208333333333333
usually asked 	 0.03125
document reader 	 0.027777777777777776
summaries depending 	 0.046511627906976744
Quechua , 	 1.0
quantitative approaches 	 0.25
a printed 	 0.001226993865030675
system on 	 0.010752688172043012
final post-processing 	 0.1111111111111111
the specific 	 0.0020761245674740486
mapping the 	 0.5
drawback of 	 1.0
progress is 	 0.14285714285714285
data in 	 0.025974025974025976
: Dynamic 	 0.00980392156862745
around 2 	 0.125
2006 hurricane 	 0.3333333333333333
IR and 	 0.3333333333333333
issued to 	 1.0
denote abbreviations 	 0.5
has interest 	 0.011904761904761904
, results 	 0.0005614823133071309
Hidden Markov 	 1.0
no subtypes 	 0.07692307692307693
Current QA 	 0.2
in word 	 0.0018726591760299626
time warping 	 0.12121212121212122
or POST 	 0.0045045045045045045
was one 	 0.012987012987012988
were similar 	 0.024390243902439025
for businesses 	 0.0036101083032490976
, Teun 	 0.0005614823133071309
Integration -LRB- 	 1.0
linguistic knowledge 	 0.0625
verbs , 	 0.6
prisoner-of-war camp 	 1.0
associated with 	 0.25
n't for 	 0.25
James Deese 	 0.25
is very 	 0.012195121951219513
other sentences 	 0.014285714285714285
on my 	 0.0047169811320754715
a component 	 0.00245398773006135
Are there 	 1.0
: compare 	 0.00980392156862745
adaptation . 	 0.6666666666666666
varying degrees 	 1.0
Henry Kucera 	 0.5
of Eastern 	 0.00089126559714795
neural approaches 	 0.06666666666666667
close the 	 1.0
these sounds 	 0.023809523809523808
information appear 	 0.021739130434782608
earlier in 	 0.25
or after 	 0.0045045045045045045
sound input 	 0.05
Janet Kolodner 	 0.5
learning and 	 0.023255813953488372
two general 	 0.034482758620689655
methods related 	 0.022727272727272728
summary and 	 0.047619047619047616
person may 	 0.05263157894736842
the N-best 	 0.0006920415224913495
Call home 	 1.0
controllers -LRB- 	 0.3333333333333333
phrase -LRB- 	 0.1
Gail Jefferson 	 1.0
ranking process 	 0.14285714285714285
characters can 	 0.1875
weather reports 	 0.2857142857142857
Workshop Hirschman 	 1.0
-LRB- especially 	 0.005420054200542005
may vary 	 0.019230769230769232
reason is 	 0.25
Markov models 	 0.3333333333333333
: Speech 	 0.00980392156862745
<s> Also 	 0.0023059185242121443
are too 	 0.004149377593360996
credit card 	 1.0
these summaries 	 0.047619047619047616
Early work 	 0.5
thought or 	 0.3333333333333333
for disambiguation 	 0.0036101083032490976
researchers must 	 0.1
extremely difficult 	 0.75
accuracy will 	 0.03225806451612903
language require 	 0.006756756756756757
or serving 	 0.0045045045045045045
Treebank . 	 0.3333333333333333
topics or 	 0.14285714285714285
be approached 	 0.004219409282700422
in systems 	 0.003745318352059925
the EVALITA 	 0.0006920415224913495
New Orleans 	 1.0
component of 	 0.6
as computational 	 0.003484320557491289
word delimiter 	 0.016666666666666666
-LRB- a 	 0.013550135501355014
tag-sets . 	 1.0
the noun 	 0.0006920415224913495
as `` 	 0.04878048780487805
dialog that 	 0.5
larger group 	 0.0625
intervals like 	 1.0
in classifying 	 0.0018726591760299626
To address 	 0.1111111111111111
and ICR 	 0.002890173410404624
length using 	 0.125
translated as 	 0.25
difficult to 	 0.39285714285714285
to act 	 0.0026560424966799467
also used 	 0.028985507246376812
the main 	 0.001384083044982699
, relationship 	 0.0005614823133071309
not predict 	 0.008928571428571428
improve . 	 0.07692307692307693
Linguistics -LRB- 	 0.3333333333333333
A feature 	 0.02
: Extract 	 0.00980392156862745
count of 	 0.4
relations to 	 0.08333333333333333
the LexRank 	 0.0006920415224913495
which describe 	 0.007246376811594203
difficulty is 	 0.14285714285714285
<s> POS-tagging 	 0.0007686395080707148
stress and 	 0.5
socio-psychological characteristics 	 1.0
and commercial 	 0.001445086705202312
characteristics of 	 0.5
corp. . 	 1.0
that shift 	 0.0035460992907801418
robust when 	 0.5
corpus -RRB- 	 0.12903225806451613
of canned 	 0.00089126559714795
its component 	 0.02857142857142857
<s> Beatrice 	 0.0007686395080707148
norm . 	 1.0
<s> Subsequently 	 0.0007686395080707148
popular example 	 0.1111111111111111
which parts 	 0.007246376811594203
against noise 	 0.2
and Spanish 	 0.001445086705202312
how phrases 	 0.034482758620689655
clean hand-printed 	 0.5
language constraints 	 0.006756756756756757
warping Dynamic 	 0.25
systems however 	 0.008928571428571428
Current state 	 0.2
accuracy above 	 0.03225806451612903
tell it 	 0.3333333333333333
generate a 	 0.3333333333333333
tried . 	 0.3333333333333333
<s> POS 	 0.0007686395080707148
in recent 	 0.003745318352059925
accepts a 	 0.5
technology useful 	 0.045454545454545456
transformation , 	 1.0
methods already 	 0.022727272727272728
from speech 	 0.009615384615384616
lexical segments 	 0.07692307692307693
and represented 	 0.001445086705202312
metric such 	 0.3333333333333333
registry . 	 1.0
on each 	 0.0047169811320754715
minimal complexity 	 1.0
for instance 	 0.018050541516245487
plus some 	 1.0
Such perceptions 	 0.125
Telematics -LRB- 	 1.0
that should 	 0.0070921985815602835
character-by-character OCR 	 1.0
treat words 	 0.5
, Vito 	 0.0005614823133071309
the country 	 0.0020761245674740486
in their 	 0.00749063670411985
Systems -RRB- 	 0.08333333333333333
The paradigm 	 0.005208333333333333
Pollen counts 	 1.0
operated successfully 	 0.5
knowledge and 	 0.07407407407407407
support question 	 0.25
decoding is 	 1.0
the phenomenon 	 0.001384083044982699
speed is 	 0.14285714285714285
of corpora 	 0.00089126559714795
can have 	 0.011049723756906077
generators of 	 0.5
appears that 	 0.2
Topic segmentation 	 1.0
specific example 	 0.047619047619047616
this can 	 0.01098901098901099
clues not 	 0.3333333333333333
, does 	 0.0011229646266142617
coherent sequences 	 0.2
documents than 	 0.02631578947368421
part of 	 0.8148148148148148
in itself 	 0.0018726591760299626
Bar-Hillel . 	 1.0
dissertation at 	 0.3333333333333333
as length 	 0.003484320557491289
of document\/text 	 0.00089126559714795
processing . 	 0.12962962962962962
The recently 	 0.005208333333333333
Corporation originally 	 0.25
the breadth 	 0.0006920415224913495
evaluation is 	 0.07407407407407407
the mean 	 0.0006920415224913495
is felt 	 0.0020325203252032522
Apple Newton 	 1.0
scholars have 	 0.5
probabilities would 	 0.09090909090909091
Dependence vs. 	 1.0
while Carnegie 	 0.05
that participate 	 0.0035460992907801418
and 2500 	 0.001445086705202312
pruned to 	 1.0
tagging systems 	 0.04
a translator 	 0.0036809815950920245
and interactive 	 0.001445086705202312
solved problem 	 0.4
and evaluation 	 0.004335260115606936
and interjection 	 0.001445086705202312
more deterministic 	 0.021052631578947368
for using 	 0.0036101083032490976
structured speech 	 0.16666666666666666
be assumed 	 0.004219409282700422
can find 	 0.0055248618784530384
lexical analysis 	 0.07692307692307693
Modern general-purpose 	 0.3333333333333333
as with 	 0.006968641114982578
blocks world 	 0.25
service . 	 0.2
to direct 	 0.0013280212483399733
perhaps trivial 	 0.16666666666666666
first raised 	 0.030303030303030304
an upper-case 	 0.007575757575757576
Romanseval campaigns 	 1.0
-- including 	 0.04
systems simply 	 0.008928571428571428
corresponded to 	 1.0
can tell 	 0.0055248618784530384
<s> LUNAR 	 0.0007686395080707148
features describing 	 0.038461538461538464
responsible for 	 1.0
difficult problems 	 0.10714285714285714
paper-to-computer text 	 1.0
for breathing 	 0.0036101083032490976
interpreter . 	 0.5
and social 	 0.004335260115606936
better decisions 	 0.1111111111111111
ranks the 	 0.5
to some 	 0.006640106241699867
examples can 	 0.041666666666666664
module looks 	 0.3333333333333333
we say 	 0.044444444444444446
a web 	 0.001226993865030675
their more 	 0.029411764705882353
Relationship extraction 	 1.0
counterparts in 	 1.0
translation when 	 0.013513513513513514
their input 	 0.029411764705882353
level of 	 0.35
that characterize 	 0.0035460992907801418
connected directly 	 0.2
different profile 	 0.02040816326530612
error rates 	 0.25
by Ask.com 	 0.005714285714285714
semantic representation 	 0.047619047619047616
on less 	 0.0047169811320754715
approaches assume 	 0.03571428571428571
grammar -RRB- 	 0.08108108108108109
for up-to-date 	 0.0036101083032490976
to reflect 	 0.0013280212483399733
understanding : 	 0.030303030303030304
of possible 	 0.00267379679144385
Intelligence Corporation 	 0.3333333333333333
defined , 	 0.16666666666666666
sophisticated understanding 	 0.14285714285714285
focus to 	 0.14285714285714285
into an 	 0.02564102564102564
coherent and 	 0.2
complicating the 	 1.0
, cited 	 0.0005614823133071309
a fluent 	 0.001226993865030675
run an 	 0.2
fulfill the 	 0.5
Church used 	 0.3333333333333333
data source 	 0.012987012987012988
converting the 	 0.5
the culture 	 0.0006920415224913495
much time 	 0.045454545454545456
be digitalized 	 0.004219409282700422
done both 	 0.09090909090909091
or function 	 0.0045045045045045045
Recall measures 	 0.3333333333333333
Keyphrase Extraction 	 0.25
i.e. relationship 	 0.05263157894736842
for sentence 	 0.0036101083032490976
the detection 	 0.0006920415224913495
not President 	 0.008928571428571428
and nouns 	 0.001445086705202312
questions and 	 0.038461538461538464
Paul Gee 	 0.2
using will 	 0.01694915254237288
unfamiliar input 	 1.0
survey of 	 1.0
of hand-printed 	 0.0017825311942959
, Art 	 0.0005614823133071309
right . 	 0.3
evaluation workshops 	 0.018518518518518517
are language-specific 	 0.004149377593360996
the expectancy 	 0.0006920415224913495
may dismiss 	 0.019230769230769232
usually done 	 0.0625
this case 	 0.01098901098901099
for real-world 	 0.007220216606498195
discussions about 	 0.3333333333333333
work from 	 0.041666666666666664
context . 	 0.21212121212121213
used over 	 0.008849557522123894
speaker of 	 0.1111111111111111
often much 	 0.022727272727272728
, plus 	 0.0005614823133071309
English speaking 	 0.02702702702702703
arithmetic expression 	 1.0
2,000 or 	 0.5
an utterance 	 0.015151515151515152
at Yale 	 0.029411764705882353
early precursor 	 0.1
than 1 	 0.022222222222222223
discourse turns 	 0.027777777777777776
factors , 	 0.3333333333333333
of co-articulation 	 0.00089126559714795
Direct Voice 	 1.0
are ambiguous 	 0.004149377593360996
vocal tract 	 1.0
, speed 	 0.0005614823133071309
search -LRB- 	 0.09090909090909091
'' vertex 	 0.005154639175257732
Tablet PC 	 1.0
and sentences 	 0.002890173410404624
was greatly 	 0.012987012987012988
-LRB- OCR 	 0.0027100271002710027
profiling for 	 1.0
wave in 	 0.1111111111111111
is routed 	 0.0040650406504065045
grammar in 	 0.02702702702702703
a Computer 	 0.001226993865030675
Robinson , 	 1.0
Full of 	 1.0
a generated 	 0.001226993865030675
plateaued and 	 1.0
-- often 	 0.04
the grammar 	 0.006228373702422145
why automatic 	 0.14285714285714285
undertaken , 	 0.5
Maximal Marginal 	 1.0
impossible . 	 0.5
functional languages 	 0.5
Avionics Research 	 1.0
identical to 	 1.0
that requires 	 0.0070921985815602835
of whole 	 0.0017825311942959
automatically learning 	 0.047619047619047616
sentenced separated 	 1.0
Digitize the 	 1.0
It refers 	 0.02631578947368421
automatically learn 	 0.09523809523809523
which kind 	 0.007246376811594203
human variability 	 0.021739130434782608
situation where 	 0.5
memory Political 	 0.5
American Recovery 	 0.2
paragraphs in 	 0.25
semantics without 	 0.07142857142857142
good insight 	 0.07692307692307693
representation and 	 0.10526315789473684
features into 	 0.038461538461538464
2007 is 	 0.2
Deaf or 	 1.0
of work 	 0.00089126559714795
reading text 	 0.125
commonly researched 	 0.125
using `` 	 0.01694915254237288
words '' 	 0.01834862385321101
for what 	 0.007220216606498195
a modal 	 0.001226993865030675
develop innovative 	 0.2
or nature 	 0.0045045045045045045
the gold 	 0.0020761245674740486
Language Workshop 	 0.08333333333333333
humans to 	 0.08333333333333333
expect that 	 0.3333333333333333
any domain 	 0.03225806451612903
intensive as 	 1.0
pattern has 	 0.16666666666666666
the SIGGEN 	 0.0006920415224913495
several million 	 0.045454545454545456
is widely 	 0.0040650406504065045
Another approach 	 0.15384615384615385
then applies 	 0.02857142857142857
Post has 	 0.5
to physicians 	 0.0013280212483399733
a typical 	 0.00245398773006135
real time 	 0.1111111111111111
a limited 	 0.00245398773006135
, weather 	 0.0005614823133071309
improvement by 	 0.25
for Larry 	 0.0036101083032490976
project were 	 0.07692307692307693
be ? 	 0.004219409282700422
, speech 	 0.004491858506457047
Management command 	 1.0
between computers 	 0.02564102564102564
or future 	 0.0045045045045045045
evaluators . 	 1.0
either as 	 0.3
described here 	 0.16666666666666666
been written 	 0.014705882352941176
automated language 	 0.14285714285714285
at NYU 	 0.014705882352941176
source language 	 0.125
with them 	 0.01639344262295082
'' This 	 0.005154639175257732
more difficult 	 0.07368421052631578
successively more 	 1.0
learning Beginning 	 0.023255813953488372
relief is 	 1.0
authenticate or 	 1.0
Examples are 	 0.6666666666666666
major database 	 0.08333333333333333
context to 	 0.030303030303030304
it aims 	 0.008547008547008548
H. Levinsohn 	 0.5
scoring function 	 0.5
Grishman R. 	 1.0
`` Dog 	 0.005291005291005291
on general 	 0.0047169811320754715
places , 	 0.5
but was 	 0.014705882352941176
Advanced Fighter 	 0.2
analysis systems 	 0.015384615384615385
with adjacent 	 0.00546448087431694
proved similarly 	 0.3333333333333333
hand , 	 0.5
classify a 	 0.5
as their 	 0.006968641114982578
because many 	 0.03333333333333333
assignment of 	 0.5
as weather 	 0.003484320557491289
These two 	 0.058823529411764705
only 10 	 0.02631578947368421
Leeuwen , 	 1.0
'' will 	 0.005154639175257732
big green 	 0.5
compared phrase-structure 	 0.14285714285714285
as those 	 0.017421602787456445
, LUNAR 	 0.0005614823133071309
choice with 	 0.125
features of 	 0.15384615384615385
samples from 	 0.5
ambiguous context-free 	 0.08333333333333333
meaningful relationships 	 0.125
extraction system 	 0.06451612903225806
Guy Cook 	 1.0
post-processing by 	 0.3333333333333333
are time-consuming 	 0.004149377593360996
gonna do 	 1.0
different meanings 	 0.02040816326530612
of modern 	 0.00089126559714795
perhaps surprisingly 	 0.16666666666666666
identified in 	 0.2
equivalence is 	 0.5
total number 	 0.5
corpora in 	 0.09090909090909091
reading is 	 0.125
Important journals 	 1.0
estimated probability 	 1.0
too similar 	 0.16666666666666666
some grammar 	 0.012048192771084338
government . 	 0.3333333333333333
and Canada 	 0.001445086705202312
non-annotated data 	 1.0
podcast where 	 1.0
actual forecast 	 0.2
ranking over 	 0.14285714285714285
the proliferation 	 0.0006920415224913495
prone to 	 1.0
that NLG 	 0.0070921985815602835
simultaneously meets 	 0.5
lack of 	 1.0
rules from 	 0.023255813953488372
both use 	 0.03225806451612903
written by 	 0.23076923076923078
as some 	 0.003484320557491289
or speed 	 0.0045045045045045045
analysis model 	 0.015384615384615385
me the 	 1.0
`` Computer 	 0.005291005291005291
of complexity 	 0.00089126559714795
answer candidate 	 0.03333333333333333
a routing 	 0.001226993865030675
of dependency 	 0.00089126559714795
he went 	 0.2857142857142857
supervised extractive 	 0.0625
task , 	 0.09523809523809523
of digital 	 0.00089126559714795
task can 	 0.023809523809523808
normal speech 	 0.5
and rule 	 0.001445086705202312
called recursively 	 0.05555555555555555
Intelligent '' 	 0.3333333333333333
<s> , 	 0.0007686395080707148
1984 . 	 1.0
and discuss 	 0.001445086705202312
include distinct 	 0.037037037037037035
, English-like 	 0.0005614823133071309
-LRB- various 	 0.0027100271002710027
fonts , 	 0.3333333333333333
shape of 	 1.0
Types of 	 1.0
Another term 	 0.07692307692307693
Methods such 	 0.25
Rate -LRB- 	 1.0
intended emotional 	 0.2
a gradually 	 0.001226993865030675
recognition Main 	 0.008264462809917356
accurate transcription 	 0.14285714285714285
that without 	 0.0035460992907801418
selling a 	 1.0
<s> Topics 	 0.0007686395080707148
will not 	 0.11428571428571428
given -LRB- 	 0.041666666666666664
-LRB- monetary 	 0.0027100271002710027
the smaller 	 0.0006920415224913495
that apply 	 0.0035460992907801418
sample of 	 0.3333333333333333
Communications -LRB- 	 1.0
about 30 	 0.025
ME -RRB- 	 0.5
formally expressed 	 0.5
using natural 	 0.01694915254237288
by using 	 0.017142857142857144
from spelling 	 0.009615384615384616
a appropriate 	 0.001226993865030675
knowledge on 	 0.037037037037037035
abilities . 	 1.0
<s> Many 	 0.008455034588777863
analyzed with 	 0.2
HMM-based approach 	 0.6666666666666666
times in 	 0.2
Morse Code 	 1.0
software technology 	 0.037037037037037035
answered about 	 0.2
dataset -RRB- 	 1.0
shown in 	 0.4
transfer-based machine 	 0.6666666666666666
operational settings 	 1.0
paragraphs . 	 0.25
Challenges shared-task 	 1.0
earlier term 	 0.25
contents of 	 1.0
important . 	 0.0625
news conference 	 0.07692307692307693
words involved 	 0.009174311926605505
fonts used 	 0.3333333333333333
of contextual 	 0.00089126559714795
but machines 	 0.014705882352941176
technology This 	 0.045454545454545456
relationships . 	 0.16666666666666666
task . 	 0.23809523809523808
's Mars 	 0.0196078431372549
independent systems 	 0.5
with specific 	 0.00546448087431694
on their 	 0.009433962264150943
web site 	 0.25
and are 	 0.0072254335260115606
the meeting 	 0.0006920415224913495
a -5 	 0.001226993865030675
topics automatically 	 0.14285714285714285
questioner 's 	 0.25
translation at 	 0.013513513513513514
more reliable 	 0.031578947368421054
lexicon representation 	 0.1111111111111111
<s> Their 	 0.0015372790161414297
using elements 	 0.01694915254237288
human vocabularies 	 0.021739130434782608
More recently 	 0.1111111111111111
becomes easier 	 0.5
-LRB- 95 	 0.0027100271002710027
is often 	 0.022357723577235773
appropriately spelled 	 0.5
cognition and 	 1.0
baseball league 	 1.0
character . 	 0.045454545454545456
distance , 	 0.6666666666666666
the LOB 	 0.0006920415224913495
calls instead 	 1.0
speech tagger 	 0.006578947368421052
, Edmund 	 0.0005614823133071309
: Interlingual 	 0.00980392156862745
, NP 	 0.0005614823133071309
and grammar 	 0.002890173410404624
+ R 	 0.16666666666666666
waves would 	 0.14285714285714285
This covers 	 0.031746031746031744
words that 	 0.009174311926605505
to automatizing 	 0.0013280212483399733
needed to 	 0.09523809523809523
As access 	 0.05555555555555555
that contains 	 0.010638297872340425
original scanned 	 0.07692307692307693
<s> OCR 	 0.0030745580322828594
sentiment with 	 0.04
Post Office 	 0.5
mine the 	 1.0
of converting 	 0.0017825311942959
which found 	 0.014492753623188406
are by 	 0.004149377593360996
organization of 	 0.4
additional citations 	 0.16666666666666666
bites '' 	 0.3333333333333333
avoids overfitting 	 1.0
purpose graph-based 	 0.4
the exception 	 0.0006920415224913495
parameters related 	 0.25
same time 	 0.12
<s> Rules 	 0.0007686395080707148
next four 	 0.14285714285714285
this information 	 0.01098901098901099
connected regions 	 0.2
like Chinese 	 0.07142857142857142
not agree 	 0.008928571428571428
making a 	 0.14285714285714285
PageRank\/TextRank on 	 1.0
list , 	 0.09090909090909091
defined by 	 0.16666666666666666
<s> Isolated 	 0.0007686395080707148
campaign is 	 0.2
In such 	 0.009523809523809525
belongs to 	 1.0
sentences presented 	 0.013157894736842105
output . 	 0.15384615384615385
one video 	 0.015384615384615385
that approximate 	 0.0035460992907801418
a binary 	 0.00245398773006135
of mouse 	 0.00089126559714795
be viewed 	 0.016877637130801686
hearings -RRB- 	 1.0
utilize large 	 0.5
binary judgement 	 0.25
processing uses 	 0.018518518518518517
for understanding 	 0.0036101083032490976
pioneered this 	 0.3333333333333333
of war 	 0.00089126559714795
Savic Naomi 	 1.0
asked and 	 0.3333333333333333
are able 	 0.012448132780082987
focusing on 	 1.0
than one 	 0.06666666666666667
sampling rate 	 1.0
when discussing 	 0.05714285714285714
approach that 	 0.05714285714285714
text and 	 0.018867924528301886
concerns finding 	 0.5
continue to 	 1.0
<s> Specifically 	 0.0007686395080707148
and here 	 0.001445086705202312
notion that 	 0.25
Vito Technology 	 1.0
be defined 	 0.004219409282700422
structure of 	 0.3333333333333333
and semantics 	 0.004335260115606936
will determine 	 0.02857142857142857
of sublanguage 	 0.0017825311942959
whom ? 	 0.5
million books 	 0.3333333333333333
Word Error 	 0.14285714285714285
have human-made 	 0.009615384615384616
startlingly human-like 	 1.0
planning an 	 0.5
7 % 	 0.14285714285714285
1991 -RRB- 	 0.6666666666666666
not capitalize 	 0.008928571428571428
systems sold 	 0.008928571428571428
hard task 	 0.16666666666666666
because punctuation 	 0.03333333333333333
the water 	 0.0006920415224913495
networks allow 	 0.07142857142857142
on new 	 0.0047169811320754715
target handover 	 0.09090909090909091
, would 	 0.0016844469399213925
This device 	 0.015873015873015872
case in 	 0.058823529411764705
<s> -LRB- 	 0.014604150653343582
patient '' 	 1.0
word accuracies 	 0.016666666666666666
in 1989 	 0.0018726591760299626
of rocks 	 0.00089126559714795
The Brill 	 0.005208333333333333
learning that 	 0.023255813953488372
card OCR 	 0.25
, document 	 0.0005614823133071309
increasingly difficult 	 0.3333333333333333
to each 	 0.006640106241699867
Potter , 	 1.0
on attaching 	 0.009433962264150943
different people 	 0.02040816326530612
of annotated 	 0.00089126559714795
more widespread 	 0.010526315789473684
candidate can 	 0.3333333333333333
costly training 	 1.0
'' `` 	 0.005154639175257732
generated is 	 0.06666666666666667
relationships of 	 0.16666666666666666
formalization of 	 0.5
Summarizers -LRB- 	 1.0
requiring all 	 0.5
control of 	 0.6
way that 	 0.125
clarification of 	 0.6666666666666666
interaction The 	 0.125
other applications 	 0.014285714285714285
between 1964 	 0.02564102564102564
file to 	 1.0
be weighted 	 0.004219409282700422
then applied 	 0.05714285714285714
and questions 	 0.001445086705202312
be understood 	 0.004219409282700422
technology that 	 0.045454545454545456
noun -LRB- 	 0.07142857142857142
coverage , 	 0.3333333333333333
for continuous 	 0.0036101083032490976
determine its 	 0.08695652173913043
an advanced 	 0.007575757575757576
taught the 	 0.6666666666666666
with increasing 	 0.00546448087431694
Knowledge on 	 0.5
can present 	 0.0055248618784530384
involves the 	 0.2
have keyphrases 	 0.019230769230769232
range from 	 0.2857142857142857
language Prolog 	 0.006756756756756757
pragmatics to 	 0.3333333333333333
David R. 	 0.25
is permuted 	 0.0020325203252032522
cases one 	 0.05555555555555555
newswire reports 	 1.0
it is 	 0.20512820512820512
they require 	 0.05
Sonic Extractor 	 1.0
; This 	 0.02127659574468085
them for 	 0.05263157894736842
reference to 	 0.25
sentence-end after 	 1.0
Wide Web 	 1.0
green fire 	 1.0
the number 	 0.004844290657439446
<s> Effective 	 0.0007686395080707148
compare them 	 0.14285714285714285
also quite 	 0.014492753623188406
if one 	 0.03571428571428571
analyzing written 	 0.2
- -RRB- 	 0.0625
small range 	 0.1111111111111111
the difficulties 	 0.0006920415224913495
With continuous 	 0.14285714285714285
that adaptation 	 0.0035460992907801418
summaries -RRB- 	 0.023255813953488372
services , 	 0.3333333333333333
the opinion 	 0.0006920415224913495
confirmed by 	 1.0
see the 	 0.1
Basic sound 	 1.0
specific letters 	 0.047619047619047616
a frame 	 0.001226993865030675
to predict 	 0.0026560424966799467
sentence -LRB- 	 0.020833333333333332
entropy , 	 0.2
the rules 	 0.0034602076124567475
VITO Voice2Go 	 1.0
fundamental errors 	 0.5
relevant summaries 	 0.14285714285714285
rules are 	 0.023255813953488372
handwritten , 	 0.5
and morphology 	 0.001445086705202312
, flexibility 	 0.0005614823133071309
processing is 	 0.037037037037037035
comprehensive knowledge 	 0.2
achieves its 	 0.5
Brown University 	 0.14285714285714285
the completion 	 0.0006920415224913495
by which 	 0.005714285714285714
The extrinsic 	 0.005208333333333333
graph . 	 0.15384615384615385
question processing 	 0.07142857142857142
from any 	 0.009615384615384616
teams to 	 0.5
applications can 	 0.04
advanced '' 	 0.2
to the 	 0.10225763612217796
-- to 	 0.04
text map 	 0.006289308176100629
into intrinsic 	 0.01282051282051282
-LRB- ATN 	 0.0027100271002710027
one word 	 0.03076923076923077
have shown 	 0.009615384615384616
common tag 	 0.04
encode in 	 1.0
Q&A systems 	 1.0
that some 	 0.014184397163120567
practice , 	 0.5
Richard Kittredge 	 1.0
perfect -LRB- 	 1.0
expensive task 	 0.14285714285714285
sentence is 	 0.041666666666666664
training on 	 0.03571428571428571
Annotate the 	 1.0
often of 	 0.022727272727272728
which consists 	 0.007246376811594203
layer of 	 1.0
to better 	 0.00398406374501992
can learn 	 0.0055248618784530384
evaluation criteria 	 0.037037037037037035
for those 	 0.007220216606498195
He pointed 	 0.125
off-line character 	 1.0
trained hidden 	 0.3333333333333333
ISRI -RRB- 	 1.0
specific summarization 	 0.047619047619047616
the Advanced 	 0.0006920415224913495
be used 	 0.08016877637130802
new utterance 	 0.041666666666666664
applications discussed 	 0.04
order to 	 0.5714285714285714
and placed 	 0.001445086705202312
for without 	 0.0036101083032490976
speech the 	 0.006578947368421052
or a 	 0.08558558558558559
, outputting 	 0.0005614823133071309
turn a 	 0.16666666666666666
immediately to 	 1.0
How to 	 0.2857142857142857
conventional computer 	 1.0
information needed 	 0.021739130434782608
reasons . 	 0.5
data mining 	 0.025974025974025976
the form 	 0.0006920415224913495
content that 	 0.08333333333333333
but we 	 0.014705882352941176
also referred 	 0.014492753623188406
still just 	 0.06666666666666667
proposed by 	 0.1111111111111111
show the 	 1.0
Jones Street 	 1.0
deciding whether 	 0.3333333333333333
Each word 	 0.16666666666666666
Schools commonly 	 1.0
system should 	 0.010752688172043012
used OCR 	 0.008849557522123894
nodes represents 	 0.14285714285714285
expression just 	 0.1
Named entity 	 1.0
need as 	 0.047619047619047616
compiled newswire 	 1.0
at input 	 0.014705882352941176
when moved 	 0.02857142857142857
found Intelligent 	 0.07142857142857142
proliferation of 	 1.0
known as 	 0.38461538461538464
Transcription -LRB- 	 1.0
written , 	 0.038461538461538464
changes which 	 1.0
a given 	 0.014723926380368098
digital dictation 	 0.14285714285714285
concepts are 	 0.4
evaluation data 	 0.018518518518518517
found in 	 0.21428571428571427
-- in 	 0.04
each one 	 0.044444444444444446
Lisp hence 	 1.0
entries for 	 0.5
several qualities 	 0.045454545454545456
the opinions 	 0.0006920415224913495
study based 	 0.25
three to 	 0.3333333333333333
words into 	 0.03669724770642202
<s> Technologies 	 0.0007686395080707148
are `` 	 0.004149377593360996
`` supervised 	 0.026455026455026454
ELIZA was 	 0.1111111111111111
the feature\/aspect-based 	 0.0006920415224913495
must be 	 0.42857142857142855
OnlineOCR practically 	 0.3333333333333333
manipulate it 	 0.3333333333333333
creating an 	 0.2857142857142857
rules of 	 0.09302325581395349
appear , 	 0.0625
scanning solution 	 0.5
As described 	 0.05555555555555555
For individuals 	 0.01639344262295082
follows a 	 0.5
additional evidence 	 0.16666666666666666
mainly came 	 0.16666666666666666
likelihood linear 	 0.6666666666666666
English POS-taggers 	 0.02702702702702703
<s> More 	 0.006149116064565719
-LRB- EBMT 	 0.0027100271002710027
with each 	 0.00546448087431694
150,000 words 	 1.0
more input 	 0.010526315789473684
put this 	 0.25
that dispense 	 0.0035460992907801418
that corresponded 	 0.0035460992907801418
recent development 	 0.125
term first 	 0.05555555555555555
critical that 	 0.25
most successful 	 0.034482758620689655
code readers 	 0.14285714285714285
to NLP 	 0.0013280212483399733
iteration , 	 1.0
a US 	 0.00245398773006135
describing language 	 0.25
recognition is 	 0.0743801652892562
Syphon -LRB- 	 1.0
automatically do 	 0.047619047619047616
the sequence 	 0.0006920415224913495
ASR in 	 0.5
or what 	 0.009009009009009009
reasons that 	 0.5
technology for 	 0.09090909090909091
diversity during 	 0.25
within a 	 0.2777777777777778
world , 	 0.06666666666666667
errors in 	 0.2
what original 	 0.03125
than supervised 	 0.022222222222222223
psychologist . 	 1.0
entrants companies 	 1.0
method simply 	 0.0625
disagree with 	 0.3333333333333333
of laws 	 0.00089126559714795
input ; 	 0.024390243902439025
<s> Maximum 	 0.0015372790161414297
output with 	 0.038461538461538464
legal documents 	 0.3333333333333333
Corpus developed 	 0.0625
1990 dissertation 	 0.3333333333333333
text-to-speech and 	 0.5
paper is 	 0.09090909090909091
to have 	 0.013280212483399735
too -RRB- 	 0.16666666666666666
Corpus . 	 0.0625
and discontinuous 	 0.001445086705202312
WordNet , 	 0.5
DeRose 's 	 0.4
discussing what 	 0.5
is unusual 	 0.0020325203252032522
clip of 	 1.0
passages . 	 0.5
entertaining can 	 0.5
whether they 	 0.07692307692307693
shifted priorities 	 1.0
be NP-complete 	 0.004219409282700422
and proper 	 0.001445086705202312
English phrase 	 0.02702702702702703
followed perhaps 	 0.25
formalization . 	 0.5
ambitious projects 	 1.0
lip-synch timing 	 1.0
vendors speech 	 0.25
the BORIS 	 0.0006920415224913495
document\/text summarization 	 0.5
responding to 	 1.0
a text 	 0.01717791411042945
multileveled pattern 	 1.0
<s> Alternatively 	 0.0015372790161414297
conversational content 	 1.0
- NC 	 0.0625
the neural-network 	 0.0006920415224913495
robot in 	 0.5
Transfer-based machine 	 1.0
LL parsers 	 1.0
<s> Recent 	 0.0023059185242121443
fields such 	 0.16666666666666666
yes-no question 	 1.0
task-effectiveness at 	 0.5
of pairs 	 0.0017825311942959
issue in 	 0.125
one observation 	 0.015384615384615385
NLP system 	 0.0851063829787234
a shortened 	 0.001226993865030675
dog '' 	 0.3333333333333333
Joe Biden 	 1.0
accuracy may 	 0.03225806451612903
, on 	 0.0039303761931499155
turn also 	 0.16666666666666666
possible , 	 0.125
choices Designing 	 0.2
training organizations 	 0.03571428571428571
corpus is 	 0.03225806451612903
, impressive 	 0.0005614823133071309
about 95 	 0.025
this data 	 0.01098901098901099
of languages 	 0.00089126559714795
to appear 	 0.0026560424966799467
Sentiment Analysis 	 0.16666666666666666
Generation Challenges 	 0.5
cares about 	 1.0
product was 	 0.14285714285714285
TextRank , 	 0.14285714285714285
presented in 	 0.5
adverb , 	 1.0
date -LRB- 	 0.6666666666666666
presented . 	 0.16666666666666666
extraction of 	 0.0967741935483871
hopefully better 	 1.0
every 10 	 0.3333333333333333
backward , 	 1.0
of his 	 0.00267379679144385
frequency -LRB- 	 0.5
employs rule-based 	 0.5
listens for 	 1.0
and Language 	 0.005780346820809248
was influenced 	 0.012987012987012988
<s> Warren 	 0.0007686395080707148
can benefit 	 0.011049723756906077
<s> Dictionary-based 	 0.0007686395080707148
corpora are 	 0.18181818181818182
classification looks 	 0.058823529411764705
reasoning for 	 0.14285714285714285
and subsequent 	 0.001445086705202312
valuable new 	 0.5
-RRB- would 	 0.005420054200542005
represent natural 	 0.1111111111111111
-RRB- Marc 	 0.0027100271002710027
of distinct 	 0.00089126559714795
rules similar 	 0.046511627906976744
discourse analysts 	 0.05555555555555555
devoted to 	 0.6
difficult and 	 0.03571428571428571
, these 	 0.0011229646266142617
elaborate theories 	 1.0
new opportunities 	 0.041666666666666664
-LRB- 2000 	 0.0027100271002710027
<s> Envelopes 	 0.0007686395080707148
summarization -RRB- 	 0.02
slang . 	 1.0
that was 	 0.010638297872340425
vary with 	 0.16666666666666666
abstractive keyphrase 	 0.16666666666666666
' is 	 0.05263157894736842
which to 	 0.007246376811594203
its best 	 0.02857142857142857
perspective , 	 0.25
talk page 	 1.0
was made 	 0.012987012987012988
also lead 	 0.014492753623188406
Units -LRB- 	 1.0
sentence with 	 0.020833333333333332
of charge 	 0.00089126559714795
categories could 	 0.1111111111111111
However even 	 0.02702702702702703
text generation 	 0.006289308176100629
examples ? 	 0.041666666666666664
register by 	 1.0
not trivial 	 0.008928571428571428
until 1970 	 0.5
deduction to 	 1.0
trade speed 	 0.5
of optical 	 0.00089126559714795
helping people 	 1.0
: Convert 	 0.0196078431372549
the way 	 0.002768166089965398
a more 	 0.0049079754601227
, statement 	 0.0005614823133071309
the Romance 	 0.0006920415224913495
with keyphrases 	 0.00546448087431694
without significant 	 0.07692307692307693
tense , 	 0.5
or they 	 0.0045045045045045045
aid in 	 0.75
work well 	 0.041666666666666664
the strength 	 0.001384083044982699
of surrounding 	 0.00089126559714795
subtask of 	 1.0
on top 	 0.0047169811320754715
system-generated summary 	 0.5
was used 	 0.05194805194805195
allowing greater 	 0.3333333333333333
original training 	 0.07692307692307693
using OCR 	 0.05084745762711865
discourse Political 	 0.027777777777777776
equivalent set 	 0.2
Separate words 	 0.5
this approach 	 0.02197802197802198
on some 	 0.04245283018867924
Ideally , 	 1.0
DARPA funding 	 0.25
mechanism for 	 1.0
are dealing 	 0.004149377593360996
now more 	 0.07692307692307693
sentences Grass 	 0.013157894736842105
answering : 	 0.08333333333333333
human-ratings and 	 1.0
Some attempts 	 0.047619047619047616
because NLP 	 0.03333333333333333
as researchers 	 0.003484320557491289
industry , 	 0.3333333333333333
that difference 	 0.0035460992907801418
desktop OCR 	 1.0
opened , 	 1.0
standards . 	 0.4
mining of 	 0.2
semantics -RRB- 	 0.07142857142857142
other research 	 0.014285714285714285
some set 	 0.012048192771084338
of sentences 	 0.006238859180035651
language can 	 0.006756756756756757
TextRank does 	 0.07142857142857142
12 such 	 0.2
'' exceeded 	 0.005154639175257732
idea but 	 0.14285714285714285
Roger Schank 	 0.75
commercial OCR 	 0.18181818181818182
as articles 	 0.003484320557491289
Where such 	 1.0
<s> Computationally 	 0.0007686395080707148
or structured 	 0.0045045045045045045
to decoding 	 0.0013280212483399733
way -- 	 0.041666666666666664
, closed-domain 	 0.0005614823133071309
input -RRB- 	 0.024390243902439025
source text 	 0.20833333333333334
hand-printed documents 	 0.25
article deal 	 0.034482758620689655
all possible 	 0.06976744186046512
A word 	 0.02
<s> Books 	 0.0007686395080707148
isloated and 	 1.0
express the 	 0.4
results are 	 0.19047619047619047
biomedical domain 	 1.0
decisions only 	 0.1
distances for 	 0.5
determining sentiment 	 0.16666666666666666
that an 	 0.0035460992907801418
by teletype 	 0.005714285714285714
Continuous speech 	 1.0
Statistics derived 	 0.3333333333333333
is related 	 0.0020325203252032522
Perhaps the 	 1.0
this vocabulary 	 0.01098901098901099
as decision 	 0.010452961672473868
assigns large 	 1.0
out that 	 0.07142857142857142
an easy 	 0.007575757575757576
is known 	 0.006097560975609756
take the 	 0.2
Beginning in 	 0.5
a small 	 0.00245398773006135
successful finished 	 0.1111111111111111
, comprehend 	 0.0005614823133071309
corpus and 	 0.03225806451612903
and answered 	 0.001445086705202312
individual vertices 	 0.08333333333333333
collection . 	 0.2
attractive recognition 	 0.3333333333333333
also granted 	 0.014492753623188406
Narrow but 	 1.0
, too 	 0.0005614823133071309
the filter 	 0.0006920415224913495
the sentences 	 0.005536332179930796
classes Different 	 0.2
-LRB- Campaigns 	 0.0027100271002710027
Hybrid machine 	 0.5
Sometimes it 	 1.0
Smartphones . 	 1.0
to specific 	 0.0013280212483399733
alphabet , 	 0.6666666666666666
of achieving 	 0.0017825311942959
French . 	 0.25
content words 	 0.08333333333333333
a matter 	 0.001226993865030675
is how 	 0.0040650406504065045
idioms , 	 1.0
duplicate typewritten 	 0.5
word that 	 0.03333333333333333
Unsupervised tagging 	 0.16666666666666666
the literature 	 0.0006920415224913495
the 1950s 	 0.0020761245674740486
pictures or 	 1.0
of Quechua 	 0.00089126559714795
de Beaugrande 	 0.5
Many ATC 	 0.08333333333333333
of arbitrary 	 0.00089126559714795
This new 	 0.015873015873015872
edges after 	 0.14285714285714285
Wikipedia and 	 0.5
of errors 	 0.00089126559714795
of classifying 	 0.00089126559714795
a corresponding 	 0.001226993865030675
attached , 	 0.5
ability of 	 0.25
-LRB- HAMS 	 0.0027100271002710027
, Robyn 	 0.0005614823133071309
but rather 	 0.029411764705882353
opinion has 	 0.2
the holder 	 0.0006920415224913495
factor -LRB- 	 0.5
more descriptive 	 0.010526315789473684
-RRB- If 	 0.005420054200542005
asked for 	 0.3333333333333333
and 1957 	 0.001445086705202312
underlie the 	 1.0
Giro , 	 1.0
result is 	 0.18181818181818182
, split 	 0.0005614823133071309
<s> Battle 	 0.0007686395080707148
computational humor 	 0.1
Fourier transform 	 0.6666666666666666
The term 	 0.020833333333333332
procedures used 	 0.25
random walks 	 0.2857142857142857
On January 	 0.16666666666666666
right information 	 0.1
a unified 	 0.001226993865030675
many advantages 	 0.019230769230769232
leading to 	 1.0
sometimes called 	 0.07692307692307693
next item 	 0.14285714285714285
kick ' 	 1.0
, or 	 0.018528916339135316
allows a 	 0.125
orally speaking 	 1.0
capture speech 	 0.5
but deep 	 0.014705882352941176
revealing socio-psychological 	 1.0
1982 -RRB- 	 0.3333333333333333
human speakers 	 0.021739130434782608
problem with 	 0.022727272727272728
tokens from 	 0.14285714285714285
calculator or 	 0.5
HAMS = 	 1.0
HMMs involve 	 0.125
Lexical choice 	 0.5
Latin very 	 0.25
steady accumulation 	 0.5
-LRB- Carbonell 	 0.0027100271002710027
Since the 	 0.2
and required 	 0.001445086705202312
multiplying together 	 1.0
by this 	 0.005714285714285714
no effects 	 0.07692307692307693
wide use 	 0.25
, linguistic 	 0.0005614823133071309
acoustic modeling 	 0.3333333333333333
<s> Progress 	 0.0007686395080707148
Answer extraction 	 0.6666666666666666
less complex 	 0.08333333333333333
no incorrect 	 0.07692307692307693
, some 	 0.0050533408197641775
Intelligence '' 	 0.3333333333333333
, D. 	 0.0005614823133071309
a display 	 0.001226993865030675
, e.g. 	 0.005614823133071308
-LRB- IE 	 0.005420054200542005
using , 	 0.01694915254237288
centroid '' 	 0.5
, ID 	 0.0005614823133071309
grammar formalisms 	 0.02702702702702703
the suffix 	 0.0006920415224913495
fully articulated 	 0.16666666666666666
did Joe 	 0.2
GRM library 	 1.0
the informational 	 0.0006920415224913495
sidestepped the 	 1.0
article : 	 0.4482758620689655
The main 	 0.015625
by the 	 0.15428571428571428
Francis , 	 1.0
appear in 	 0.4375
contains no 	 0.1
1965 , 	 0.5
translate between 	 0.16666666666666666
-LRB- MLLR 	 0.0027100271002710027
control when 	 0.2
step -RRB- 	 0.06666666666666667
Strzalkowski T. 	 1.0
In both 	 0.01904761904761905
often and 	 0.022727272727272728
typically based 	 0.05555555555555555
key phrases 	 0.16666666666666666
ELIZA might 	 0.1111111111111111
with isolated 	 0.00546448087431694
natural -RRB- 	 0.013333333333333334
the controller 	 0.001384083044982699
the relevant 	 0.0006920415224913495
, Marcus 	 0.0005614823133071309
with -LRB- 	 0.00546448087431694
modeling , 	 0.14285714285714285
or profession 	 0.0045045045045045045
-LRB- 2001 	 0.0027100271002710027
similar sentences 	 0.1111111111111111
, humans 	 0.0005614823133071309
more categories 	 0.010526315789473684
segmentation between 	 0.030303030303030304
legends and 	 1.0
a naive 	 0.001226993865030675
but is 	 0.029411764705882353
subsequent concepts 	 0.5
languages -LRB- 	 0.04
and reporting 	 0.001445086705202312
the structure 	 0.0020761245674740486
NLP , 	 0.02127659574468085
or cross-lingual 	 0.0045045045045045045
classification task 	 0.058823529411764705
Europe , 	 0.6
things . 	 0.3333333333333333
of off-line 	 0.00089126559714795
capitalized , 	 0.6666666666666666
of formalisms\/languages 	 0.00089126559714795
processing group 	 0.018518518518518517
a stationary 	 0.00245398773006135
consonants , 	 0.3333333333333333
be more 	 0.02109704641350211
fields . 	 0.16666666666666666
SPHINX toolkit 	 1.0
of research 	 0.0071301247771836
distinction with 	 0.2
will indicate 	 0.02857142857142857
But then 	 0.16666666666666666
and writing 	 0.001445086705202312
and minimum 	 0.001445086705202312
test environment 	 0.1
coupons returned 	 1.0
more formally 	 0.010526315789473684
typical to 	 0.1111111111111111
, morphology 	 0.0011229646266142617
<s> Design 	 0.0023059185242121443
-RRB- `` 	 0.0027100271002710027
500 texts 	 0.5
Thai do 	 0.5
and example 	 0.001445086705202312
<s> Unsupervised 	 0.003843197540353574
morphology of 	 0.14285714285714285
of constraints 	 0.00089126559714795
many instances 	 0.019230769230769232
and far 	 0.002890173410404624
by analyzing 	 0.011428571428571429
where sentences 	 0.05714285714285714
fighter trainer 	 0.16666666666666666
volume , 	 0.25
text corpus 	 0.012578616352201259
hybrid approach 	 0.5
different possible 	 0.02040816326530612
well as 	 0.4642857142857143
-RRB- securely 	 0.0027100271002710027
resources such 	 0.16666666666666666
step towards 	 0.06666666666666667
for machine 	 0.010830324909747292
13 , 	 0.5
from Moore 	 0.009615384615384616
other English 	 0.014285714285714285
using general 	 0.01694915254237288
comprehension , 	 0.14285714285714285
or query 	 0.0045045045045045045
discourse are 	 0.027777777777777776
him perform 	 0.5
did cause 	 0.2
with low 	 0.00546448087431694
, i.e. 	 0.0039303761931499155
space is 	 0.2
to query 	 0.0013280212483399733
lies a 	 0.5
or turns-at-talk 	 0.0045045045045045045
part -LRB- 	 0.037037037037037035
which occur 	 0.007246376811594203
by hand 	 0.03428571428571429
ASR is 	 0.16666666666666666
sentence boundaries 	 0.08333333333333333
NER -RRB- 	 1.0
slowly but 	 0.5
One way 	 0.07692307692307693
of about 	 0.00089126559714795
it accepts 	 0.008547008547008548
allowing for 	 0.3333333333333333
grammars . 	 0.14285714285714285
the ANR-Passage 	 0.0006920415224913495
metrics : 	 0.1111111111111111
Navigation Systems 	 1.0
<s> Named 	 0.0007686395080707148
a rainbow 	 0.001226993865030675
<s> Open 	 0.0007686395080707148
networks have 	 0.07142857142857142
of words 	 0.013368983957219251
George Lakoff 	 1.0
can prove 	 0.0055248618784530384
following are 	 0.06666666666666667
legal and 	 0.3333333333333333
and manage 	 0.001445086705202312
especially of 	 0.06666666666666667
identify the 	 0.5
coarticulation , 	 1.0
the nautical 	 0.0006920415224913495
is facing 	 0.0020325203252032522
on one 	 0.009433962264150943
speaking , 	 0.625
humor -RRB- 	 1.0
T. 1991 	 1.0
management applications 	 0.14285714285714285
appropriate syntactic 	 0.25
similar `` 	 0.037037037037037035
<s> LexisNexis 	 0.0007686395080707148
has many 	 0.023809523809523808
is affected 	 0.0020325203252032522
list approach 	 0.09090909090909091
serve as 	 0.8
translator must 	 0.14285714285714285
the other 	 0.005536332179930796
Semi-supervised and 	 1.0
new methods 	 0.041666666666666664
which the 	 0.057971014492753624
tagged as 	 0.3333333333333333
QA Questions 	 0.047619047619047616
to that 	 0.0026560424966799467
ambiguity and 	 0.125
in turn 	 0.009363295880149813
Command Success 	 0.5
A random 	 0.02
a factory 	 0.001226993865030675
results when 	 0.09523809523809523
replicated his 	 1.0
speech Adverse 	 0.006578947368421052
Symantec changed 	 0.5
commonly tagged 	 0.125
if and 	 0.03571428571428571
Monroe and 	 1.0
generally without 	 0.09090909090909091
boundary disambiguation 	 0.3333333333333333
news article 	 0.15384615384615385
methods for 	 0.045454545454545456
rapidly growing 	 0.5
revolutionized bill 	 1.0
on such 	 0.0047169811320754715
many more 	 0.019230769230769232
subtasks . 	 0.5
` nice 	 0.0625
co-occur with 	 0.5
recommendations and 	 1.0
Lemke , 	 1.0
and linguistics 	 0.002890173410404624
can discriminate 	 0.0055248618784530384
NLG -RRB- 	 0.047619047619047616
camp '' 	 0.25
, modules 	 0.0005614823133071309
Penpoint OS 	 1.0
personal digital 	 0.25
be referred 	 0.004219409282700422
numbers that 	 0.14285714285714285
the draft 	 0.0006920415224913495
thus speech 	 0.1
commercially available 	 1.0
computer analysis 	 0.022727272727272728
1982 Gary 	 0.3333333333333333
commercial systems 	 0.09090909090909091
methods assess 	 0.022727272727272728
use . 	 0.041666666666666664
different times 	 0.02040816326530612
evaluate an 	 0.25
language with 	 0.006756756756756757
recognition models 	 0.008264462809917356
syntactic parsers 	 0.07692307692307693
KEA -LRB- 	 1.0
is accomplished 	 0.0020325203252032522
at a 	 0.058823529411764705
LinguaSys , 	 1.0
was attempted 	 0.012987012987012988
world with 	 0.06666666666666667
early 1990s 	 0.1
the construction 	 0.0006920415224913495
collect call 	 1.0
shops in 	 1.0
-LRB- such 	 0.02168021680216802
have not 	 0.019230769230769232
inter-word spaces 	 1.0
usually involves 	 0.03125
famous QA 	 0.3333333333333333
an interactive 	 0.007575757575757576
The lexer 	 0.005208333333333333
generation techniques 	 0.1111111111111111
freely available 	 1.0
found . 	 0.07142857142857142
measurement of 	 0.5
form filling 	 0.05
conversation , 	 0.25
, information 	 0.0011229646266142617
recognition efforts 	 0.008264462809917356
Goodwin , 	 1.0
understanding machine 	 0.030303030303030304
strokes for 	 1.0
some systems 	 0.024096385542168676
strings of 	 1.0
are marked 	 0.004149377593360996
of abbreviations 	 0.0017825311942959
system , 	 0.10752688172043011
score if 	 0.16666666666666666
affect is 	 0.3333333333333333
easily , 	 0.1111111111111111
an error 	 0.007575757575757576
input . 	 0.04878048780487805
sound pattern 	 0.05
increasingly focused 	 0.3333333333333333
1980s the 	 0.1111111111111111
different dialogues 	 0.02040816326530612
A second 	 0.02
even the 	 0.07407407407407407
-LRB- Cullingford 	 0.0027100271002710027
with respect 	 0.03825136612021858
, Paroubek 	 0.0005614823133071309
was -LRB- 	 0.012987012987012988
statistical distribution 	 0.030303030303030304
compiler due 	 0.3333333333333333
these T 	 0.023809523809523808
far is 	 0.125
to change 	 0.0013280212483399733
and taking 	 0.001445086705202312
by Zellig 	 0.005714285714285714
hurricane season 	 1.0
organizations such 	 1.0
front-end SR 	 1.0
dependency for 	 0.2
theoretical aspects 	 0.3333333333333333
affective state 	 1.0
some cases 	 0.04819277108433735
consecutive words 	 0.5
beyond which 	 0.16666666666666666
, despite 	 0.0005614823133071309
in that 	 0.003745318352059925
used English 	 0.008849557522123894
levels of 	 0.3181818181818182
and simulation 	 0.001445086705202312
questions are 	 0.07692307692307693
Reiter and 	 1.0
avoiding some 	 0.5
noise but 	 0.125
psychology Response 	 0.25
biographical questions 	 1.0
realistic grammars 	 1.0
all the 	 0.13953488372093023
hybrid system 	 0.5
while an 	 0.05
contain punctuation 	 0.08333333333333333
The topic 	 0.005208333333333333
leftmost derivation 	 1.0
blue '' 	 1.0
text to 	 0.0440251572327044
is clearly 	 0.0040650406504065045
evaluation : 	 0.037037037037037035
given data 	 0.041666666666666664
little inflectional 	 0.3333333333333333
it -RRB- 	 0.008547008547008548
operating on 	 0.5
encyclopedia '' 	 1.0
, along 	 0.0005614823133071309
improved their 	 0.25
to resort 	 0.0013280212483399733
walks and 	 0.5
realizations as 	 1.0
-RRB- -RRB- 	 0.005420054200542005
of anaphora 	 0.00089126559714795
for machine-translation 	 0.0036101083032490976
feasible the 	 0.5
of grammatical 	 0.00089126559714795
the outside 	 0.0006920415224913495
in a 	 0.09363295880149813
isolation of 	 0.5
attained . 	 1.0
not a 	 0.008928571428571428
-RRB- refer 	 0.0027100271002710027
Dictionary-based machine 	 0.5
related . 	 0.06666666666666667
ranked by 	 0.2
typically one 	 0.05555555555555555
and curves 	 0.001445086705202312
it up 	 0.008547008547008548
by Kurzweil 	 0.005714285714285714
, many 	 0.0039303761931499155
Interface , 	 1.0
representation of 	 0.10526315789473684
for errors 	 0.0036101083032490976
similarities in 	 0.5
pages concluded 	 0.14285714285714285
a bank 	 0.001226993865030675
LILOG , 	 0.5
<s> Perhaps 	 0.0007686395080707148
generate jokes 	 0.05555555555555555
varies greatly 	 1.0
Questions are 	 1.0
done by 	 0.18181818181818182
Since then 	 0.2
possibility of 	 0.75
user about 	 0.07142857142857142
choices What 	 0.4
in more 	 0.0018726591760299626
is using 	 0.0040650406504065045
a relaxed 	 0.001226993865030675
capabilities by 	 0.2
lessons learned 	 1.0
The readers 	 0.005208333333333333
probably `` 	 0.25
implicate `` 	 1.0
field . 	 0.14814814814814814
the hypothesis 	 0.0006920415224913495
corpus contains 	 0.03225806451612903
1954 -RRB- 	 0.3333333333333333
a calculator 	 0.00245398773006135
translate text 	 0.3333333333333333
character groups 	 0.045454545454545456
deterministic problem 	 0.25
Constraints may 	 0.3333333333333333
test and 	 0.2
speech-to-text processing 	 0.5
how often 	 0.034482758620689655
be semantic 	 0.004219409282700422
Intrinsic evaluation 	 0.3333333333333333
Adda 1999 	 0.5
this aim 	 0.01098901098901099
strong correlation 	 0.25
This problem 	 0.06349206349206349
some common 	 0.012048192771084338
pronunciation , 	 1.0
, unlike 	 0.0005614823133071309
is testing 	 0.0020325203252032522
generate translations 	 0.05555555555555555
faster but 	 0.3333333333333333
In particular 	 0.02857142857142857
between closely 	 0.02564102564102564
both cases 	 0.03225806451612903
any previous 	 0.03225806451612903
state computers 	 0.07142857142857142
affect the 	 0.3333333333333333
given corpus 	 0.041666666666666664
shallow approaches 	 0.16666666666666666
notoriously , 	 1.0
barmaid '' 	 0.3333333333333333
the effort 	 0.0006920415224913495
above are 	 0.07692307692307693
could often 	 0.0625
represent as 	 0.1111111111111111
code on 	 0.14285714285714285
OCR service 	 0.04081632653061224
Features might 	 1.0
, often 	 0.0016844469399213925
procedures can 	 0.5
<s> Vulcan 	 0.0007686395080707148
natural speech 	 0.02666666666666667
500,000 . 	 1.0
would produce 	 0.03773584905660377
`` What 	 0.015873015873015872
year or 	 0.16666666666666666
and treat 	 0.001445086705202312
: A 	 0.00980392156862745
, was 	 0.0022459292532285235
is Hard 	 0.0020325203252032522
superimpose one 	 1.0
of action 	 0.00089126559714795
word problems 	 0.016666666666666666
while ratings 	 0.05
focused on 	 0.9090909090909091
Canada ? 	 0.16666666666666666
similar to 	 0.5555555555555556
in popular 	 0.0018726591760299626
8000 samples 	 1.0
translate spoken 	 0.16666666666666666
at that 	 0.014705882352941176
text into 	 0.0440251572327044
artificial processes 	 0.18181818181818182
evaluations , 	 0.16666666666666666
, idioms 	 0.0005614823133071309
Peru . 	 0.5
breadth '' 	 0.5
summaries can 	 0.046511627906976744
was historically 	 0.012987012987012988
Another good 	 0.07692307692307693
OCR or 	 0.02040816326530612
handling such 	 0.5
of at 	 0.00089126559714795
program were 	 0.045454545454545456
humanities and 	 1.0
and makes 	 0.001445086705202312
then there 	 0.02857142857142857
analyses , 	 0.2
combine various 	 0.6666666666666666
multilingual textual 	 0.3333333333333333
of AI 	 0.00089126559714795
labels to 	 1.0
is at 	 0.0020325203252032522
of social 	 0.0017825311942959
inputting approximately 	 1.0
Inc. and 	 0.5
<s> ROUGE-1 	 0.0007686395080707148
-LRB- Kittredge 	 0.0027100271002710027
utterance `` 	 0.3333333333333333
proposed keyphrases 	 0.2222222222222222
end abruptly 	 0.125
1966 . 	 0.3333333333333333
only unigrams 	 0.02631578947368421
of reviews 	 0.00089126559714795
features ? 	 0.038461538461538464
logic oriented 	 0.25
voice in 	 0.07692307692307693
corpora on 	 0.09090909090909091
model taggers 	 0.03333333333333333
problem involves 	 0.045454545454545456
character stream 	 0.045454545454545456
's Law 	 0.0196078431372549
as supervised 	 0.003484320557491289
makes use 	 0.125
internal organization 	 0.2
, Speereo 	 0.0005614823133071309
being considered 	 0.1111111111111111
system takes 	 0.010752688172043012
representation into 	 0.05263157894736842
in full 	 0.0018726591760299626
great success 	 0.3333333333333333
classifier so 	 0.14285714285714285
or existing 	 0.0045045045045045045
the eigenvector 	 0.0006920415224913495
are displayed 	 0.004149377593360996
features\/aspects , 	 1.0
user are 	 0.07142857142857142
rates . 	 0.125
how to 	 0.10344827586206896
Martin presents 	 0.5
a subset 	 0.0036809815950920245
of evaluation 	 0.004456327985739751
not mark 	 0.008928571428571428
then common 	 0.02857142857142857
like `` 	 0.10714285714285714
that PageRank 	 0.0035460992907801418
taught to 	 0.3333333333333333
MPE -RRB- 	 1.0
ambiguity by 	 0.125
finding non-existent 	 0.2
many consecutive 	 0.019230769230769232
parsing In 	 0.03571428571428571
most fonts 	 0.017241379310344827
's the 	 0.0196078431372549
the keyphrases 	 0.001384083044982699
are both 	 0.008298755186721992
linguistic cues 	 0.0625
Kurzweil Applied 	 0.14285714285714285
BLEU is 	 0.3333333333333333
material . 	 0.5
output than 	 0.038461538461538464
-LRB- Dec. 	 0.0027100271002710027
differently on 	 1.0
marketing . 	 1.0
highlighted , 	 1.0
RAF employs 	 1.0
letters of 	 0.2
continuous speech 	 0.5
Su , 	 1.0
the analysis 	 0.002768166089965398
word boundary 	 0.016666666666666666
first commercial 	 0.06060606060606061
years after 	 0.047619047619047616
, Jan 	 0.0005614823133071309
hierarchy of 	 1.0
put a 	 0.25
not resulted 	 0.008928571428571428
answering have 	 0.08333333333333333
planning and 	 0.5
feature . 	 0.15384615384615385
removes the 	 1.0
to erroneous 	 0.0013280212483399733
unigrams to 	 0.08333333333333333
direction is 	 0.3333333333333333
which showed 	 0.007246376811594203
<s> Importance 	 0.0007686395080707148
such keyphrases 	 0.008130081300813009
and speaker 	 0.001445086705202312
, deciding 	 0.0022459292532285235
facts about 	 1.0
: noun 	 0.00980392156862745
, because 	 0.004491858506457047
-LRB- c 	 0.0027100271002710027
language have 	 0.006756756756756757
called GRASSHOPPER 	 0.05555555555555555
and post-secondary 	 0.001445086705202312
often ambiguous 	 0.022727272727272728
the Eagles 	 0.0006920415224913495
legal word 	 0.3333333333333333
nuances and 	 1.0
total accuracy 	 0.5
problem was 	 0.022727272727272728
relative probability 	 0.3333333333333333
paper . 	 0.09090909090909091
measure , 	 0.09090909090909091
should figure 	 0.05263157894736842
reader . 	 0.2
any significant 	 0.03225806451612903
, gestures 	 0.0005614823133071309
texts can 	 0.058823529411764705
, Animate 	 0.0005614823133071309
method used 	 0.0625
rushing to 	 1.0
Control -RRB- 	 1.0
spaces . 	 0.2
including morphemes 	 0.07142857142857142
, especially 	 0.0050533408197641775
various aspects 	 0.05555555555555555
, abstracts 	 0.0005614823133071309
system-generated summaries 	 0.5
parsing comes 	 0.03571428571428571
been especially 	 0.014705882352941176
as English 	 0.010452961672473868
over vertices 	 0.08333333333333333
Speereo Software 	 0.5
One step 	 0.07692307692307693
humans often 	 0.08333333333333333
between them 	 0.05128205128205128
to TextRank 	 0.0013280212483399733
subject . 	 0.25
been devised 	 0.014705882352941176
signals are 	 1.0
In computer 	 0.009523809523809525
named entity 	 0.2857142857142857
on complex 	 0.0047169811320754715
learn about 	 0.07692307692307693
rev , 	 1.0
given CFG 	 0.041666666666666664
an American 	 0.007575757575757576
Therein lies 	 1.0
to existing 	 0.0013280212483399733
an allowable 	 0.007575757575757576
expanding all 	 1.0
smoothly with 	 0.5
's polarity 	 0.0196078431372549
, approach 	 0.0005614823133071309
repeated relations 	 0.5
words have 	 0.009174311926605505
Machines Research 	 1.0
topic -RRB- 	 0.125
ontology are 	 0.5
or any 	 0.013513513513513514
language using 	 0.006756756756756757
research necessary 	 0.023809523809523808
grounded in 	 1.0
Speaking '' 	 1.0
degrees depending 	 0.5
The hidden 	 0.005208333333333333
and how 	 0.004335260115606936
` hit 	 0.0625
may suffer 	 0.019230769230769232
has now 	 0.011904761904761904
other work 	 0.014285714285714285
repeated as 	 0.5
Studies -RRB- 	 1.0
agreement is 	 0.3333333333333333
the closest 	 0.0006920415224913495
effort should 	 0.25
telegraph code 	 1.0
B , 	 1.0
look -LRB- 	 0.2
real ATC 	 0.1111111111111111
other automatic 	 0.014285714285714285
shipment of 	 1.0
techniques merely 	 0.043478260869565216
language-processing tasks 	 1.0
and later 	 0.001445086705202312
know document 	 0.5
sociolinguistics Ethnography 	 0.5
overriding issue 	 1.0
other languages 	 0.07142857142857142
and widely 	 0.001445086705202312
and confusability 	 0.001445086705202312
Word splitting 	 0.2857142857142857
with models 	 0.00546448087431694
express this 	 0.2
conditions Environmental 	 0.2
be run 	 0.004219409282700422
paste relevant 	 1.0
The vertices 	 0.005208333333333333
TextRank is 	 0.07142857142857142
-LRB- also 	 0.01084010840108401
, where 	 0.008422234699606962
or applying 	 0.0045045045045045045
Relevance -LRB- 	 1.0
polarity and 	 0.125
domains , 	 0.125
module uses 	 0.3333333333333333
Substantial efforts 	 0.5
science -LRB- 	 0.1
assume no 	 0.5
of cepstral 	 0.00089126559714795
require in 	 0.045454545454545456
the complexity 	 0.005536332179930796
are surprisingly 	 0.004149377593360996
determination : 	 1.0
natural language 	 0.76
to move 	 0.0013280212483399733
speaking -RRB- 	 0.125
contains additional 	 0.1
anymore , 	 1.0
following issues 	 0.06666666666666667
been undertaken 	 0.014705882352941176
A morphosyntactic 	 0.02
of `` 	 0.0071301247771836
; `` 	 0.02127659574468085
qualitatively The 	 1.0
boundaries may 	 0.09090909090909091
from weather 	 0.009615384615384616
typewritten reports 	 0.2
or to 	 0.009009009009009009
Unsupervised approaches 	 0.16666666666666666
Chantal Mouffe 	 1.0
of active 	 0.00089126559714795
sentences weighted 	 0.013157894736842105
a universal 	 0.001226993865030675
V , 	 1.0
to automatically 	 0.00796812749003984
score -LRB- 	 0.16666666666666666
, decimal 	 0.0005614823133071309
be resolved 	 0.004219409282700422
Canadian Hansard 	 0.5
levels first 	 0.045454545454545456
output to 	 0.038461538461538464
Task description 	 0.3333333333333333
titles , 	 0.5
so simply 	 0.03333333333333333
with other 	 0.00546448087431694
the barmaid 	 0.0006920415224913495
; total 	 0.02127659574468085
relying on 	 1.0
ROUGE-1 scores 	 0.2
you must 	 0.07692307692307693
- Top-down 	 0.0625
disappear . 	 1.0
task it 	 0.023809523809523808
as categories 	 0.003484320557491289
way as 	 0.041666666666666664
of handwritten 	 0.00089126559714795
answering There 	 0.08333333333333333
cases -- 	 0.05555555555555555
English and 	 0.08108108108108109
found by 	 0.07142857142857142
PC + 	 0.25
new complex 	 0.041666666666666664
from 50 	 0.009615384615384616
instead recognizes 	 0.14285714285714285
a grammar 	 0.00245398773006135
refers to 	 1.0
a chunk 	 0.007361963190184049
-LRB- NLG 	 0.008130081300813009
been used 	 0.07352941176470588
Greek -LRB- 	 0.3333333333333333
parsers which 	 0.07692307692307693
6 to 	 0.75
annotation has 	 0.25
placement of 	 1.0
MIT wrote 	 0.5
for an 	 0.0036101083032490976
<s> Basically 	 0.0007686395080707148
Austrian emigre 	 1.0
text categorization 	 0.006289308176100629
producing more 	 0.3333333333333333
keyphrases assigned 	 0.02857142857142857
to capture 	 0.0013280212483399733
grammar to 	 0.02702702702702703
by heteroscedastic 	 0.005714285714285714
system for 	 0.021505376344086023
beginning in 	 0.5
semitied covariance 	 1.0
: Digitize 	 0.00980392156862745
posts and 	 1.0
mid-1960s . 	 1.0
to parse 	 0.005312084993359893
which arise 	 0.007246376811594203
numbers after 	 0.14285714285714285
with two 	 0.01092896174863388
answer to 	 0.16666666666666666
`` Tell 	 0.005291005291005291
`` black 	 0.005291005291005291
e-communities die 	 0.5
use it 	 0.027777777777777776
word use 	 0.016666666666666666
library , 	 0.5
dictionary entry 	 0.14285714285714285
Schiffrin , 	 1.0
evaluation tests 	 0.037037037037037035
readability and 	 1.0
larger collection 	 0.0625
techniques for 	 0.08695652173913043
SHRDLU , 	 0.16666666666666666
algorithm . 	 0.14285714285714285
recognition from 	 0.01652892561983471
there were 	 0.075
phenomenon may 	 0.2
segmentation depends 	 0.030303030303030304
dimensions of 	 0.6666666666666666
see for 	 0.05
75 % 	 1.0
is based 	 0.008130081300813009
speech of 	 0.006578947368421052
parsing have 	 0.03571428571428571
corpus has 	 0.03225806451612903
large ; 	 0.043478260869565216
explicitly delimited 	 0.25
2007 . 	 0.4
others assign 	 0.08333333333333333
complex expressions 	 0.041666666666666664
do all 	 0.038461538461538464
make up 	 0.1
require their 	 0.045454545454545456
sentences , 	 0.10526315789473684
recognition benchmark 	 0.008264462809917356
highest probability 	 0.3333333333333333
successive words 	 0.5
open-access journal 	 1.0
years to 	 0.047619047619047616
's first 	 0.0196078431372549
system with 	 0.010752688172043012
% error 	 0.02564102564102564
or aspects 	 0.0045045045045045045
is best 	 0.0020325203252032522
word -LRB- 	 0.016666666666666666
modules that 	 0.5
available -LRB- 	 0.058823529411764705
commercial version 	 0.09090909090909091
the typewritten 	 0.0006920415224913495
entities '' 	 0.14285714285714285
background noise 	 0.3333333333333333
linguistically meaningful 	 1.0
A year 	 0.02
often quoted 	 0.022727272727272728
an attractive 	 0.007575757575757576
discussions in 	 0.3333333333333333
suitability as 	 0.5
and Sentences 	 0.001445086705202312
the Royal 	 0.001384083044982699
, GRASSHOPPER 	 0.0005614823133071309
same characters 	 0.04
logic -RRB- 	 0.25
basic categories 	 0.07692307692307693
2 , 	 0.2
turned into 	 1.0
summaries known 	 0.023255813953488372
set that 	 0.02564102564102564
that time 	 0.0035460992907801418
known keyphrase 	 0.038461538461538464
Japanese prisoner 	 0.125
may chose 	 0.019230769230769232
may depend 	 0.019230769230769232
previously prepared 	 0.5
software is 	 0.07407407407407407
sort mail 	 0.3333333333333333
printed in 	 0.08333333333333333
a sensible 	 0.001226993865030675
1981 -RRB- 	 1.0
a '' 	 0.001226993865030675
like supervised 	 0.03571428571428571
we construct 	 0.022222222222222223
supplying more 	 1.0
under stress 	 0.2
numeric scores 	 1.0
but they 	 0.04411764705882353
solve a 	 0.25
, encoding 	 0.0005614823133071309
, funding 	 0.0016844469399213925
the evaluation 	 0.0034602076124567475
require a 	 0.22727272727272727
official languages 	 1.0
series -RRB- 	 0.125
just validated 	 0.1111111111111111
real-world data 	 0.3333333333333333
Processes may 	 1.0
, needed 	 0.0005614823133071309
of Sydney 	 0.00089126559714795
conditions . 	 0.2
NIST role 	 0.5
, meaning 	 0.0005614823133071309
Sager , 	 0.5
complexity while 	 0.08333333333333333
light -RRB- 	 0.3333333333333333
be around 	 0.004219409282700422
sounds representing 	 0.06666666666666667
together ? 	 0.125
or uncertainties 	 0.0045045045045045045
journal abstracts 	 0.3333333333333333
properties . 	 0.25
annual Document 	 0.5
in Statistical 	 0.0018726591760299626
improvements . 	 0.5
when working 	 0.02857142857142857
A problem 	 0.02
and we 	 0.001445086705202312
the answer 	 0.009688581314878892
a string 	 0.00245398773006135
concern . 	 1.0
systems , 	 0.05357142857142857
resulting graph 	 0.25
alternative courses 	 0.3333333333333333
it becomes 	 0.03418803418803419
is subject 	 0.0020325203252032522
are exploited 	 0.004149377593360996
in 1987 	 0.003745318352059925
and corrected 	 0.001445086705202312
the reason 	 0.0006920415224913495
word sequences 	 0.016666666666666666
mining have 	 0.2
they take 	 0.025
data annotation 	 0.012987012987012988
heavy-noise , 	 1.0
entire banking 	 0.3333333333333333
reporting -RRB- 	 0.3333333333333333
an understanding 	 0.015151515151515152
Markov model 	 0.4444444444444444
Web . 	 0.2222222222222222
not an 	 0.008928571428571428
in specific 	 0.003745318352059925
, John 	 0.002807411566535654
Machine Learning 	 0.1111111111111111
Each frame 	 0.16666666666666666
vs. Independence 	 0.08333333333333333
<s> Described 	 0.0007686395080707148
where the 	 0.37142857142857144
This analysis 	 0.015873015873015872
accommodate ambiguity 	 0.4
simply focused 	 0.08333333333333333
variability , 	 1.0
be approximately 	 0.004219409282700422
domain . 	 0.3
structure grammar 	 0.08333333333333333
adaptive document\/text 	 0.3333333333333333
background knowledge 	 0.3333333333333333
OCR to 	 0.061224489795918366
of developing 	 0.00089126559714795
in part 	 0.0018726591760299626
at University 	 0.014705882352941176
averaged . 	 1.0
use following 	 0.013888888888888888
be statistically 	 0.004219409282700422
builds a 	 0.5
Beatrice Santorini 	 1.0
advances in 	 1.0
any capitalization 	 0.03225806451612903
Although humans 	 0.125
discontinuous speech 	 0.6666666666666666
supervised learning 	 0.3125
which itself 	 0.007246376811594203
from data 	 0.019230769230769232
human geography 	 0.021739130434782608
to eigenvalue 	 0.0013280212483399733
analytical artificial 	 0.5
usually do 	 0.03125
post-processed by 	 1.0
User profiling 	 0.5
the ranking 	 0.001384083044982699
to summarization 	 0.0026560424966799467
: rule-based 	 0.00980392156862745
, size 	 0.0005614823133071309
individuals that 	 1.0
found that 	 0.35714285714285715
not rare 	 0.008928571428571428
Marcus M. 	 1.0
that connects 	 0.0035460992907801418
depend on 	 1.0
rainbow form 	 1.0
language . 	 0.07432432432432433
categorical . 	 1.0
In 2006 	 0.009523809523809525
your head 	 0.5
to integrate 	 0.0013280212483399733
<s> Our 	 0.0023059185242121443
Although it 	 0.125
devoted in 	 0.2
`` Who 	 0.010582010582010581
Naomi Sager 	 1.0
vectors -LRB- 	 0.3333333333333333
in generating 	 0.0018726591760299626
among humans 	 0.125
possible task 	 0.041666666666666664
Shallow parsing 	 0.5
The 10 	 0.005208333333333333
Jaworski , 	 1.0
Northern Isles 	 0.6666666666666666
JSF -RRB- 	 1.0
sub-categories . 	 1.0
system typically 	 0.010752688172043012
, minimum 	 0.0005614823133071309
of nouns 	 0.00089126559714795
language might 	 0.006756756756756757
Canada are 	 0.16666666666666666
often involve 	 0.022727272727272728
topics in 	 0.14285714285714285
<s> Keyphrases 	 0.0007686395080707148
their `` 	 0.029411764705882353
or semantics 	 0.0045045045045045045
case '' 	 0.058823529411764705
and split 	 0.001445086705202312
field , 	 0.037037037037037035
that allows 	 0.0035460992907801418
& Online 	 0.125
to current 	 0.0013280212483399733
Beaugrande , 	 1.0
complex images 	 0.041666666666666664
The difference 	 0.005208333333333333
far northeast 	 0.25
a pre-existing 	 0.001226993865030675
printed page 	 0.08333333333333333
translated in 	 0.25
translation Main 	 0.013513513513513514
these two 	 0.023809523809523808
answer temporal 	 0.03333333333333333
Corpus , 	 0.0625
- A 	 0.0625
of product 	 0.00089126559714795
save time 	 1.0
example : 	 0.024691358024691357
a sequence 	 0.007361963190184049
the computerization 	 0.0006920415224913495
probabilistic division 	 0.14285714285714285
fields , 	 0.3333333333333333
resources and 	 0.16666666666666666
optical character 	 1.0
for 5 	 0.0036101083032490976
the mental 	 0.0006920415224913495
free speech 	 0.25
, Brazil 	 0.0005614823133071309
add them 	 1.0
meaning -LRB- 	 0.043478260869565216
7 in 	 0.2857142857142857
to any 	 0.00398406374501992
rules based 	 0.023255813953488372
among sentences 	 0.125
. '' 	 0.00546021840873635
is probably 	 0.0020325203252032522
data sets 	 0.03896103896103896
detailed discussions 	 0.5
of Turney 	 0.0017825311942959
Asian language 	 1.0
Fourier Transform 	 0.3333333333333333
proved far 	 0.3333333333333333
prepare formal 	 1.0
headlines , 	 1.0
1950s included 	 0.25
estimation and 	 1.0
a user 	 0.00245398773006135
recognize For 	 0.1111111111111111
are simple 	 0.004149377593360996
and regions 	 0.001445086705202312
of disassembling 	 0.00089126559714795
aural feedback 	 1.0
Duchess was 	 1.0
MEAD -RRB- 	 1.0
with realistic 	 0.00546448087431694
describing each 	 0.25
probabilities of 	 0.2727272727272727
entry from 	 0.25
to consult 	 0.0013280212483399733
focused solely 	 0.09090909090909091
: the 	 0.0196078431372549
skilled linguist 	 1.0
edges are 	 0.14285714285714285
: statistical 	 0.00980392156862745
text-understanding system 	 1.0
models , 	 0.07692307692307693
analysts This 	 0.5
its designers 	 0.02857142857142857
positive or 	 0.2857142857142857
ports . 	 1.0
constraint , 	 1.0
portion of 	 1.0
Semantic Evaluation 	 0.3333333333333333
models for 	 0.23076923076923078
images environment 	 0.16666666666666666
profession -LRB- 	 1.0
another linguistic 	 0.07692307692307693
the relationships 	 0.0006920415224913495
When used 	 0.14285714285714285
readers processed 	 0.5
subfields of 	 1.0
manually or 	 0.25
if-then rules 	 1.0
extraction -LRB- 	 0.06451612903225806
both affine 	 0.03225806451612903
human language 	 0.06521739130434782
; these 	 0.0425531914893617
relatively simple 	 1.0
psychotherapist , 	 1.0
are robust 	 0.004149377593360996
filter preselects 	 0.5
of some 	 0.004456327985739751
a relic 	 0.001226993865030675
`` Call 	 0.005291005291005291
is where 	 0.0040650406504065045
and naturalness 	 0.001445086705202312
William A. 	 0.5
Given the 	 0.07142857142857142
, instead 	 0.0005614823133071309
vs. spontaneous 	 0.08333333333333333
results can 	 0.047619047619047616
the analog 	 0.0006920415224913495
POST -RRB- 	 1.0
of examples 	 0.00089126559714795
waves that 	 0.14285714285714285
offering WebOCR 	 1.0
the printed 	 0.0006920415224913495
five pages 	 0.2
different features 	 0.02040816326530612
to found 	 0.0013280212483399733
display format 	 0.5
<s> Individuals 	 0.0007686395080707148
symbols , 	 0.3333333333333333
which should 	 0.007246376811594203
keeping a 	 0.5
will give 	 0.02857142857142857
and human-generated 	 0.001445086705202312
formatted output 	 1.0
rudimentary translation 	 0.5
about specific 	 0.025
laughter -RRB- 	 1.0
the realm 	 0.0006920415224913495
even lower 	 0.037037037037037035
that interact 	 0.0035460992907801418
by experts 	 0.005714285714285714
a rules 	 0.001226993865030675
without it 	 0.07692307692307693
to OCR 	 0.0013280212483399733
extraction : 	 0.06451612903225806
and achieved 	 0.001445086705202312
issues Disambiguation 	 0.2
and trigram 	 0.001445086705202312
<s> Helicopters 	 0.0007686395080707148
methods did 	 0.022727272727272728
Adverse conditions 	 1.0
could just 	 0.0625
broken into 	 0.6
pre-defined or 	 0.5
The authors 	 0.015625
corresponding systems 	 0.16666666666666666
especially input 	 0.13333333333333333
work at 	 0.041666666666666664
They simply 	 0.3333333333333333
stress injuries 	 0.5
taggers and 	 0.2857142857142857
them attractive 	 0.05263157894736842
letter shapes 	 0.3333333333333333
created based 	 0.14285714285714285
since they 	 0.1
meaning then 	 0.043478260869565216
test example 	 0.1
another problem 	 0.15384615384615385
but triples 	 0.014705882352941176
is similar 	 0.0040650406504065045
Speech '' 	 0.03225806451612903
are confirmed 	 0.004149377593360996
important -RRB- 	 0.0625
grammar can 	 0.02702702702702703
the grammatical 	 0.0020761245674740486
machines to 	 0.25
, 13 	 0.0005614823133071309
note is 	 1.0
linguistics that 	 0.1
hence reducing 	 0.5
which bore 	 0.007246376811594203
Functional grammar 	 1.0
data that 	 0.025974025974025976
adverbs , 	 1.0
organization -RRB- 	 0.2
same words 	 0.04
other medium 	 0.014285714285714285
<s> Beginning 	 0.0007686395080707148
of generating 	 0.00089126559714795
new insights 	 0.041666666666666664
an application 	 0.015151515151515152
' language 	 0.05263157894736842
product line 	 0.14285714285714285
words by 	 0.009174311926605505
n-gram overlaps 	 0.5
Society , 	 1.0
the Levenshtein 	 0.0006920415224913495
corresponds to 	 1.0
: an 	 0.00980392156862745
speech processing 	 0.006578947368421052
on corpora 	 0.0047169811320754715
require extensive 	 0.09090909090909091
then compute 	 0.02857142857142857
Compare speech 	 1.0
-LRB- for 	 0.018970189701897018
-LRB- grammar 	 0.0027100271002710027
processor speeds 	 1.0
the error 	 0.0006920415224913495
are easier 	 0.004149377593360996
of answer 	 0.00089126559714795
, handling 	 0.0011229646266142617
algorithms optimized 	 0.02857142857142857
de l'assignation 	 0.5
: give 	 0.0196078431372549
sentence of 	 0.020833333333333332
might not 	 0.07692307692307693
choices that 	 0.2
simple and 	 0.07692307692307693
translation requires 	 0.013513513513513514
Consider the 	 1.0
improve readability 	 0.07692307692307693
sales data 	 0.3333333333333333
as humans 	 0.003484320557491289
<s> Jump 	 0.0007686395080707148
assumptions such 	 0.2
summary based 	 0.023809523809523808
make it 	 0.1
international relations 	 0.5
This corpus 	 0.031746031746031744
you have 	 0.15384615384615385
presence of 	 1.0
-RRB- work 	 0.0027100271002710027
user , 	 0.07142857142857142
accusative , 	 1.0
Automatic segmentation 	 0.3333333333333333
an internal 	 0.022727272727272728
such as 	 0.7317073170731707
profile captures 	 0.3333333333333333
merely assigning 	 0.5
be selected 	 0.004219409282700422
's instructions 	 0.0196078431372549
, education 	 0.0005614823133071309
a co-occurrence 	 0.001226993865030675
humans transcribe 	 0.08333333333333333
recognition but 	 0.008264462809917356
developing new 	 0.25
any speaker 	 0.03225806451612903
basic level 	 0.07692307692307693
more like 	 0.010526315789473684
conducted in 	 0.4
by concatenating 	 0.005714285714285714
by decomposing 	 0.005714285714285714
considered separately 	 0.1111111111111111
to enhance 	 0.0013280212483399733
the F35 	 0.0006920415224913495
, candidacies 	 0.0005614823133071309
was done 	 0.012987012987012988
often possible 	 0.022727272727272728
-LRB- which 	 0.008130081300813009
term language 	 0.05555555555555555
computer performance 	 0.022727272727272728
world . 	 0.13333333333333333
or phones 	 0.0045045045045045045
combining it 	 0.25
they helped 	 0.025
the object 	 0.0006920415224913495
current QA 	 0.14285714285714285
documents containing 	 0.02631578947368421
first order 	 0.030303030303030304
Sydney Lamb 	 1.0
However such 	 0.02702702702702703
of continuous 	 0.0017825311942959
Rule-based The 	 0.5
agree about 	 0.3333333333333333
's NLP 	 0.0196078431372549
<s> Starting 	 0.0007686395080707148
comes into 	 0.2
are less 	 0.004149377593360996
in deaf 	 0.0018726591760299626
conjunction with 	 0.6666666666666666
system is 	 0.0967741935483871
contain names 	 0.08333333333333333
such a 	 0.04878048780487805
this shifting 	 0.01098901098901099
Maximum entropy 	 0.6666666666666666
Black E. 	 0.5
, relatively 	 0.0005614823133071309
Algorithm -RRB- 	 1.0
collection of 	 0.4
combining decisions 	 0.25
characterize a 	 0.5
and Unsupervised 	 0.001445086705202312
accidentally omitted 	 1.0
`` proper 	 0.005291005291005291
or continuous 	 0.0045045045045045045
cursive handwriting 	 0.2
usually from 	 0.03125
them out 	 0.05263157894736842
statistics : 	 0.125
, business 	 0.0005614823133071309
summarization approaches 	 0.02
spite of 	 1.0
universal encyclopedia 	 0.3333333333333333
ASR . 	 0.16666666666666666
that there 	 0.0070921985815602835
not be 	 0.10714285714285714
were then 	 0.024390243902439025
is Shift-Reduce 	 0.0020325203252032522
simultaneously with 	 0.5
Winograd was 	 0.3333333333333333
and then 	 0.010115606936416185
13 parsers 	 0.5
-LRB- or 	 0.02710027100271003
level 7 	 0.05
6 over 	 0.25
<s> DeRose 	 0.0015372790161414297
dynamic character 	 0.2
accuracy of 	 0.12903225806451613
context data 	 0.030303030303030304
not map 	 0.008928571428571428
of more 	 0.0035650623885918
possible unigrams 	 0.041666666666666664
segments each 	 0.2
`` Did 	 0.005291005291005291
and weaknesses 	 0.001445086705202312
examples and 	 0.16666666666666666
of using 	 0.00089126559714795
are needed 	 0.004149377593360996
Hence -LRB- 	 0.5
and Rubin 	 0.001445086705202312
some programming 	 0.012048192771084338
action is 	 0.2
source document 	 0.08333333333333333
various optimization 	 0.05555555555555555
recognized with 	 0.16666666666666666
of Speaker 	 0.00089126559714795
able to 	 1.0
technology is 	 0.13636363636363635
to deal 	 0.0013280212483399733
answering deals 	 0.16666666666666666
approaches used 	 0.03571428571428571
digital texts 	 0.14285714285714285
published his 	 0.14285714285714285
state automata 	 0.07142857142857142
a flight 	 0.001226993865030675
as opposed 	 0.003484320557491289
is likely 	 0.006097560975609756
salience . 	 1.0
classifiers -RRB- 	 0.5
scale , 	 0.3333333333333333
this is 	 0.0989010989010989
value , 	 0.3333333333333333
actual measurement 	 0.2
foreign word 	 0.5
the serial 	 0.0006920415224913495
linear discriminant 	 0.2857142857142857
began selling 	 0.14285714285714285
spoken version 	 0.07142857142857142
databases . 	 0.5
how it 	 0.06896551724137931
which is 	 0.09420289855072464
delta and 	 1.0
real difference 	 0.1111111111111111
users sometimes 	 0.1111111111111111
<s> Automatic 	 0.005380476556495004
Fowler , 	 1.0
restrictions . 	 1.0
is speaking 	 0.0020325203252032522
explicit by 	 0.2
coordinates and 	 1.0
same person 	 0.04
summers of 	 1.0
generating examples 	 0.2
the editor 	 0.0006920415224913495
collection sizes 	 0.2
into individual 	 0.01282051282051282
a date 	 0.001226993865030675
quality and 	 0.1
evaluating NLG 	 0.2
view -LRB- 	 0.3333333333333333
blogs , 	 0.5
wrote The 	 0.16666666666666666
speech feature 	 0.006578947368421052
frequency with 	 0.5
deep '' 	 0.14285714285714285
typically given 	 0.05555555555555555
sentences in 	 0.10526315789473684
this article 	 0.04395604395604396
learned on 	 0.2
people from 	 0.0625
`` Army 	 0.005291005291005291
to 1966 	 0.0013280212483399733
using edges 	 0.01694915254237288
Jabberwacky . 	 1.0
machine-encoded text 	 1.0
-LRB- end 	 0.0027100271002710027
A popular 	 0.02
has two 	 0.011904761904761904
stochastic purposes 	 0.125
semantically constrained 	 1.0
character error 	 0.045454545454545456
, if 	 0.005614823133071308
sentence breaks 	 0.020833333333333332
help improve 	 0.1111111111111111
Applications The 	 0.5
network is 	 0.16666666666666666
is also 	 0.02032520325203252
to medical 	 0.0013280212483399733
text or 	 0.018867924528301886
disfluences -LRB- 	 1.0
to assist 	 0.0013280212483399733
500 samples 	 0.5
MT performs 	 0.2
On the 	 0.3333333333333333
methods more 	 0.022727272727272728
current research 	 0.14285714285714285
Applied linguistics 	 0.5
language during 	 0.006756756756756757
searched , 	 1.0
and answer 	 0.001445086705202312
known what 	 0.038461538461538464
letters blend 	 0.1
Often a 	 0.3333333333333333
be applied 	 0.012658227848101266
competitions devoted 	 1.0
top T 	 0.4
different ones 	 0.02040816326530612
DA -RRB- 	 0.6666666666666666
also help 	 0.014492753623188406
grid & 	 1.0
the mid-1960s 	 0.0006920415224913495
recorded speech 	 0.5
he or 	 0.14285714285714285
etc. , 	 0.045454545454545456
abbreviations that 	 0.2
If `` 	 0.1
linked in 	 0.3333333333333333
to clarify 	 0.0013280212483399733
with known 	 0.01092896174863388
specially designed 	 1.0
OnlineOCR With 	 0.3333333333333333
a semantic 	 0.001226993865030675
1978 Kurzweil 	 0.3333333333333333
by simple 	 0.005714285714285714
<s> Reading 	 0.0007686395080707148
Project , 	 1.0
approach to 	 0.17142857142857143
to '' 	 0.0026560424966799467
surprisingly difficult 	 0.3333333333333333
Optical character 	 0.6666666666666666
corresponding text 	 0.16666666666666666
algorithm could 	 0.03571428571428571
e.g. elaboration 	 0.017857142857142856
, leaving 	 0.0005614823133071309
1 -RRB- 	 0.25
the recognition 	 0.001384083044982699
The approaches 	 0.005208333333333333
in computational 	 0.003745318352059925
and named 	 0.001445086705202312
emerged as 	 1.0
is specifically 	 0.0020325203252032522
to overfitting 	 0.0013280212483399733
pro or 	 1.0
and count 	 0.001445086705202312
by providing 	 0.005714285714285714
and characterizes 	 0.001445086705202312
general learning 	 0.045454545454545456
singular proper 	 0.25
detail . 	 0.5
all possibilities 	 0.023255813953488372
art for 	 0.5
still not 	 0.06666666666666667
<s> Open-domain 	 0.0007686395080707148
produce output 	 0.09090909090909091
text conversion 	 0.006289308176100629
word blend 	 0.016666666666666666
ATC training 	 0.4
CWA -RRB- 	 1.0
and waves 	 0.001445086705202312
fixed static 	 0.5
for various 	 0.0036101083032490976
tongues sharing 	 1.0
an eyes-busy 	 0.007575757575757576
but instead 	 0.014705882352941176
with regards 	 0.00546448087431694
earliest such 	 0.5
or 4-gram 	 0.0045045045045045045
or its 	 0.0045045045045045045
systems include 	 0.008928571428571428
visual detection 	 0.5
technologies -- 	 0.25
how close 	 0.034482758620689655
Referring expression 	 1.0
Inter-rater reliability 	 1.0
in `` 	 0.0056179775280898875
learn tag 	 0.07692307692307693
mentions within 	 0.3333333333333333
, Margaret 	 0.0005614823133071309
the typical 	 0.0006920415224913495
text ; 	 0.006289308176100629
theory of 	 0.07692307692307693
these problems 	 0.023809523809523808
learn the 	 0.07692307692307693
solve properly 	 0.25
a simple 	 0.001226993865030675
the morphemes 	 0.0006920415224913495
had an 	 0.07142857142857142
decisions probabilistically 	 0.1
genre and 	 0.5
desired language 	 0.2
of effort 	 0.00089126559714795
procedural information 	 1.0
an algorithm 	 0.022727272727272728
potentially more 	 0.3333333333333333
mixture of 	 1.0
simplest -LRB- 	 1.0
11 point 	 1.0
RCA collaborated 	 0.2
problem from 	 0.022727272727272728
a communicative 	 0.001226993865030675
-- whole 	 0.04
, Santoni 	 0.0005614823133071309
that they 	 0.024822695035460994
, shallow 	 0.0005614823133071309
the word 	 0.005536332179930796
speech caused 	 0.006578947368421052
My head 	 1.0
to databases 	 0.0013280212483399733
his `` 	 0.08333333333333333
separate parts 	 0.1
machine would 	 0.02531645569620253
summaries helps 	 0.023255813953488372
dogs the 	 0.14285714285714285
<s> Solutions 	 0.0007686395080707148
during World 	 0.1
the art 	 0.001384083044982699
where it 	 0.02857142857142857
When we 	 0.14285714285714285
, regardless 	 0.0016844469399213925
At Stanford 	 0.3333333333333333
exist . 	 1.0
this character 	 0.01098901098901099
e.g. Querying 	 0.017857142857142856
capitalizes all 	 1.0
primary topics 	 0.5
translation Machine 	 0.013513513513513514
was nearly 	 0.012987012987012988
speaker , 	 0.05555555555555555
classification : 	 0.058823529411764705
translation -LRB- 	 0.02702702702702703
the testing 	 0.0006920415224913495
be electronically 	 0.004219409282700422
Statistical machine 	 0.2222222222222222
some variant 	 0.012048192771084338
, style 	 0.0005614823133071309
verb -LRB- 	 0.15384615384615385
model and 	 0.03333333333333333
Norman , 	 0.5
be thought 	 0.004219409282700422
every 10msec 	 0.3333333333333333
uses spontaneous 	 0.07142857142857142
parse tree 	 0.1111111111111111
to set 	 0.00398406374501992
, merging 	 0.0005614823133071309
more quickly 	 0.010526315789473684
algorithm implicitly 	 0.03571428571428571
historically used 	 0.5
, rule-based 	 0.0005614823133071309
Improved output 	 1.0
probability , 	 0.14285714285714285
recognition since 	 0.008264462809917356
and end 	 0.001445086705202312
The two 	 0.010416666666666666
full progress 	 0.2
as interlingual 	 0.003484320557491289
XML documents 	 1.0
errors or 	 0.2
the very 	 0.0006920415224913495
that `` 	 0.02127659574468085
example type 	 0.012345679012345678
a 3 	 0.001226993865030675
best one 	 0.05555555555555555
Bernard Vauquois 	 1.0
NLP algorithms 	 0.0425531914893617
NLP The 	 0.0425531914893617
the author 	 0.0020761245674740486
<s> Knowledge 	 0.0007686395080707148
greater than 	 0.3333333333333333
it helps 	 0.008547008547008548
widely-reported news 	 1.0
Recall this 	 0.3333333333333333
documents onto 	 0.02631578947368421
well captured 	 0.03571428571428571
vector . 	 0.3333333333333333
<s> Full 	 0.0007686395080707148
disambiguation often 	 0.1
caps -RRB- 	 1.0
, P 	 0.0005614823133071309
probabilistic modeling 	 0.14285714285714285
Microsoft Voice 	 0.5
from section 	 0.009615384615384616
campaign compared 	 0.2
, large-vocabulary 	 0.0005614823133071309
and ELIZA 	 0.002890173410404624
collections , 	 0.5
translator 's 	 0.2857142857142857
restricted vocabulary 	 0.25
ROUGE is 	 0.4
evaluation step 	 0.037037037037037035
voicemail to 	 1.0
pre-processing e.g. 	 1.0
and maximum 	 0.001445086705202312
required . 	 0.14285714285714285
never be 	 0.4
semantic schemes 	 0.047619047619047616
ambiguous there 	 0.08333333333333333
after removing 	 0.08333333333333333
One might 	 0.07692307692307693
this point 	 0.01098901098901099
`` blocks 	 0.010582010582010581
feature of 	 0.23076923076923078
more accurately 	 0.010526315789473684
to aid 	 0.0013280212483399733
: Category 	 0.00980392156862745
linguist to 	 0.5
-LRB- Recall-Oriented 	 0.005420054200542005
stochastic taggers 	 0.125
a rapidly 	 0.001226993865030675
`` Speaker 	 0.010582010582010581
short intervals 	 0.125
seminal paper 	 1.0
eigenvalue 1 	 1.0
seen as 	 0.3
such ambiguity 	 0.024390243902439025
been produced 	 0.014705882352941176
models ... 	 0.038461538461538464
a program 	 0.0049079754601227
mathematical rules 	 0.5
consonants depends 	 0.3333333333333333
distortions -LRB- 	 1.0
sometimes preferred 	 0.07692307692307693
extensively used 	 1.0
workshops , 	 0.5
, for 	 0.012352610892756879
noun can 	 0.07142857142857142
have assessed 	 0.009615384615384616
do '' 	 0.038461538461538464
at the 	 0.22058823529411764
facing more 	 1.0
standard random 	 0.07142857142857142
QA systems 	 0.2857142857142857
speech act 	 0.006578947368421052
of filtering 	 0.00089126559714795
overall speech 	 0.16666666666666666
versions of 	 0.3333333333333333
Books like 	 1.0
the shops 	 0.0006920415224913495
and phrases 	 0.002890173410404624
a context 	 0.00245398773006135
to machine-learning 	 0.0013280212483399733
and Callaghan 	 0.001445086705202312
good translation 	 0.07692307692307693
and Grass 	 0.001445086705202312
benefits : 	 0.5
feature which 	 0.07692307692307693
tokens like 	 0.14285714285714285
Rates Increase 	 1.0
the peak 	 0.0006920415224913495
in differing 	 0.0018726591760299626
October 2007 	 1.0
language for 	 0.02027027027027027
and Snyder 	 0.001445086705202312
well be 	 0.03571428571428571
known word 	 0.038461538461538464
said with 	 1.0
only rely 	 0.02631578947368421
allows for 	 0.125
to gather 	 0.0013280212483399733
indifferent to 	 1.0
Union as 	 1.0
A linguist 	 0.02
used all 	 0.008849557522123894
ranked highly 	 0.4
Anaphora resolution 	 1.0
the continuously 	 0.0006920415224913495
analysis system 	 0.015384615384615385
subjective . 	 0.3333333333333333
Each article 	 0.16666666666666666
can occur 	 0.0055248618784530384
by interactive 	 0.005714285714285714
user has 	 0.07142857142857142
right-hand-sides of 	 1.0
Marc Angenot 	 1.0
with his 	 0.00546448087431694
doing as 	 0.5
some classification-related 	 0.012048192771084338
fail for 	 0.3333333333333333
12 , 	 0.2
languages like 	 0.02
learn such 	 0.07692307692307693
examples in 	 0.041666666666666664
not use 	 0.017857142857142856
<s> They 	 0.0023059185242121443
, though 	 0.003368893879842785
text image 	 0.006289308176100629
campaign was 	 0.2
approaches , 	 0.03571428571428571
that area 	 0.0035460992907801418
are hardly 	 0.004149377593360996
of TextRank 	 0.00089126559714795
shared-task events 	 1.0
negative examples 	 0.125
and stochastic 	 0.001445086705202312
noun , 	 0.42857142857142855
-RRB- break 	 0.0027100271002710027
learning As 	 0.023255813953488372
similarity . 	 0.1
world '' 	 0.06666666666666667
<s> Efficient 	 0.0007686395080707148
standards require 	 0.2
organization documents 	 0.2
include contractions 	 0.037037037037037035
<s> Evaluating 	 0.0015372790161414297
the task 	 0.004844290657439446
reading credit 	 0.125
a specialised 	 0.001226993865030675
tags . 	 0.3333333333333333
and right 	 0.002890173410404624
purposes . 	 0.5
more times 	 0.010526315789473684
Constraint Grammar 	 1.0
language in 	 0.006756756756756757
other cockpit 	 0.014285714285714285
while `` 	 0.05
appropriate action 	 0.25
of unstructured 	 0.00089126559714795
we may 	 0.022222222222222223
, false 	 0.0005614823133071309
direct hand 	 0.16666666666666666
number on 	 0.023255813953488372
to 90 	 0.0013280212483399733
higher degree 	 0.14285714285714285
sense and 	 0.125
above text 	 0.07692307692307693
a row 	 0.001226993865030675
appears multiple 	 0.2
of possibilities 	 0.00089126559714795
distinguish between 	 0.4
to unsupervised 	 0.0026560424966799467
separators -RRB- 	 1.0
around 6 	 0.375
is prone 	 0.0020325203252032522
limits -LRB- 	 1.0
semantics . 	 0.07142857142857142
and data 	 0.005780346820809248
matching the 	 0.2
the E-set 	 0.0006920415224913495
or XML 	 0.0045045045045045045
the application 	 0.0006920415224913495
are averaged 	 0.004149377593360996
adjacent instances 	 0.16666666666666666
also that 	 0.014492753623188406
is entirely 	 0.0020325203252032522
such template-matching 	 0.008130081300813009
The NIST 	 0.005208333333333333
one reference 	 0.015384615384615385
Applications include 	 0.5
as interactivity 	 0.003484320557491289
regions . 	 0.5
with human-made 	 0.00546448087431694
now called 	 0.07692307692307693
the graph 	 0.004152249134948097
many significant 	 0.019230769230769232
, recall 	 0.0005614823133071309
H. Shepard 	 0.5
, however 	 0.006176305446378439
's and 	 0.0196078431372549
huge handmade 	 1.0
when inter-annotator 	 0.02857142857142857
to save 	 0.0013280212483399733
purpose of 	 0.2
1629 , 	 1.0
than 98 	 0.022222222222222223
many systems 	 0.019230769230769232
disambiguation on 	 0.1
also obtained 	 0.014492753623188406
captioned telephone 	 1.0
time question 	 0.030303030303030304
existing so 	 0.2
basic technology 	 0.07692307692307693
after stemming 	 0.08333333333333333
of assertions 	 0.00089126559714795
content selection 	 0.08333333333333333
groups within 	 0.2
term , 	 0.05555555555555555
dogs '' 	 0.5714285714285714
simulation is 	 0.3333333333333333
several modules 	 0.045454545454545456
define several 	 0.5
CSR have 	 0.3333333333333333
multi-platforms such 	 1.0
mouse driven 	 1.0
contextual polarity 	 0.5
svg This 	 1.0
systems usually 	 0.017857142857142856
most positive 	 0.017241379310344827
an extension 	 0.007575757575757576
make use 	 0.05
criteria and 	 0.25
be measured 	 0.004219409282700422
, shop 	 0.0005614823133071309
visited and 	 1.0
utterance and 	 0.3333333333333333
is it 	 0.0020325203252032522
entity about 	 0.2
generate text 	 0.05555555555555555
create more 	 0.058823529411764705
automatically generated 	 0.14285714285714285
decided without 	 0.3333333333333333
-LRB- POS 	 0.005420054200542005
understanding or 	 0.030303030303030304
notably to 	 0.3333333333333333
needs . 	 0.1
Obama , 	 1.0
largely been 	 0.2
set '' 	 0.05128205128205128
analysis '' 	 0.015384615384615385
utility with 	 0.5
syntactic features 	 0.07692307692307693
complex recognition 	 0.041666666666666664
impact of 	 0.5
subset of 	 1.0
and related 	 0.004335260115606936
each state 	 0.022222222222222223
the identity 	 0.002768166089965398
of an 	 0.011586452762923352
the isolation 	 0.0006920415224913495
for language 	 0.0036101083032490976
of people 	 0.00089126559714795
can exploit 	 0.0055248618784530384
known for 	 0.038461538461538464
explicit formalization 	 0.2
a very 	 0.013496932515337423
weaker . 	 1.0
2008 -RRB- 	 1.0
Prominent discourse 	 1.0
<s> Issues 	 0.0015372790161414297
split it 	 0.25
rates can 	 0.125
syntactic , 	 0.07692307692307693
sets from 	 0.09090909090909091
comprehension of 	 0.2857142857142857
and natural 	 0.005780346820809248
accurately -LRB- 	 0.5
involves several 	 0.1
networks Neural 	 0.07142857142857142
continuous text 	 0.16666666666666666
understood only 	 1.0
on Speech 	 0.0047169811320754715
, syntactic 	 0.0016844469399213925
release beyond 	 0.3333333333333333
Ohio Bell 	 1.0
records and 	 0.25
What are 	 0.36363636363636365
Nearest-neighbor have 	 1.0
an attribute 	 0.007575757575757576
of candidates 	 0.00089126559714795
recognition -RRB- 	 0.008264462809917356
representing printed 	 0.5
identified . 	 0.2
<s> Of 	 0.0007686395080707148
articles on 	 0.125
`` I 	 0.005291005291005291
or an 	 0.009009009009009009
both rules 	 0.03225806451612903
using punctuation 	 0.01694915254237288
be an 	 0.004219409282700422
taggers can 	 0.14285714285714285
Lexical segmentation 	 0.5
simplify the 	 1.0
vowels depends 	 0.3333333333333333
The essential 	 0.005208333333333333
the examples 	 0.0020761245674740486
increasing G-loads 	 0.3333333333333333
-RRB- Modern 	 0.0027100271002710027
demonstration was 	 0.2
titles and 	 0.5
is worth 	 0.0040650406504065045
the resulting 	 0.001384083044982699
by resorting 	 0.005714285714285714
including images 	 0.07142857142857142
Handel also 	 1.0
analysis has 	 0.015384615384615385
humans in 	 0.08333333333333333
same summary 	 0.04
% , 	 0.05128205128205128
rules that 	 0.09302325581395349
procedures , 	 0.25
millions of 	 1.0
attempted by 	 1.0
customize OCR 	 0.5
recognition task 	 0.01652892561983471
Hard to 	 0.5
Extraction Algorithm 	 0.3333333333333333
associated number 	 0.25
by transfer-based 	 0.005714285714285714
issue , 	 0.125
contains all 	 0.1
are keyphrase 	 0.004149377593360996
correct result 	 0.06666666666666667
and allows 	 0.002890173410404624
the Web 	 0.001384083044982699
search Query 	 0.09090909090909091
the parse 	 0.0006920415224913495
the current 	 0.001384083044982699
+5 scale 	 1.0
OCR Document 	 0.02040816326530612
campaign on 	 0.2
, opened 	 0.0005614823133071309
controversy is 	 1.0
understand '' 	 0.14285714285714285
but when 	 0.014705882352941176
difficulties discussed 	 0.5
sentiment words 	 0.04
by Henry 	 0.005714285714285714
larger set 	 0.0625
method and 	 0.0625
be put 	 0.004219409282700422
one element 	 0.015384615384615385
active area 	 0.5
within Tipster 	 0.05555555555555555
experience , 	 0.5
computational linguistics 	 0.6
, tables 	 0.0005614823133071309
Computers can 	 1.0
about to 	 0.025
which of 	 0.007246376811594203
can decide 	 0.0055248618784530384
modern statistically-based 	 0.2
-LRB- e.g. 	 0.10298102981029811
the source 	 0.008304498269896194
two given 	 0.034482758620689655
topic of 	 0.125
<s> Further 	 0.0023059185242121443
values for 	 0.125
in 1966 	 0.0018726591760299626
language model 	 0.006756756756756757
weighted to 	 0.3333333333333333
based therapy 	 0.018518518518518517
can both 	 0.0055248618784530384
in helicopters 	 0.003745318352059925
problem than 	 0.022727272727272728
and see 	 0.001445086705202312
, means 	 0.0005614823133071309
strategies to 	 0.5
Handwriting recognition 	 1.0
be done 	 0.02109704641350211
with `` 	 0.01639344262295082
expected . 	 0.14285714285714285
word-frequency and 	 1.0
Spoken Language 	 1.0
helicopters is 	 0.5
identification pattern 	 0.2
problem because 	 0.022727272727272728
not rely 	 0.017857142857142856
, strategies 	 0.0005614823133071309
can improve 	 0.0055248618784530384
into all 	 0.01282051282051282
the theoretical 	 0.0006920415224913495
the desired 	 0.002768166089965398
plural , 	 0.4
rudimentary way 	 0.5
and syntax 	 0.001445086705202312
keyboard a 	 0.3333333333333333
heuristics to 	 0.5
College -LRB- 	 0.5
finished writing 	 0.5
as 50 	 0.003484320557491289
paraphrases -LRB- 	 1.0
notably by 	 0.3333333333333333
individual speaker 	 0.08333333333333333
size N 	 0.16666666666666666
R. McDonald 	 0.16666666666666666
definition of 	 0.6
automatically the 	 0.047619047619047616
Penn tag 	 0.2222222222222222
product became 	 0.14285714285714285
all lower 	 0.023255813953488372
or characters 	 0.0045045045045045045
rule-based machine 	 0.14285714285714285
candidates for 	 0.2
a vocabulary 	 0.001226993865030675
problems for 	 0.058823529411764705
While there 	 0.2
the letters 	 0.0006920415224913495
In many 	 0.01904761904761905
immediate neighbors 	 1.0
It was 	 0.05263157894736842
as named 	 0.003484320557491289
the Ohio 	 0.0006920415224913495
grammars -LRB- 	 0.07142857142857142
resulting in 	 0.25
sentiment about 	 0.04
a rate 	 0.001226993865030675
Technolangue\/Easy Text 	 0.5
1955 , 	 1.0
such system 	 0.008130081300813009
are concerned 	 0.004149377593360996
rule-based translation 	 0.14285714285714285
the preceding 	 0.0006920415224913495
as EAGLi 	 0.003484320557491289
were conducted 	 0.024390243902439025
journals include 	 0.5
project and 	 0.07692307692307693
the ROUGE-1 	 0.0006920415224913495
to determining 	 0.0013280212483399733
practical systems 	 0.5
in medical 	 0.0018726591760299626
area include 	 0.09090909090909091
as phoneme 	 0.003484320557491289
implemented on 	 0.2
thousands of 	 0.6666666666666666
SR while 	 0.3333333333333333
at conclusions 	 0.014705882352941176
1995 -RRB- 	 1.0
OCR in 	 0.04081632653061224
creates a 	 0.5
accommodate left 	 0.2
both of 	 0.06451612903225806
or sentences 	 0.0045045045045045045
was developed 	 0.012987012987012988
name -LRB- 	 0.2
'' sentiment 	 0.005154639175257732
In common 	 0.009523809523809525
ratings usually 	 0.1111111111111111
partially influenced 	 1.0
`` Apparatus 	 0.005291005291005291
Chinese characters 	 0.14285714285714285
reasoning approach 	 0.14285714285714285
testing accuracy 	 0.2
input six 	 0.024390243902439025
background material 	 0.3333333333333333
it difficult 	 0.017094017094017096
much less 	 0.045454545454545456
see appraisal 	 0.05
complex problem 	 0.041666666666666664
other forms 	 0.014285714285714285
worldwide view 	 1.0
, French 	 0.0005614823133071309
was recognized 	 0.012987012987012988
Morpheme Analysis 	 1.0
, Steven 	 0.0005614823133071309
This can 	 0.015873015873015872
currently require 	 0.14285714285714285
1969 , 	 0.5
already placed 	 0.2
accuracy because 	 0.03225806451612903
must interpret 	 0.07142857142857142
to another 	 0.00398406374501992
approximately 200 	 0.5
including mobile 	 0.07142857142857142
Hirschman L. 	 0.5
Such strategy 	 0.125
also general 	 0.014492753623188406
sentence also 	 0.020833333333333332
state of 	 0.35714285714285715
D. Booth 	 0.2
camp is 	 0.25
erroneous input 	 1.0
non-existent words 	 1.0
a single 	 0.011042944785276074
to refer 	 0.0013280212483399733
developing text 	 0.25
thus reducing 	 0.1
evaluated using 	 0.14285714285714285
a referring 	 0.001226993865030675
Technolangue\/Easy project 	 0.5
disambiguation concerns 	 0.1
been more 	 0.029411764705882353
-LRB- predict 	 0.0027100271002710027
to by 	 0.0026560424966799467
kind to 	 0.09090909090909091
performed in 	 0.2
was also 	 0.025974025974025976
phrases and 	 0.1875
interoperability between 	 1.0
12 \* 	 0.4
which undertook 	 0.007246376811594203
and the 	 0.059248554913294796
additional features 	 0.16666666666666666
-LRB- context-free 	 0.0027100271002710027
than precision 	 0.022222222222222223
aircraft Substantial 	 0.14285714285714285
quite different 	 0.375
diversity : 	 0.25
compare various 	 0.14285714285714285
very much 	 0.024390243902439025
called morphological 	 0.05555555555555555
units -- 	 0.14285714285714285
point , 	 0.6666666666666666
the sentiment 	 0.001384083044982699
of grammar 	 0.0017825311942959
e.g. Constraints 	 0.017857142857142856
each whole 	 0.022222222222222223
a significant 	 0.001226993865030675
successfully in 	 0.3333333333333333
real-world information 	 0.16666666666666666
, Roger 	 0.0005614823133071309
top-down parsing 	 0.5
personal computers 	 0.25
<s> the 	 0.0007686395080707148
like in 	 0.03571428571428571
<s> On-line 	 0.0023059185242121443
option . 	 1.0
importance of 	 0.5
this task 	 0.04395604395604396
recall-based measure 	 0.5
unigram , 	 0.6
for word 	 0.0036101083032490976
contexts . 	 0.2857142857142857
due to 	 0.4
tried to 	 0.3333333333333333
skew , 	 1.0
distinguish names 	 0.2
is farther 	 0.0020325203252032522
length , 	 0.25
calling for 	 1.0
requires significant 	 0.0625
be required 	 0.004219409282700422
interest in 	 0.6363636363636364
coherent or 	 0.2
to converse 	 0.0013280212483399733
translation . 	 0.05405405405405406
commonly referred 	 0.125
on-line character 	 0.3333333333333333
output distribution 	 0.038461538461538464
Basically , 	 1.0
provide manually 	 0.16666666666666666
See machine 	 0.16666666666666666
gone into 	 1.0
abstraction . 	 0.25
machine language 	 0.0379746835443038
, symbolic 	 0.0005614823133071309
approach 90 	 0.02857142857142857
known to 	 0.07692307692307693
, understanding 	 0.0005614823133071309
more time-consuming 	 0.010526315789473684
Word-sense disambiguation 	 1.0
rule induction 	 0.3333333333333333
Art Graesser 	 1.0
and smaller 	 0.001445086705202312
Recent research 	 0.6666666666666666
reduce acoustic 	 1.0
English in 	 0.02702702702702703
in 2007 	 0.0018726591760299626
historically -RRB- 	 0.5
-LRB- VITO 	 0.0027100271002710027
spacecraft , 	 1.0
those pauses 	 0.045454545454545456
by A. 	 0.005714285714285714
commercializing paper-to-computer 	 1.0
Interactive QA 	 0.5
physician , 	 1.0
<s> Extractive 	 0.0007686395080707148
problems Processes 	 0.058823529411764705
occurred in 	 1.0
Rhetoric Stylistics 	 1.0
when annotating 	 0.02857142857142857
comprises all 	 1.0
them . 	 0.10526315789473684
intended for 	 0.4
keyphrases and 	 0.02857142857142857
Veterans Administration 	 1.0
techniques offer 	 0.043478260869565216
analysis was 	 0.03076923076923077
translator needs 	 0.14285714285714285
embedded lists 	 0.25
a variety 	 0.008588957055214725
inadequate protection 	 1.0
processes used 	 0.4
<s> Conferences 	 0.0007686395080707148
condensation operations 	 1.0
line of 	 0.3333333333333333
\* , 	 0.5
discussed between 	 0.14285714285714285
this meaning 	 0.01098901098901099
common-sense reasoning 	 1.0
that serve 	 0.0035460992907801418
training documents 	 0.10714285714285714
maintenance -RRB- 	 1.0
noting that 	 1.0
talk-in-interaction . 	 1.0
Accuracy rates 	 0.2857142857142857
The late 	 0.005208333333333333
algorithms is 	 0.02857142857142857
Digitized Sound 	 1.0
`` an 	 0.005291005291005291
that use 	 0.0070921985815602835
HTK toolkit 	 0.5
suitable -LRB- 	 0.25
for words 	 0.0036101083032490976
determine `` 	 0.043478260869565216
simple keyword 	 0.038461538461538464
`` semi-supervised 	 0.005291005291005291
on summarization 	 0.0047169811320754715
devices for 	 0.25
had plateaued 	 0.07142857142857142
representation framework 	 0.05263157894736842
question classifier 	 0.023809523809523808
resolved : 	 1.0
, Screenshot 	 0.0005614823133071309
Precision measures 	 1.0
understanding programs 	 0.030303030303030304
convey intended 	 0.3333333333333333
or printed 	 0.0045045045045045045
interactivity -LRB- 	 1.0
digitized , 	 1.0
1976 -RRB- 	 0.5
positive sentiment 	 0.14285714285714285
-LRB- University 	 0.0027100271002710027
phrases are 	 0.0625
excerpt containing 	 1.0
their system 	 0.029411764705882353
Natural Language 	 0.25
the single 	 0.0006920415224913495
and signing 	 0.001445086705202312
left , 	 0.16666666666666666
Bush '' 	 0.5
French in 	 0.125
most basic 	 0.017241379310344827
copied and 	 0.5
translation of 	 0.14864864864864866
reading comprehension 	 0.25
an online 	 0.007575757575757576
, after 	 0.0005614823133071309
techniques can 	 0.043478260869565216
Nelson Francis 	 1.0
smoothly or 	 0.5
<s> About 	 0.0007686395080707148
speeches , 	 1.0
underpinnings discouraged 	 1.0
human judges 	 0.043478260869565216
more subjective 	 0.010526315789473684
3 '' 	 0.2
its nascent 	 0.02857142857142857
whole of 	 0.1111111111111111
an extractive 	 0.007575757575757576
Eight years 	 1.0
of knowledge 	 0.0017825311942959
neural nets 	 0.06666666666666667
-LRB- Nuance 	 0.0027100271002710027
Lao , 	 1.0
information in 	 0.043478260869565216
average text 	 0.5
maintains that 	 1.0
Other tasks 	 0.14285714285714285
five different 	 0.2
features '' 	 0.038461538461538464
algorithm is 	 0.17857142857142858
as in 	 0.027874564459930314
has a 	 0.047619047619047616
`` do 	 0.005291005291005291
both left-most 	 0.03225806451612903
ask for 	 0.25
given a 	 0.16666666666666666
campaigns within 	 0.5
coherent summary 	 0.2
paper-intensive industry 	 1.0
common to 	 0.04
Customized OCR 	 1.0
less expensive 	 0.08333333333333333
of faster 	 0.00089126559714795
input gracefully 	 0.024390243902439025
systems can 	 0.026785714285714284
computing . 	 0.5
of 1,500 	 0.00089126559714795
work has 	 0.08333333333333333
continuous recognition 	 0.16666666666666666
relationships among 	 0.16666666666666666
? '' 	 0.375
a hybrid 	 0.00245398773006135
evaluation In 	 0.018518518518518517
have limited 	 0.009615384615384616
not its 	 0.008928571428571428
case of 	 0.35294117647058826
recognizing difficult 	 0.2
perform functions 	 0.09090909090909091
the possibility 	 0.002768166089965398
looks natural 	 0.25
feature in 	 0.07692307692307693
would run 	 0.03773584905660377
items in 	 0.5
or paragraphs 	 0.009009009009009009
additional clues 	 0.16666666666666666
in predicate 	 0.0018726591760299626
demonstration of 	 0.4
absorbing states 	 0.3333333333333333
evaluation Depending 	 0.018518518518518517
wrote an 	 0.3333333333333333
e.g. who 	 0.017857142857142856
discourse begin 	 0.027777777777777776
phoneme , 	 0.5
to input 	 0.0026560424966799467
table that 	 0.14285714285714285
knowledge representation 	 0.037037037037037035
keyphrases that 	 0.08571428571428572
statistical ; 	 0.030303030303030304
the important 	 0.0006920415224913495
Manfred R. 	 1.0
conduct with 	 1.0
a nautical 	 0.001226993865030675
for parse 	 0.0036101083032490976
effectiveness . 	 0.3333333333333333
or subjective 	 0.009009009009009009
book '' 	 0.125
system determine 	 0.010752688172043012
category registry 	 0.5
harder it 	 0.14285714285714285
The human 	 0.005208333333333333
nice properties 	 0.25
question and 	 0.023809523809523808
communication radios 	 0.2
of word-forms 	 0.00089126559714795
performance is 	 0.1111111111111111
-- indeed 	 0.04
of physics 	 0.00089126559714795
of ISO\/TC37 	 0.00089126559714795
Records -LRB- 	 1.0
units ; 	 0.14285714285714285
to mimic 	 0.0013280212483399733
the expected 	 0.001384083044982699
and control 	 0.004335260115606936
also programs 	 0.014492753623188406
Man bites 	 0.5
system might 	 0.010752688172043012
that TextRank 	 0.0070921985815602835
her judgement 	 0.5
Company and 	 0.5
its application 	 0.02857142857142857
algorithms which 	 0.02857142857142857
methods to 	 0.09090909090909091
ICR . 	 0.3333333333333333
single datum 	 0.07142857142857142
2002 a 	 0.5
while Church 	 0.05
document , 	 0.1388888888888889
reference for 	 0.125
of public 	 0.00089126559714795
describe the 	 0.3333333333333333
automated semantic 	 0.14285714285714285
achieved with 	 0.2
surrounding vowels 	 0.2
Rabiner can 	 1.0
the elements 	 0.0006920415224913495
be a 	 0.05485232067510549
possible on 	 0.041666666666666664
answer highlighted 	 0.03333333333333333
Fundamentals of 	 1.0
can even 	 0.0055248618784530384
difference can 	 0.25
articles rarely 	 0.125
sentiment analysis 	 0.52
In natural 	 0.009523809523809525
captures data 	 1.0
multilingual questions 	 0.3333333333333333
<s> Programming 	 0.0015372790161414297
syllables but 	 0.5
are statistical 	 0.004149377593360996
Kintsch , 	 1.0
cockpit , 	 0.5
usually with 	 0.0625
Agency in 	 0.5
usually not 	 0.03125
paper -RRB- 	 0.09090909090909091
; it 	 0.02127659574468085
, multi-document 	 0.0005614823133071309
banking system 	 1.0
`` deep 	 0.005291005291005291
or sentiments 	 0.0045045045045045045
communicative event 	 0.3333333333333333
or more 	 0.018018018018018018
DTW . 	 0.3333333333333333
`` pseudo-pilot 	 0.005291005291005291
translation programs 	 0.013513513513513514
produce useful 	 0.045454545454545456
system generates 	 0.010752688172043012
issue . 	 0.25
of distinctions 	 0.00089126559714795
probabilistic and 	 0.14285714285714285
summary might 	 0.023809523809523808
large corpus 	 0.043478260869565216
been built 	 0.014705882352941176
the extent 	 0.001384083044982699
history of 	 0.5
In 1983 	 0.009523809523809525
night . 	 1.0
Schank , 	 0.2
1997 -LRB- 	 0.5
by no 	 0.005714285714285714
in some 	 0.00749063670411985
among named 	 0.125
the information 	 0.0034602076124567475
-LRB- ending 	 0.0027100271002710027
Why does 	 0.2857142857142857
shared tasks 	 0.5
rules for 	 0.046511627906976744
tables to 	 0.3333333333333333
particularly the 	 0.2
texts of 	 0.11764705882352941
`` fire 	 0.005291005291005291
can increase 	 0.0055248618784530384
other systems 	 0.02857142857142857
theories in 	 0.2
languages See 	 0.02
parties du 	 1.0
-LRB- creating 	 0.0027100271002710027
' lengths 	 0.05263157894736842
we try 	 0.022222222222222223
together . 	 0.125
conference rooms 	 0.5
specific trade 	 0.047619047619047616
A. van 	 0.2
to finding 	 0.0013280212483399733
`` set 	 0.005291005291005291
might generate 	 0.038461538461538464
then extrapolate 	 0.02857142857142857
Some writing 	 0.047619047619047616
The learning 	 0.005208333333333333
include all 	 0.037037037037037035
Japanese prisoners 	 0.125
problems , 	 0.35294117647058826
, possessive 	 0.0005614823133071309
the importance 	 0.001384083044982699
reCAPTCHA system 	 1.0
inference -- 	 0.25
which generates 	 0.007246376811594203
itself is 	 0.2
statistics of 	 0.125
Translation '' 	 0.3333333333333333
This system 	 0.015873015873015872
on statistical 	 0.009433962264150943
probabilities not 	 0.09090909090909091
reports , 	 0.2
NLP and 	 0.02127659574468085
grammatical tagging 	 0.18181818181818182
contains a 	 0.1
unambiguously identified 	 1.0
breaking , 	 0.5
continued more 	 0.1111111111111111
to what 	 0.005312084993359893
structured databases 	 0.16666666666666666
sound really 	 0.05
and transmitting 	 0.001445086705202312
-LRB- May 	 0.005420054200542005
underlies the 	 1.0
the vagueness 	 0.0006920415224913495
boundaries between 	 0.18181818181818182
essentially they 	 0.125
and\/or producing 	 0.3333333333333333
Part-of-Speech Tagset 	 1.0
, Hafiz 	 0.0005614823133071309
of giving 	 0.00089126559714795
models upon 	 0.038461538461538464
and -RRB- 	 0.001445086705202312
as simply 	 0.003484320557491289
decisions or 	 0.1
morphology -LRB- 	 0.14285714285714285
news-gathering , 	 1.0
Moreover , 	 1.0
` kit 	 0.125
-RRB- MorphoChallenge 	 0.0027100271002710027
as weapon 	 0.003484320557491289
system had 	 0.010752688172043012
all nouns 	 0.023255813953488372
telephone . 	 0.5
purely statistical 	 1.0
use cepstral 	 0.013888888888888888
governmental proceedings 	 1.0
, sentences 	 0.0011229646266142617
non-trivial , 	 0.5
ATNs '' 	 0.3333333333333333
distinction is 	 0.4
and 1970s 	 0.001445086705202312
track of 	 1.0
Australian Air 	 0.5
ideas in 	 0.5
where syllables 	 0.02857142857142857
humans , 	 0.16666666666666666
Often natural 	 0.3333333333333333
sold by 	 0.3333333333333333
structured documents 	 0.16666666666666666
The objects 	 0.005208333333333333
the sequences 	 0.0006920415224913495
, installed 	 0.0005614823133071309
Aletta Norval 	 1.0
estimating the 	 1.0
questioners expect 	 1.0
requires humans 	 0.0625
other types 	 0.014285714285714285
given rise 	 0.041666666666666664
the expression 	 0.001384083044982699
in virtually 	 0.0018726591760299626
ARRA -RRB- 	 1.0
out . 	 0.07142857142857142
or might 	 0.0045045045045045045
For a 	 0.03278688524590164
Speech Writing 	 0.03225806451612903
<s> Again 	 0.0007686395080707148
timing for 	 1.0
medical professionals 	 0.16666666666666666
authoritative of 	 1.0
Instead of 	 1.0
another , 	 0.07692307692307693
versions needed 	 0.3333333333333333
in computer-aided 	 0.0018726591760299626
this issue 	 0.01098901098901099
above -RRB- 	 0.07692307692307693
create features 	 0.058823529411764705
that contain 	 0.010638297872340425
more robust 	 0.021052631578947368
the pragmatics 	 0.0006920415224913495
worth noting 	 0.5
on Mandarin 	 0.0047169811320754715
the same 	 0.01522491349480969
committed into 	 1.0
approached keyphrase 	 0.5
sense that 	 0.125
Mr. Smith 	 0.5
not always 	 0.008928571428571428
engines such 	 0.3333333333333333
President Bush 	 0.5
hidden parts 	 0.125
Chafe , 	 1.0
whether it 	 0.07692307692307693
when there 	 0.02857142857142857
recognition technology 	 0.008264462809917356
learning automatically 	 0.023255813953488372
during machine 	 0.1
but may 	 0.029411764705882353
Dyer developed 	 1.0
agreement about 	 0.3333333333333333
more effectively 	 0.010526315789473684
and Church 	 0.001445086705202312
used for 	 0.13274336283185842
Teun A. 	 1.0
approaches . 	 0.10714285714285714
given formal 	 0.041666666666666664
Nations materials 	 0.5
, Abney 	 0.0005614823133071309
a preliminary 	 0.001226993865030675
bore similarities 	 1.0
thus beyond 	 0.1
`` AI-complete 	 0.010582010582010581
from both 	 0.009615384615384616
crucial to 	 1.0
software finding 	 0.037037037037037035
, possibly 	 0.0005614823133071309
, up 	 0.0005614823133071309
stutering , 	 1.0
writing rules 	 0.1111111111111111
possibly linked 	 0.5
is recognizing 	 0.0020325203252032522
humans would 	 0.08333333333333333
In 1994 	 0.009523809523809525
-RRB- formal 	 0.0027100271002710027
Research Projects 	 0.125
probability what 	 0.14285714285714285
still '' 	 0.06666666666666667
in Canada 	 0.0018726591760299626
, setting 	 0.0011229646266142617
evaluation has 	 0.018518518518518517
, Walter 	 0.0005614823133071309
1970 , 	 0.3333333333333333
: Rule-based 	 0.00980392156862745
and simply 	 0.001445086705202312
big a 	 0.5
Huang etc. 	 1.0
as voice 	 0.003484320557491289
of objects 	 0.00089126559714795
<s> A 	 0.033820138355111454
a strong 	 0.00245398773006135
overlaps between 	 0.5
two sentences 	 0.06896551724137931
can effectively 	 0.0055248618784530384
thus makes 	 0.1
functional grammar 	 0.5
automatic keyphrase 	 0.043478260869565216
applications fall 	 0.04
Statistical Main 	 0.1111111111111111
phrasing the 	 1.0
parsing , 	 0.07142857142857142
pointed out 	 1.0
Informally , 	 1.0
John 's 	 0.25
search by 	 0.09090909090909091
Marginal Relevance 	 1.0
markers . 	 0.3333333333333333
systems favor 	 0.008928571428571428
From these 	 1.0
some nice 	 0.012048192771084338
voice user 	 0.07692307692307693
developments in 	 0.6666666666666666
sentence and 	 0.020833333333333332
that attempt 	 0.0035460992907801418
cost related 	 0.5
Example-based machine 	 0.6666666666666666
Warren Weaver 	 1.0
would both 	 0.018867924528301886
his book 	 0.08333333333333333
ways by 	 0.125
system working 	 0.010752688172043012
new part 	 0.041666666666666664
ones that 	 0.1
etc. -- 	 0.045454545454545456
processing step 	 0.018518518518518517
aim of 	 0.5
approach ; 	 0.02857142857142857
addition to 	 0.5
or by 	 0.0045045045045045045
useful as 	 0.07142857142857142
TextRank results 	 0.07142857142857142
each year 	 0.022222222222222223
, propositions 	 0.0011229646266142617
is critical 	 0.0020325203252032522
in fighter 	 0.0056179775280898875
success of 	 0.6
notations of 	 0.5
to create 	 0.01195219123505976
popular journals 	 0.1111111111111111
is a 	 0.10975609756097561
recognizers into 	 0.5
examples Performance 	 0.041666666666666664
information Popular 	 0.021739130434782608
Spitzer 's 	 1.0
web , 	 0.125
extractive approach 	 0.14285714285714285
relative position 	 0.3333333333333333
are arranged 	 0.004149377593360996
among other 	 0.375
retrained to 	 1.0
infinitive marker 	 1.0
help determine 	 0.1111111111111111
: lessons 	 0.00980392156862745
call to 	 0.3333333333333333
into English 	 0.02564102564102564
carry out 	 1.0
The grammar 	 0.010416666666666666
social media 	 0.2857142857142857
not words 	 0.026785714285714284
integrating speech 	 1.0
of course 	 0.0017825311942959
The technology 	 0.005208333333333333
Some SR 	 0.047619047619047616
simple sentence 	 0.038461538461538464
VRX and 	 1.0
usually a 	 0.03125
, 4 	 0.0005614823133071309
unlike brain 	 1.0
learning is 	 0.023255813953488372
<s> Referring 	 0.0007686395080707148
Journal . 	 0.3333333333333333
Wireless World 	 1.0
of potential 	 0.0017825311942959
meaningful . 	 0.125
you can 	 0.15384615384615385
lexicon required 	 0.1111111111111111
intelligence . 	 0.125
match certain 	 0.16666666666666666
, question 	 0.0011229646266142617
and character 	 0.001445086705202312
are useful 	 0.004149377593360996
extraction removes 	 0.03225806451612903
, called 	 0.0005614823133071309
type as 	 0.07142857142857142
dynamic programming 	 0.2
efficient parsers 	 0.3333333333333333
they think 	 0.025
desired identification 	 0.2
the operation 	 0.0006920415224913495
top-down expansion 	 0.25
unable to 	 1.0
OS . 	 0.5
this reason 	 0.02197802197802198
the 70 	 0.0006920415224913495
within documents 	 0.05555555555555555
`` 12 	 0.010582010582010581
grammatical relationships 	 0.09090909090909091
As in 	 0.2222222222222222
currently the 	 0.14285714285714285
important words 	 0.0625
critical new 	 0.25
what categories 	 0.03125
was entertaining 	 0.012987012987012988
workday to 	 1.0
commonly teach 	 0.125
scanner that 	 0.3333333333333333
one typically 	 0.015384615384615385
but it 	 0.058823529411764705
improve robustness 	 0.07692307692307693
periods or 	 0.3333333333333333
has specialized 	 0.011904761904761904
In general 	 0.02857142857142857
Message Understanding 	 1.0
vector machines 	 0.3333333333333333
feature statistical 	 0.07692307692307693
into machine 	 0.01282051282051282
of Speech 	 0.00089126559714795
of high 	 0.00089126559714795
the quantitative 	 0.0006920415224913495
SBD -RRB- 	 1.0
the Morpholympics 	 0.0006920415224913495
them good 	 0.05263157894736842
SYSTRAN for 	 1.0
overlaps to 	 0.5
air traffic 	 0.6
Conference Evaluation 	 0.5
evaluated . 	 0.14285714285714285
the standard 	 0.001384083044982699
the dialog 	 0.0006920415224913495
actions . 	 1.0
standard method 	 0.07142857142857142
and common-sense 	 0.001445086705202312
techniques . 	 0.043478260869565216
, extracting 	 0.0005614823133071309
structures that 	 0.4
degree -LRB- 	 0.16666666666666666
detect the 	 1.0
unsupervised '' 	 0.125
Lightning II 	 1.0
be seen 	 0.012658227848101266
are analyzed 	 0.004149377593360996
identification of 	 0.4
curves , 	 1.0
human ratings 	 0.08695652173913043
a cosine 	 0.001226993865030675
a reference 	 0.00245398773006135
2,663,758 . 	 1.0
speakers were 	 0.25
introducing models 	 1.0
number . 	 0.046511627906976744
of non-annotated 	 0.00089126559714795
determine the 	 0.391304347826087
then taking 	 0.02857142857142857
clusters . 	 1.0
sentences or 	 0.013157894736842105
English or 	 0.02702702702702703
determining the 	 0.6666666666666666
book -LRB- 	 0.25
See Peter 	 0.16666666666666666
error-prone and 	 1.0
<s> Up 	 0.0007686395080707148
meaning into 	 0.043478260869565216
, Racter 	 0.0005614823133071309
distinguishes two 	 0.5
-LRB- IMR 	 0.0027100271002710027
generally rely 	 0.09090909090909091
, Ford 	 0.0005614823133071309
writing style 	 0.1111111111111111
for POS 	 0.0036101083032490976
for clarification 	 0.0036101083032490976
needs a 	 0.2
the approximate 	 0.0006920415224913495
for sound 	 0.0036101083032490976
Emergent grammar 	 1.0
invalid constructs 	 1.0
for commercial 	 0.0036101083032490976
artificial neural 	 0.09090909090909091
for use 	 0.007220216606498195
moon missions 	 1.0
or topics 	 0.0045045045045045045
field is 	 0.037037037037037035
that speech 	 0.0035460992907801418
example Wireless 	 0.012345679012345678
several words 	 0.045454545454545456
... About 	 0.5
been hand-annotated 	 0.029411764705882353
can greatly 	 0.0055248618784530384
images . 	 0.16666666666666666
, low-resolution 	 0.0005614823133071309
left-most derivations 	 0.5
the difficulty 	 0.0006920415224913495
clauses , 	 1.0
combinations of 	 1.0
thousands or 	 0.3333333333333333
the basics 	 0.0006920415224913495
and length 	 0.001445086705202312
PageRank , 	 0.16666666666666666
algorithms to 	 0.08571428571428572
harder tasks 	 0.14285714285714285
Known word 	 1.0
important memorandum 	 0.0625
those surrounding 	 0.045454545454545456
market their 	 0.3333333333333333
an idea 	 0.007575757575757576
relationships between 	 0.16666666666666666
MySpace -RRB- 	 1.0
predicted pollen 	 0.5
or keyphrases 	 0.0045045045045045045
Australian physician 	 0.5
produced systems 	 0.2222222222222222
software varied 	 0.037037037037037035
been much 	 0.014705882352941176
-- between 	 0.04
sequences are 	 0.1111111111111111
eigenvector corresponding 	 0.5
Many machine 	 0.08333333333333333
Braille texts 	 1.0
and if 	 0.001445086705202312
they all 	 0.05
<s> Evaluation 	 0.003843197540353574
small text 	 0.1111111111111111
example through 	 0.012345679012345678
quantity . 	 0.3333333333333333
-LRB- Computational 	 0.0027100271002710027
subtopic of 	 1.0
the dynamic 	 0.0006920415224913495
even produce 	 0.037037037037037035
page including 	 0.14285714285714285
clearly many 	 0.3333333333333333
Voice Translator 	 0.2
<s> Commercial 	 0.0015372790161414297
readily conveyed 	 0.3333333333333333
a speaker-dependent 	 0.001226993865030675
a German 	 0.001226993865030675
preferable , 	 1.0
the 2006 	 0.0006920415224913495
, Harrison 	 0.0005614823133071309
to assign 	 0.00398406374501992
<s> Black-box 	 0.0007686395080707148
historical data 	 1.0
judge is 	 0.25
considered -LRB- 	 0.1111111111111111
the degree 	 0.0006920415224913495
break hyphenated 	 0.5
each lexical 	 0.022222222222222223
'' standards 	 0.005154639175257732
large corpora 	 0.043478260869565216
Trained linguists 	 1.0
problem can 	 0.022727272727272728
of allowing 	 0.00089126559714795
commonplace and 	 1.0
mainly evaluation 	 0.16666666666666666
is 8000 	 0.0020325203252032522
, assertion 	 0.0005614823133071309
AVRADA -RRB- 	 0.5
have humans 	 0.009615384615384616
the introduction 	 0.0006920415224913495
not also 	 0.008928571428571428
probabilities are 	 0.09090909090909091
DeRose used 	 0.2
to sort 	 0.0013280212483399733
internet discussion 	 1.0
different tongues 	 0.02040816326530612
diagramming of 	 1.0
van Leeuwen 	 0.5
sound . 	 0.05
the syntactic 	 0.0006920415224913495
temporal and 	 0.5
C , 	 1.0
more appropriately 	 0.010526315789473684
fighter environment 	 0.16666666666666666
questions . 	 0.07692307692307693
lexicons -LRB- 	 0.5
; no 	 0.02127659574468085
lead to 	 1.0
two sequences 	 0.034482758620689655
large probabilities 	 0.043478260869565216
Speech is 	 0.06451612903225806
function with 	 0.125
as first-order 	 0.003484320557491289
of document 	 0.00089126559714795
Mirage aircraft 	 1.0
installed defective 	 0.3333333333333333
late 70s 	 0.1111111111111111
also be 	 0.10144927536231885
translation Example-based 	 0.013513513513513514
country , 	 0.25
vendors began 	 0.25
verification . 	 1.0
and syntactic 	 0.002890173410404624
form of 	 0.35
of standard 	 0.00089126559714795
the most 	 0.01314878892733564
its immediate 	 0.02857142857142857
and Intelligent 	 0.001445086705202312
while on-line 	 0.05
presume a 	 1.0
language -LRB- 	 0.013513513513513514
Inc. in 	 0.5
create some 	 0.058823529411764705
; for 	 0.0425531914893617
with images 	 0.00546448087431694
expanded the 	 1.0
cost of 	 0.5
the purpose 	 0.001384083044982699
`` Sentiment 	 0.005291005291005291
can deal 	 0.0055248618784530384
instead for 	 0.14285714285714285
, stored 	 0.0005614823133071309
and decorrelating 	 0.001445086705202312
Today there 	 1.0
process , 	 0.027777777777777776
-RRB- the 	 0.0027100271002710027
, Jim 	 0.0005614823133071309
'' the 	 0.005154639175257732
Schank 's 	 0.2
Sentence segmentation 	 0.4
aims to 	 0.6666666666666666
method in 	 0.0625
Tauschek was 	 0.5
tools usually 	 0.16666666666666666
'' has 	 0.010309278350515464
Establishment -LRB- 	 1.0
alphabetic heritage 	 1.0
method of 	 0.125
relevant content 	 0.14285714285714285
help blind 	 0.1111111111111111
geospatial questions 	 1.0
coughing , 	 1.0
the requirements 	 0.0006920415224913495
Bobrow 's 	 1.0
technology . 	 0.09090909090909091
these topics 	 0.023809523809523808
expressions . 	 0.6666666666666666
, object 	 0.0005614823133071309
Some current 	 0.09523809523809523
of context 	 0.0017825311942959
using logical 	 0.01694915254237288
of Michigan 	 0.00089126559714795
those of 	 0.045454545454545456
is digital 	 0.0020325203252032522
answer questions 	 0.03333333333333333
or aspect 	 0.0045045045045045045
NLG input 	 0.047619047619047616
capitalize names 	 1.0
Edward Robinson 	 1.0
target -LRB- 	 0.09090909090909091
Computer Science 	 0.16666666666666666
four step 	 0.14285714285714285
particular types 	 0.07692307692307693
strategy to 	 0.6
for more 	 0.007220216606498195
hub '' 	 1.0
information , 	 0.043478260869565216
between positive 	 0.02564102564102564
of John 	 0.00089126559714795
be as 	 0.012658227848101266
only want 	 0.02631578947368421
an extrinsic 	 0.007575757575757576
could be 	 0.25
content present 	 0.08333333333333333
, entering 	 0.0011229646266142617
was considered 	 0.012987012987012988
compare automatic 	 0.14285714285714285
to clean 	 0.0013280212483399733
fairly non-trivial 	 0.25
DeRose 1990 	 0.2
the legends 	 0.0006920415224913495
side effect 	 1.0
Trek . 	 1.0
meanings according 	 0.25
10 milliseconds 	 0.25
Act of 	 1.0
additional constraints 	 0.16666666666666666
diagonal covariance 	 1.0
each for 	 0.022222222222222223
the foreign 	 0.0006920415224913495
have higher 	 0.009615384615384616
, Carmen 	 0.0005614823133071309
they join 	 0.025
though ROUGE-1 	 0.1
DTW has 	 0.3333333333333333
the fidelity 	 0.0006920415224913495
much like 	 0.045454545454545456
the noise 	 0.0006920415224913495
-LRB- AVRADA 	 0.0027100271002710027
final letter 	 0.1111111111111111
a proper 	 0.001226993865030675
language processing 	 0.22297297297297297
correct answer 	 0.06666666666666667
using an 	 0.03389830508474576
70 % 	 0.75
design of 	 0.25
for part-of-speech 	 0.007220216606498195
reads sections 	 0.5
with equipment 	 0.00546448087431694
grouped into 	 0.5
symbol of 	 0.25
they had 	 0.025
the clusters 	 0.0006920415224913495
more precise 	 0.010526315789473684
text from 	 0.012578616352201259
being based 	 0.05555555555555555
Languages with 	 0.3333333333333333
typical sentences 	 0.1111111111111111
where particular 	 0.02857142857142857
or from 	 0.0045045045045045045
wishes to 	 1.0
-LRB- linguistics 	 0.005420054200542005
Anthology . 	 1.0
Critical discourse 	 0.5
approximation . 	 0.16666666666666666
a maximum 	 0.00245398773006135
opens , 	 1.0
false positives 	 0.5
constraints ; 	 0.25
other things 	 0.04285714285714286
: Hidden 	 0.00980392156862745
reported there 	 0.2
highly . 	 0.1111111111111111
Piron 's 	 0.3333333333333333
grammar for 	 0.02702702702702703
elements , 	 0.25
-RRB- -- 	 0.0027100271002710027
necessary . 	 0.1
information databases 	 0.021739130434782608
computational concerns 	 0.1
of real-world 	 0.00089126559714795
more widely 	 0.010526315789473684
determine which 	 0.08695652173913043
many times 	 0.019230769230769232
of major 	 0.00089126559714795
depth of 	 0.3333333333333333
still have 	 0.06666666666666667
stages are 	 0.5
Kurzweil and 	 0.14285714285714285
dictionary can 	 0.14285714285714285
-LRB- rescoring 	 0.0027100271002710027
, generally 	 0.0005614823133071309
are sometimes 	 0.004149377593360996
the SPOTLIGHT 	 0.0006920415224913495
Schroeder , 	 1.0
1,000 parts 	 0.5
and print 	 0.001445086705202312
inference within 	 0.25
at . 	 0.014705882352941176
showing comparative 	 0.5
the presence 	 0.0006920415224913495
metric for 	 0.3333333333333333
<s> You 	 0.0007686395080707148
LexRank is 	 0.08333333333333333
medical data 	 0.3333333333333333
2002 evaluation 	 0.5
leads , 	 1.0
digital computers 	 0.14285714285714285
Jump to 	 1.0
four steps 	 0.14285714285714285
storing , 	 1.0
Current machine 	 0.2
summarization systems 	 0.1
it extremely 	 0.008547008547008548
are presented 	 0.004149377593360996
same topic 	 0.04
the training 	 0.002768166089965398
ELIZA worked 	 0.1111111111111111
articles from 	 0.125
about any 	 0.025
developed RSI 	 0.038461538461538464
a set 	 0.01717791411042945
meaning in 	 0.08695652173913043
corpora '' 	 0.09090909090909091
data for 	 0.012987012987012988
way : 	 0.041666666666666664
in politics 	 0.0018726591760299626
companies -LRB- 	 0.5
lexical functional 	 0.07692307692307693
under ultraviolet 	 0.2
broken English 	 0.2
unstructured text 	 1.0
for accuracy 	 0.0036101083032490976
ten years 	 1.0
so-called delta 	 0.3333333333333333
AVRADA tests 	 0.5
How , 	 0.14285714285714285
they observe 	 0.025
Linguistics defines 	 0.3333333333333333
in . 	 0.0018726591760299626
taggers The 	 0.14285714285714285
1953 U.S. 	 1.0
distinct vowels 	 0.14285714285714285
process The 	 0.027777777777777776
reduced set 	 0.25
information then 	 0.021739130434782608
know is 	 0.5
to message 	 0.0013280212483399733
perform adaptive 	 0.09090909090909091
computer vision 	 0.022727272727272728
ones , 	 0.3
needs to 	 0.4
having a 	 0.2
<s> Anaphor 	 0.0007686395080707148
photographing data 	 1.0
actioning it 	 1.0
as OnlineOCR 	 0.003484320557491289
further condensation 	 0.125
interface for 	 0.25
driving social 	 1.0
can not 	 0.08287292817679558
require significant 	 0.045454545454545456
the evaluators 	 0.0006920415224913495
The sailor 	 0.005208333333333333
or weapon 	 0.0045045045045045045
this , 	 0.04395604395604396
sources are 	 0.16666666666666666
most negative 	 0.017241379310344827
traffic controllers 	 1.0
element of 	 1.0
principle , 	 1.0
, on-line 	 0.0005614823133071309
Lander Automatic 	 0.5
statistical analysis 	 0.030303030303030304
acoustic signal 	 0.16666666666666666
core elements 	 0.5
extraction depends 	 0.03225806451612903
involves visual 	 0.1
50 % 	 0.6666666666666666
creating more 	 0.14285714285714285
Question Answering 	 0.14285714285714285
selecting examples 	 0.2
that tell 	 0.0035460992907801418
new domains 	 0.041666666666666664
or word-category 	 0.0045045045045045045
real-valued vectors 	 0.3333333333333333
descriptor in 	 1.0
human-generated model 	 0.5
E. , 	 0.25
anaphora . 	 1.0
handheld scanner 	 1.0
unsupervised `` 	 0.125
good for 	 0.07692307692307693
systems that 	 0.0625
platforms . 	 1.0
like Ncmsan 	 0.03571428571428571
impressive . 	 0.5
ranks , 	 0.5
as a 	 0.11846689895470383
still the 	 0.06666666666666667
successive letters 	 0.5
with recognition 	 0.00546448087431694
of Chinese 	 0.00089126559714795
, from 	 0.0005614823133071309
texts seems 	 0.058823529411764705
not found 	 0.008928571428571428
step and 	 0.06666666666666667
, that 	 0.0022459292532285235
advantage of 	 0.8
require subjects 	 0.045454545454545456
comes to 	 0.2
parsing of 	 0.07142857142857142
out other 	 0.07142857142857142
Pyramid Method 	 1.0
although capitalization 	 0.16666666666666666
established within 	 1.0
The umbrella 	 0.005208333333333333
Rescoring is 	 1.0
and decelerations 	 0.001445086705202312
but discards 	 0.014705882352941176
organization , 	 0.2
chose different 	 1.0
east . 	 1.0
even several 	 0.037037037037037035
your system 	 0.5
graph specially 	 0.07692307692307693
language search 	 0.006756756756756757
networks discussions 	 0.07142857142857142
data entry 	 0.03896103896103896
returning a 	 1.0
the probabilities 	 0.002768166089965398
etc. -RRB- 	 0.4090909090909091
The Archaeology 	 0.005208333333333333
the Blind 	 0.001384083044982699
database queries 	 0.1
became one 	 0.2
extraction , 	 0.1935483870967742
fact , 	 0.45454545454545453
end a 	 0.125
for parsing 	 0.0036101083032490976
, when 	 0.003368893879842785
decided that 	 0.3333333333333333
syntax , 	 0.45454545454545453
therefore it 	 0.6
new set 	 0.041666666666666664
interference between 	 1.0
the World 	 0.0034602076124567475
those two 	 0.045454545454545456
easily when 	 0.1111111111111111
enterprise customers 	 1.0
results over 	 0.047619047619047616
forums -LRB- 	 1.0
models . 	 0.11538461538461539
the desktop 	 0.0006920415224913495
Text Segmentation 	 0.16666666666666666
character alone 	 0.045454545454545456
markup like 	 1.0
if we 	 0.07142857142857142
WER -RRB- 	 1.0
questions , 	 0.3076923076923077
, although 	 0.0022459292532285235
A semantic 	 0.02
professor at 	 1.0
-RRB- question 	 0.0027100271002710027
to analyzing 	 0.0013280212483399733
machine and 	 0.012658227848101266
looks , 	 0.25
that combines 	 0.0035460992907801418
<s> What 	 0.0030745580322828594
to book 	 0.0013280212483399733
help speakers 	 0.1111111111111111
In 1929 	 0.009523809523809525
undirected and 	 1.0
Willig , 	 1.0
hear sound 	 0.5
Interlingual Main 	 0.3333333333333333
steady increase 	 0.5
surprisingly disruptive 	 0.3333333333333333
`` dogs 	 0.021164021164021163
lend well 	 1.0
mobile email 	 0.5
: expanded 	 0.00980392156862745
scanned images 	 0.3333333333333333
abstractive methods 	 0.3333333333333333
still translates 	 0.06666666666666667
features or 	 0.038461538461538464
is extremely 	 0.0040650406504065045
weighted finite 	 0.3333333333333333
objects . 	 0.2
Multilingual -LRB- 	 1.0
It stands 	 0.02631578947368421
provided significant 	 0.2
output by 	 0.038461538461538464
search method 	 0.09090909090909091
'' highly 	 0.005154639175257732
accuracy . 	 0.22580645161290322
of structured 	 0.00089126559714795
dynamic background 	 0.2
An absorbing 	 0.0625
-LRB- normalized 	 0.0027100271002710027
the input 	 0.005536332179930796
IBM . 	 0.3333333333333333
possible -LRB- 	 0.041666666666666664
argued that 	 1.0
<s> might 	 0.0007686395080707148
unknown and 	 1.0
Callaghan which 	 1.0
consisting of 	 1.0
people , 	 0.0625
formed Symantec 	 0.2
a part 	 0.00245398773006135
IR -RRB- 	 0.3333333333333333
is true 	 0.0020325203252032522
States , 	 0.14285714285714285
rely are 	 0.14285714285714285
especially inflectional 	 0.06666666666666667
indicate speech 	 0.3333333333333333
Most modern 	 0.5
major constituents 	 0.08333333333333333
writing custom 	 0.1111111111111111
a handheld 	 0.001226993865030675
more successful 	 0.031578947368421054
get bunch 	 0.14285714285714285
printed characters 	 0.08333333333333333
's tagger 	 0.0196078431372549
<s> increases 	 0.0007686395080707148
, adverb 	 0.0005614823133071309
the software 	 0.0020761245674740486
, common 	 0.0005614823133071309
of Lichtenstein 	 0.00089126559714795
by context-free 	 0.005714285714285714
prove impossible 	 1.0
Bolivar , 	 1.0
<s> CLAWS 	 0.0015372790161414297
composing Braille 	 1.0
-LRB- 1954 	 0.0027100271002710027
business rules 	 0.25
segmentation process 	 0.030303030303030304
should be 	 0.47368421052631576
on tasks 	 0.009433962264150943
, ICASSP 	 0.0005614823133071309
to generate 	 0.00796812749003984
, phrases 	 0.0005614823133071309
often disagree 	 0.022727272727272728
is whether 	 0.0020325203252032522
, Sandra 	 0.0005614823133071309
extraction module 	 0.03225806451612903
model would 	 0.1
<s> Sentiment 	 0.003843197540353574
carried out 	 0.5
only way 	 0.02631578947368421
interactive program 	 0.25
are beginning 	 0.004149377593360996
output nodes 	 0.07692307692307693
themselves simply 	 0.25
school-age children 	 1.0
; nor 	 0.02127659574468085
showed that 	 0.75
start to 	 0.14285714285714285
has the 	 0.023809523809523808
assumption , 	 0.5
selecting and 	 0.2
a name 	 0.00245398773006135
using the 	 0.1016949152542373
decisions . 	 0.1
some NLP 	 0.012048192771084338
four decades 	 0.14285714285714285
whether documents 	 0.07692307692307693
Recently , 	 1.0
the Bayes 	 0.0006920415224913495
techniques that 	 0.08695652173913043
to develop 	 0.006640106241699867
LexRank simply 	 0.08333333333333333
, length 	 0.0005614823133071309
<s> Vocalizations 	 0.0007686395080707148
Homayoon Beigi 	 1.0
most sense 	 0.017241379310344827
the voice 	 0.0006920415224913495
interest . 	 0.09090909090909091
human linguists 	 0.021739130434782608
Deborah Tannen 	 0.5
, TextRank 	 0.0011229646266142617
accumulation of 	 1.0
approximating sentence 	 1.0
to prune 	 0.0013280212483399733
structure that 	 0.08333333333333333
has also 	 0.03571428571428571
partial answers 	 1.0
difficulty in 	 0.2857142857142857
as 7 	 0.003484320557491289
for computer 	 0.010830324909747292
must compute 	 0.07142857142857142
slot represents 	 1.0
for Reading 	 0.0036101083032490976
detected , 	 0.5
shifting to 	 1.0
, its 	 0.0005614823133071309
to incorporate 	 0.0013280212483399733
entropy classifier 	 0.2
probability is 	 0.14285714285714285
worlds '' 	 1.0
their features\/aspects 	 0.029411764705882353
<s> Xerox 	 0.0007686395080707148
The construction 	 0.005208333333333333
particular note 	 0.07692307692307693
field which 	 0.037037037037037035
These rules 	 0.058823529411764705
rule that 	 0.3333333333333333
fail to 	 0.3333333333333333
Oklahoma , 	 1.0
, D 	 0.0005614823133071309
machine is 	 0.012658227848101266
hand printing 	 0.07142857142857142
main knowledge 	 0.125
even if 	 0.1111111111111111
Components and 	 1.0
, analyze 	 0.0005614823133071309
one there 	 0.015384615384615385
direct and 	 0.16666666666666666
Jay Lemke 	 1.0
5 % 	 0.5
Deirdre Wilson 	 1.0
research focus 	 0.023809523809523808
database look-up 	 0.1
, MT 	 0.0011229646266142617
model avoids 	 0.03333333333333333
Schank at 	 0.2
natural '' 	 0.02666666666666667
competence , 	 1.0
approximate meaning 	 0.5
parsing - 	 0.07142857142857142
product of 	 0.14285714285714285
trained on 	 0.3333333333333333
many applications 	 0.038461538461538464
some but 	 0.012048192771084338
to documents 	 0.0013280212483399733
languages is 	 0.02
program in 	 0.09090909090909091
garden path 	 1.0
human summary 	 0.021739130434782608
possible without 	 0.041666666666666664
directly comparable 	 0.2
Guzman , 	 1.0
from technology 	 0.009615384615384616
summarization works 	 0.02
, Barbara 	 0.0005614823133071309
in spoken 	 0.003745318352059925
covers speech 	 0.25
when a 	 0.11428571428571428
automatically The 	 0.047619047619047616
, relative 	 0.0005614823133071309
States . 	 0.2857142857142857
used . 	 0.04424778761061947
states are 	 0.25
using their 	 0.01694915254237288
of question 	 0.00267379679144385
input is 	 0.024390243902439025
DTW -RRB- 	 0.3333333333333333
a common 	 0.00245398773006135
understanding . 	 0.06060606060606061
not describe 	 0.008928571428571428
LexRank differences 	 0.08333333333333333
internal representation 	 0.6
's methods 	 0.0196078431372549
analyzing the 	 0.2
East Asian 	 1.0
than processing 	 0.022222222222222223
summarization system 	 0.06
very preliminary 	 0.024390243902439025
is then 	 0.01016260162601626
hitcha ' 	 1.0
and computationally 	 0.001445086705202312
standards are 	 0.2
language sentences 	 0.006756756756756757
produce vowels 	 0.045454545454545456
University included 	 0.1111111111111111
Sentence breaking 	 0.2
input features 	 0.024390243902439025
When several 	 0.14285714285714285
before it 	 0.16666666666666666
or language 	 0.0045045045045045045
its main 	 0.02857142857142857
take advantage 	 0.4
which may 	 0.007246376811594203
phrase that 	 0.1
QUALM -LRB- 	 1.0
levels as 	 0.045454545454545456
and informativeness 	 0.001445086705202312
of analysis 	 0.00089126559714795
after John 	 0.08333333333333333
syntax is 	 0.09090909090909091
translations using 	 0.5
is fairly 	 0.0020325203252032522
speech in 	 0.013157894736842105
or uttered 	 0.0045045045045045045
harder when 	 0.14285714285714285
also very 	 0.028985507246376812
transition network 	 1.0
in NLP 	 0.0149812734082397
many sentences 	 0.019230769230769232
same example-generation 	 0.04
are good 	 0.004149377593360996
to impersonate 	 0.0013280212483399733
extrinsic -RRB- 	 0.16666666666666666
part-of-speech possibilities 	 0.06666666666666667
annotating texts 	 1.0
by DARPA 	 0.005714285714285714
orange in 	 1.0
chunk of 	 1.0
highly interactive 	 0.1111111111111111
with deep 	 0.00546448087431694
Technology Center 	 0.3333333333333333
having the 	 0.2
largely dependent 	 0.2
to tell 	 0.0013280212483399733
vocabulary and 	 0.25
a conversation 	 0.001226993865030675
text units 	 0.018867924528301886
of ambitious 	 0.00089126559714795
Flickinger D. 	 1.0
proceeds in 	 1.0
main underlying 	 0.125
programs sponsored 	 0.09090909090909091
the summers 	 0.0006920415224913495
Ernesto Laclau 	 1.0
and Tigrinya 	 0.001445086705202312
called machine 	 0.05555555555555555
linguistic analysis 	 0.0625
form logical 	 0.05
truck A 	 1.0
eyes-busy environment 	 1.0
real progress 	 0.1111111111111111
Many words 	 0.16666666666666666
comprising context 	 0.5
filter returns 	 0.5
the simulation 	 0.0006920415224913495
effectively utilize 	 0.3333333333333333
two distinct 	 0.034482758620689655
, typically 	 0.0016844469399213925
Glass-box evaluation 	 1.0
, second 	 0.0005614823133071309
task requires 	 0.023809523809523808
a dictionary 	 0.0036809815950920245
be learned 	 0.004219409282700422
-LRB- F 	 0.0027100271002710027
QA computer 	 0.047619047619047616
Task and 	 0.6666666666666666
be manually 	 0.004219409282700422
are typically 	 0.012448132780082987
program got 	 0.045454545454545456
this genre 	 0.01098901098901099
input such 	 0.024390243902439025
own expert 	 0.16666666666666666
with Fourier 	 0.00546448087431694
developed the 	 0.15384615384615385
finite state 	 0.8
to most 	 0.0013280212483399733
results in 	 0.047619047619047616
least to 	 0.2
; Compute 	 0.02127659574468085
not represent 	 0.008928571428571428
, Constraint 	 0.0005614823133071309
considered . 	 0.1111111111111111
recording conditions 	 1.0
on Mirage 	 0.0047169811320754715
domain or 	 0.05
that operated 	 0.0035460992907801418
ELIZA . 	 0.1111111111111111
took Harris 	 1.0
<s> Given 	 0.0023059185242121443
TaleSpin -LRB- 	 1.0
routing bar 	 0.3333333333333333
not worked 	 0.008928571428571428
EBMT -RRB- 	 1.0
The methods 	 0.010416666666666666
recognition problems 	 0.008264462809917356
human-written ones 	 0.5
by searching 	 0.005714285714285714
method for 	 0.125
noise levels 	 0.125
split into 	 0.5
He tried 	 0.125
as easily 	 0.006968641114982578
rank individual 	 0.16666666666666666
a standard 	 0.00245398773006135
consistently available 	 0.3333333333333333
a voice 	 0.001226993865030675
published at 	 0.14285714285714285
Perceptron , 	 1.0
and an 	 0.004335260115606936
re-encode the 	 1.0
1949 RCA 	 0.5
Throughout the 	 1.0
on hand-written 	 0.0047169811320754715
out from 	 0.07142857142857142
major component 	 0.08333333333333333
overall polarity 	 0.16666666666666666
inherent expressivity 	 1.0
damping factor 	 1.0
inferior results 	 1.0
in computers 	 0.0018726591760299626
just `` 	 0.1111111111111111
phrase-structure grammars 	 1.0
are starting 	 0.004149377593360996
sources for 	 0.16666666666666666
summary -RRB- 	 0.023809523809523808
2012 -RRB- 	 1.0
the approach 	 0.0006920415224913495
Systems released 	 0.16666666666666666
problem overlaps 	 0.022727272727272728
, disturbed 	 0.0005614823133071309
`` unsupervised 	 0.005291005291005291
utility and 	 0.5
already been 	 0.4
most commonly 	 0.017241379310344827
in Northern 	 0.0018726591760299626
correspond to 	 1.0
so phonemes 	 0.03333333333333333
evaluation technique 	 0.018518518518518517
over 95 	 0.08333333333333333
felt -RRB- 	 1.0
systems trade 	 0.008928571428571428
12 categories 	 0.2
then it 	 0.05714285714285714
only five 	 0.02631578947368421
`` good 	 0.005291005291005291
speaker reads 	 0.05555555555555555
priorities . 	 1.0
or electronic 	 0.0045045045045045045
and making 	 0.002890173410404624
students at 	 0.6666666666666666
this ostensibly 	 0.01098901098901099
This term 	 0.015873015873015872
open world 	 0.25
of campaigns 	 0.00089126559714795
Query expansion 	 1.0
between successive 	 0.02564102564102564
the reported 	 0.0006920415224913495
a phrase 	 0.00245398773006135
the boundaries 	 0.0020761245674740486
NLP Main 	 0.02127659574468085
printer using 	 1.0
problem setting 	 0.022727272727272728
<s> Artificial 	 0.0007686395080707148
the ambiguous 	 0.001384083044982699
the National 	 0.001384083044982699
Army , 	 0.25
Research into 	 0.125
observed vector 	 1.0
that did 	 0.0070921985815602835
within the 	 0.16666666666666666
by supplying 	 0.005714285714285714
on word 	 0.0047169811320754715
-LRB- orange 	 0.0027100271002710027
more and 	 0.021052631578947368
, identify 	 0.0011229646266142617
an image 	 0.007575757575757576
despite being 	 0.3333333333333333
<s> Significant 	 0.0007686395080707148
<s> Schools 	 0.0007686395080707148
concept is 	 0.25
soft , 	 0.5
the notion 	 0.001384083044982699
<s> At 	 0.0023059185242121443
question focus 	 0.023809523809523808
e.g. containing 	 0.017857142857142856
These waves 	 0.058823529411764705
OCR accuracy 	 0.02040816326530612
Problem Solving 	 1.0
these techniques 	 0.023809523809523808
Web is 	 0.1111111111111111
This work 	 0.031746031746031744
keyphrases will 	 0.02857142857142857
criterion depends 	 0.5
all cases 	 0.023255813953488372
reducing training 	 0.5
shapes of 	 0.6666666666666666
parameters for 	 0.5
is unclear 	 0.0020325203252032522
ambiguous when 	 0.08333333333333333
1993 . 	 0.3333333333333333
individual users 	 0.08333333333333333
strength at 	 0.2
the metrics 	 0.0006920415224913495
are available 	 0.008298755186721992
regard to 	 0.8
Decoding the 	 0.5
and limited 	 0.001445086705202312
result of 	 0.2727272727272727
be performed 	 0.008438818565400843
was a 	 0.03896103896103896
On-line character 	 0.6666666666666666
extensive knowledge 	 0.3333333333333333
and mapping 	 0.001445086705202312
government and 	 0.3333333333333333
graph-based ranking 	 1.0
blocks worlds 	 0.25
are divided 	 0.004149377593360996
possible sentences 	 0.041666666666666664
each feature\/aspect 	 0.022222222222222223
be automated 	 0.004219409282700422
for a 	 0.10469314079422383
Sublanguage analysis 	 1.0
predefined template 	 1.0
Georgetown experiment 	 1.0
queries , 	 0.3333333333333333
recognize text 	 0.1111111111111111
in reverse 	 0.0018726591760299626
were the 	 0.024390243902439025
the reCAPTCHA 	 0.0006920415224913495
worked , 	 0.2
an abstract 	 0.007575757575757576
Consortium has 	 1.0
out -LRB- 	 0.07142857142857142
how the 	 0.034482758620689655
Turing test 	 0.5
giving the 	 0.5
data was 	 0.012987012987012988
small knowledge 	 0.1111111111111111
<s> Recognition 	 0.0015372790161414297
-LRB- Realtime 	 0.0027100271002710027
co-occurrence in 	 0.3333333333333333
: Manual 	 0.00980392156862745
of marking 	 0.00089126559714795
-RRB- <s/> 	 0.037940379403794036
usually termed 	 0.03125
as controlled 	 0.003484320557491289
label discourse 	 1.0
full comprehension 	 0.2
has thousands 	 0.011904761904761904
Lehnert , 	 0.6666666666666666
depth '' 	 0.3333333333333333
direct translation 	 0.16666666666666666
of documents 	 0.004456327985739751
and intra-texual 	 0.001445086705202312
context or 	 0.030303030303030304
'' other 	 0.005154639175257732
ELIZA gained 	 0.1111111111111111
language input 	 0.02027027027027027
reviewed and 	 1.0
coefficients , 	 0.25
that form 	 0.0035460992907801418
, Kurzweil 	 0.0016844469399213925
the RCA 	 0.0006920415224913495
studied more 	 1.0
involves paraphrasing 	 0.1
of integration 	 0.00089126559714795
In 1914 	 0.009523809523809525
about this 	 0.025
rightmost derivation 	 1.0
<s> Major 	 0.0015372790161414297
A first 	 0.02
disabilities People 	 0.25
containing these 	 0.125
Several MT 	 0.3333333333333333
hyphenation . 	 1.0
post-processing step 	 0.6666666666666666
to millions 	 0.0013280212483399733
The features 	 0.005208333333333333
; otherwise 	 0.02127659574468085
probabilistic decisions 	 0.2857142857142857
images of 	 0.3333333333333333
generic machine-generated 	 0.3333333333333333
by metrics 	 0.005714285714285714
US Veterans 	 0.14285714285714285
generally refers 	 0.09090909090909091
when writing 	 0.05714285714285714
the fields 	 0.0006920415224913495
are then 	 0.004149377593360996
in evaluating 	 0.0018726591760299626
Kurzweil Computer 	 0.2857142857142857
be adequately 	 0.004219409282700422
American English 	 0.2
those using 	 0.045454545454545456
good '' 	 0.07692307692307693
citations to 	 0.6666666666666666
reports into 	 0.4
recognition algorithms 	 0.008264462809917356
the informal 	 0.0006920415224913495
language metamodel 	 0.006756756756756757
ranking sentences 	 0.14285714285714285
existing hand-written 	 0.2
dominance of 	 1.0
cheque -LRB- 	 1.0
one 10msec 	 0.015384615384615385
tune the 	 1.0
users . 	 0.2222222222222222
identify objects 	 0.08333333333333333
write ` 	 1.0
National Giro 	 0.3333333333333333
interaction by 	 0.125
databases as 	 0.125
space character 	 0.2
<s> Deep 	 0.0007686395080707148
columns and 	 1.0
of computer 	 0.0035650623885918
section titles 	 0.16666666666666666
by extracting 	 0.005714285714285714
radio frequencies 	 1.0
usually has 	 0.03125
expected -LRB- 	 0.14285714285714285
approach used 	 0.02857142857142857
decade to 	 0.3333333333333333
<s> Knowing 	 0.0007686395080707148
talking about 	 1.0
structure is 	 0.08333333333333333
keyphrases `` 	 0.02857142857142857
itself to 	 0.2
Rules are 	 0.6666666666666666
and pragmatics 	 0.001445086705202312
of repeated 	 0.00089126559714795
facemask , 	 1.0
disambiguation -RRB- 	 0.2
funding to 	 0.125
examples . 	 0.16666666666666666
Understanding Conference 	 0.5
and what 	 0.001445086705202312
Input -RRB- 	 0.5
languages semantics 	 0.02
most natural 	 0.034482758620689655
continued research 	 0.2222222222222222
and became 	 0.001445086705202312
quantitative evaluation 	 0.5
programer 's 	 1.0
other country 	 0.014285714285714285
need context 	 0.047619047619047616
same way 	 0.04
input character 	 0.024390243902439025
implemented by 	 0.2
person uses 	 0.05263157894736842
much funding 	 0.045454545454545456
first of 	 0.030303030303030304
match between 	 0.16666666666666666
converting printed 	 0.5
with capitalization 	 0.00546448087431694
system would 	 0.021505376344086023
rules should 	 0.023255813953488372
of interaction 	 0.00089126559714795
a good 	 0.0049079754601227
various attempts 	 0.05555555555555555
recognize all 	 0.1111111111111111
be analyzed 	 0.004219409282700422
PDF to 	 1.0
language by 	 0.006756756756756757
movie together 	 0.3333333333333333
TF-IDF vectors 	 1.0
input feature 	 0.024390243902439025
periods in 	 0.3333333333333333
features . 	 0.07692307692307693
selected based 	 0.5
the formal 	 0.001384083044982699
transcribe such 	 1.0
translation has 	 0.02702702702702703
multi-document summarization 	 0.75
High-performance fighter 	 1.0
similarity to 	 0.1
see and 	 0.05
<s> Large-scale 	 0.0007686395080707148
software for 	 0.037037037037037035
`` machine 	 0.005291005291005291
<s> Morphological 	 0.0007686395080707148
were developed 	 0.12195121951219512
of hyphenation 	 0.00089126559714795
early successes 	 0.1
judges can 	 0.5
entered the 	 0.5
of triple 	 0.00089126559714795
-RRB- into 	 0.005420054200542005
itself or 	 0.2
performance has 	 0.05555555555555555
segment to 	 0.1111111111111111
core database 	 0.5
documents at 	 0.02631578947368421
response , 	 0.5
relevance or 	 0.3333333333333333
these systems 	 0.11904761904761904
American camp 	 0.2
develop in 	 0.2
, beyond 	 0.0005614823133071309
In 1970 	 0.009523809523809525
<s> Encouraging 	 0.0007686395080707148
projects in 	 0.5
for detecting 	 0.0036101083032490976
Web-based OCR 	 0.6666666666666666
` discourse 	 0.0625
texts by 	 0.058823529411764705
date is 	 0.3333333333333333
rooms , 	 1.0
possessive , 	 1.0
the letter 	 0.0006920415224913495
accelerations and 	 1.0
that make 	 0.010638297872340425
the data 	 0.002768166089965398
& lines 	 0.125
for verification 	 0.0036101083032490976
which were 	 0.014492753623188406
answers can 	 0.08333333333333333
do so 	 0.038461538461538464
two -LRB- 	 0.034482758620689655
be the 	 0.012658227848101266
tagging : 	 0.04
Gismo '' 	 0.5
<s> N 	 0.0007686395080707148
France installing 	 0.25
input , 	 0.07317073170731707
measure based 	 0.09090909090909091
is available 	 0.0020325203252032522
interrogative -LRB- 	 1.0
result in 	 0.09090909090909091
of existing 	 0.0017825311942959
Press '' 	 1.0
grammatical analysis 	 0.09090909090909091
: Statistical 	 0.00980392156862745
potential parses 	 0.14285714285714285
are delimited 	 0.012448132780082987
, each 	 0.003368893879842785
FAA as 	 0.5
theory it 	 0.07692307692307693
fully automatic 	 0.5
lexicon , 	 0.1111111111111111
a tagging 	 0.001226993865030675
get the 	 0.2857142857142857
performed an 	 0.1
even with 	 0.037037037037037035
program that 	 0.09090909090909091
, displayed 	 0.0005614823133071309
: it 	 0.00980392156862745
sentence-level syntax 	 1.0
The experiment 	 0.005208333333333333
hyphenated words 	 1.0
-LRB- subject 	 0.0027100271002710027
waves are 	 0.14285714285714285
after 2,000 	 0.08333333333333333
commercial interest 	 0.09090909090909091
following -LRB- 	 0.06666666666666667
<s> Head-driven 	 0.0007686395080707148
for different 	 0.0036101083032490976
gender , 	 1.0
begin and 	 0.3333333333333333
<s> Typically 	 0.0007686395080707148
few sentences 	 1.0
as smart 	 0.003484320557491289
with morphological 	 0.00546448087431694
even for 	 0.037037037037037035
there may 	 0.025
occur on 	 0.2
Levenshtein distance 	 1.0
is formed 	 0.0020325203252032522
taking only 	 0.2
the final 	 0.002768166089965398
data records 	 0.012987012987012988
impossible when 	 0.5
parsing refers 	 0.03571428571428571
the annual 	 0.0006920415224913495
however , 	 0.9230769230769231
dialing -LRB- 	 1.0
, Judith 	 0.0005614823133071309
labor involved 	 0.5
syntactic . 	 0.07692307692307693
high . 	 0.05555555555555555
19th - 	 1.0
<s> Example-based 	 0.0007686395080707148
parsers . 	 0.07692307692307693
then appear 	 0.02857142857142857
not present 	 0.026785714285714284
abstracts or 	 0.5
the content 	 0.0020761245674740486
Digest and 	 0.3333333333333333
remember the 	 1.0
<s> Even 	 0.0007686395080707148
weights to 	 0.4
the original 	 0.006920415224913495
the Annual 	 0.0006920415224913495
: lexical 	 0.00980392156862745
-LRB- most 	 0.01084010840108401
the helicopter 	 0.0020761245674740486
, Shipibo 	 0.0005614823133071309
like writing 	 0.03571428571428571
abbreviations -RRB- 	 0.2
some glue 	 0.012048192771084338
entity recognition 	 0.4
was walking 	 0.012987012987012988
singular , 	 0.25
launched the 	 1.0
necessary , 	 0.1
derivations of 	 1.0
not pre 	 0.008928571428571428
, ratings 	 0.0005614823133071309
in vastly 	 0.0018726591760299626
software libraries 	 0.037037037037037035
has turned 	 0.011904761904761904
its polarity 	 0.02857142857142857
Roger Fowler 	 0.25
`` create 	 0.005291005291005291
speech recognition-related 	 0.006578947368421052
1977 -RRB- 	 1.0
patterns , 	 0.2
rate of 	 0.2727272727272727
A relationship 	 0.02
were written 	 0.024390243902439025
supervised methods 	 0.125
find left-most 	 0.07692307692307693
<s> ELIZA 	 0.0023059185242121443
be answered 	 0.004219409282700422
of morphologically 	 0.00089126559714795
probability -RRB- 	 0.2857142857142857
word senses 	 0.016666666666666666
later users 	 0.1
the spectrum 	 0.0006920415224913495
science convention 	 0.1
therapy -LRB- 	 1.0
The second 	 0.005208333333333333
might refer 	 0.038461538461538464
database or 	 0.2
Audio , 	 0.5
pseudo-pilot '' 	 0.5
soon . 	 0.3333333333333333
, rushing 	 0.0005614823133071309
simple voice 	 0.038461538461538464
resolve some 	 0.25
the highest 	 0.0006920415224913495
substantially . 	 1.0
extension of 	 1.0
training and 	 0.03571428571428571
graph is 	 0.23076923076923078
about 1,000,000 	 0.025
likelihood for 	 0.3333333333333333
much about 	 0.045454545454545456
primary output 	 0.5
rarely successful 	 0.3333333333333333
to is 	 0.0013280212483399733
their suitability 	 0.029411764705882353
candidates so 	 0.2
associate or 	 0.5
published . 	 0.14285714285714285
term used 	 0.1111111111111111
In the 	 0.13333333333333333
are multiple 	 0.004149377593360996
might appear 	 0.038461538461538464
were SHRDLU 	 0.024390243902439025
for tense 	 0.0036101083032490976
Annex on 	 1.0
a speech-recognition 	 0.0036809815950920245
by Naomi 	 0.005714285714285714
Consultant -LRB- 	 1.0
versus `` 	 1.0
own sentence 	 0.16666666666666666
processing -LRB- 	 0.07407407407407407
more basic 	 0.021052631578947368
likely to 	 0.4375
with edit 	 0.00546448087431694
analysis could 	 0.015384615384615385
learning methods 	 0.023255813953488372
was by 	 0.025974025974025976
`` random 	 0.005291005291005291
machine speech 	 0.012658227848101266
Carla Willig 	 1.0
editing the 	 0.5
and non-linear 	 0.001445086705202312
have '' 	 0.009615384615384616
maybe to 	 1.0
logic structures 	 0.25
into play 	 0.01282051282051282
dimensionality reduction 	 1.0
2007 -RRB- 	 0.2
first approximation 	 0.030303030303030304
-RRB- in 	 0.01084010840108401
the forward-backward 	 0.0006920415224913495
, recently 	 0.0005614823133071309
second important 	 0.1
binary classifier 	 0.25
part-of-speech markers 	 0.06666666666666667
in 1949 	 0.0018726591760299626
a situation 	 0.001226993865030675
, could 	 0.0005614823133071309
Dijk , 	 1.0
piece of 	 1.0
score . 	 0.5
its history 	 0.02857142857142857
learning techniques 	 0.023255813953488372
-LRB- Asia 	 0.0027100271002710027
claimed that 	 1.0
and know 	 0.001445086705202312
-LRB- how 	 0.008130081300813009
poetry passages 	 1.0
depends greatly 	 0.125
they would 	 0.025
given text 	 0.041666666666666664
of linguistics 	 0.00089126559714795
and Lao 	 0.001445086705202312
values , 	 0.125
systems of 	 0.05357142857142857
Multimodal interaction 	 1.0
translation tasks 	 0.013513513513513514
Evaluation -LRB- 	 0.1111111111111111
quantities of 	 0.6666666666666666
the design 	 0.001384083044982699
field of 	 0.4444444444444444
and substitution 	 0.001445086705202312
of pilot 	 0.00089126559714795
from left 	 0.009615384615384616
which words 	 0.014492753623188406
Symbian and 	 1.0
online databases 	 0.125
, sometimes 	 0.0005614823133071309
system was 	 0.053763440860215055
the picture 	 0.001384083044982699
is growing 	 0.0020325203252032522
originally developed 	 0.5
smoothing to 	 1.0
discussed below 	 0.42857142857142855
the grammar-based 	 0.0006920415224913495
that pollen 	 0.0035460992907801418
company to 	 0.3333333333333333
Commanders and 	 1.0
culture of 	 1.0
due especially 	 0.2
<s> He 	 0.005380476556495004
involve various 	 0.16666666666666666
keyphrases . 	 0.3142857142857143
and features 	 0.001445086705202312
the keyboard 	 0.0006920415224913495
Carbonell , 	 1.0
not as 	 0.008928571428571428
included -LRB- 	 0.125
Robert de 	 0.25
might expect 	 0.038461538461538464
develop OCR 	 0.2
second aim 	 0.1
complex endeavors 	 0.041666666666666664
ambiguities and 	 0.25
The nodes 	 0.005208333333333333
to and 	 0.0013280212483399733
-LRB- corpus 	 0.0027100271002710027
unigrams . 	 0.16666666666666666
than metrics 	 0.022222222222222223
common ground 	 0.04
as content 	 0.003484320557491289
first sentence-end 	 0.030303030303030304
labeled as 	 0.3333333333333333
with boundary 	 0.00546448087431694
Running PageRank\/TextRank 	 1.0
Speech can 	 0.03225806451612903
to see 	 0.0026560424966799467
dissertation -LRB- 	 0.3333333333333333
the expectations 	 0.0006920415224913495
, during 	 0.0005614823133071309
of these 	 0.00980392156862745
tagging work 	 0.04
who utilize 	 0.1
the discourse 	 0.0020761245674740486
a succession 	 0.001226993865030675
Training for 	 0.5
possess -LRB- 	 1.0
Brill Tagger 	 0.3333333333333333
when necessary 	 0.02857142857142857
as Scansoft 	 0.003484320557491289
to corpus 	 0.0013280212483399733
people . 	 0.125
ELIZA sometimes 	 0.1111111111111111
, models 	 0.0005614823133071309
-LRB- more 	 0.005420054200542005
acoustic and 	 0.16666666666666666
that merges 	 0.0035460992907801418
translation capabilities 	 0.013513513513513514
walk , 	 0.2
an embedded 	 0.007575757575757576
Response based 	 1.0
various levels 	 0.05555555555555555
high level 	 0.16666666666666666
OCR -RRB- 	 0.02040816326530612
feature dependencies 	 0.07692307692307693
ROUGE metric 	 0.2
largely similar 	 0.2
are content 	 0.004149377593360996
returned with 	 0.25
NN for 	 1.0
be expected 	 0.012658227848101266
20,000 words 	 1.0
alone may 	 0.25
as gold 	 0.003484320557491289
science of 	 0.1
Future research 	 0.5
e.g. the 	 0.05357142857142857
card spending 	 0.25
progress was 	 0.2857142857142857
ranging from 	 1.0
yielding thousands 	 1.0
tractability . 	 1.0
digital . 	 0.14285714285714285
<s> Higher 	 0.0007686395080707148
summaries humans 	 0.023255813953488372
of edge 	 0.00089126559714795
to accurately 	 0.0013280212483399733
was unveiled 	 0.012987012987012988
, are 	 0.0011229646266142617
as relations 	 0.003484320557491289
is identifying 	 0.0020325203252032522
form multi-word 	 0.05
, stutering 	 0.0005614823133071309
than text 	 0.022222222222222223
the Pyramid 	 0.0006920415224913495
sample is 	 0.3333333333333333
interaction Genres 	 0.125
part-of-speech , 	 0.06666666666666667
uses a 	 0.2857142857142857
semantic or 	 0.047619047619047616
an example 	 0.03787878787878788
up or 	 0.045454545454545456
proper evaluation 	 0.14285714285714285
Another project 	 0.07692307692307693
it works 	 0.008547008547008548
produce such 	 0.045454545454545456
-LRB- the 	 0.02168021680216802
200 , 	 0.5
English are 	 0.02702702702702703
which proposed 	 0.007246376811594203
of artifacts 	 0.00089126559714795
company Kurzweil 	 0.3333333333333333
resource -LRB- 	 0.2
modeling salience 	 0.14285714285714285
wrote a 	 0.16666666666666666
Black-box evaluation 	 0.5
light on 	 0.3333333333333333
social psychology 	 0.07142857142857142
-LRB- Loriot 	 0.0027100271002710027
does n't 	 0.1
keyphrases available 	 0.02857142857142857
a system-generated 	 0.001226993865030675
` summary 	 0.0625
a specialist 	 0.001226993865030675
datum is 	 1.0
high probability 	 0.05555555555555555
objects and 	 0.2
through a 	 0.25
Automated Summarizers 	 0.5
Blommaert , 	 1.0
speech , 	 0.07236842105263158
Navy , 	 1.0
something similar 	 1.0
discourse processing 	 0.027777777777777776
translation , 	 0.10810810810810811
be taken 	 0.004219409282700422
into basic 	 0.01282051282051282
<s> Once 	 0.003843197540353574
vs. glass-box 	 0.08333333333333333
When processing 	 0.14285714285714285
the industry 	 0.0006920415224913495
a test 	 0.0036809815950920245
of Scotland 	 0.0017825311942959
ratings on 	 0.1111111111111111
fail during 	 0.3333333333333333
would somehow 	 0.018867924528301886
, tag 	 0.0005614823133071309
decades -LRB- 	 1.0
English grammars 	 0.02702702702702703
on integrating 	 0.0047169811320754715
typical accuracy 	 0.1111111111111111
<s> Note 	 0.006917755572636433
blind , 	 0.25
3 % 	 0.2
stemming or 	 0.5
phrased in 	 1.0
<s> Chinese 	 0.0007686395080707148
`` hub 	 0.005291005291005291
sources . 	 0.3333333333333333
from floods 	 0.009615384615384616
and use 	 0.004335260115606936
form an 	 0.05
over the 	 0.25
post - 	 1.0
recommend '' 	 1.0
word-category disambiguation 	 1.0
interaction Pronunciation 	 0.125
the probable 	 0.0006920415224913495
and whether 	 0.001445086705202312
suggest valuable 	 0.3333333333333333
format . 	 0.5
represent . 	 0.2222222222222222
, grounded 	 0.0005614823133071309
with Nuance 	 0.00546448087431694
Turney with 	 0.1111111111111111
EMR -RRB- 	 0.3333333333333333
even so 	 0.037037037037037035
idea that 	 0.2857142857142857
regular expressions 	 1.0
following words 	 0.06666666666666667
faster to 	 0.3333333333333333
, unless 	 0.0005614823133071309
subject of 	 0.25
find a 	 0.15384615384615385
Size Grows 	 1.0
provide additional 	 0.16666666666666666
dictionary or 	 0.14285714285714285
<s> LR 	 0.0007686395080707148
sentence Grass 	 0.020833333333333332
human knowledge 	 0.021739130434782608
the models 	 0.0006920415224913495
been heard 	 0.014705882352941176
not initially 	 0.008928571428571428
output from 	 0.038461538461538464
critics claim 	 1.0
<s> If 	 0.006149116064565719
allowable substitutions 	 0.5
systems remains 	 0.008928571428571428
level 6 	 0.05
backgrounds , 	 1.0
10 parsers 	 0.125
machine translation 	 0.4936708860759494
which often 	 0.007246376811594203
dry up 	 1.0
stage of 	 0.4
usually thought 	 0.03125
corpus such 	 0.03225806451612903
machine-translation research 	 0.5
1971 Terry 	 0.3333333333333333
decide that 	 0.25
be detected 	 0.004219409282700422
tasks from 	 0.03125
reader -RRB- 	 0.1
insurance bills 	 1.0
in color 	 0.0018726591760299626
Machine Aided 	 0.1111111111111111
choice between 	 0.125
sometimes confused 	 0.07692307692307693
serial numbers 	 1.0
the full 	 0.0020761245674740486
the second 	 0.001384083044982699
generally lend 	 0.09090909090909091
Robert E. 	 0.5
appears in 	 0.2
mechanisms of 	 0.5
of human 	 0.004456327985739751
remains the 	 0.25
though it 	 0.2
not produce 	 0.008928571428571428
evaluation campaign 	 0.018518518518518517
seen in 	 0.1
phases . 	 1.0
a topic 	 0.001226993865030675
manual annotation 	 0.5
use a 	 0.05555555555555555
same in 	 0.04
only do 	 0.02631578947368421
using both 	 0.01694915254237288
response Mobile 	 0.5
-LRB- Meehan 	 0.0027100271002710027
What learning 	 0.09090909090909091
may result 	 0.019230769230769232
person 's 	 0.21052631578947367
keyphrase containing 	 0.05263157894736842
social science 	 0.07142857142857142
as they 	 0.010452961672473868
that when 	 0.0035460992907801418
you ' 	 0.07692307692307693
and to 	 0.015895953757225433
approaches are 	 0.03571428571428571
methods based 	 0.022727272727272728
concerned with 	 0.8
did exactly 	 0.2
by Greene 	 0.005714285714285714
multiple languages 	 0.07692307692307693
our everyday 	 0.2
-LRB- JSF 	 0.0027100271002710027
obstacles to 	 1.0
exactly as 	 0.3333333333333333
' stability 	 0.05263157894736842
run-time . 	 1.0
commonly defined 	 0.125
You are 	 1.0
of 5 	 0.00089126559714795
past the 	 0.3333333333333333
analysis tasks 	 0.015384615384615385
reader installed 	 0.1
printed documents 	 0.08333333333333333
capital of 	 0.6666666666666666
for Gisting 	 0.007220216606498195
to grow 	 0.0013280212483399733
developed to 	 0.038461538461538464
, words 	 0.0011229646266142617
on personal 	 0.0047169811320754715
related data 	 0.06666666666666667
Rule-based machine 	 0.5
<s> LexRank 	 0.0015372790161414297
the APEXC 	 0.0006920415224913495
often marked 	 0.022727272727272728
The importance 	 0.005208333333333333
represents an 	 0.25
linear algebra 	 0.14285714285714285
validity and 	 1.0
word recognition 	 0.016666666666666666
been displaced 	 0.014705882352941176
together , 	 0.125
Eurofighter Typhoon 	 1.0
grammatical structure 	 0.09090909090909091
text accordingly 	 0.006289308176100629
easily copied 	 0.1111111111111111
the earliest 	 0.0006920415224913495
corrected by 	 1.0
entities often 	 0.14285714285714285
or desired 	 0.0045045045045045045
cut and 	 1.0
RCA engineers 	 0.2
lunar science 	 1.0
modern parsers 	 0.2
by silence 	 0.005714285714285714
negligence '' 	 1.0
A technique 	 0.02
^ 2 	 0.3333333333333333
attitude of 	 0.5
Electronic Medical 	 0.5
components . 	 0.2
thus avoiding 	 0.1
it led 	 0.008547008547008548
different strategies 	 0.02040816326530612
Evaluation exercises 	 0.1111111111111111
has improved 	 0.011904761904761904
One example 	 0.07692307692307693
results may 	 0.047619047619047616
These standards 	 0.058823529411764705
etc. . 	 0.4090909090909091
components operating 	 0.2
represent a 	 0.2222222222222222
question can 	 0.023809523809523808
% ; 	 0.02564102564102564
experimented with 	 1.0
many authors 	 0.019230769230769232
bought the 	 1.0
a keyphrase 	 0.00245398773006135
Accuracy of 	 0.42857142857142855
approach was 	 0.02857142857142857
GRASSHOPPER incorporates 	 0.3333333333333333
itself while 	 0.2
rocks returned 	 1.0
Windows Mobile 	 1.0
LexRank and 	 0.08333333333333333
improve the 	 0.07692307692307693
funding Measuring 	 0.125
probability of 	 0.14285714285714285
in errata 	 0.0018726591760299626
digital speech-to-text 	 0.14285714285714285
of discourses 	 0.00089126559714795
all get 	 0.023255813953488372
<s> Search 	 0.0015372790161414297
keyphrases can 	 0.05714285714285714
this level 	 0.01098901098901099
via the 	 1.0
string of 	 1.0
division of 	 0.5
impersonate a 	 1.0
grammar , 	 0.10810810810810811
Discontinuous or 	 1.0
the memory 	 0.0006920415224913495
separate words 	 0.3
candidates can 	 0.2
improve results 	 0.07692307692307693
like what 	 0.03571428571428571
a stochastic 	 0.001226993865030675
verbs -LRB- 	 0.2
be recognized 	 0.008438818565400843
'' which 	 0.005154639175257732
sad , 	 1.0
involve the 	 0.16666666666666666
in advanced 	 0.0018726591760299626
new application 	 0.041666666666666664
, political 	 0.0005614823133071309
do not 	 0.5
The vectors 	 0.005208333333333333
three or 	 0.3333333333333333
first customers 	 0.030303030303030304
, particularly 	 0.0011229646266142617
formal grammar 	 0.2222222222222222
which makes 	 0.021739130434782608
automatic analysis 	 0.043478260869565216
TextRank . 	 0.07142857142857142
years researchers 	 0.047619047619047616
written text 	 0.11538461538461539
overlap . 	 0.25
splitting is 	 0.5
which a 	 0.014492753623188406
instead of 	 0.5714285714285714
smaller tag-sets 	 0.14285714285714285
samples per 	 0.5
by different 	 0.005714285714285714
for plural 	 0.0036101083032490976
wide variance 	 0.25
feature\/aspect-based sentiment 	 1.0
size , 	 0.16666666666666666
single verbal 	 0.07142857142857142
overall task 	 0.16666666666666666
correct software 	 0.06666666666666667
as ACL 	 0.003484320557491289
98.5 % 	 1.0
raters typically 	 1.0
abstractive method 	 0.16666666666666666
<s> Discourse 	 0.0023059185242121443
A different 	 0.04
ranking task 	 0.14285714285714285
integration with 	 1.0
continued development 	 0.1111111111111111
whereby words 	 1.0
BASEBALL and 	 0.5
optimizing a 	 1.0
successful in 	 0.1111111111111111
the basis 	 0.002768166089965398
dividing a 	 0.3333333333333333
the one 	 0.0006920415224913495
answer the 	 0.03333333333333333
following years 	 0.06666666666666667
tagging or 	 0.08
Greek , 	 0.3333333333333333
Once the 	 0.2
<s> Human 	 0.0023059185242121443
germane to 	 1.0
Contains Confusable 	 1.0
benefits to 	 0.5
address field 	 0.25
Naturally Speaking 	 1.0
necessary anymore 	 0.1
translation tries 	 0.013513513513513514
more dynamic 	 0.010526315789473684
decade , 	 0.3333333333333333
networks make 	 0.07142857142857142
visible Markov 	 0.3333333333333333
have problems 	 0.009615384615384616
to authenticate 	 0.0013280212483399733
patterns of 	 0.2
and analysis 	 0.001445086705202312
Charles Goodwin 	 1.0
-RRB- Commissioned 	 0.0027100271002710027
-LRB- Some 	 0.0027100271002710027
achieved , 	 0.2
the Lancaster-Oslo-Bergen 	 0.0006920415224913495
punctuation marks 	 0.2857142857142857
Unsupervised taggers 	 0.16666666666666666
feedback . 	 0.5
reviews and 	 0.16666666666666666
and Re-encoding 	 0.001445086705202312
why Vice 	 0.14285714285714285
require the 	 0.18181818181818182
across most 	 0.6
this in 	 0.01098901098901099
statistical output 	 0.030303030303030304
robots , 	 1.0
UK . 	 0.5
emigre Leo 	 1.0
morphemes -LRB- 	 0.3333333333333333
with neural 	 0.00546448087431694
that closely 	 0.0035460992907801418
an open 	 0.015151515151515152
different languages 	 0.02040816326530612
English like 	 0.02702702702702703
100000 may 	 1.0
is Reiter 	 0.0020325203252032522
, pollen 	 0.0005614823133071309
the narrowest 	 0.0006920415224913495
views as 	 1.0
phrases , 	 0.125
language production 	 0.006756756756756757
A well-known 	 0.02
insight into 	 1.0
Google used 	 0.25
the differences 	 0.0006920415224913495
level ; 	 0.1
search for 	 0.09090909090909091
Another research 	 0.07692307692307693
went in 	 0.2
as semantic 	 0.003484320557491289
ambiguity '' 	 0.125
text -RRB- 	 0.006289308176100629
to select 	 0.005312084993359893
by precision 	 0.005714285714285714
about feature 	 0.025
reasoning schemes 	 0.14285714285714285
with having 	 0.00546448087431694
discourse grammar 	 0.027777777777777776
, sociolinguistics 	 0.0005614823133071309
, fuse 	 0.0005614823133071309
icon -RRB- 	 1.0
a nice 	 0.00245398773006135
Much effort 	 0.3333333333333333
questioner , 	 0.5
contain rules 	 0.08333333333333333
As mentioned 	 0.16666666666666666
true keyphrases 	 0.5
In 1974 	 0.009523809523809525
importance is 	 0.16666666666666666
who used 	 0.1
compiler or 	 0.3333333333333333
preliminary , 	 0.3333333333333333
assertion , 	 1.0
maximal probability 	 1.0
a psychologist 	 0.001226993865030675
photocells , 	 1.0
Scotland to 	 0.2
have error 	 0.009615384615384616
would not 	 0.018867924528301886
Bayes risk 	 0.3333333333333333
a pre-processing 	 0.001226993865030675
only because 	 0.02631578947368421
creates new 	 0.5
<s> Collection 	 0.0007686395080707148
a revolution 	 0.001226993865030675
grammars are 	 0.07142857142857142
requires each 	 0.0625
into subfields 	 0.01282051282051282
ideal deep 	 1.0
solutions to 	 0.5
impossibility of 	 1.0
is being 	 0.006097560975609756
based speech 	 0.018518518518518517
ratings : 	 0.1111111111111111
animation , 	 1.0
, Number 	 0.0005614823133071309
by programs 	 0.005714285714285714
punctuation and 	 0.2857142857142857
Machine learning 	 0.1111111111111111
Internet financial 	 0.5
token , 	 0.25
in this 	 0.018726591760299626
particular method 	 0.07692307692307693
not determine 	 0.008928571428571428
, roughness 	 0.0005614823133071309
may well 	 0.019230769230769232
that the 	 0.08156028368794327
Mellon University 	 1.0
interactive translation 	 0.25
and documents 	 0.001445086705202312
are , 	 0.004149377593360996
vertex for 	 0.6666666666666666
himself with 	 0.5
using paraphrases 	 0.01694915254237288
see computational 	 0.05
which ? 	 0.007246376811594203
discourse is 	 0.08333333333333333
Objectives The 	 1.0
techniques , 	 0.08695652173913043
classification '' 	 0.29411764705882354
the ' 	 0.0006920415224913495
, and\/or 	 0.0005614823133071309
The Future 	 0.005208333333333333
rate crossed 	 0.09090909090909091
vowels and 	 0.3333333333333333
participate in 	 1.0
to cope 	 0.0013280212483399733
, Svenka 	 0.0005614823133071309
ostensibly simple 	 1.0
local collection 	 0.3333333333333333
in machine 	 0.009363295880149813
for health 	 0.0036101083032490976
called ISO\/TC37\/SC4 	 0.05555555555555555
BLEU . 	 0.3333333333333333
sufficient include 	 0.2
-LRB- VTLN 	 0.0027100271002710027
extrapolate that 	 1.0
of simple 	 0.0017825311942959
pumps last 	 0.5
GRASSHOPPER algorithm 	 0.3333333333333333
summary -LRB- 	 0.023809523809523808
electronic conversion 	 0.5
ones already 	 0.1
world of 	 0.06666666666666667
barmaid -RRB- 	 0.5
be directed 	 0.004219409282700422
to derive 	 0.0013280212483399733
1960s . 	 0.3333333333333333
bites man 	 0.3333333333333333
and 2002 	 0.001445086705202312
other purposes 	 0.014285714285714285
sentences . 	 0.10526315789473684
LexRank deals 	 0.08333333333333333
, supervised 	 0.0005614823133071309
to search 	 0.0013280212483399733
news release 	 0.07692307692307693
any arbitrary 	 0.06451612903225806
to conduct 	 0.0013280212483399733
Method -RRB- 	 1.0
purpose at 	 0.2
usually takes 	 0.03125
technique is 	 0.14285714285714285
column of 	 1.0
language characters 	 0.006756756756756757
extractors are 	 1.0
computer can 	 0.022727272727272728
a sense 	 0.001226993865030675
the core 	 0.0006920415224913495
books a 	 1.0
relations between 	 0.4166666666666667
retrieval -LRB- 	 0.14285714285714285
in depth 	 0.0018726591760299626
Bayes classifier 	 0.3333333333333333
cited the 	 1.0
identify new 	 0.08333333333333333
can of 	 0.0055248618784530384
have started 	 0.009615384615384616
sentences correct 	 0.013157894736842105
`` recommend 	 0.010582010582010581
addressed the 	 0.5
<s> Several 	 0.0023059185242121443
helped improve 	 0.3333333333333333
of units 	 0.00089126559714795
use heteroscedastic 	 0.013888888888888888
to say 	 0.00398406374501992
removal of 	 1.0
been a 	 0.029411764705882353
of tourism 	 0.00089126559714795
there as 	 0.025
enable the 	 1.0
with weights 	 0.00546448087431694
1930s . 	 1.0
the assistance 	 0.0006920415224913495
-LRB- FAS 	 0.0027100271002710027
speed for 	 0.14285714285714285
on this 	 0.014150943396226415
Two years 	 0.42857142857142855
guide the 	 1.0
answers the 	 0.08333333333333333
rules defining 	 0.023255813953488372
us to 	 0.5
`` The 	 0.015873015873015872
<s> User 	 0.0007686395080707148
common . 	 0.08
pronoun , 	 1.0
content-analysis . 	 1.0
consists of 	 1.0
thereby editing 	 1.0
virtual currency 	 1.0
answering -LRB- 	 0.08333333333333333
, Deborah 	 0.0011229646266142617
applications include 	 0.08
would correspond 	 0.018867924528301886
intelligence technologies 	 0.125
using NLG 	 0.05084745762711865
be nested 	 0.004219409282700422
analysis to 	 0.015384615384615385
An important 	 0.125
explicitly promoting 	 0.25
each ambiguity 	 0.022222222222222223
receivers . 	 1.0
for multi-document 	 0.0036101083032490976
of scanned 	 0.00089126559714795
for 1-July-2005 	 0.0036101083032490976
was considerable 	 0.012987012987012988
predict task-effectiveness 	 0.3333333333333333
compared . 	 0.2857142857142857
Cross-Sentence Information 	 1.0
using votes 	 0.01694915254237288
, conversation 	 0.0005614823133071309
termed `` 	 0.5
and isolated 	 0.001445086705202312
& Critical 	 0.125
or most 	 0.0045045045045045045
of its 	 0.0071301247771836
tag set 	 0.3125
right answer 	 0.1
the high 	 0.001384083044982699
about how 	 0.025
with , 	 0.00546448087431694
on . 	 0.018867924528301886
training techniques 	 0.03571428571428571
punctuation characters 	 0.14285714285714285
Such a 	 0.125
Putting words 	 1.0
He received 	 0.125
be implemented 	 0.008438818565400843
, NN 	 0.0005614823133071309
40 % 	 1.0
Who invented 	 0.5
Gisting Evaluation 	 1.0
virtually impossible 	 0.5
science disciplines 	 0.1
are based 	 0.02074688796680498
other to 	 0.014285714285714285
the web 	 0.001384083044982699
process broken 	 0.027777777777777776
model Modern 	 0.03333333333333333
the common 	 0.0006920415224913495
available to 	 0.058823529411764705
answer extraction 	 0.06666666666666667
capable of 	 1.0
user-provided number 	 1.0
abstraction Broadly 	 0.25
Sept. 1955 	 1.0
Another reason 	 0.07692307692307693
or text 	 0.009009009009009009
probabilities . 	 0.18181818181818182
<s> Answer 	 0.0015372790161414297
documents with 	 0.05263157894736842
aircraft . 	 0.14285714285714285
ASRU . 	 1.0
-LRB- MAHS 	 0.0027100271002710027
is getting 	 0.0040650406504065045
meantime , 	 1.0
`` mentions 	 0.005291005291005291
comprehensive model 	 0.2
out of 	 0.07142857142857142
He entered 	 0.125
, pronoun 	 0.0005614823133071309
sounds of 	 0.13333333333333333
<s> Compare 	 0.0007686395080707148
, wrote 	 0.0005614823133071309
a professional 	 0.001226993865030675
Australia . 	 1.0
transform -RRB- 	 0.2
most of 	 0.08620689655172414
, 1977 	 0.0005614823133071309
for large-vocabulary 	 0.0036101083032490976
attempts at 	 0.3333333333333333
Record or 	 1.0
This way 	 0.015873015873015872
input -LRB- 	 0.04878048780487805
to extract 	 0.00398406374501992
use an 	 0.013888888888888888
P , 	 0.5
, mainly 	 0.0005614823133071309
American Bible 	 0.2
, large 	 0.0005614823133071309
writing systems 	 0.2222222222222222
and indirect 	 0.001445086705202312
successfully adapted 	 0.3333333333333333
different levels 	 0.02040816326530612
user . 	 0.21428571428571427
<s> Acoustical 	 0.0007686395080707148
common nouns 	 0.08
the relevance 	 0.0006920415224913495
`` understand 	 0.005291005291005291
a domain-specific 	 0.001226993865030675
its speakers 	 0.02857142857142857
David Nunan 	 0.25
one has 	 0.015384615384615385
the platform 	 0.0006920415224913495
the surrounding 	 0.001384083044982699
WebOCR also 	 0.25
neural networks 	 0.5333333333333333
lot more 	 0.3333333333333333
web 2.0 	 0.125
alone -- 	 0.25
This task 	 0.031746031746031744
CFG -LRB- 	 1.0
those languages 	 0.09090909090909091
on text 	 0.0047169811320754715
it quite 	 0.008547008547008548
both the 	 0.06451612903225806
tool to 	 0.5
review as 	 0.3333333333333333
summarization research 	 0.02
is closer 	 0.0020325203252032522
-LRB- written 	 0.0027100271002710027
selection is 	 1.0
an inseparable 	 0.007575757575757576
best that 	 0.1111111111111111
Snyder -LRB- 	 0.5
reasoning to 	 0.14285714285714285
to travel 	 0.0013280212483399733
GRASSHOPPER for 	 0.3333333333333333
are remarkably 	 0.004149377593360996
highly ambiguous 	 0.1111111111111111
lexicon . 	 0.1111111111111111
small set 	 0.1111111111111111
schemes to 	 0.5
ELIZA , 	 0.3333333333333333
speech choice 	 0.006578947368421052
especially interested 	 0.06666666666666667
case . 	 0.17647058823529413
the key 	 0.0006920415224913495
the terms 	 0.0006920415224913495
reasonable approximation 	 0.5
-LRB- GPO 	 0.0027100271002710027
were able 	 0.024390243902439025
system or 	 0.021505376344086023
horizontal mark 	 1.0
in software 	 0.0018726591760299626
the text-to-speech 	 0.0006920415224913495
scale -LRB- 	 0.16666666666666666
-- Pointwise 	 0.04
applied to 	 0.7333333333333333
<s> Throughout 	 0.0007686395080707148
, Wendy 	 0.0005614823133071309
context-free grammars 	 0.36363636363636365
more or 	 0.031578947368421054
text linguistics 	 0.006289308176100629
meaning ; 	 0.043478260869565216
recognition tasks 	 0.01652892561983471
a large 	 0.0098159509202454
comparable . 	 1.0
This convinced 	 0.015873015873015872
challenge in 	 1.0
Rabinow . 	 1.0
testing for 	 0.2
neural network 	 0.3333333333333333
word-forms are 	 1.0
overfitting and 	 0.5
useful in 	 0.14285714285714285
undercarriage , 	 1.0
sentence , 	 0.125
than has 	 0.022222222222222223
and duplicate 	 0.001445086705202312
travel . 	 1.0
journals -LRB- 	 0.5
fundamental , 	 0.5
-LRB- MMI 	 0.0027100271002710027
from those 	 0.019230769230769232
research teams 	 0.023809523809523808
generating index 	 0.2
although capabilities 	 0.16666666666666666
highly and 	 0.1111111111111111
related words 	 0.06666666666666667
either positive 	 0.1
`` not 	 0.005291005291005291
always a 	 0.3333333333333333
to bootstrap 	 0.0013280212483399733
paragraphs -RRB- 	 0.25
base of 	 0.25
Nikolas Rose 	 1.0
formed by 	 0.2
uttered before 	 0.3333333333333333
-LRB- speed 	 0.0027100271002710027
getting ranked 	 0.25
a concept 	 0.001226993865030675
human-generated summaries 	 0.5
discourse structure 	 0.027777777777777776
find the 	 0.3076923076923077
same type 	 0.04
more easily 	 0.010526315789473684
and associated 	 0.001445086705202312
words two 	 0.009174311926605505
software Annotate 	 0.037037037037037035
that were 	 0.014184397163120567
experiment was 	 0.4
speech sounds 	 0.006578947368421052
Code , 	 1.0
marked by 	 0.3333333333333333
more complex 	 0.08421052631578947
Pragmatics , 	 1.0
parser often 	 0.0625
and controversial 	 0.001445086705202312
we use 	 0.022222222222222223
Test of 	 1.0
4 . 	 0.4
has plateaued 	 0.011904761904761904
A high 	 0.02
often using 	 0.022727272727272728
statically beforehand 	 1.0
, systems 	 0.0011229646266142617
and many 	 0.001445086705202312
or five 	 0.0045045045045045045
recognized essentially 	 0.16666666666666666
systems and 	 0.008928571428571428
feature transformation 	 0.07692307692307693
classifier and 	 0.14285714285714285
unsupervised keyphrase 	 0.125
as commercial 	 0.003484320557491289
as Greek 	 0.003484320557491289
either explicit 	 0.1
be for 	 0.004219409282700422
and possessives 	 0.001445086705202312
' structures 	 0.10526315789473684
because fast 	 0.03333333333333333
extraction as 	 0.06451612903225806
text structure 	 0.006289308176100629
exploit domain-specific 	 1.0
, can 	 0.003368893879842785
Schank and 	 0.4
coined the 	 1.0
aim is 	 0.5
qualities making 	 0.5
context and 	 0.12121212121212122
better guide 	 0.1111111111111111
co-founded Google 	 1.0
learn features 	 0.07692307692307693
or classified 	 0.0045045045045045045
work in 	 0.125
also similar 	 0.014492753623188406
<s> Therefore 	 0.0015372790161414297
Such systems 	 0.125
Paul Chilton 	 0.2
Brenton D. 	 1.0
can indeed 	 0.0055248618784530384
study , 	 0.25
would like 	 0.03773584905660377
simulation vendors 	 0.3333333333333333
categories and 	 0.2222222222222222
system exhibited 	 0.010752688172043012
very widely 	 0.024390243902439025
in question 	 0.003745318352059925
the theory 	 0.0020761245674740486
segmentation approaches 	 0.030303030303030304
produces Grass 	 0.25
on SourceForge 	 0.0047169811320754715
text about 	 0.012578616352201259
be any 	 0.004219409282700422
tagging . 	 0.08
characterized DARPA 	 0.25
be sequences 	 0.004219409282700422
step -- 	 0.13333333333333333
sentence -RRB- 	 0.041666666666666664
considered an 	 0.1111111111111111
help to 	 0.1111111111111111
sociolinguistics , 	 0.5
sizes of 	 0.6666666666666666
grammar is 	 0.05405405405405406
is adaptive 	 0.0020325203252032522
were started 	 0.024390243902439025
deemed most 	 0.5
Increasingly , 	 1.0
documents per 	 0.02631578947368421
which has 	 0.050724637681159424
of mainland 	 0.0017825311942959
book applies 	 0.125
is on 	 0.0040650406504065045
processors or 	 1.0
to explore 	 0.0026560424966799467
trigram found 	 0.3333333333333333
or Hard 	 0.0045045045045045045
complexity of 	 0.6666666666666666
concerns ; 	 0.5
numerous approaches 	 1.0
In English 	 0.01904761904761905
, opens 	 0.0005614823133071309
first word 	 0.030303030303030304
speech from 	 0.006578947368421052
1980s , 	 0.5555555555555556
using conventional 	 0.01694915254237288
in ` 	 0.003745318352059925
<s> Why 	 0.0015372790161414297
potentially exponential 	 0.3333333333333333
generate polynomial-size 	 0.05555555555555555
can translate 	 0.0055248618784530384
Angenot , 	 1.0
see context-free 	 0.05
question types 	 0.023809523809523808
is having 	 0.0020325203252032522
, coreference 	 0.0005614823133071309
some degree 	 0.024096385542168676
<s> Performing 	 0.0007686395080707148
least five 	 0.2
locate the 	 1.0
significant task 	 0.1111111111111111
the token 	 0.0006920415224913495
predict performance 	 0.16666666666666666
one must 	 0.015384615384615385
Although the 	 0.375
the speaker 	 0.001384083044982699
common use 	 0.08
documents in 	 0.02631578947368421
to tag 	 0.0013280212483399733
period may 	 0.5
Ontology -RRB- 	 1.0
percentage of 	 1.0
article such 	 0.034482758620689655
, which 	 0.031443009545199324
questions have 	 0.038461538461538464
subtasks that 	 0.5
in accordance 	 0.0018726591760299626
to meet 	 0.005312084993359893
, call 	 0.0005614823133071309
and Nearest-neighbor 	 0.001445086705202312
'' do 	 0.005154639175257732
In 1982 	 0.009523809523809525
negative emotions 	 0.125
language without 	 0.006756756756756757
what new 	 0.03125
where metrics 	 0.02857142857142857
January , 	 0.25
numbers which 	 0.14285714285714285
news documents 	 0.07692307692307693
related information 	 0.06666666666666667
and Arabic 	 0.002890173410404624
particularly speech 	 0.2
with 12 	 0.00546448087431694
perform complex 	 0.09090909090909091
specification is 	 0.5
analyzed , 	 0.2
the idea 	 0.001384083044982699
, Jay 	 0.0005614823133071309
, international 	 0.0005614823133071309
were performed 	 0.024390243902439025
most prior 	 0.017241379310344827
how summarization 	 0.034482758620689655
perform a 	 0.2727272727272727
<s> Question 	 0.003843197540353574
other related 	 0.014285714285714285
computer database 	 0.022727272727272728
most notoriously 	 0.017241379310344827
`` He 	 0.005291005291005291
why he 	 0.2857142857142857
's speech 	 0.0196078431372549
waves and 	 0.14285714285714285
, Driver-license 	 0.0005614823133071309
applications seek 	 0.04
a precise 	 0.001226993865030675
the robot 	 0.0006920415224913495
the advent 	 0.0006920415224913495
, among 	 0.0005614823133071309
sublanguage domains 	 0.3333333333333333
chosen domain 	 0.2
on automatically 	 0.0047169811320754715
32 -RRB- 	 1.0
size -RRB- 	 0.16666666666666666
naive semantics 	 0.5
information extraction 	 0.021739130434782608
great deal 	 0.3333333333333333
Some notably 	 0.047619047619047616
visit Iraq 	 0.5
especially to 	 0.06666666666666667
Collection of 	 1.0
metamodel and 	 1.0
time in 	 0.030303030303030304
what class 	 0.03125
from 71 	 0.009615384615384616
, V 	 0.0005614823133071309
probably the 	 0.25
- keyphrases 	 0.0625
toolkit -RRB- 	 0.5
detail but 	 0.5
the written 	 0.0006920415224913495
risk of 	 0.5
scale . 	 0.16666666666666666
figure on 	 0.5
not seen 	 0.008928571428571428
transcription of 	 0.5
find ways 	 0.07692307692307693
providers began 	 1.0
mechanical or 	 1.0
+ Cloud 	 0.16666666666666666
not hear 	 0.008928571428571428
common for 	 0.08
We assume 	 0.14285714285714285
Words and 	 0.25
or Arabic 	 0.0045045045045045045
reference model 	 0.125
`` automatic 	 0.005291005291005291
not Afghanistan 	 0.008928571428571428
Merging of 	 1.0
one another 	 0.015384615384615385
of typical 	 0.00089126559714795
most can 	 0.017241379310344827
or interpreter 	 0.009009009009009009
Determine the 	 1.0
same objects 	 0.04
instances of 	 0.6666666666666666
would enable 	 0.018867924528301886
the 1970s 	 0.0006920415224913495
Genre Analysis 	 1.0
Parsing : 	 0.2
a Rogerian 	 0.001226993865030675
Ensemble methods 	 1.0
<s> How 	 0.0030745580322828594
changing information 	 1.0
Coreference resolution 	 1.0
than when 	 0.022222222222222223
also experimented 	 0.014492753623188406
give the 	 0.5
`` bag 	 0.005291005291005291
wave as 	 0.1111111111111111
accommodate various 	 0.2
a data 	 0.001226993865030675
fire truck 	 0.5
application requirements 	 0.07142857142857142
by visual 	 0.005714285714285714
pragmatics . 	 0.3333333333333333
'' is 	 0.04639175257731959
speech segmentation 	 0.03289473684210526
-RRB- BioCreative 	 0.0027100271002710027
boundary identification 	 0.16666666666666666
and NLP 	 0.002890173410404624
<s> Applications 	 0.0015372790161414297
A. D. 	 0.2
Alternatively , 	 1.0
final sounds 	 0.1111111111111111
the issues 	 0.0006920415224913495
often used 	 0.022727272727272728
describe developments 	 0.16666666666666666
Since OCR 	 0.2
CKY algorithm 	 1.0
speaker can 	 0.05555555555555555
version ; 	 0.3333333333333333
Canada to 	 0.16666666666666666
not readily 	 0.008928571428571428
the Turing 	 0.0006920415224913495
QA research 	 0.047619047619047616
capabilities were 	 0.2
, whether 	 0.0005614823133071309
2000 , 	 0.3333333333333333
misspelled words 	 1.0
to support 	 0.0026560424966799467
character -RRB- 	 0.045454545454545456
the wave 	 0.0006920415224913495
intended meaning 	 0.2
Generation -LRB- 	 0.5
Baum-Welch algorithm 	 1.0
human . 	 0.06521739130434782
known labeled 	 0.038461538461538464
the large 	 0.0006920415224913495
extraction Answer 	 0.03225806451612903
presentation of 	 1.0
The Eurofighter 	 0.005208333333333333
even larger 	 0.037037037037037035
Air Force 	 0.6666666666666666
weights . 	 0.4
model . 	 0.06666666666666667
d'évaluation de 	 1.0
generated out 	 0.06666666666666667
management system 	 0.14285714285714285
-LRB- unigram 	 0.0027100271002710027
preferred computer-generated 	 1.0
Style Studies 	 1.0
bar code 	 1.0
publication of 	 0.6666666666666666
of system-generated 	 0.00089126559714795
to make 	 0.005312084993359893
<s> WebOCR 	 0.0015372790161414297
without having 	 0.07692307692307693
as dynamic 	 0.003484320557491289
word spaces 	 0.016666666666666666
by Environment 	 0.005714285714285714
gradually reduced 	 1.0
time and 	 0.09090909090909091
perhaps because 	 0.16666666666666666
million words 	 0.3333333333333333
from one 	 0.028846153846153848
notion of 	 0.75
of phrases 	 0.00089126559714795
or English-like 	 0.0045045045045045045
linguistic controversy 	 0.0625
techniques in 	 0.043478260869565216
` naturally 	 0.0625
1971 -LRB- 	 0.3333333333333333
model '' 	 0.03333333333333333
and Intelligence 	 0.001445086705202312
they must 	 0.025
meanings , 	 0.25
for gestures 	 0.0036101083032490976
is written 	 0.0020325203252032522
of automatically 	 0.0017825311942959
Labov , 	 1.0
, Edward 	 0.0005614823133071309
`` B 	 0.005291005291005291
was CANDIDE 	 0.012987012987012988
of sounds 	 0.00089126559714795
Two vertices 	 0.14285714285714285
structures in 	 0.2
as social 	 0.003484320557491289
expression and 	 0.2
gives less 	 0.5
USA in 	 1.0
With the 	 0.2857142857142857
It proved 	 0.02631578947368421
a threshold 	 0.0036809815950920245
, bigrams 	 0.0011229646266142617
necessary therefore 	 0.1
approximated as 	 1.0
paper documents 	 0.09090909090909091
recognition : 	 0.01652892561983471
trillion-word corpus 	 1.0
lexical segment 	 0.07692307692307693
, search 	 0.0011229646266142617
since one 	 0.1
special types 	 0.2
classifies features 	 1.0
boundaries are 	 0.09090909090909091
-LRB- NLP 	 0.008130081300813009
text summarization 	 0.006289308176100629
advent of 	 1.0
processing would 	 0.018518518518518517
works It 	 0.5
more -RRB- 	 0.010526315789473684
the schematic 	 0.0006920415224913495
angry , 	 0.5
by its 	 0.005714285714285714
du discors 	 1.0
these other 	 0.023809523809523808
scope of 	 1.0
a QA 	 0.0049079754601227
into text 	 0.038461538461538464
incorrect assignment 	 0.3333333333333333
including : 	 0.14285714285714285
general concepts 	 0.045454545454545456
human would 	 0.021739130434782608
approaches Automatic 	 0.03571428571428571
parsers and 	 0.07692307692307693
were limited 	 0.024390243902439025
certain region 	 0.14285714285714285
been believed 	 0.014705882352941176
solid state 	 1.0
made feasible 	 0.0625
Programming methods 	 0.3333333333333333
phrase appears 	 0.1
classified into 	 1.0
in garden 	 0.0018726591760299626
application may 	 0.07142857142857142
each unigram 	 0.044444444444444446
on dictionary 	 0.0047169811320754715
filling may 	 1.0
part-of-speech tagging 	 0.4666666666666667
-LRB- parsed 	 0.0027100271002710027
worth remembering 	 0.5
from all 	 0.009615384615384616
of information 	 0.004456327985739751
SourceForge . 	 1.0
should indicate 	 0.05263157894736842
related fields 	 0.06666666666666667
301 computer 	 1.0
reached 20,000 	 0.5
the adviser 	 0.0006920415224913495
parser generates 	 0.0625
analyzed using 	 0.2
ways in 	 0.125
summarization algorithms 	 0.02
at characters 	 0.014705882352941176
usually evaluated 	 0.03125
line as 	 0.3333333333333333
low-resolution , 	 1.0
tag the 	 0.0625
sentiment -LRB- 	 0.04
boundary markers 	 0.16666666666666666
Master lead-in 	 1.0
synopsis like 	 1.0
closed world 	 1.0
expectations , 	 1.0
to them 	 0.0026560424966799467
Technology Integration 	 0.3333333333333333
-RRB- Speech 	 0.0027100271002710027
Solving System 	 0.5
an opinion 	 0.007575757575757576
has characterized 	 0.011904761904761904
bigrams , 	 1.0
Airline Ticket 	 1.0
of selecting 	 0.00089126559714795
A large 	 0.02
turn . 	 0.16666666666666666
and typical 	 0.001445086705202312
for these 	 0.0036101083032490976
financial message 	 0.25
one on 	 0.015384615384615385
particular event 	 0.07692307692307693
7 distinct 	 0.14285714285714285
all be 	 0.023255813953488372
and computational 	 0.001445086705202312
single document 	 0.07142857142857142
Besides the 	 1.0
Manual analysis 	 0.3333333333333333
screen of 	 1.0
various fine 	 0.05555555555555555
Granada Pallet 	 0.5
discourse analyst 	 0.027777777777777776
to define 	 0.0026560424966799467
POS-taggers , 	 1.0
were easy 	 0.024390243902439025
recogniton vary 	 0.5
for statistical 	 0.007220216606498195
in e-communities 	 0.0018726591760299626
input devices 	 0.04878048780487805
process may 	 0.027777777777777776
-RRB- Critical 	 0.0027100271002710027
Guidelines see 	 0.5
1929 Gustav 	 1.0
contains errors 	 0.2
harder 75 	 0.14285714285714285
has dried 	 0.011904761904761904
Oil Company 	 1.0
its domain 	 0.05714285714285714
deep are 	 0.14285714285714285
information retrieval 	 0.10869565217391304
name and 	 0.2
adjective or 	 0.42857142857142855
identifying trends 	 0.16666666666666666
head hurts 	 1.0
of unknown 	 0.00089126559714795
summary in 	 0.047619047619047616
'' versus 	 0.005154639175257732
human capabilities 	 0.021739130434782608
at hand 	 0.014705882352941176
to examples 	 0.0013280212483399733
text by 	 0.006289308176100629
reported -LRB- 	 0.2
above techniques 	 0.07692307692307693
certain sequences 	 0.14285714285714285
Stages The 	 1.0
and find 	 0.001445086705202312
named Interspeech 	 0.14285714285714285
more severe 	 0.010526315789473684
2 descriptions 	 0.2
-LRB- like 	 0.0027100271002710027
which simulates 	 0.007246376811594203
with specialised 	 0.00546448087431694
enabling technologies 	 1.0
mention how 	 0.3333333333333333
the accompanying 	 0.0006920415224913495
how people 	 0.034482758620689655
to many 	 0.005312084993359893
<s> Moreover 	 0.0030745580322828594
in natural 	 0.013108614232209739
the broken 	 0.0006920415224913495
described as 	 0.3333333333333333
The AT&T 	 0.005208333333333333
meanings depending 	 0.25
performance . 	 0.16666666666666666
quantities . 	 0.3333333333333333
time , 	 0.3333333333333333
and composing 	 0.001445086705202312
scientific fields 	 0.5
Note , 	 0.1111111111111111
recognizer , 	 1.0
for substantial 	 0.0036101083032490976
the larger 	 0.0006920415224913495
many chatterbots 	 0.019230769230769232
<s> Anaphora 	 0.0007686395080707148
semiotics , 	 1.0
electrical characteristics 	 1.0
cues help 	 1.0
a series 	 0.007361963190184049
we could 	 0.022222222222222223
, is 	 0.0072992700729927005
deep understanding 	 0.2857142857142857
So far 	 0.3333333333333333
above , 	 0.3076923076923077
available on 	 0.058823529411764705
using random 	 0.01694915254237288
several other 	 0.045454545454545456
rules engine 	 0.023255813953488372
can compensate 	 0.0055248618784530384
obtained by 	 0.5714285714285714
rapidly changing 	 0.5
insights . 	 1.0
call '' 	 0.3333333333333333
deploy machine 	 1.0
system selects 	 0.010752688172043012
she were 	 1.0
Typical questions 	 0.5
was hoping 	 0.012987012987012988
systems are 	 0.11607142857142858
discourse . 	 0.027777777777777776
doctors , 	 0.3333333333333333
unambiguous . 	 0.5
The reader 	 0.005208333333333333
new sentences 	 0.041666666666666664
similarity score 	 0.1
different contexts 	 0.02040816326530612
is searched 	 0.0020325203252032522
in non-Western 	 0.0018726591760299626
systems included 	 0.008928571428571428
than whole 	 0.022222222222222223
has wide 	 0.011904761904761904
task because 	 0.023809523809523808
capitalization at 	 0.3333333333333333
questions can 	 0.038461538461538464
NLG system 	 0.09523809523809523
success in 	 0.2
proper nouns 	 0.14285714285714285
Joseph Weizenbaum 	 1.0
is considerable 	 0.0020325203252032522
this product 	 0.01098901098901099
capitalization may 	 0.3333333333333333
<s> Please 	 0.0023059185242121443
from knowledge 	 0.009615384615384616
example text 	 0.012345679012345678
meets two 	 0.5
basis for 	 0.3333333333333333
, key 	 0.0005614823133071309
<s> Few 	 0.0007686395080707148
program is 	 0.045454545454545456
majority of 	 1.0
translating speech 	 0.25
recognition such 	 0.008264462809917356
part-of-speech tag 	 0.06666666666666667
Section , 	 1.0
Germany , 	 0.5
major algorithms 	 0.08333333333333333
and interaction 	 0.001445086705202312
delimited , 	 0.5
like to 	 0.07142857142857142
that access 	 0.0035460992907801418
summaries of 	 0.09302325581395349
, current 	 0.0005614823133071309
was based 	 0.012987012987012988
Auto plant 	 1.0
content alone 	 0.08333333333333333
tag '' 	 0.0625
decimal point 	 1.0
conditions Accuracy 	 0.2
since it 	 0.2
well their 	 0.03571428571428571
the problem 	 0.006228373702422145
effort . 	 0.25
to publish 	 0.0013280212483399733
likely related 	 0.0625
example is 	 0.012345679012345678
generic response 	 0.3333333333333333
prior ranking 	 0.3333333333333333
settle on 	 1.0
non-linear transformations 	 1.0
to Iraq 	 0.0013280212483399733
named entities 	 0.42857142857142855
different from 	 0.12244897959183673
'' about 	 0.005154639175257732
task entirely 	 0.023809523809523808
that have 	 0.02127659574468085
for reasons 	 0.0036101083032490976
typically the 	 0.05555555555555555
is what 	 0.0040650406504065045
Languages which 	 0.3333333333333333
operating system 	 0.5
adviser for 	 1.0
between Internet 	 0.02564102564102564
happens when 	 1.0
, 1978 	 0.0011229646266142617
writing SHRDLU 	 0.1111111111111111
theoretical perspectives 	 0.3333333333333333
cover . 	 1.0
choose from 	 0.5
would use 	 0.018867924528301886
relationships can 	 0.16666666666666666
Paul Hopper 	 0.2
disturbed by 	 1.0
meant and 	 0.5
-RRB- hours 	 0.0027100271002710027
<s> Examples 	 0.0023059185242121443
on its 	 0.009433962264150943
N in 	 0.3333333333333333
as keeping 	 0.003484320557491289
What is 	 0.2727272727272727
and combine 	 0.001445086705202312
report -RRB- 	 0.25
various types 	 0.1111111111111111
vertices are 	 0.1111111111111111
path , 	 0.5
<s> Main 	 0.0007686395080707148
, recorded 	 0.0005614823133071309
for Amharic 	 0.0036101083032490976
estimate sentence 	 0.25
both be 	 0.03225806451612903
recognizes the 	 1.0
answer a 	 0.03333333333333333
'' arguably 	 0.005154639175257732
OCR Software 	 0.02040816326530612
candidate passages 	 0.3333333333333333
EMR according 	 0.3333333333333333
that is 	 0.05673758865248227
which , 	 0.007246376811594203
of running 	 0.00089126559714795
of identifiers 	 0.00089126559714795
deliberately inserts 	 1.0
start experimenting 	 0.14285714285714285
trained automatically 	 0.3333333333333333
setting radio 	 0.2
and conversations 	 0.001445086705202312
while the 	 0.05
Speech Communication 	 0.03225806451612903
Rose , 	 1.0
computationally feasible 	 0.5
their solutions 	 0.029411764705882353
of tasks 	 0.00089126559714795
APEXC machine 	 1.0
e.g. , 	 0.4642857142857143
useful NLG 	 0.07142857142857142
<s> Manual 	 0.0015372790161414297
only be 	 0.02631578947368421
using database 	 0.01694915254237288
sentiment in 	 0.08
, negative 	 0.0005614823133071309
Nielsen automatically 	 1.0
efforts have 	 0.5714285714285714
writer with 	 1.0
systems indicate 	 0.008928571428571428
`` sailor 	 0.010582010582010581
the individual 	 0.001384083044982699
'' it 	 0.005154639175257732
Although , 	 0.125
has 4 	 0.011904761904761904
lexical resources 	 0.07692307692307693
Psycholinguists prefer 	 1.0
and support 	 0.001445086705202312
was quite 	 0.012987012987012988
has fairly 	 0.011904761904761904
also continue 	 0.014492753623188406
copy the 	 1.0
between IR 	 0.02564102564102564
Initial results 	 1.0
uses the 	 0.07142857142857142
it enumerated 	 0.008547008547008548
apply statistical 	 0.2
domain-specific knowledge 	 0.5
ATNs and 	 0.3333333333333333
Information -LRB- 	 0.2
inference algorithm 	 0.25
interfaces such 	 0.5
from Turney 	 0.009615384615384616
or fuse 	 0.0045045045045045045
grammar methods 	 0.02702702702702703
systems developed 	 0.017857142857142856
the dictator 	 0.0006920415224913495
, Facebook 	 0.0005614823133071309
translation or 	 0.013513513513513514
computer databases 	 0.022727272727272728
, taking 	 0.0005614823133071309
The LexRank 	 0.005208333333333333
algorithm like 	 0.03571428571428571
then can 	 0.02857142857142857
choice : 	 0.125
Corpus tag 	 0.125
to pre-process 	 0.0013280212483399733
<s> Word 	 0.003843197540353574
learning algorithms 	 0.11627906976744186
of context-free 	 0.00089126559714795
new scientific 	 0.041666666666666664
themselves sometimes 	 0.25
algorithms use 	 0.02857142857142857
in free 	 0.003745318352059925
commonly associated 	 0.125
meet larger 	 0.25
translation paradigm 	 0.013513513513513514
identified which 	 0.2
World War 	 0.14285714285714285
neighbors . 	 0.3333333333333333
closely approximates 	 0.2
normalization and 	 0.16666666666666666
in text 	 0.0018726591760299626
actual text 	 0.2
text rather 	 0.006289308176100629
They combine 	 0.3333333333333333
efforts are 	 0.14285714285714285
hoping to 	 1.0
The idea 	 0.010416666666666666
or shifted 	 0.0045045045045045045
extremes , 	 1.0
The final 	 0.005208333333333333
challenging because 	 1.0
In 1965 	 0.009523809523809525
the hidden 	 0.0006920415224913495
An ongoing 	 0.0625
NLP . 	 0.10638297872340426
because of 	 0.2
tokens -LRB- 	 0.14285714285714285
rich lexicon 	 0.6
rank `` 	 0.16666666666666666
tagging was 	 0.08
directly from 	 0.2
symbol . 	 0.5
'' occurs 	 0.005154639175257732
by many 	 0.005714285714285714
than of 	 0.022222222222222223
smaller . 	 0.14285714285714285
levels will 	 0.09090909090909091
Some text 	 0.047619047619047616
, engaging 	 0.0005614823133071309
distinguished . 	 1.0
to accommodate 	 0.0026560424966799467
verb , 	 0.38461538461538464
constraints Read 	 0.25
An ISO 	 0.0625
it easily 	 0.008547008547008548
dialogues between 	 1.0
linguistic formalism 	 0.0625
some work 	 0.012048192771084338
adjectives , 	 0.3333333333333333
measured can 	 0.16666666666666666
of summarization 	 0.0071301247771836
improve recognition 	 0.15384615384615385
lexicon reached 	 0.1111111111111111
especially if 	 0.06666666666666667
-LRB- `` 	 0.02168021680216802
LUNAR , 	 0.6666666666666666
made available 	 0.0625
casual speech 	 1.0
linguistic way 	 0.0625
The problem 	 0.015625
2,000 words 	 0.5
i.e. text 	 0.05263157894736842
i.e. determining 	 0.05263157894736842
makes it 	 0.25
other features 	 0.014285714285714285
best application 	 0.05555555555555555
disambiguation -LRB- 	 0.1
each other 	 0.13333333333333333
very slow 	 0.024390243902439025
be his 	 0.004219409282700422
-LRB- although 	 0.005420054200542005
as : 	 0.003484320557491289
and news 	 0.001445086705202312
Convert information 	 0.5
on hand-crafted 	 0.0047169811320754715
also require 	 0.014492753623188406
postal code 	 1.0
who co-founded 	 0.1
developing a 	 0.25
the special 	 0.0006920415224913495
picture quality 	 0.25
Court reporting 	 1.0
PangeaMT , 	 1.0
disambiguation : 	 0.1
systems will 	 0.008928571428571428
Typically features 	 1.0
data table 	 0.012987012987012988
follows that 	 0.5
is computed 	 0.0020325203252032522
document is 	 0.027777777777777776
used that 	 0.008849557522123894
and expensive 	 0.002890173410404624
EAGLi for 	 1.0
statistical methods 	 0.12121212121212122
us with 	 0.5
larger system 	 0.125
the opposite 	 0.001384083044982699
of sentence-level 	 0.00089126559714795
because two 	 0.03333333333333333
rarely have 	 0.3333333333333333
interpretable rules 	 1.0
Royal Aerospace 	 0.5
read 23 	 0.14285714285714285
look at 	 0.4
`` main 	 0.005291005291005291
reviews to 	 0.16666666666666666
very rudimentary 	 0.024390243902439025
their reputations 	 0.029411764705882353
the length 	 0.001384083044982699
US Army 	 0.14285714285714285
could usefully 	 0.0625
a purpose 	 0.001226993865030675
can function 	 0.0055248618784530384
was evident 	 0.012987012987012988
Journal -RRB- 	 0.3333333333333333
content overlap 	 0.16666666666666666
<s> Based 	 0.0007686395080707148
<s> These 	 0.012298232129131437
are 9 	 0.004149377593360996
British General 	 0.3333333333333333
a leftmost 	 0.00245398773006135
, QUALM 	 0.0005614823133071309
Words in 	 0.25
, largely 	 0.0005614823133071309
output , 	 0.038461538461538464
discourse in 	 0.05555555555555555
symbolic representation 	 1.0
The 1970s 	 0.005208333333333333
better scoring 	 0.1111111111111111
<s> Canada 	 0.0007686395080707148
hard to 	 0.3333333333333333
as short 	 0.003484320557491289
The problems 	 0.005208333333333333
However some 	 0.02702702702702703
Adam Jaworski 	 1.0
was only 	 0.012987012987012988
league over 	 1.0
user could 	 0.07142857142857142
giving these 	 0.5
language data 	 0.006756756756756757
or Spanish 	 0.0045045045045045045
one sentence 	 0.03076923076923077
representation system 	 0.05263157894736842
enumerated all 	 1.0
to texts 	 0.0013280212483399733
recursion in 	 1.0
product or 	 0.14285714285714285
in following 	 0.0018726591760299626
of international 	 0.00089126559714795
applied successfully 	 0.06666666666666667
, resource 	 0.0005614823133071309
The attitude 	 0.005208333333333333
angle . 	 1.0
al. 1989 	 1.0
language interaction 	 0.006756756756756757
Gene Ontology 	 1.0
group developed 	 0.25
Gaussians , 	 1.0
NYU , 	 1.0
, Brill 	 0.0005614823133071309
being the 	 0.05555555555555555
the Grace 	 0.0006920415224913495
Please improve 	 0.3333333333333333
using all 	 0.01694915254237288
or just 	 0.0045045045045045045
, predicting 	 0.0005614823133071309
published but 	 0.14285714285714285
rare . 	 0.5
rescore lattices 	 1.0
Statistical natural-language 	 0.1111111111111111
Corpus -RRB- 	 0.1875
functioning of 	 0.3333333333333333
to right 	 0.0013280212483399733
which associate 	 0.007246376811594203
, based 	 0.0011229646266142617
Ford Sync 	 1.0
Transform , 	 1.0
NLP comprises 	 0.02127659574468085
selects important 	 0.5
techniques is 	 0.043478260869565216
of hours 	 0.00089126559714795
Programming languages 	 0.6666666666666666
RSI became 	 1.0
is included 	 0.0020325203252032522
one human 	 0.015384615384615385
look '' 	 0.2
text -LRB- 	 0.03773584905660377
<s> Context-free 	 0.0007686395080707148
visible light 	 0.3333333333333333
Different types 	 1.0
a sample 	 0.001226993865030675
Chinese and 	 0.14285714285714285
ROUGE-1 -LRB- 	 0.2
Modern speech 	 0.3333333333333333
factors affect 	 0.3333333333333333
engine page 	 0.16666666666666666
the new 	 0.0006920415224913495
is not 	 0.03861788617886179
its use 	 0.02857142857142857
domain posed 	 0.05
phoneme classification 	 0.5
degree to 	 0.16666666666666666
active research 	 0.5
systems '' 	 0.008928571428571428
the segment 	 0.0006920415224913495
evaluated by 	 0.14285714285714285
a company 	 0.001226993865030675
Training air 	 0.5
of aircraft 	 0.00089126559714795
would require 	 0.05660377358490566
first occurrence 	 0.030303030303030304
questions under 	 0.038461538461538464
a machine 	 0.008588957055214725
complex task 	 0.041666666666666664
Because progress 	 0.5
sentences at 	 0.013157894736842105
is especially 	 0.0040650406504065045
at least 	 0.07352941176470588
given sentence 	 0.08333333333333333
, Speech 	 0.0016844469399213925
using either 	 0.01694915254237288
meaning part 	 0.043478260869565216
7110.65 details 	 1.0
it belongs 	 0.008547008547008548
within three 	 0.1111111111111111
One study 	 0.07692307692307693
lookup algorithms 	 1.0
opinion mining 	 0.2
can make 	 0.016574585635359115
organized notations 	 1.0
into machine-encoded 	 0.01282051282051282
speech for 	 0.02631578947368421
greatly with 	 0.14285714285714285
they disagree 	 0.025
the answers 	 0.0006920415224913495
common way 	 0.08
offs in 	 1.0
of ways 	 0.0017825311942959
can use 	 0.016574585635359115
John Heritage 	 0.125
is compounded 	 0.0020325203252032522
meant that 	 0.5
the types 	 0.001384083044982699
ease interoperability 	 1.0
journal article 	 0.3333333333333333
create tokens 	 0.058823529411764705
stage is 	 0.4
Tokens are 	 1.0
1960s and 	 0.3333333333333333
Depending on 	 1.0
a collection 	 0.00245398773006135
technique referred 	 0.14285714285714285
eliminate the 	 0.5
must first 	 0.07142857142857142
software has 	 0.037037037037037035
a computer 	 0.018404907975460124
Search collections 	 0.5
societal problem 	 1.0
' pyramid 	 0.05263157894736842
different output 	 0.02040816326530612
grammars often 	 0.07142857142857142
experimenting . 	 1.0
dictionary-based machine 	 1.0
The target 	 0.005208333333333333
lexicon of 	 0.2222222222222222
individual characters 	 0.08333333333333333
proceedings into 	 1.0
sets for 	 0.09090909090909091
start with 	 0.14285714285714285
are used 	 0.03319502074688797
Users were 	 1.0
orthography to 	 0.5
IR relies 	 0.3333333333333333
called `` 	 0.2777777777777778
particular , 	 0.23076923076923078
non-trivial techniques 	 0.5
its lexicon 	 0.02857142857142857
a native 	 0.00245398773006135
the capital 	 0.001384083044982699
there are 	 0.375
to adapt 	 0.0013280212483399733
R. , 	 0.3333333333333333
Querying application 	 1.0
in fundamentally 	 0.0018726591760299626
`` right 	 0.005291005291005291
are discussed 	 0.004149377593360996
also : 	 0.028985507246376812
a speaker 	 0.00245398773006135
-RRB- Interactional 	 0.0027100271002710027
languages tend 	 0.02
page . 	 0.14285714285714285
As a 	 0.1111111111111111
HTK book 	 0.5
context can 	 0.030303030303030304
includes transfer-based 	 0.14285714285714285
research had 	 0.047619047619047616
the potential 	 0.0020761245674740486
optimize some 	 1.0
Morpholympics compared 	 1.0
it , 	 0.017094017094017096
<s> In 	 0.07455803228285934
assigned keywords 	 0.5
larger text 	 0.0625
cosine similarity 	 0.3333333333333333
tagging Koine 	 0.04
much useful 	 0.045454545454545456
learning approaches 	 0.023255813953488372
Some unsupervised 	 0.047619047619047616
efficient manner 	 0.3333333333333333
Often used 	 0.3333333333333333
Keyphrases have 	 1.0
limited vocabulary 	 0.1
if you 	 0.07142857142857142
<s> This 	 0.03996925441967717
summary '' 	 0.047619047619047616
F-16 aircraft 	 0.5
time . 	 0.12121212121212122
Natural '' 	 0.08333333333333333
most authoritative 	 0.017241379310344827
in 1997 	 0.003745318352059925
training are 	 0.07142857142857142
<s> Working 	 0.0007686395080707148
sound waves 	 0.05
to overcome 	 0.0013280212483399733
contrast , 	 0.625
positives by 	 1.0
use ` 	 0.013888888888888888
over many 	 0.08333333333333333
and larger 	 0.002890173410404624
'' for 	 0.005154639175257732
whether `` 	 0.07692307692307693
The common 	 0.005208333333333333
Semantic analysis 	 0.3333333333333333
The extractor 	 0.005208333333333333
medium levels 	 0.3333333333333333
deployed in 	 0.5
other POS 	 0.014285714285714285
CSIS -RRB- 	 0.5
on part-of-speech 	 0.0047169811320754715
Kurzweil started 	 0.14285714285714285
was shown 	 0.025974025974025976
's Digest 	 0.058823529411764705
earliest example 	 0.5
`` advanced 	 0.005291005291005291
mark the 	 0.3333333333333333
of virtual 	 0.00089126559714795
to carefully 	 0.0013280212483399733
hurts ? 	 0.5
these programs 	 0.023809523809523808
pre-structured database 	 1.0
computer process 	 0.022727272727272728
stemming -RRB- 	 0.5
the Cuzco 	 0.0006920415224913495
represents a 	 0.75
to English 	 0.0013280212483399733
Aerospace Establishment 	 0.5
Activity -LRB- 	 1.0
a trend 	 0.001226993865030675
readily reveal 	 0.3333333333333333
Shepard went 	 0.3333333333333333
by analogy 	 0.005714285714285714
could co-occur 	 0.0625
project was 	 0.07692307692307693
new entrants 	 0.041666666666666664
their training 	 0.029411764705882353
such formal 	 0.008130081300813009
Health Record 	 0.5
lexical units 	 0.07692307692307693
examples are 	 0.041666666666666664
step the 	 0.06666666666666667
direct real-world 	 0.16666666666666666
Once examples 	 0.2
tests the 	 0.5
a Cognitive 	 0.001226993865030675
higher level 	 0.14285714285714285
, 7 	 0.0005614823133071309
speech commands 	 0.006578947368421052
be represented 	 0.008438818565400843
at an 	 0.014705882352941176
examples produces 	 0.041666666666666664
soon become 	 0.3333333333333333
SRI International 	 1.0
generally used 	 0.09090909090909091
the summaries 	 0.002768166089965398
be output 	 0.004219409282700422
a skilled 	 0.001226993865030675
Speaker dependence 	 0.16666666666666666
from multimedia 	 0.009615384615384616
names that 	 0.2857142857142857
Ingria R. 	 1.0
of 21 	 0.00089126559714795
corresponding increase 	 0.16666666666666666
sailor ! 	 0.2
displayed as 	 0.5
71 % 	 1.0
operators need 	 1.0
progress and 	 0.14285714285714285
related in 	 0.06666666666666667
research is 	 0.047619047619047616
quickly , 	 1.0
of cursive 	 0.00089126559714795
the unwanted 	 0.0006920415224913495
a rightmost 	 0.00245398773006135
although there 	 0.16666666666666666
computers for 	 0.1111111111111111
The set 	 0.005208333333333333
the summary 	 0.005536332179930796
typically produces 	 0.05555555555555555
or syntactic 	 0.0045045045045045045
specific error 	 0.047619047619047616
learn parameters 	 0.07692307692307693
between sentences 	 0.05128205128205128
summaries but 	 0.023255813953488372
another verb 	 0.07692307692307693
entropy has 	 0.2
understanding involves 	 0.030303030303030304
universal '' 	 0.3333333333333333
model that 	 0.13333333333333333
typically include 	 0.05555555555555555
them in 	 0.05263157894736842
all written 	 0.06976744186046512
of restaurant 	 0.00089126559714795
verb : 	 0.07692307692307693
should soon 	 0.05263157894736842
the polarity 	 0.0006920415224913495
John Swales 	 0.25
-LRB- 3 	 0.005420054200542005
list -LRB- 	 0.09090909090909091
if the 	 0.35714285714285715
a heuristic 	 0.00245398773006135
a background 	 0.001226993865030675
-RRB- This 	 0.0027100271002710027
in massive 	 0.0018726591760299626
costs -LRB- 	 1.0
which had 	 0.007246376811594203
is why 	 0.0020325203252032522
you would 	 0.07692307692307693
Hulth showed 	 0.3333333333333333
transcriptions -LRB- 	 0.5
individual phones 	 0.08333333333333333
following example 	 0.13333333333333333
<s> Commanders 	 0.0007686395080707148
, adjectives 	 0.0005614823133071309
of small 	 0.00089126559714795
into methods 	 0.01282051282051282
against feature 	 0.2
are in-principle 	 0.004149377593360996
'' each 	 0.005154639175257732
leaving the 	 1.0
of idioms 	 0.00089126559714795
applications including 	 0.04
Recall can 	 0.3333333333333333
patented and 	 1.0
In order 	 0.01904761904761905
1978 -RRB- 	 0.6666666666666666
ones focus 	 0.1
, going 	 0.0005614823133071309
name is 	 0.2
of commercial 	 0.00089126559714795
text documents 	 0.006289308176100629
Convert chunks 	 0.5
substantial financial 	 0.2
accompanying HTK 	 1.0
France has 	 0.25
-RRB- dogs 	 0.0027100271002710027
products in 	 0.25
Treebank -RRB- 	 0.16666666666666666
for many 	 0.007220216606498195
speech-enabled Symbian 	 1.0
syntactic parser 	 0.07692307692307693
so it 	 0.06666666666666667
command centres 	 0.5
reading machine 	 0.125
<s> Realisation 	 0.0007686395080707148
<s> NLP 	 0.0007686395080707148
spirit to 	 1.0
masculine , 	 1.0
SHRDLU and 	 0.16666666666666666
ca n't 	 1.0
parsing systems 	 0.03571428571428571
recorded all 	 0.5
, Hindle 	 0.0005614823133071309
report -LRB- 	 0.25
medium or 	 0.3333333333333333
provided by 	 0.4
text more 	 0.006289308176100629
methods have 	 0.045454545454545456
of NLP 	 0.004456327985739751
Turkish , 	 1.0
, searching 	 0.0005614823133071309
management environments 	 0.14285714285714285
step neural 	 0.06666666666666667
of over 	 0.00089126559714795
learned . 	 0.2
split , 	 0.25
Administration , 	 1.0
the technique 	 0.0006920415224913495
, thanks 	 0.0005614823133071309
'' and 	 0.06701030927835051
is preferable 	 0.0020325203252032522
arm to 	 1.0
and searching 	 0.001445086705202312
domains . 	 0.25
from that 	 0.009615384615384616
a basis 	 0.00245398773006135
recognition products 	 0.008264462809917356
technology development 	 0.045454545454545456
vice versa 	 1.0
of British 	 0.00089126559714795
constructed , 	 0.5
delimited -LRB- 	 0.25
of allowable 	 0.00089126559714795
the functioning 	 0.0006920415224913495
features like 	 0.038461538461538464
, 3 	 0.0005614823133071309
it in 	 0.008547008547008548
features in 	 0.038461538461538464
An explicit 	 0.0625
, who 	 0.0005614823133071309
image , 	 0.3333333333333333
` global 	 0.0625
In 1949 	 0.009523809523809525
a combination 	 0.00245398773006135
researchers wrote 	 0.1
data at 	 0.012987012987012988
closely tied 	 0.2
became the 	 0.2
precursor to 	 1.0
by Frederick 	 0.005714285714285714
two ways 	 0.034482758620689655
parsing input 	 0.03571428571428571
identify keyphrases 	 0.08333333333333333
by Manfred 	 0.005714285714285714
can be 	 0.5027624309392266
representative of 	 1.0
, dimensions 	 0.0005614823133071309
generated texts 	 0.06666666666666667
connected Web 	 0.2
Alenia Aermacchi 	 1.0
or custom 	 0.0045045045045045045
human review 	 0.021739130434782608
To mine 	 0.1111111111111111
a core 	 0.001226993865030675
text are 	 0.006289308176100629
very useful 	 0.04878048780487805
a noun 	 0.007361963190184049
and experience 	 0.001445086705202312
Profile templates 	 1.0
<s> Bottom-up 	 0.0007686395080707148
we are 	 0.044444444444444446
select individual 	 0.16666666666666666
In France 	 0.01904761904761905
example -LRB- 	 0.012345679012345678
deployed was 	 0.5
were undertaken 	 0.024390243902439025
analysts not 	 0.5
describe a 	 0.16666666666666666
conveyed via 	 1.0
'' where 	 0.005154639175257732
The systems 	 0.005208333333333333
spoken words 	 0.14285714285714285
<s> Comparing 	 0.0007686395080707148
later the 	 0.1
checked each 	 0.5
linguistics -LRB- 	 0.05
: Word 	 0.00980392156862745
other are 	 0.014285714285714285
independence Isolated 	 1.0
Noun , 	 1.0
approximates that 	 0.5
separate lexical 	 0.1
is publicly 	 0.0020325203252032522
a suitable 	 0.0036809815950920245
the delta 	 0.0006920415224913495
restricted world 	 0.25
only study 	 0.02631578947368421
vs. objective 	 0.08333333333333333
normalization to 	 0.16666666666666666
T , 	 0.16666666666666666
OCR Accuracy 	 0.02040816326530612
The method 	 0.005208333333333333
in research 	 0.0018726591760299626
vocabulary . 	 0.25
, except 	 0.0005614823133071309
and ushered 	 0.001445086705202312
applications in 	 0.08
hours of 	 0.5
to more 	 0.0026560424966799467
Answering QA 	 1.0
, heavy-noise 	 0.0005614823133071309
<s> Statistical 	 0.0023059185242121443
scores as 	 0.2
meet a 	 0.25
way . 	 0.041666666666666664
speech attached 	 0.006578947368421052
of 19th 	 0.00089126559714795
translator that 	 0.14285714285714285
Harrison P. 	 1.0
of global 	 0.00089126559714795
segment , 	 0.2222222222222222
languages contain 	 0.02
each with 	 0.022222222222222223
paper , 	 0.09090909090909091
densely connected 	 1.0
M. 1999 	 0.25
<s> Today 	 0.0007686395080707148
`` Computing 	 0.005291005291005291
, voice-activation 	 0.0005614823133071309
Jonathan Potter 	 1.0
a result 	 0.0036809815950920245
parse a 	 0.1111111111111111
one summary 	 0.015384615384615385
generating too 	 0.2
previous training 	 0.3333333333333333
II in 	 0.5
-LRB- 1966 	 0.0027100271002710027
state -RRB- 	 0.07142857142857142
-- on 	 0.04
important by 	 0.0625
efficiently . 	 1.0
generic summaries 	 0.3333333333333333
higher levels 	 0.2857142857142857
is positive 	 0.0020325203252032522
`` barmaid 	 0.010582010582010581
the field 	 0.011764705882352941
Products , 	 0.5
and classification 	 0.001445086705202312
disambiguation , 	 0.1
of smoothing 	 0.00089126559714795
Intra-texual methods 	 1.0
man-hours worked 	 1.0
font at 	 0.3333333333333333
of extractive 	 0.00089126559714795
An extrinsic 	 0.0625
no significant 	 0.07692307692307693
human might 	 0.021739130434782608
some context 	 0.012048192771084338
though much 	 0.1
, whose 	 0.0011229646266142617
a facemask 	 0.001226993865030675
-LRB- NER 	 0.0027100271002710027
or other 	 0.009009009009009009
NLP tasks 	 0.0425531914893617
systems dynamically 	 0.008928571428571428
overlap metrics 	 0.25
structure The 	 0.08333333333333333
and reasoning 	 0.002890173410404624
technology devised 	 0.045454545454545456
experience is 	 0.5
generally amenable 	 0.09090909090909091
translating Quechua 	 0.25
analogy and 	 1.0
LREC Granada 	 1.0
type of 	 0.5714285714285714
discussion of 	 0.5
better data 	 0.1111111111111111
transform of 	 0.2
recognition conferences 	 0.008264462809917356
Before getting 	 0.5
% in 	 0.02564102564102564
verb . 	 0.15384615384615385
but IE 	 0.014705882352941176
on developing 	 0.0047169811320754715
no. . 	 1.0
model information 	 0.03333333333333333
hand-written by 	 0.14285714285714285
Elinor Ochs 	 1.0
commands are 	 0.2
individual unigrams 	 0.08333333333333333
and typically 	 0.002890173410404624
; later 	 0.02127659574468085
of most 	 0.00089126559714795
Why unsupervised 	 0.14285714285714285
vertices\/unigrams are 	 1.0
<s> Statistics 	 0.0023059185242121443
all governmental 	 0.023255813953488372
The acoustic 	 0.005208333333333333
examples to 	 0.041666666666666664
mainly from 	 0.16666666666666666
proposed photographing 	 0.1111111111111111
users to 	 0.2222222222222222
painstakingly `` 	 1.0
3 or 	 0.2
unigram `` 	 0.2
treat them 	 0.5
, Ann 	 0.0005614823133071309
selecting duplicate 	 0.2
summarization program 	 0.02
to return 	 0.0026560424966799467
produce numeric 	 0.045454545454545456
their linguistic 	 0.029411764705882353
available , 	 0.23529411764705882
are complicated 	 0.004149377593360996
a summary 	 0.0098159509202454
Peru , 	 0.5
piecewise stationary 	 1.0
major corpus 	 0.08333333333333333
-LRB- of 	 0.005420054200542005
frequently formalized 	 0.5
of neural 	 0.00089126559714795
we think 	 0.022222222222222223
very deep 	 0.024390243902439025
i.e. it 	 0.05263157894736842
German city 	 0.25
systems depended 	 0.008928571428571428
algorithm -LRB- 	 0.03571428571428571
extracted summaries 	 1.0
the values 	 0.0006920415224913495
the can 	 0.0006920415224913495
noise and 	 0.125
person to 	 0.10526315789473684
unverified or 	 1.0
Kolodner . 	 1.0
a compiler 	 0.0036809815950920245
large-vocabulary system 	 0.3333333333333333
technique which 	 0.14285714285714285
with disabilities 	 0.01092896174863388
those concerning 	 0.045454545454545456
linear combination 	 0.14285714285714285
a non-whitespace 	 0.001226993865030675
can represent 	 0.011049723756906077
a credit 	 0.00245398773006135
in further 	 0.0018726591760299626
recognition accuracy 	 0.05785123966942149
& OnlineOCR 	 0.25
single PC 	 0.07142857142857142
is insufficient 	 0.0020325203252032522
shorter and 	 0.5
computing : 	 0.5
indicate that 	 0.3333333333333333
corpus in 	 0.06451612903225806
as vertices 	 0.003484320557491289
were question 	 0.024390243902439025
in operational 	 0.0018726591760299626
2010 -RRB- 	 0.3333333333333333
many languages 	 0.019230769230769232
while verbs 	 0.05
step -LRB- 	 0.06666666666666667
metrics in 	 0.1111111111111111
assistant providing 	 1.0
aims at 	 0.3333333333333333
simple world 	 0.038461538461538464
so meaningless 	 0.03333333333333333
→ barmaid 	 0.3333333333333333
Postal Service 	 1.0
or less 	 0.018018018018018018
different parts 	 0.04081632653061224
Michael Dyer 	 0.25
removing stopwords 	 0.5
Produce a 	 1.0
described above 	 0.5
subsequent application 	 0.5
hand . 	 0.07142857142857142
any human 	 0.03225806451612903
-RRB- vibration 	 0.0027100271002710027
were spoken 	 0.024390243902439025
each dictionary 	 0.022222222222222223
network approaches 	 0.3333333333333333
management task 	 0.14285714285714285
of yesterday 	 0.00267379679144385
up In 	 0.045454545454545456
styles itself 	 1.0
`` central 	 0.010582010582010581
M. , 	 0.5
Telephony and 	 1.0
of SHRDLU 	 0.00089126559714795
WebOCR & 	 0.75
new text 	 0.08333333333333333
autopilot system 	 1.0
intrinsic evaluation 	 0.5
individual morphemes 	 0.08333333333333333
or analysis 	 0.0045045045045045045
phrase How 	 0.1
it began 	 0.008547008547008548
UK dealing 	 0.25
a basic 	 0.00245398773006135
been attained 	 0.014705882352941176
as Google 	 0.003484320557491289
and 2 	 0.001445086705202312
introduced the 	 1.0
adjacent sounds 	 0.16666666666666666
are clearly 	 0.004149377593360996
the natural 	 0.001384083044982699
by standard 	 0.005714285714285714
formed ? 	 0.4
an EMR 	 0.007575757575757576
means that 	 0.6666666666666666
UC and 	 0.5
matching , 	 0.2
Cognitive Process 	 0.3333333333333333
one of 	 0.2153846153846154
's necessary 	 0.0196078431372549
Liu 's 	 1.0
a roadmap 	 0.001226993865030675
with some 	 0.02185792349726776
kind of 	 0.7272727272727273
algorithms are 	 0.05714285714285714
be labeled 	 0.004219409282700422
Specifically , 	 1.0
and depth 	 0.001445086705202312
forecasts . 	 0.2
recognition deteriorated 	 0.008264462809917356
-LRB- probabilistic 	 0.0027100271002710027
and Reinvestment 	 0.001445086705202312
By combining 	 0.3333333333333333
, pitch 	 0.0005614823133071309
in driving 	 0.0018726591760299626
also prefer 	 0.014492753623188406
area is 	 0.18181818181818182
-LRB- MCE 	 0.0027100271002710027
co-occurrence graph 	 0.6666666666666666
genres of 	 1.0
not in 	 0.008928571428571428
being processed 	 0.05555555555555555
the annotation 	 0.0006920415224913495
paragraph . 	 0.3333333333333333
to , 	 0.0026560424966799467
research articles 	 0.023809523809523808
and orthography 	 0.001445086705202312
world 's 	 0.06666666666666667
parsing for 	 0.03571428571428571
but in 	 0.029411764705882353
south east 	 1.0
engaging in 	 1.0
E-set : 	 1.0
basic task 	 0.07692307692307693
summaries formed 	 0.023255813953488372
of software 	 0.00089126559714795
University 's 	 0.1111111111111111
difficult tasks 	 0.07142857142857142
, different 	 0.0016844469399213925
predicting star 	 0.5
retrieval or 	 0.14285714285714285
Ticket stock 	 1.0
similar contexts 	 0.037037037037037035
= common 	 0.1111111111111111
Pronunciation evaluation 	 1.0
generation because 	 0.1111111111111111
one , 	 0.06153846153846154
-RRB- : 	 0.024390243902439025
<s> Little 	 0.0007686395080707148
will generate 	 0.08571428571428572
specifically developed 	 0.5
the linguistic 	 0.0006920415224913495
; An 	 0.02127659574468085
that causes 	 0.0035460992907801418
Document structuring 	 0.25
first pass 	 0.030303030303030304
, similarity 	 0.0005614823133071309
although these 	 0.16666666666666666
<s> Using 	 0.0015372790161414297
extent to 	 0.25
be thresholded 	 0.004219409282700422
<s> However 	 0.02843966179861645
analysis on 	 0.015384615384615385
of Roger 	 0.00089126559714795
, Invoice 	 0.0005614823133071309
will tend 	 0.02857142857142857
tagging will 	 0.04
many higher 	 0.019230769230769232
all view 	 0.023255813953488372
and LUNAR 	 0.001445086705202312
metrics used 	 0.1111111111111111
to know 	 0.0013280212483399733
that performance 	 0.010638297872340425
produced by 	 0.3333333333333333
use splicing 	 0.013888888888888888
Structure , 	 1.0
they rephrase 	 0.025
includes -LRB- 	 0.14285714285714285
this constraint 	 0.01098901098901099
According to 	 1.0
each such 	 0.022222222222222223
into the 	 0.10256410256410256
details the 	 0.5
line in 	 0.3333333333333333
the identities 	 0.0006920415224913495
of Arabic 	 0.00089126559714795
major OCR 	 0.16666666666666666
end of 	 0.25
or generated 	 0.0045045045045045045
For telephone 	 0.01639344262295082
stages of 	 0.5
answers -RRB- 	 0.08333333333333333
versions for 	 0.3333333333333333
structure with 	 0.08333333333333333
final stage 	 0.1111111111111111
used speech 	 0.008849557522123894
inclusion in 	 1.0
die or 	 1.0
Arbor , 	 1.0
Harris had 	 0.1111111111111111
them automatically 	 0.05263157894736842
new data 	 0.041666666666666664
interpretation . 	 0.5
the Tablet 	 0.0006920415224913495
or highly 	 0.0045045045045045045
measures of 	 0.16666666666666666
not accommodate 	 0.017857142857142856
which discourse 	 0.007246376811594203
Standardization in 	 1.0
morphemes . 	 0.3333333333333333
continuous similarity 	 0.16666666666666666
indicates that 	 1.0
upload paper 	 1.0
2000 . 	 0.3333333333333333
In Italy 	 0.009523809523809525
use of 	 0.2916666666666667
RCA 301 	 0.2
= no. 	 0.1111111111111111
Bois , 	 1.0
<s> → 	 0.0007686395080707148
since the 	 0.1
adjacent words 	 0.3333333333333333
a dog 	 0.001226993865030675
requiring knowledge 	 0.5
: for 	 0.00980392156862745
proposed . 	 0.1111111111111111
for can 	 0.0036101083032490976
objective sentences 	 0.2
many other 	 0.09615384615384616
have included 	 0.009615384615384616
person-years of 	 1.0
often represented 	 0.022727272727272728
answering a 	 0.08333333333333333
compounded by 	 1.0
in reconfiguring 	 0.0018726591760299626
into readable 	 0.01282051282051282
, previously 	 0.0005614823133071309
almost no 	 1.0
effects of 	 1.0
also need 	 0.014492753623188406
`` political 	 0.005291005291005291
much more 	 0.18181818181818182
from Latin 	 0.009615384615384616
natural-language processing 	 1.0
food and 	 1.0
speaker independent 	 0.05555555555555555
to words 	 0.0013280212483399733
recognition computer 	 0.01652892561983471
first major 	 0.06060606060606061
be broken 	 0.004219409282700422
distinct parts 	 0.14285714285714285
uses search 	 0.07142857142857142
recognition systems 	 0.08264462809917356
projects never 	 0.5
common -LRB- 	 0.04
of statistical 	 0.0017825311942959
tract length 	 1.0
important distinction 	 0.125
signing off 	 1.0
can produce 	 0.0055248618784530384
of input 	 0.00267379679144385
High-order n-gram 	 1.0
device converted 	 0.5
easily retrieved 	 0.1111111111111111
been based 	 0.014705882352941176
advanced pattern 	 0.2
target value 	 0.09090909090909091
that perform 	 0.0035460992907801418
user profile 	 0.07142857142857142
for modeling 	 0.0036101083032490976
-LRB- 1993 	 0.0027100271002710027
that most 	 0.0035460992907801418
<s> Voice 	 0.0007686395080707148
1,915,993 -RRB- 	 1.0
is checking 	 0.0020325203252032522
US baseball 	 0.14285714285714285
artificial intelligence 	 0.6363636363636364
prize . 	 1.0
or content 	 0.0045045045045045045
to date 	 0.0026560424966799467
started to 	 0.25
without much 	 0.07692307692307693
things , 	 0.3333333333333333
Langues vol-2 	 1.0
where successively 	 0.02857142857142857
improved computer 	 0.25
linguists decided 	 0.3333333333333333
these simple 	 0.023809523809523808
an NLP 	 0.022727272727272728
, William 	 0.0011229646266142617
using a 	 0.1694915254237288
Dale -LRB- 	 1.0
speech that 	 0.006578947368421052
join different 	 1.0
be necessary 	 0.008438818565400843
the naval 	 0.0006920415224913495
processing '' 	 0.037037037037037035
correct values 	 0.06666666666666667
data can 	 0.025974025974025976
processing techniques 	 0.037037037037037035
inter-texual ones 	 0.5
use lexical 	 0.027777777777777776
so-called ROUGE 	 0.3333333333333333
inserts those 	 1.0
helps him 	 0.5
archiving and 	 1.0
sentence position 	 0.041666666666666664
idea of 	 0.2857142857142857
limited applications 	 0.1
to machine 	 0.00398406374501992
been characterized 	 0.014705882352941176
overcome these 	 0.5
towards this 	 1.0
term frequencies 	 0.05555555555555555
Screenshot OCR 	 1.0
it listens 	 0.008547008547008548
implementation of 	 1.0
University researchers 	 0.1111111111111111
for recognizing 	 0.0036101083032490976
logical inference 	 0.16666666666666666
there was 	 0.075
source . 	 0.041666666666666664
source documents 	 0.125
the partial 	 0.0006920415224913495
software tools 	 0.037037037037037035
, part-of-speech 	 0.0011229646266142617
and without 	 0.002890173410404624
`` recommendation 	 0.010582010582010581
see LMF 	 0.05
the CKY 	 0.0006920415224913495
the occurrence 	 0.0006920415224913495
of any 	 0.00267379679144385
the so-called 	 0.0006920415224913495
that transcended 	 0.0035460992907801418
phrases may 	 0.0625
See also 	 0.5
taggers are 	 0.14285714285714285
needs the 	 0.1
Handbook chapter 	 1.0
explanation , 	 1.0
is functioning 	 0.0020325203252032522
<s> Languages 	 0.0023059185242121443
The sentences 	 0.005208333333333333
more deeply 	 0.010526315789473684
output quality 	 0.038461538461538464
discussed in 	 0.14285714285714285
SpeechTEK and 	 0.5
a translation 	 0.00245398773006135
the computer-aided 	 0.0006920415224913495
linguist working 	 0.5
would . 	 0.018867924528301886
unveiled during 	 1.0
automatic summary 	 0.08695652173913043
system needs 	 0.043010752688172046
evaluation programs 	 0.018518518518518517
normalization -LRB- 	 0.16666666666666666
without the 	 0.07692307692307693
, German 	 0.0011229646266142617
, interlingual 	 0.0011229646266142617
task often 	 0.023809523809523808
Extractor -RRB- 	 1.0
might include 	 0.038461538461538464
dynamically create 	 0.5
made explicit 	 0.0625
against any 	 0.2
evaluation in 	 0.05555555555555555
system using 	 0.010752688172043012
are unable 	 0.004149377593360996
's subscription 	 0.0196078431372549
\* -LRB- 	 0.25
emotional states 	 0.25
experiment -LRB- 	 0.2
attempt to 	 1.0
text that 	 0.025157232704402517
person or 	 0.10526315789473684
a newspaper 	 0.001226993865030675
outputting one 	 0.5
to +5 	 0.0013280212483399733
verify the 	 1.0
make ` 	 0.05
several choices 	 0.045454545454545456
, Sept. 	 0.0005614823133071309
the semantics 	 0.001384083044982699
such application 	 0.008130081300813009
sound into 	 0.05
novel proposal 	 1.0
segmentation may 	 0.030303030303030304
been automatic 	 0.014705882352941176
James Paul 	 0.25
grammar Text 	 0.02702702702702703
most often 	 0.017241379310344827
might skip 	 0.038461538461538464
are outside 	 0.004149377593360996
or transfer-based 	 0.0045045045045045045
going to 	 0.25
rather to 	 0.0625
can form 	 0.0055248618784530384
the separate 	 0.0006920415224913495
well known 	 0.03571428571428571
consonants and 	 0.3333333333333333
pilots in 	 0.5
now commonplace 	 0.07692307692307693
work of 	 0.08333333333333333
translation -RRB- 	 0.02702702702702703
forth . 	 1.0
often characterised 	 0.022727272727272728
good summary 	 0.15384615384615385
routed along 	 0.5
a feature 	 0.00245398773006135
journal `` 	 0.3333333333333333
LexRank applies 	 0.08333333333333333
-RRB- Parsing 	 0.0027100271002710027
use various 	 0.013888888888888888
case is 	 0.058823529411764705
NLP is 	 0.02127659574468085
Liberman M. 	 1.0
research direction 	 0.023809523809523808
most popular 	 0.05172413793103448
<s> Most 	 0.0015372790161414297
like PDF 	 0.03571428571428571
FST , 	 1.0
moderate to 	 0.6
a podcast 	 0.001226993865030675
interaction . 	 0.25
and artificial 	 0.001445086705202312
centres require 	 1.0
pulled directly 	 1.0
an epidemic 	 0.007575757575757576
NLP problems 	 0.0425531914893617
exploited ; 	 1.0
performs simple 	 1.0
algorithm to 	 0.07142857142857142
sentences can 	 0.039473684210526314
tasks typically 	 0.03125
be obtained 	 0.004219409282700422
other levels 	 0.014285714285714285
formalized in 	 1.0
to ease 	 0.0013280212483399733
are grounded 	 0.008298755186721992
say that 	 0.14285714285714285
to involved 	 0.0013280212483399733
abbreviations , 	 0.4
sentences have 	 0.02631578947368421
over hand-produced 	 0.08333333333333333
use this 	 0.027777777777777776
, sentence 	 0.0005614823133071309
explicit models 	 0.2
be similar 	 0.004219409282700422
stationary distribution 	 0.2857142857142857
experiment in 	 0.2
single character 	 0.07142857142857142
disabilities who 	 0.25
narrowest and 	 1.0
are harder 	 0.004149377593360996
tends to 	 1.0
fine-grained analysis 	 1.0
`` prestige 	 0.005291005291005291
or the 	 0.04054054054054054
direction . 	 0.3333333333333333
'' non-linearly 	 0.005154639175257732
<s> We 	 0.005380476556495004
context , 	 0.12121212121212122
and Subjectivity 	 0.001445086705202312
segmentation , 	 0.09090909090909091
Essentially , 	 1.0
many forms 	 0.019230769230769232
<s> Finally 	 0.0007686395080707148
, and 	 0.10612015721504772
clear . 	 0.25
late 1960s 	 0.1111111111111111
question posed 	 0.047619047619047616
computer-generated weather 	 1.0
summarization task 	 0.02
or program 	 0.0045045045045045045
even languages 	 0.037037037037037035
this prior 	 0.01098901098901099
concatenating the 	 1.0
otherwise , 	 0.5
goal . 	 0.14285714285714285
n-dimensional real-valued 	 1.0
summarization Machine 	 0.02
of rule-based 	 0.00089126559714795
Scotland with 	 0.2
assembling output 	 1.0
-LRB- role 	 0.0027100271002710027
Harris 1991 	 0.1111111111111111
Extraction techniques 	 0.3333333333333333
methodology was 	 0.5
-RRB- it 	 0.0027100271002710027
are domain-independent 	 0.004149377593360996
finding a 	 0.4
class they 	 0.25
vendors . 	 0.25
For sentiment 	 0.01639344262295082
language documents 	 0.006756756756756757
can also 	 0.04419889502762431
an original 	 0.007575757575757576
and manipulate 	 0.001445086705202312
only at 	 0.02631578947368421
Up to 	 1.0
Ncmsan , 	 1.0
meaning `` 	 0.043478260869565216
process termed 	 0.027777777777777776
token is 	 0.5
used varies 	 0.008849557522123894
a knowledge 	 0.001226993865030675
Journal corpus 	 0.3333333333333333
news , 	 0.07692307692307693
often it 	 0.022727272727272728
the behavior 	 0.0006920415224913495
-RRB- \/ 	 0.0027100271002710027
is both 	 0.0020325203252032522
of dividing 	 0.00267379679144385
up '' 	 0.045454545454545456
reformulate the 	 1.0
to read 	 0.0013280212483399733
ranking algorithm 	 0.2857142857142857
It follows 	 0.02631578947368421
Caldas-Coulthard , 	 1.0
to paper-intensive 	 0.0013280212483399733
further clues 	 0.125
algorithm ? 	 0.03571428571428571
basic elements 	 0.07692307692307693
implemented in 	 0.2
With discontinuous 	 0.14285714285714285
synthesizer . 	 1.0
each known 	 0.022222222222222223
as paraphrase 	 0.003484320557491289
by combining 	 0.005714285714285714
considerably from 	 1.0
the impossibility 	 0.0006920415224913495
the previous 	 0.001384083044982699
made in 	 0.125
marker vs. 	 1.0
uses . 	 0.07142857142857142
to identify 	 0.006640106241699867
the essence 	 0.001384083044982699
instructed to 	 1.0
other terms 	 0.014285714285714285
others seem 	 0.08333333333333333
agree on 	 0.3333333333333333
word and 	 0.016666666666666666
although usually 	 0.16666666666666666
Starting in 	 1.0
enough for 	 0.2
measure for 	 0.09090909090909091
summaries -LRB- 	 0.046511627906976744
basic and 	 0.07692307692307693
characters which 	 0.125
often the 	 0.045454545454545456
RDF . 	 1.0
massive collections 	 1.0
stock market 	 0.3333333333333333
computer program 	 0.11363636363636363
it aimed 	 0.008547008547008548
most linguists 	 0.017241379310344827
aimed at 	 1.0
ratings are 	 0.2222222222222222
been changed 	 0.014705882352941176
the Artificial 	 0.0006920415224913495
database tables 	 0.1
have been 	 0.25
negative or 	 0.125
identifying relevant 	 0.16666666666666666
; we 	 0.02127659574468085
in addition 	 0.0056179775280898875
developed for 	 0.038461538461538464
answer 90 	 0.03333333333333333
Vice President 	 1.0
a vertex 	 0.00245398773006135
The FAA 	 0.005208333333333333
might ask 	 0.038461538461538464
the sample 	 0.0006920415224913495
most importantly 	 0.017241379310344827
by Homayoon 	 0.005714285714285714
parse individual 	 0.1111111111111111
The unsupervised 	 0.005208333333333333
make them 	 0.05
where clear 	 0.02857142857142857
2PR \/ 	 1.0
build an 	 0.6666666666666666
similar measure 	 0.037037037037037035
Fighter Technology 	 1.0
use so-called 	 0.013888888888888888
approach has 	 0.02857142857142857
the initial 	 0.0006920415224913495
sizes generally 	 0.3333333333333333
predicting ratings 	 0.5
speech which 	 0.006578947368421052
syntactic parsing 	 0.07692307692307693
patent on 	 0.75
different classes 	 0.02040816326530612
typical real-world 	 0.1111111111111111
which recursively 	 0.007246376811594203
measure summary 	 0.09090909090909091
within an 	 0.05555555555555555
: Dictionary-based 	 0.00980392156862745
usability . 	 1.0
Turney paper 	 0.2222222222222222
than a 	 0.1111111111111111
problems with 	 0.058823529411764705
this sentence 	 0.01098901098901099
application , 	 0.14285714285714285
Furthermore , 	 1.0
taken to 	 0.3333333333333333
learning . 	 0.09302325581395349
letter , 	 0.16666666666666666
interactive clarification 	 0.25
More advanced 	 0.1111111111111111
kit ' 	 1.0
equipment based 	 0.3333333333333333
toward actions 	 1.0
statistics . 	 0.375
as people 	 0.003484320557491289
SATZ architecture 	 1.0
semantic information 	 0.09523809523809523
, Nuance 	 0.0005614823133071309
, Digital 	 0.0005614823133071309
the total 	 0.0006920415224913495
discors pour 	 1.0
<s> As 	 0.010760953112990008
Words : 	 0.25
subject to 	 0.125
, Wayne 	 0.0005614823133071309
vary from 	 0.16666666666666666
accurate recognition 	 0.14285714285714285
Algorithms Both 	 0.5
titled Natural 	 1.0
that deals 	 0.0035460992907801418
The improvement 	 0.005208333333333333
standard corpora 	 0.07142857142857142
information from 	 0.06521739130434782
a quantitative 	 0.00245398773006135
of all 	 0.0035650623885918
, Carla 	 0.0005614823133071309
degree of 	 0.5
MLLR -RRB- 	 1.0
prior work 	 0.3333333333333333
formulation The 	 1.0
classify properly 	 0.5
as WordNet 	 0.003484320557491289
tones that 	 1.0
was installed 	 0.012987012987012988
variables such 	 1.0
in USA 	 0.0018726591760299626
fade away 	 1.0
-RRB- or 	 0.01084010840108401
from 10,000 	 0.009615384615384616
assess how 	 0.6666666666666666
Approaches which 	 0.3333333333333333
can get 	 0.0055248618784530384
step is 	 0.06666666666666667
prepared , 	 1.0
currency for 	 1.0
the questions 	 0.0006920415224913495
all unknowns 	 0.023255813953488372
leverages the 	 1.0
NLP task 	 0.02127659574468085
randomly chosen 	 1.0
that investigates 	 0.0035460992907801418
often contains 	 0.022727272727272728
generate high-quality 	 0.05555555555555555
computer that 	 0.022727272727272728
diverse set 	 0.5
onto its 	 1.0
speech . 	 0.06578947368421052
according to 	 1.0
then use 	 0.02857142857142857
give a 	 0.25
coherence . 	 0.3333333333333333
, symbols 	 0.0005614823133071309
minute , 	 1.0
recognition within 	 0.008264462809917356
relationship between 	 0.16666666666666666
digits `` 	 1.0
devices . 	 0.5
what linguistic 	 0.03125
its context 	 0.02857142857142857
providing customer 	 0.5
and left 	 0.001445086705202312
text contains 	 0.006289308176100629
topic , 	 0.25
that capture 	 0.0035460992907801418
NLG ; 	 0.047619047619047616
psychology , 	 0.75
Speech understanding 	 0.03225806451612903
scholars -LRB- 	 0.5
messages . 	 0.5
S. , 	 1.0
the sort 	 0.0006920415224913495
corpus linguistics 	 0.0967741935483871
market for 	 0.3333333333333333
computer extracting 	 0.022727272727272728
setting steer-point 	 0.2
about social 	 0.025
a horizontal 	 0.001226993865030675
in determining 	 0.0018726591760299626
support the 	 0.25
to action 	 0.0013280212483399733
and segment 	 0.001445086705202312
problem is 	 0.11363636363636363
the order 	 0.001384083044982699
: Determine 	 0.00980392156862745
Arabic -RRB- 	 0.25
others can 	 0.08333333333333333
largely an 	 0.2
data-to-text systems 	 1.0
, by 	 0.002807411566535654
unified mathematical 	 1.0
expressed on 	 0.3333333333333333
simplified form 	 0.5
answers rather 	 0.08333333333333333
accordance with 	 1.0
embedded quotations 	 0.25
now the 	 0.07692307692307693
Pang showed 	 0.3333333333333333
alone , 	 0.25
into meaningful 	 0.02564102564102564
usually perform 	 0.03125
or real 	 0.0045045045045045045
and would 	 0.001445086705202312
strongly to 	 0.5
the actual 	 0.0020761245674740486
Zacharov -RRB- 	 1.0
Information Science 	 0.2
questions in 	 0.038461538461538464
likely a 	 0.0625
classroom lectures 	 1.0
-LRB- English 	 0.0027100271002710027
An automated 	 0.0625
example for 	 0.024691358024691357
the Parliament 	 0.0006920415224913495
advantages over 	 1.0
on Reader 	 0.0047169811320754715
Givón , 	 1.0
has continued 	 0.011904761904761904
are beyond 	 0.004149377593360996
covers tasks 	 0.25
French and 	 0.25
discrete characters 	 0.3333333333333333
processes implemented 	 0.2
rank . 	 0.16666666666666666
left-to-right . 	 1.0
without inter-word 	 0.07692307692307693
-LRB- 2005 	 0.0027100271002710027
counts are 	 1.0
two categories 	 0.034482758620689655
pyramid showing 	 1.0
to HMMs 	 0.0013280212483399733
these good 	 0.023809523809523808
easier for 	 0.125
include SpeechTEK 	 0.037037037037037035
clues in 	 0.3333333333333333
users with 	 0.1111111111111111
to rewrite 	 0.0013280212483399733
`` diversity 	 0.005291005291005291
is called 	 0.012195121951219513
as models 	 0.003484320557491289
output scores 	 0.038461538461538464
running Palm 	 0.3333333333333333
<s> Unlike 	 0.0007686395080707148
not have 	 0.017857142857142856
of sound 	 0.0017825311942959
, hypothetical 	 0.0005614823133071309
`` entities 	 0.005291005291005291
Several research 	 0.3333333333333333
unlikely analyses 	 1.0
as well 	 0.04878048780487805
knowledge to 	 0.037037037037037035
to language 	 0.0013280212483399733
<s> Hidden 	 0.0023059185242121443
English text 	 0.02702702702702703
, DeRose 	 0.0005614823133071309
Health Organization 	 0.5
USMC , 	 1.0
than trying 	 0.022222222222222223
G , 	 1.0
and Semantic 	 0.001445086705202312
for generating 	 0.0036101083032490976
well a 	 0.03571428571428571
test as 	 0.1
developed and 	 0.038461538461538464
whole word 	 0.1111111111111111
a probabilistic 	 0.001226993865030675
-RRB- - 	 0.0027100271002710027
more probabilistic 	 0.010526315789473684
: Error 	 0.00980392156862745
new odd 	 0.041666666666666664
what is 	 0.09375
that aid 	 0.0035460992907801418
voices or 	 1.0
The shapes 	 0.005208333333333333
social interaction 	 0.07142857142857142
what knowledge 	 0.03125
Palm OS 	 1.0
learning such 	 0.023255813953488372
into punched 	 0.01282051282051282
Brill 's 	 0.3333333333333333
the reverse 	 0.0006920415224913495
successful . 	 0.1111111111111111
tagged '' 	 0.6666666666666666
released speech 	 0.5
flow together 	 1.0
categorization , 	 1.0
disagree on 	 0.3333333333333333
, Nikolas 	 0.0005614823133071309
number should 	 0.023255813953488372
pre-determined when 	 1.0
Hulth uses 	 0.3333333333333333
OCR and 	 0.02040816326530612
currently using 	 0.14285714285714285
of taking 	 0.00089126559714795
similarity between 	 0.2
papers on 	 0.6666666666666666
, Charles 	 0.0005614823133071309
natural as 	 0.013333333333333334
then we 	 0.05714285714285714
at TWA 	 0.014705882352941176
had the 	 0.07142857142857142
of elementary 	 0.00089126559714795
dictates into 	 1.0
that occurs 	 0.0035460992907801418
, human 	 0.003368893879842785
the food 	 0.0006920415224913495
access to 	 1.0
tagging include 	 0.04
equivalent to 	 0.2
performed more 	 0.1
it formed 	 0.008547008547008548
identities of 	 1.0
informatics . 	 1.0
as machine 	 0.003484320557491289
descriptions ; 	 1.0
words or 	 0.06422018348623854
aids for 	 1.0
well that 	 0.03571428571428571
-LRB- icon 	 0.0027100271002710027
values correlate 	 0.125
1954 involved 	 0.3333333333333333
derived by 	 0.3333333333333333
algorithm for 	 0.07142857142857142
n't see 	 0.25
English into 	 0.02702702702702703
developed in 	 0.23076923076923078
trainer . 	 1.0
`` un-supervised 	 0.005291005291005291
technique used 	 0.14285714285714285
to provide 	 0.005312084993359893
, paper 	 0.0005614823133071309
located anywhere 	 1.0
Application-Oriented OCR 	 1.0
acts in 	 0.3333333333333333
input with 	 0.024390243902439025
semantic constraints 	 0.047619047619047616
of 4 	 0.0017825311942959
the late 	 0.005536332179930796
DA began 	 0.3333333333333333
a supervised 	 0.00245398773006135
by Joseph 	 0.005714285714285714
see above 	 0.05
OCR , 	 0.14285714285714285
domotic appliance 	 1.0
summary that 	 0.047619047619047616
AI-complete problem 	 0.3333333333333333
select keyphrases 	 0.16666666666666666
first , 	 0.030303030303030304
classifier module 	 0.14285714285714285
says phrases 	 1.0
This comparison 	 0.015873015873015872
How should 	 0.14285714285714285
minimizes the 	 1.0
perspective . 	 0.25
`` uh 	 0.005291005291005291
that sentences 	 0.0035460992907801418
such input 	 0.008130081300813009
components -RRB- 	 0.2
translator for 	 0.14285714285714285
The algorithms 	 0.010416666666666666
query these 	 0.3333333333333333
Some speech 	 0.047619047619047616
the ACL 	 0.0006920415224913495
More detailed 	 0.1111111111111111
the US 	 0.001384083044982699
Armed Forces 	 1.0
good approximation 	 0.07692307692307693
developments of 	 0.3333333333333333
the contents 	 0.0006920415224913495
programs . 	 0.2727272727272727
prune away 	 1.0
sounds , 	 0.13333333333333333
Treebank data 	 0.16666666666666666
a generic 	 0.001226993865030675
as working 	 0.003484320557491289
highest ROUGE-1 	 0.3333333333333333
summarization : 	 0.02
Military High-performance 	 1.0
echoes , 	 1.0
canonical form 	 1.0
video the 	 0.2
from the 	 0.21153846153846154
functioning as 	 0.6666666666666666
above all 	 0.07692307692307693
a classification 	 0.001226993865030675
grammars for 	 0.07142857142857142
Speaker independent 	 0.16666666666666666
model all 	 0.03333333333333333
research devoted 	 0.023809523809523808
, US 	 0.0011229646266142617
Models are 	 0.3333333333333333
generate keyphrases 	 0.05555555555555555
Archaeology of 	 1.0
the complex 	 0.0006920415224913495
a high 	 0.0036809815950920245
These methods 	 0.17647058823529413
evaluated to 	 0.2857142857142857
mobile processor 	 0.5
modern approaches 	 0.2
cockpit functions 	 0.5
a human-language 	 0.001226993865030675
increase recognition 	 0.25
to unfamiliar 	 0.0013280212483399733
`` up 	 0.005291005291005291
: Vocabulary 	 0.00980392156862745
their associated 	 0.029411764705882353
whether medium 	 0.07692307692307693
unique and 	 1.0
University introduced 	 0.1111111111111111
the extremely 	 0.0006920415224913495
analyser to 	 1.0
it would 	 0.03418803418803419
all such 	 0.023255813953488372
and system 	 0.001445086705202312
TextRank was 	 0.14285714285714285
Nuance Communications 	 0.6666666666666666
whole discourses 	 0.1111111111111111
the effectiveness 	 0.0006920415224913495
more general 	 0.031578947368421054
sharing one 	 1.0
For most 	 0.01639344262295082
of machine 	 0.0071301247771836
a user-provided 	 0.001226993865030675
analyst is 	 1.0
answers are 	 0.08333333333333333
parse natural 	 0.1111111111111111
'' in 	 0.03608247422680412
in 1952 	 0.0018726591760299626
1969 Roger 	 0.5
expected , 	 0.14285714285714285
metric -LRB- 	 0.3333333333333333
others have 	 0.08333333333333333
are broader 	 0.004149377593360996
out loud 	 0.07142857142857142
of hard 	 0.0017825311942959
units are 	 0.2857142857142857
created rules 	 0.14285714285714285
opportunity rather 	 0.5
data about 	 0.012987012987012988
final phase 	 0.1111111111111111
The context 	 0.005208333333333333
are largely 	 0.004149377593360996
-LRB- sometimes 	 0.0027100271002710027
their routing 	 0.029411764705882353
René Descartes 	 1.0
called the 	 0.1111111111111111
method can 	 0.0625
of code 	 0.00089126559714795
a choice 	 0.001226993865030675
usually faster 	 0.03125
edge cases 	 0.3333333333333333
that converts 	 0.0035460992907801418
issue of 	 0.125
any answer 	 0.03225806451612903
or deferred 	 0.0045045045045045045
were accelerations 	 0.024390243902439025
of intermediary 	 0.00089126559714795
such large 	 0.008130081300813009
same method 	 0.04
25 % 	 1.0
say , 	 0.2857142857142857
major influence 	 0.08333333333333333
The rise 	 0.005208333333333333
, leads 	 0.0005614823133071309
achieve performance 	 0.5
possible forms 	 0.041666666666666664
about a 	 0.025
viewed as 	 1.0
German capitalizes 	 0.25
sounds '' 	 0.06666666666666667
normalization it 	 0.16666666666666666
are two 	 0.008298755186721992
OCR products 	 0.02040816326530612
Information retrieval 	 0.2
but to 	 0.014705882352941176
<s> Instead 	 0.0015372790161414297
have some 	 0.009615384615384616
measure a 	 0.09090909090909091
prose text 	 1.0
recursive-descent parser 	 1.0
any data 	 0.03225806451612903
, 2 	 0.0005614823133071309
PhD thesis 	 1.0
been shown 	 0.014705882352941176
intra-texual . 	 1.0
versa first-cut 	 1.0
vowels . 	 0.3333333333333333
Louise J. 	 1.0
less uninterrupted 	 0.08333333333333333
as 1 	 0.003484320557491289
words -- 	 0.009174311926605505
triples and 	 0.3333333333333333
a fashion 	 0.001226993865030675
the CoNLL 	 0.0006920415224913495
from which 	 0.028846153846153848
is distinct 	 0.0020325203252032522
referenced . 	 1.0
L'action GRACE 	 1.0
highly complex 	 0.1111111111111111
the book 	 0.001384083044982699
All the 	 1.0
Corpus contains 	 0.0625
N words 	 0.3333333333333333
both to 	 0.12903225806451613
document classification 	 0.05555555555555555
moves , 	 1.0
approach would 	 0.02857142857142857
straightforward PCFGs 	 1.0
systems now 	 0.008928571428571428
task -LRB- 	 0.023809523809523808
NLP programs 	 0.02127659574468085
likely be 	 0.0625
somewhat recent 	 0.5
for document 	 0.0036101083032490976
is vital 	 0.0020325203252032522
on pilot 	 0.0047169811320754715
to ones 	 0.0013280212483399733
on democratizing 	 0.0047169811320754715
grammar because 	 0.02702702702702703
to specify 	 0.0013280212483399733
feasibility study 	 0.5
individual cursive 	 0.08333333333333333
informational content 	 0.5
whose theoretical 	 0.3333333333333333
a task 	 0.0036809815950920245
the Puma 	 0.0006920415224913495
assign labels 	 0.2
Prolog generally 	 1.0
properties and 	 0.25
true only 	 0.5
to interface 	 0.0013280212483399733
domain and 	 0.05
be moderate 	 0.004219409282700422
45 % 	 1.0
requires fairly 	 0.0625
, substantial 	 0.0005614823133071309
Rubin , 	 1.0
LILOG projects 	 0.5
explored using 	 0.5
a reading 	 0.001226993865030675
non-whitespace character 	 1.0
Automated essay 	 0.5
-RRB- Hands-free 	 0.0027100271002710027
individual words 	 0.08333333333333333
confusion with 	 1.0
evaluation looks 	 0.018518518518518517
simple natural 	 0.038461538461538464
billing purposes 	 1.0
first agree 	 0.030303030303030304
language question-answering 	 0.006756756756756757
between `` 	 0.02564102564102564
literature are 	 1.0
Another area 	 0.07692307692307693
specialized output 	 0.5
very optimistic 	 0.024390243902439025
management Question 	 0.14285714285714285
until the 	 0.5
generate textual 	 0.05555555555555555
is substantial 	 0.0020325203252032522
analysis : 	 0.06153846153846154
extent of 	 0.25
ambiguities one 	 0.25
Levinsohn , 	 1.0
2011 -RRB- 	 0.5
appears to 	 0.2
Substantial test 	 0.5
language -RRB- 	 0.013513513513513514
accurate program 	 0.14285714285714285
upper-case letter 	 1.0
expended to 	 1.0
subjective Yes\/No 	 0.16666666666666666
then characterized 	 0.02857142857142857
famous article 	 0.3333333333333333
condense a 	 1.0
classes of 	 0.4
sentence there 	 0.020833333333333332
whether -LRB- 	 0.07692307692307693
than sixty 	 0.022222222222222223
It essentially 	 0.02631578947368421
In 1955 	 0.009523809523809525
documents generally 	 0.02631578947368421
learned from 	 0.2
guided by 	 1.0
translating texts 	 0.25
developed dynamic 	 0.038461538461538464
`` Meaningful 	 0.005291005291005291
past decade 	 0.3333333333333333
the table 	 0.001384083044982699
's Stilstudien 	 0.0196078431372549
probability to 	 0.14285714285714285
`` Speech 	 0.005291005291005291
Stephen H. 	 1.0
topics and 	 0.14285714285714285
thought of 	 0.6666666666666666
This hierarchy 	 0.015873015873015872
final keyphrases 	 0.2222222222222222
the provider 	 0.001384083044982699
carried on 	 0.5
<s> Ideally 	 0.0015372790161414297
or identical 	 0.0045045045045045045
category that 	 0.5
, contain 	 0.0005614823133071309
e.g. Syntactic 	 0.017857142857142856
this are 	 0.02197802197802198
speech as 	 0.013157894736842105
focus and 	 0.14285714285714285
but far 	 0.014705882352941176
of written 	 0.0035650623885918
levels but 	 0.045454545454545456
exponential number 	 0.5
any safety 	 0.03225806451612903
that human 	 0.0070921985815602835
improve machine 	 0.07692307692307693
can achieve 	 0.0055248618784530384
's students 	 0.0196078431372549
Sound is 	 0.3333333333333333
in service 	 0.0018726591760299626
demonstrated at 	 1.0
compute the 	 0.5
well , 	 0.03571428571428571
department in 	 0.5
LexRank has 	 0.08333333333333333
handles both 	 1.0
message understanding 	 0.5
to assess 	 0.0013280212483399733
and determining 	 0.001445086705202312
incorrect ones 	 0.3333333333333333
text-to-speech synthesizer 	 0.25
Parsing can 	 0.2
language constructs 	 0.006756756756756757
effectively learning 	 0.3333333333333333
to train 	 0.0013280212483399733
express sentiment 	 0.2
texts . 	 0.17647058823529413
proposed a 	 0.2222222222222222
A number 	 0.06
the context 	 0.004152249134948097
because translation 	 0.03333333333333333
its entirety 	 0.02857142857142857
van Dijk 	 0.5
detecting the 	 1.0
create edges 	 0.058823529411764705
improve performance 	 0.07692307692307693
evaluate summaries 	 0.25
a human 	 0.013496932515337423
QA is 	 0.047619047619047616
real-valued weights 	 0.6666666666666666
integrated information 	 0.3333333333333333
merging the 	 0.5
expertise , 	 1.0
who '' 	 0.1
using sentence 	 0.01694915254237288
Design choices 	 1.0
with highest 	 0.01092896174863388
to -RRB- 	 0.0013280212483399733
would be 	 0.16981132075471697
account several 	 0.3333333333333333
calculator program 	 0.5
U.S. Department 	 0.14285714285714285
and must 	 0.001445086705202312
than that 	 0.044444444444444446
times a 	 0.2
the phonemes 	 0.001384083044982699
speeds . 	 0.5
distances represented 	 0.5
very rare 	 0.024390243902439025
inter-annotator agreement 	 1.0
of domain 	 0.00089126559714795
highly structured 	 0.1111111111111111
funding was 	 0.125
The shorter 	 0.005208333333333333
written language 	 0.11538461538461539
corpora of 	 0.09090909090909091
to settle 	 0.0013280212483399733
-- makes 	 0.04
improvement to 	 0.25
syntactic analysis 	 0.15384615384615385
context of 	 0.15151515151515152
made more 	 0.125
Science , 	 0.5
application to 	 0.07142857142857142
he talking 	 0.14285714285714285
of pollen 	 0.0017825311942959
: Sample 	 0.00980392156862745
sub-sounds , 	 1.0
lines and 	 0.3333333333333333
if they 	 0.03571428571428571
some heuristic 	 0.012048192771084338
structured real-world 	 0.16666666666666666
overload has 	 1.0
for extracting 	 0.0036101083032490976
different distances 	 0.02040816326530612
use the 	 0.013888888888888888
example Mr. 	 0.012345679012345678
some interrogative 	 0.012048192771084338
in 1982 	 0.0018726591760299626
much through 	 0.045454545454545456
his or 	 0.08333333333333333
from social 	 0.009615384615384616
grammar of 	 0.05405405405405406
OnlineOCR or 	 0.3333333333333333
, therefore 	 0.0016844469399213925
the search 	 0.0006920415224913495
task also 	 0.023809523809523808
been taken 	 0.014705882352941176
support personnel 	 0.25
a quantity 	 0.001226993865030675
fighter applications 	 0.16666666666666666
, ranging 	 0.0011229646266142617
the 1980s 	 0.001384083044982699
tell whether 	 0.3333333333333333
language into 	 0.006756756756756757
current efforts 	 0.14285714285714285
Segmentation , 	 1.0
highly by 	 0.1111111111111111
which accommodate 	 0.007246376811594203
gained surprising 	 0.5
While LexRank 	 0.2
by deep 	 0.005714285714285714
not before 	 0.008928571428571428
a model 	 0.001226993865030675
considerable success 	 0.2
VTLN -RRB- 	 1.0
information contained 	 0.021739130434782608
-LRB- extrinsic 	 0.0027100271002710027
: objective 	 0.00980392156862745
classification-related measure 	 1.0
isolated NLP 	 0.2
<s> Among 	 0.0007686395080707148
make decisions 	 0.05
very limited 	 0.04878048780487805
human assessments 	 0.021739130434782608
than procedural 	 0.022222222222222223
to model 	 0.0013280212483399733
documents obtained 	 0.02631578947368421
interlingual , 	 0.25
sound clip 	 0.1
, coughing 	 0.0005614823133071309
Hybrid MT 	 0.5
the possibilities 	 0.0006920415224913495
is typical 	 0.0020325203252032522
words in 	 0.09174311926605505
on speaker 	 0.0047169811320754715
similar kind 	 0.037037037037037035
Few assumptions 	 1.0
theoretical underpinnings 	 0.3333333333333333
question , 	 0.2619047619047619
It is 	 0.6052631578947368
a form 	 0.001226993865030675
are ambiguities 	 0.004149377593360996
vector of 	 0.3333333333333333
Shepard , 	 0.3333333333333333
people who 	 0.125
using precision 	 0.01694915254237288
of new 	 0.00089126559714795
attribute grammars 	 0.5
software often 	 0.037037037037037035
in quite 	 0.0018726591760299626
a probability 	 0.001226993865030675
simplification Text-to-speech 	 1.0
popularity as 	 1.0
, \* 	 0.0005614823133071309
2001 -RRB- 	 0.5
also important 	 0.014492753623188406
in performance 	 0.0018726591760299626
publicly available 	 1.0
speech to 	 0.019736842105263157
implied challenges 	 1.0
also the 	 0.028985507246376812
Word segmentation 	 0.14285714285714285
research may 	 0.023809523809523808
The most 	 0.026041666666666668
have any 	 0.009615384615384616
understanding '' 	 0.06060606060606061
, Guy 	 0.0005614823133071309
5 consecutive 	 0.5
greater risk 	 0.3333333333333333
and even 	 0.008670520231213872
transcended the 	 1.0
into its 	 0.01282051282051282
process automatic 	 0.027777777777777776
a huge 	 0.001226993865030675
the presentation 	 0.0006920415224913495
-RRB- coefficients 	 0.0027100271002710027
Arabic in 	 0.25
the name 	 0.001384083044982699
title concentrates 	 1.0
as blogs 	 0.003484320557491289
users and 	 0.1111111111111111
possessives into 	 1.0
have more 	 0.028846153846153848
holes '' 	 1.0
actually more 	 0.3333333333333333
are provided 	 0.004149377593360996
are confusable 	 0.004149377593360996
approaches have 	 0.07142857142857142
recently updated 	 0.3333333333333333
weapon release 	 0.5
Hulth used 	 0.3333333333333333
network has 	 0.16666666666666666
seen before 	 0.2
traditionally made 	 0.5
free of 	 0.25
one or 	 0.03076923076923077
the stage 	 0.0006920415224913495
could do 	 0.0625
the Armed 	 0.0006920415224913495
Maximum entropy-based 	 0.3333333333333333
word processing 	 0.016666666666666666
before . 	 0.16666666666666666
right-most derivations 	 1.0
the subject 	 0.0034602076124567475
input characters 	 0.024390243902439025
a past-tense 	 0.001226993865030675
built that 	 0.3333333333333333
The Turney 	 0.005208333333333333
an NLG 	 0.007575757575757576
computed as 	 0.5
information into 	 0.043478260869565216
scores for 	 0.2
the harmonic 	 0.0006920415224913495
-LRB- DTW 	 0.0027100271002710027
Some tag 	 0.047619047619047616
improved the 	 0.25
and algorithms 	 0.001445086705202312
Emanuel Schegloff 	 0.5
dictionary . 	 0.14285714285714285
intrinsic and 	 0.25
Human languages 	 0.2
running English 	 0.3333333333333333
times , 	 0.2
automatically generate 	 0.047619047619047616
States Postal 	 0.14285714285714285
<s> Parsers 	 0.0015372790161414297
normalization . 	 0.3333333333333333
of a 	 0.08199643493761141
more complicated 	 0.010526315789473684
approximate at 	 0.5
front door 	 1.0
also characterized 	 0.014492753623188406
translation may 	 0.013513513513513514
dried up 	 1.0
to carry 	 0.0013280212483399733
to separate 	 0.0013280212483399733
of extracted 	 0.00089126559714795
1993 -RRB- 	 0.3333333333333333
of images 	 0.00089126559714795
place at 	 0.25
larger summarization 	 0.0625
applying a 	 0.25
on several 	 0.0047169811320754715
Popular speech 	 1.0
-RRB- for 	 0.005420054200542005
contrast -RRB- 	 0.125
adjacent unigrams 	 0.16666666666666666
wide range 	 0.5
complex system 	 0.08333333333333333
say your 	 0.14285714285714285
robotic arm 	 1.0
like ca 	 0.03571428571428571
many aspects 	 0.019230769230769232
this basic 	 0.01098901098901099
some writing 	 0.012048192771084338
are four 	 0.004149377593360996
, glossary 	 0.0011229646266142617
coding of 	 1.0
strong is 	 0.25
measured with 	 0.16666666666666666
accuracy and 	 0.03225806451612903
gained by 	 0.5
application has 	 0.07142857142857142
pre-existing corpus 	 0.5
which are 	 0.08695652173913043
summaries qualitatively 	 0.023255813953488372
across a 	 0.2
FAS -RRB- 	 1.0
hand-crafted rules 	 0.5
which research 	 0.007246376811594203
Garfinkel who 	 1.0
expression . 	 0.2
billion words 	 1.0
several sub-problems 	 0.045454545454545456
not been 	 0.017857142857142856
tagger . 	 0.1111111111111111
systems applications 	 0.008928571428571428
best with 	 0.05555555555555555
the strengths 	 0.0006920415224913495
signed language 	 1.0
quite weak 	 0.125
first-order logic 	 1.0
<s> Another 	 0.009992313604919293
discourses , 	 0.5
performance only 	 0.05555555555555555
English is 	 0.02702702702702703
assistants such 	 1.0
latter as 	 1.0
of Knowledge 	 0.00089126559714795
text cohesion 	 0.006289308176100629
fusion techniques 	 1.0
in both 	 0.0018726591760299626
= Noun 	 0.1111111111111111
, need 	 0.0005614823133071309
compare the 	 0.2857142857142857
sample corpus 	 0.3333333333333333
Aggregation : 	 1.0
generalizes as 	 1.0
more human-generated 	 0.010526315789473684
The notion 	 0.010416666666666666
using journal 	 0.01694915254237288
in document 	 0.003745318352059925
and generally 	 0.001445086705202312
makes sense 	 0.125
to which 	 0.006640106241699867
then using 	 0.02857142857142857
the edges 	 0.001384083044982699
created , 	 0.2857142857142857
is -LRB- 	 0.0040650406504065045
spoken natural 	 0.07142857142857142
on paper 	 0.0047169811320754715
How many 	 0.14285714285714285
useful to 	 0.14285714285714285
practical dimensions 	 0.5
Business-card OCR 	 1.0
likely another 	 0.0625
Systems corp. 	 0.08333333333333333
good search 	 0.07692307692307693
Evaluation techniques 	 0.1111111111111111
taggers . 	 0.14285714285714285
correlate with 	 0.6666666666666666
a damping 	 0.001226993865030675
` conceptual 	 0.0625
structured into 	 0.16666666666666666
The apple 	 0.010416666666666666
or keep 	 0.0045045045045045045
Ratliff originally 	 1.0
between ` 	 0.02564102564102564
POS tagger 	 0.3076923076923077
it does 	 0.008547008547008548
data , 	 0.12987012987012986
done using 	 0.09090909090909091
understanding to 	 0.06060606060606061
will replace 	 0.02857142857142857
<s> Approaches 	 0.0023059185242121443
and EUROPARL 	 0.001445086705202312
-LRB- actual 	 0.0027100271002710027
member of 	 1.0
frame ; 	 0.5
data and 	 0.025974025974025976
, widely 	 0.0005614823133071309
characterised by 	 1.0
closer to 	 1.0
context-free approximation 	 0.09090909090909091
indicating important 	 1.0
rules . 	 0.13953488372093023
analysis Genre 	 0.015384615384615385
suggest that 	 0.3333333333333333
, recommendations 	 0.0005614823133071309
covariance transform 	 0.5
extensive lexicons 	 0.3333333333333333
theory Functional 	 0.07692307692307693
performed through 	 0.1
continues to 	 1.0
noun reading 	 0.07142857142857142
several classifiers 	 0.045454545454545456
defective flood-control 	 1.0
phrase begin 	 0.1
which generate 	 0.021739130434782608
work , 	 0.125
a short-time 	 0.001226993865030675
visit . 	 0.5
b -RRB- 	 1.0
you say 	 0.07692307692307693
such problems 	 0.008130081300813009
remains to 	 0.25
primarily with 	 0.5
specified in 	 1.0
, paragraphs 	 0.0005614823133071309
shed light 	 1.0
we learn 	 0.022222222222222223
democratizing data 	 0.5
now we 	 0.07692307692307693
purposes -LRB- 	 0.25
is steered 	 0.0020325203252032522
form the 	 0.05
work for 	 0.041666666666666664
passages to 	 0.5
or ICR 	 0.0045045045045045045
70s the 	 1.0
OCR . 	 0.02040816326530612
likely part 	 0.0625
theory , 	 0.3076923076923077
a negative 	 0.001226993865030675
linked because 	 0.3333333333333333
better measure 	 0.1111111111111111
so has 	 0.03333333333333333
turns and 	 0.3333333333333333
the humanities 	 0.0006920415224913495
are structured 	 0.004149377593360996
about 1965 	 0.025
tables of 	 0.3333333333333333
descriptive and 	 0.3333333333333333
in reference 	 0.003745318352059925
can understand 	 0.0055248618784530384
Commissioned by 	 1.0
not become 	 0.008928571428571428
different method 	 0.02040816326530612
corpus as 	 0.03225806451612903
nouns or 	 0.1111111111111111
if in 	 0.07142857142857142
e.g. echoes 	 0.017857142857142856
vs. open 	 0.08333333333333333
customize the 	 0.5
as subtasks 	 0.003484320557491289
the disfluences 	 0.0006920415224913495
in excess 	 0.003745318352059925
<s> Goldberg 	 0.0007686395080707148
education , 	 1.0
aspects such 	 0.14285714285714285
Information Subsumption 	 0.2
ROUGE-1 only 	 0.2
Parsing is 	 0.4
specific to 	 0.047619047619047616
shallow-transfer machine 	 1.0
the test 	 0.001384083044982699
determine if 	 0.21739130434782608
is actually 	 0.0040650406504065045
effective and 	 0.16666666666666666
as just 	 0.003484320557491289
glue text 	 1.0
; Each 	 0.02127659574468085
with using 	 0.00546448087431694
life . 	 0.25
-LRB- CSIS 	 0.0027100271002710027
be made 	 0.016877637130801686
Mention must 	 1.0
been applied 	 0.08823529411764706
A document 	 0.02
Other segmentation 	 0.14285714285714285
precise set 	 0.3333333333333333
highest level 	 0.3333333333333333
Turney 's 	 0.4444444444444444
require to 	 0.045454545454545456
As this 	 0.05555555555555555
for tagging 	 0.0036101083032490976
blocks , 	 0.25
with initial 	 0.00546448087431694
long-time translator 	 1.0
continuously rendered 	 1.0
tagger , 	 0.4444444444444444
texts , 	 0.11764705882352941
to one 	 0.0026560424966799467
<s> Where 	 0.0007686395080707148
or stochastic 	 0.0045045045045045045
informational structures 	 0.5
in healthcare 	 0.0018726591760299626
forecasts used 	 0.2
summary tackles 	 0.023809523809523808
sales receipts 	 0.3333333333333333
analysis algorithms 	 0.015384615384615385
systems in 	 0.008928571428571428
in-depth analysis 	 0.3333333333333333
given , 	 0.041666666666666664
smaller lexical 	 0.14285714285714285
framework . 	 0.75
The best 	 0.005208333333333333
funding of 	 0.125
peak , 	 1.0
sounds are 	 0.13333333333333333
led to 	 0.6666666666666666
-RRB- and 	 0.05420054200542006
data . 	 0.22077922077922077
based , 	 0.037037037037037035
likely not 	 0.0625
second ; 	 0.1
of 1,000 	 0.00089126559714795
table lookup 	 0.14285714285714285
NLP Handbook 	 0.02127659574468085
, special 	 0.0005614823133071309
speaking speeds 	 0.125
boolean syntactic 	 1.0
are likely 	 0.016597510373443983
to natural 	 0.0013280212483399733
application of 	 0.2857142857142857
is 2,000 	 0.0020325203252032522
real-time written 	 0.5
include stages 	 0.037037037037037035
and continued 	 0.001445086705202312
-RRB- from 	 0.0027100271002710027
English for 	 0.02702702702702703
topic were 	 0.125
different vendors 	 0.02040816326530612
, Air 	 0.0005614823133071309
recent news 	 0.125
work based 	 0.041666666666666664
that ROUGE 	 0.0035460992907801418
EndWar and 	 1.0
software had 	 0.037037037037037035
appraisal theory 	 1.0
1975 -RRB- 	 1.0
graph using 	 0.07692307692307693
how a 	 0.06896551724137931
environments . 	 1.0
speaker dependent 	 0.05555555555555555
program may 	 0.045454545454545456
We then 	 0.14285714285714285
of 2007 	 0.0017825311942959
cache language 	 1.0
paradigm includes 	 0.3333333333333333
and consonants 	 0.001445086705202312
employed within 	 1.0
PAM -LRB- 	 1.0
as statistical 	 0.003484320557491289
the right 	 0.0020761245674740486
By 1985 	 0.3333333333333333
first choice 	 0.030303030303030304
, classroom 	 0.0005614823133071309
types including 	 0.07142857142857142
time-scales -LRB- 	 1.0
smaller ones 	 0.14285714285714285
an opportunity 	 0.007575757575757576
remain high 	 1.0
book-new . 	 1.0
source , 	 0.041666666666666664
semantic from 	 0.047619047619047616
methods try 	 0.022727272727272728
transducer , 	 0.5
dialogue with 	 0.5
`` the 	 0.026455026455026454
thus returning 	 0.1
ontologies -LRB- 	 0.16666666666666666
Isolated , 	 1.0
the mission 	 0.0006920415224913495
models that 	 0.11538461538461539
language representation 	 0.006756756756756757
as information 	 0.003484320557491289
's 1990 	 0.0196078431372549
emergence of 	 1.0
<s> Features 	 0.0007686395080707148
phonemes -LRB- 	 0.16666666666666666
sequence of 	 0.875
black holes 	 1.0
assistance of 	 1.0
tools starts 	 0.16666666666666666
ALPAC report 	 1.0
with manually 	 0.00546448087431694
and after 	 0.004335260115606936
spoken language 	 0.14285714285714285
matching and 	 0.2
BASEBALL answered 	 0.5
phenomenon of 	 0.6
statistical models 	 0.21212121212121213
<s> Searches 	 0.0007686395080707148
verbalization . 	 1.0
quantity -LRB- 	 0.3333333333333333
various algorithms 	 0.05555555555555555
accuracy over 	 0.03225806451612903
HMM parameter 	 0.3333333333333333
recall . 	 0.6666666666666666
produces less 	 0.25
gradual lessening 	 1.0
quantity of 	 0.3333333333333333
rewrite it 	 1.0
: Decoding 	 0.00980392156862745
: Microsoft 	 0.00980392156862745
is brought 	 0.0020325203252032522
posed in 	 0.6666666666666666
regards to 	 1.0
existing summaries 	 0.2
often do 	 0.022727272727272728
considerable commercial 	 0.2
recognition , 	 0.11570247933884298
preparation of 	 1.0
project compared 	 0.15384615384615385
<s> e.g. 	 0.0015372790161414297
during handwriting 	 0.1
the SR 	 0.0006920415224913495
-- the 	 0.12
linguistic nuances 	 0.0625
commercial . 	 0.09090909090909091
2010 and 	 0.3333333333333333
polynomial-size representations 	 1.0
algorithm or 	 0.03571428571428571
to end 	 0.0026560424966799467
-LRB- intonation 	 0.0027100271002710027
LR parsers 	 1.0
the tasks 	 0.0006920415224913495
that match 	 0.0035460992907801418
systems analyze 	 0.008928571428571428
to computer 	 0.0013280212483399733
and LR 	 0.001445086705202312
various natural 	 0.05555555555555555
not possible 	 0.008928571428571428
, Case 	 0.0005614823133071309
, Cynthia 	 0.0005614823133071309
OS or 	 0.5
sub-signals . 	 1.0
criteria , 	 0.25
<s> With 	 0.003843197540353574
the given 	 0.001384083044982699
Organization , 	 1.0
automate about 	 0.3333333333333333
well-known application 	 1.0
from naive 	 0.009615384615384616
algorithms fall 	 0.02857142857142857
In languages 	 0.009523809523809525
relations , 	 0.16666666666666666
become well 	 0.25
the Medical 	 0.0006920415224913495
to low 	 0.0013280212483399733
first step 	 0.06060606060606061
, Winograd 	 0.0005614823133071309
when we 	 0.05714285714285714
whether to 	 0.07692307692307693
summary , 	 0.14285714285714285
to rank 	 0.00398406374501992
Shallow approaches 	 0.5
Winograd continued 	 0.3333333333333333
ways , 	 0.25
in artificial 	 0.0018726591760299626
culminating in 	 1.0
space complexity 	 0.2
senses , 	 0.5
are designed 	 0.004149377593360996
as ME 	 0.003484320557491289
common ones 	 0.04
in restricted 	 0.0018726591760299626
, 1976 	 0.0011229646266142617
is much 	 0.0040650406504065045
Introduction and 	 1.0
require some 	 0.045454545454545456
and Latin 	 0.001445086705202312
Jim Martin 	 1.0
attempts have 	 0.16666666666666666
that might 	 0.0070921985815602835
NLP methods 	 0.02127659574468085
Gismo . 	 0.5
steps , 	 0.5
did fail 	 0.2
resource such 	 0.2
users after 	 0.1111111111111111
scanning applications 	 0.5
it uses 	 0.017094017094017096
form ? 	 0.05
Word error 	 0.14285714285714285
grammars can 	 0.07142857142857142
Tom Clancy 	 1.0
DCD library 	 1.0
summaries or 	 0.023255813953488372
Harold Garfinkel 	 1.0
he developed 	 0.14285714285714285
-LRB- 2004 	 0.0027100271002710027
, Wallace 	 0.0005614823133071309
language analysis 	 0.006756756756756757
US ports 	 0.14285714285714285
top level 	 0.2
physics that 	 1.0
the derived 	 0.0006920415224913495
, nasality 	 0.0005614823133071309
to reformulate 	 0.0013280212483399733
plant OCR 	 1.0
<s> Running 	 0.0007686395080707148
and , 	 0.004335260115606936
semi-supervised '' 	 0.5
and for 	 0.001445086705202312
for database 	 0.0036101083032490976
consider sequences 	 0.25
as early 	 0.003484320557491289
`` My 	 0.005291005291005291
address of 	 0.25
toy project 	 0.5
European Parliament 	 0.3333333333333333
sentence to 	 0.020833333333333332
in French 	 0.0018726591760299626
an implementation 	 0.007575757575757576
rules , 	 0.11627906976744186
larger chunk 	 0.0625
This article 	 0.015873015873015872
exclamation marks 	 1.0
thus be 	 0.1
came from 	 0.5
of 80 	 0.00089126559714795
as knowledge 	 0.003484320557491289
as if 	 0.003484320557491289
abbreviated to 	 1.0
rules through 	 0.023255813953488372
new . 	 0.041666666666666664
limitation in 	 1.0
on Audio 	 0.0047169811320754715
have explicit 	 0.009615384615384616
judges , 	 0.5
bilingual text 	 0.5
edges between 	 0.14285714285714285
exchange of 	 1.0
alternative approach 	 0.3333333333333333
be asking 	 0.004219409282700422
wave which 	 0.1111111111111111
text can 	 0.006289308176100629
sound , 	 0.1
HTML and 	 1.0
to break 	 0.0013280212483399733
technologies for 	 0.25
, answered 	 0.0005614823133071309
: Merging 	 0.00980392156862745
much harder 	 0.045454545454545456
sentences into 	 0.039473684210526314
sufficient iteration 	 0.2
problem may 	 0.022727272727272728
tags , 	 0.3333333333333333
interpreters require 	 1.0
the example 	 0.0020761245674740486
authors decide 	 0.2
given unfamiliar 	 0.08333333333333333
two illustrates 	 0.034482758620689655
-LRB- closer 	 0.0027100271002710027
messages into 	 0.5
databases and 	 0.125
this centroid 	 0.01098901098901099
are expected 	 0.004149377593360996
into each 	 0.01282051282051282
True\/False is 	 1.0
main ideas 	 0.125
lengths -RRB- 	 1.0
widespread . 	 1.0
pasted , 	 1.0
and development 	 0.002890173410404624
units such 	 0.14285714285714285
against which 	 0.2
sounds : 	 0.06666666666666667
to Xerox 	 0.0013280212483399733
traditionally written 	 0.5
Stylistics -LRB- 	 1.0
when summarizing 	 0.02857142857142857
knowledge that 	 0.037037037037037035
describing graphs 	 0.25
a speech 	 0.00245398773006135
model is 	 0.1
mapping each 	 0.5
tokens 12 	 0.14285714285714285
easier than 	 0.25
figure out 	 0.5
suitable ontology 	 0.25
University of 	 0.3333333333333333
processing text 	 0.018518518518518517
many neighbors 	 0.019230769230769232
differences in 	 0.3333333333333333
entry has 	 0.25
in one 	 0.00749063670411985
: Many 	 0.00980392156862745
﻿Natural language 	 1.0
Ray Kurzweil 	 1.0
by other 	 0.005714285714285714
systems with 	 0.017857142857142856
to require 	 0.0013280212483399733
controller tasks 	 0.25
Statistical Methods 	 0.1111111111111111
shift-reduce algorithm 	 1.0
the OCR-A 	 0.0006920415224913495
representing successive 	 0.5
important points 	 0.0625
Hardy , 	 1.0
very small 	 0.04878048780487805
better translations 	 0.1111111111111111
tokens form 	 0.14285714285714285
pioneered at 	 0.3333333333333333
simple extraction 	 0.038461538461538464
grown . 	 1.0
Use '' 	 0.5
part usually 	 0.037037037037037035
walks . 	 0.5
the TextRank 	 0.001384083044982699
languages . 	 0.16
Larry Page 	 0.5
speech-to-text -RRB- 	 0.5
expressivity of 	 1.0
of stock 	 0.00089126559714795
analysis aims 	 0.015384615384615385
of parsing 	 0.0017825311942959
relaxed parser 	 1.0
. <s/> 	 0.9875195007800313
about democratizing 	 0.025
or even 	 0.02252252252252252
which was 	 0.036231884057971016
Computer Products 	 0.3333333333333333
and others 	 0.002890173410404624
machine-aided human 	 1.0
information about 	 0.043478260869565216
case with 	 0.058823529411764705
people 's 	 0.0625
hurts '' 	 0.5
happen between 	 1.0
several variables 	 0.045454545454545456
before or 	 0.16666666666666666
vendors in 	 0.25
labeled data 	 0.3333333333333333
Systems based 	 0.25
recognized normal 	 0.16666666666666666
chart parsing 	 1.0
weighted by 	 0.3333333333333333
of sentence 	 0.00089126559714795
training in 	 0.03571428571428571
in another 	 0.00749063670411985
reliance on 	 1.0
, abstractive 	 0.0005614823133071309
voice file 	 0.07692307692307693
to text 	 0.0026560424966799467
object `` 	 0.5
Michigan , 	 1.0
be considered 	 0.008438818565400843
formalism which 	 1.0
, thus 	 0.0011229646266142617
`` Customized 	 0.005291005291005291
June 1990 	 1.0
various NLP 	 0.05555555555555555
usually be 	 0.03125
reviews , 	 0.5
are selected 	 0.004149377593360996
analyze ` 	 0.25
proportional to 	 1.0
dogs → 	 0.14285714285714285
been parse 	 0.014705882352941176
complex setting 	 0.041666666666666664
document such 	 0.027777777777777776
movies , 	 1.0
the details 	 0.0006920415224913495
and VOLSUNGA 	 0.001445086705202312
an urgent 	 0.007575757575757576
, automatic 	 0.0016844469399213925
metrics often 	 0.1111111111111111
due both 	 0.4
on either 	 0.0047169811320754715
while abstraction 	 0.05
phrase ` 	 0.1
lot of 	 0.3333333333333333
a construct 	 0.001226993865030675
Stilstudien -LRB- 	 1.0
explained by 	 1.0
and very 	 0.001445086705202312
these apply 	 0.023809523809523808
also includes 	 0.014492753623188406
analysis is 	 0.015384615384615385
one natural 	 0.03076923076923077
Phillips . 	 1.0
or emotion 	 0.0045045045045045045
can say 	 0.0055248618784530384
feasibility demonstration 	 0.5
to it 	 0.0013280212483399733
Star Trek 	 1.0
most spoken 	 0.034482758620689655
the big 	 0.0006920415224913495
on sentences 	 0.0047169811320754715
the start 	 0.002768166089965398
the subsequent 	 0.0006920415224913495
than extraction 	 0.022222222222222223
English-like sentences 	 0.3333333333333333
names of 	 0.14285714285714285
and you 	 0.004335260115606936
William Labov 	 0.5
high-quality weather 	 1.0
OCR technology 	 0.1836734693877551
involves preliminary 	 0.1
Web-based + 	 0.3333333333333333
questions pertaining 	 0.038461538461538464
where natural 	 0.02857142857142857
tagger is 	 0.1111111111111111
specialist textbook 	 1.0
techniques use 	 0.043478260869565216
a qualitative 	 0.00245398773006135
aircraft platforms 	 0.14285714285714285
any sentences 	 0.03225806451612903
commonly serve 	 0.125
the algorithm 	 0.0006920415224913495
in 2004 	 0.0018726591760299626
solve algebra 	 0.25
Greene and 	 1.0
new wave 	 0.041666666666666664
choices -LRB- 	 0.2
was the 	 0.05194805194805195
written English 	 0.038461538461538464
, probabilistic 	 0.0016844469399213925
second layer 	 0.2
Abney S. 	 1.0
-LRB- http:\/\/haydn.isi.edu\/ROUGE\/ 	 0.0027100271002710027
that person 	 0.0035460992907801418
Santorini gives 	 1.0
Sentiment analysis 	 0.8333333333333334
, web 	 0.0005614823133071309
spend much 	 1.0
Biden visit 	 0.3333333333333333
experts of 	 1.0
the quality 	 0.0034602076124567475
informative enough 	 0.5
the potentially 	 0.0006920415224913495
walk on 	 0.4
try to 	 1.0
linguistics is 	 0.15
four together 	 0.14285714285714285
it even 	 0.008547008547008548
time-consuming part 	 0.3333333333333333
conceptual dependency 	 0.5
e-communities through 	 0.5
qualitative automatic 	 0.5
a question 	 0.008588957055214725
elements from 	 0.25
be translated 	 0.012658227848101266
an autopilot 	 0.007575757575757576
as classifying 	 0.003484320557491289
emotions in 	 1.0
human-readable address 	 1.0
the machine-learning 	 0.0006920415224913495
analyze the 	 0.25
with the 	 0.15300546448087432
: List 	 0.00980392156862745
recall may 	 0.3333333333333333
similarly effective 	 1.0
Compute features 	 1.0
-RRB- Current 	 0.0027100271002710027
it to 	 0.042735042735042736
require that 	 0.045454545454545456
1950 , 	 1.0
exceptions -RRB- 	 1.0
from NLP 	 0.009615384615384616
equipment would 	 0.3333333333333333
high degree 	 0.05555555555555555
to 7 	 0.00398406374501992
, + 	 0.0011229646266142617
pieces as 	 1.0
heuristic to 	 0.3333333333333333
action of 	 0.2
A subtask 	 0.02
finished product 	 0.5
absorbing random 	 0.3333333333333333
stream is 	 0.5
MMI -RRB- 	 1.0
1960s were 	 0.3333333333333333
relevance assessment 	 0.3333333333333333
of analyses 	 0.00089126559714795
the results 	 0.002768166089965398
speaker . 	 0.16666666666666666
in walking 	 0.0018726591760299626
mentioned by 	 0.16666666666666666
probabilities returned 	 0.09090909090909091
, Adriana 	 0.0005614823133071309
air is 	 0.2
solved in 	 0.2
1996 , 	 1.0
Extractive methods 	 1.0
that revolutionized 	 0.0035460992907801418
from computer 	 0.009615384615384616
document formats 	 0.027777777777777776
the relationship 	 0.0006920415224913495
a page 	 0.001226993865030675
comparing its 	 0.5
i.e. so 	 0.05263157894736842
sequence alignment 	 0.125
analysis Variation 	 0.015384615384615385
semantics which 	 0.14285714285714285
rate is 	 0.18181818181818182
During this 	 0.5
increases in 	 1.0
'' set 	 0.005154639175257732
the action 	 0.0006920415224913495
for simple 	 0.0036101083032490976
of speech 	 0.040998217468805706
considered or 	 0.1111111111111111
represented themselves 	 0.16666666666666666
video captioning 	 0.2
stories on 	 1.0
the volume 	 0.0006920415224913495
direct a 	 0.16666666666666666
Shepard 's 	 0.3333333333333333
slow and 	 0.5
Spontaneous Speech 	 1.0
MorphoChallenge Semi-supervised 	 1.0
or opinion 	 0.0045045045045045045
outside the 	 0.5
may have 	 0.038461538461538464
learning -RRB- 	 0.023255813953488372
of large 	 0.0035650623885918
1946 by 	 1.0
-RRB- Some 	 0.0027100271002710027
might contain 	 0.038461538461538464
a graph 	 0.0036809815950920245
Du Bois 	 1.0
, sales 	 0.0005614823133071309
processing natural 	 0.018518518518518517
an early 	 0.007575757575757576
evaluating automatically 	 0.2
fairly often 	 0.25
, rejecting 	 0.0005614823133071309
media such 	 0.16666666666666666
native speaker 	 1.0
to densely 	 0.0013280212483399733
'' appears 	 0.005154639175257732
machine digitized 	 0.012658227848101266
the recognized 	 0.001384083044982699
utilize an 	 0.5
e.g. space 	 0.017857142857142856
of hidden 	 0.00089126559714795
to turn 	 0.0013280212483399733
achieving high 	 0.5
appear more 	 0.0625
expressed with 	 0.16666666666666666
evaluation procedures 	 0.018518518518518517
to explicitly 	 0.0026560424966799467
is 7 	 0.0020325203252032522
networks Main 	 0.07142857142857142
of marketing 	 0.00089126559714795
the WYSIWYM 	 0.0006920415224913495
, Court 	 0.0005614823133071309
performed , 	 0.2
% on 	 0.02564102564102564
sense of 	 0.125
is easily 	 0.0020325203252032522
one needs 	 0.015384615384615385
Asia Online 	 1.0
: Deciding 	 0.00980392156862745
possibilities . 	 0.2
language usually 	 0.006756756756756757
'' a 	 0.015463917525773196
omni-font OCR 	 1.0
choice is 	 0.25
was extensively 	 0.012987012987012988
many different 	 0.07692307692307693
essentially identical 	 0.125
include an 	 0.07407407407407407
more sophisticated 	 0.021052631578947368
paper Shipibo 	 0.09090909090909091
Google published 	 0.25
English has 	 0.05405405405405406
lattices represented 	 1.0
example by 	 0.024691358024691357
learning , 	 0.09302325581395349
needed without 	 0.047619047619047616
so we 	 0.03333333333333333
, according 	 0.0005614823133071309
to closely 	 0.0013280212483399733
training data 	 0.35714285714285715
affects the 	 1.0
States ? 	 0.14285714285714285
role in 	 0.25
evident way 	 0.5
sentiments expressed 	 1.0
many programmers 	 0.019230769230769232
service , 	 0.4
of count 	 0.00089126559714795
which accepts 	 0.007246376811594203
software Current 	 0.037037037037037035
Results have 	 1.0
Unix Consultant 	 0.5
their chosen 	 0.029411764705882353
Models In 	 0.3333333333333333
is capitalized 	 0.0020325203252032522
door being 	 0.25
, T 	 0.0005614823133071309
terms , 	 0.07692307692307693
normal human 	 0.5
converse on 	 1.0
been data-to-text 	 0.014705882352941176
is particularly 	 0.0040650406504065045
business , 	 0.25
over an 	 0.08333333333333333
account how 	 0.3333333333333333
in visible 	 0.0018726591760299626
on how 	 0.009433962264150943
sentiment based 	 0.04
summarization technology 	 0.02
where phrases 	 0.02857142857142857
program can 	 0.045454545454545456
NC '' 	 1.0
commands issued 	 0.2
'' might 	 0.010309278350515464
declared before 	 0.5
email Multimodal 	 0.5
no assumptions 	 0.07692307692307693
it appropriately 	 0.008547008547008548
T unigrams 	 0.3333333333333333
emotional effect 	 0.25
been encouraging 	 0.014705882352941176
pollen levels 	 0.6923076923076923
can express 	 0.022099447513812154
Working with 	 1.0
speech interface 	 0.006578947368421052
of each 	 0.006238859180035651
F = 	 1.0
easily be 	 0.1111111111111111
, Z 	 0.0005614823133071309
keyphrases formed 	 0.02857142857142857
with implicit 	 0.00546448087431694
pseudo-pilot , 	 0.5
called evaluation 	 0.1111111111111111
controllers . 	 0.3333333333333333
anthropology , 	 1.0
is reported 	 0.0020325203252032522
not absolutely 	 0.008928571428571428
environment where 	 0.16666666666666666
media . 	 0.16666666666666666
agrees with 	 1.0
word forms 	 0.016666666666666666
to include 	 0.009296148738379814
10 -RRB- 	 0.125
and may 	 0.002890173410404624
but steadily 	 0.014705882352941176
the fly 	 0.0006920415224913495
perspectives and 	 1.0
As well 	 0.05555555555555555
judge its 	 0.25
of assembling 	 0.00089126559714795
standard can 	 0.07142857142857142
health and 	 1.0
Dynamic time 	 0.8
an evaluation 	 0.007575757575757576
The only 	 0.010416666666666666
somewhat shallow 	 0.5
in time 	 0.0018726591760299626
serving a 	 1.0
on specific 	 0.0047169811320754715
by their 	 0.005714285714285714
reason , 	 0.5
The edges 	 0.005208333333333333
is far 	 0.0040650406504065045
word breaks 	 0.016666666666666666
output in 	 0.038461538461538464
construct lightweight 	 0.3333333333333333
business letters 	 0.25
kind appears 	 0.09090909090909091
International continued 	 1.0
different problem 	 0.02040816326530612
process Flow 	 0.027777777777777776
systems currently 	 0.008928571428571428
basic approach 	 0.07692307692307693
objective or 	 0.2
classifying a 	 0.4
as 1946 	 0.003484320557491289
distinction , 	 0.2
, grammars 	 0.0005614823133071309
first mechanized 	 0.030303030303030304
to run 	 0.0013280212483399733
quoted in 	 1.0
Smith went 	 1.0
resource management 	 0.4
by any 	 0.005714285714285714
accurate even 	 0.14285714285714285
like this 	 0.03571428571428571
-RRB- showed 	 0.0027100271002710027
effort has 	 0.25
having to 	 0.2
The popular 	 0.005208333333333333
to help 	 0.0026560424966799467
text of 	 0.006289308176100629
quoting people 	 1.0
January 13 	 0.25
previous questions 	 0.3333333333333333
Latin alphabet 	 0.25
communicative goal 	 0.6666666666666666
meaningful symbol 	 0.125
projection followed 	 1.0
belong to 	 1.0
classification error 	 0.058823529411764705
and Cary 	 0.001445086705202312
left recursion 	 0.16666666666666666
domain might 	 0.05
of 15-20 	 0.00089126559714795
and linear 	 0.001445086705202312
NLP systems 	 0.06382978723404255
system proposed 	 0.010752688172043012
benchmark tests 	 1.0
as noun 	 0.003484320557491289
designed to 	 0.7142857142857143
excellent application 	 1.0
Success Rate 	 1.0
of algorithms 	 0.00089126559714795
them -LRB- 	 0.05263157894736842
get high 	 0.14285714285714285
constituents such 	 0.5
fully up 	 0.16666666666666666
graphic user 	 1.0
research , 	 0.07142857142857142
we normally 	 0.022222222222222223
, similarities 	 0.0005614823133071309
stems in 	 0.5
merging of 	 0.5
-LRB- HLDA 	 0.0027100271002710027
Michael Schober 	 0.25
In theory 	 0.01904761904761905
common case 	 0.04
following manner 	 0.06666666666666667
and paste 	 0.001445086705202312
-RRB- with 	 0.008130081300813009
Jacob Rabinow 	 1.0
n being 	 0.5
specifically concerned 	 0.5
input a 	 0.024390243902439025
and LILOG 	 0.001445086705202312
This research 	 0.015873015873015872
most notably 	 0.017241379310344827
dedicated to 	 0.6666666666666666
a simulation 	 0.001226993865030675
discriminate because 	 0.3333333333333333
As businesses 	 0.05555555555555555
; Computed 	 0.02127659574468085
address - 	 0.25
companies to 	 0.5
article contains 	 0.034482758620689655
Question classes 	 0.2857142857142857
item may 	 1.0
are popular 	 0.004149377593360996
motion during 	 1.0
extraction process 	 0.03225806451612903
, neutral 	 0.0005614823133071309
`` President 	 0.005291005291005291
the parser 	 0.002768166089965398
pertaining to 	 1.0
for data 	 0.007220216606498195
growing field 	 0.5
a score 	 0.001226993865030675
rank unigrams 	 0.16666666666666666
extracting their 	 0.2
http:\/\/arxiv.org\/abs\/1104.2086 -RRB- 	 1.0
For instance 	 0.11475409836065574
read aloud 	 0.14285714285714285
1 -LRB- 	 0.25
for . 	 0.0036101083032490976
multiple parts 	 0.07692307692307693
bilingual corpus 	 0.5
ambiguity of 	 0.125
gap between 	 1.0
an input 	 0.022727272727272728
both learn 	 0.03225806451612903
article on 	 0.034482758620689655
be decided 	 0.004219409282700422
all caps 	 0.023255813953488372
`` classification 	 0.015873015873015872
persuasion , 	 1.0
and patented 	 0.001445086705202312
easier to 	 0.25
and introducing 	 0.001445086705202312
examples have 	 0.041666666666666664
and Dragon 	 0.001445086705202312
, text 	 0.0016844469399213925
and large 	 0.001445086705202312
large-vocabulary speech 	 0.6666666666666666
always , 	 0.3333333333333333
are analytical 	 0.004149377593360996
segment in 	 0.1111111111111111
80 % 	 1.0
it ends 	 0.017094017094017096
NLP -RRB- 	 0.0851063829787234
PCFGs -LRB- 	 1.0
: By 	 0.00980392156862745
all . 	 0.023255813953488372
tied to 	 1.0
eliminate redundancy 	 0.5
displayed on-line 	 0.5
results . 	 0.09523809523809523
among others 	 0.125
dramatically reduced 	 1.0
<s> After 	 0.0023059185242121443
processing Objectives 	 0.018518518518518517
, hence 	 0.0005614823133071309
another . 	 0.23076923076923078
required the 	 0.14285714285714285
and delta-delta 	 0.002890173410404624
primarily by 	 0.5
design and 	 0.25
considered a 	 0.1111111111111111
Kurzweil 's 	 0.14285714285714285
create '' 	 0.058823529411764705
those proved 	 0.045454545454545456
of science 	 0.00089126559714795
summarization involves 	 0.02
term `` 	 0.1111111111111111
Coulthard , 	 1.0
do . 	 0.038461538461538464
explicitly mention 	 0.25
thereof -RRB- 	 1.0
shorter the 	 0.5
unmanageable . 	 1.0
of to 	 0.00089126559714795
; applies 	 0.02127659574468085
, distance 	 0.0005614823133071309
specific domains 	 0.047619047619047616
still largely 	 0.06666666666666667
effective decision-support 	 0.16666666666666666
digital camera 	 0.14285714285714285
Neural Network 	 0.25
precise ones 	 0.3333333333333333
statistics -LRB- 	 0.125
going backward 	 0.25
: how 	 0.00980392156862745
example shows 	 0.012345679012345678
numbers represent 	 0.14285714285714285
equal to 	 1.0
which various 	 0.007246376811594203
is crucial 	 0.0020325203252032522
the door 	 0.0006920415224913495
1998 The 	 0.25
fine tune 	 0.5
floods '' 	 1.0
readable English 	 0.3333333333333333
for reading 	 0.007220216606498195
growing interest 	 0.5
for male-female 	 0.0036101083032490976
and was 	 0.001445086705202312
then the 	 0.05714285714285714
to answer 	 0.00398406374501992
`` Why 	 0.015873015873015872
SR system 	 0.3333333333333333
approach -RRB- 	 0.05714285714285714
and Audio 	 0.001445086705202312
models have 	 0.038461538461538464
use multiple 	 0.013888888888888888
Neural networks 	 0.75
to reliable 	 0.0013280212483399733
an edge 	 0.015151515151515152
complex matter 	 0.041666666666666664
Question book-new 	 0.14285714285714285
more common 	 0.010526315789473684
their underlying 	 0.029411764705882353
patents . 	 1.0
<s> Grammatical 	 0.0007686395080707148
were ambiguous 	 0.024390243902439025
house through 	 0.5
<s> ﻿Natural 	 0.0007686395080707148
May 2009 	 0.5
correct part 	 0.2
large text 	 0.043478260869565216
, why 	 0.0011229646266142617
tell us 	 0.3333333333333333
collecting a 	 1.0
that spontaneous 	 0.0035460992907801418
Case = 	 1.0
spaces of 	 0.2
format called 	 0.5
1933 -LRB- 	 1.0
very dependent 	 0.024390243902439025
systems perform 	 0.008928571428571428
classifying the 	 0.2
be explained 	 0.004219409282700422
at its 	 0.014705882352941176
instead optimize 	 0.14285714285714285
and insurance 	 0.001445086705202312
answer is 	 0.06666666666666667
with medium 	 0.00546448087431694
as pseudo-pilot 	 0.003484320557491289
edge if 	 0.3333333333333333
, culminating 	 0.0005614823133071309
research in 	 0.14285714285714285
Kittredge & 	 0.5
diversity '' 	 0.25
are difficult 	 0.004149377593360996
World , 	 0.14285714285714285
Cuzco area 	 1.0
CSIS , 	 0.5
some topic 	 0.012048192771084338
keyphrase system 	 0.05263157894736842
best modern 	 0.05555555555555555
arguably function 	 0.5
Computational Linguistics 	 1.0
F-16 VISTA 	 0.5
has more 	 0.011904761904761904
minimize the 	 1.0
merge adjacent 	 1.0
though we 	 0.1
including PARRY 	 0.07142857142857142
In 2002 	 0.009523809523809525
and assessing 	 0.001445086705202312
reduced . 	 0.5
successes occurred 	 1.0
several systems 	 0.045454545454545456
evaluation would 	 0.05555555555555555
a rule 	 0.001226993865030675
step . 	 0.13333333333333333
Z '' 	 1.0
is commercially 	 0.0020325203252032522
In addition 	 0.02857142857142857
converts a 	 1.0
unigrams appear 	 0.08333333333333333
warnings from 	 1.0
very hard 	 0.024390243902439025
will need 	 0.02857142857142857
observation . 	 1.0
semantics , 	 0.2857142857142857
filtered from 	 0.3333333333333333
into a 	 0.21794871794871795
author when 	 0.3333333333333333
, cultural 	 0.0005614823133071309
BioCreative Message 	 1.0
translation components 	 0.013513513513513514
, Jonathan 	 0.0005614823133071309
process -LRB- 	 0.027777777777777776
controlled by 	 1.0
Perspectives The 	 1.0
l'assignation des 	 1.0
, inter-texual 	 0.0005614823133071309
a feasibility 	 0.001226993865030675
input-stream by 	 1.0
Intrinsic vs. 	 0.3333333333333333
Aerospace -LRB- 	 0.5
factor . 	 0.5
words surrounding 	 0.009174311926605505
text fragments 	 0.006289308176100629
, taught 	 0.0005614823133071309
word pronunciations 	 0.016666666666666666
one symbol 	 0.015384615384615385
sequences , 	 0.1111111111111111
reconfiguring them 	 1.0
that users 	 0.0035460992907801418
-RRB- up 	 0.0027100271002710027
often addressed 	 0.022727272727272728
some new 	 0.012048192771084338
new , 	 0.041666666666666664
McCarthy coined 	 1.0
to discover 	 0.0013280212483399733
keyphrase , 	 0.05263157894736842
and turns 	 0.001445086705202312
practically available 	 1.0
The relationships 	 0.005208333333333333
abstracts , 	 0.5
dictionary is 	 0.14285714285714285
levels . 	 0.045454545454545456
domain ontologies 	 0.1
<s> Current 	 0.0023059185242121443
entered John 	 0.5
meaning and 	 0.043478260869565216
person\/persons rather 	 1.0
where `` 	 0.02857142857142857
with it 	 0.01092896174863388
Cullingford , 	 1.0
indeed that 	 0.3333333333333333
and Natural 	 0.001445086705202312
saying . 	 1.0
at each 	 0.014705882352941176
paper used 	 0.09090909090909091
pollen count 	 0.07692307692307693
sentence importance 	 0.020833333333333332
because a 	 0.03333333333333333
explicit word 	 0.2
single sentence 	 0.07142857142857142
print a 	 1.0
Unlike PageRank 	 1.0
a tool 	 0.00245398773006135
or probabilities 	 0.0045045045045045045
general-purpose speech 	 1.0
interjection . 	 1.0
Authorities in 	 1.0
Army Corps 	 0.5
Stanford , 	 0.5
, researchers 	 0.0011229646266142617
around Documents 	 0.125
keywords or 	 0.5
Analysis Although 	 0.2
like English 	 0.07142857142857142
French , 	 0.125
part because 	 0.037037037037037035
's coherence 	 0.0196078431372549
beyond the 	 0.5
level is 	 0.05
reference summary 	 0.375
-LRB- and 	 0.013550135501355014
's SPHINX 	 0.0196078431372549
system comprising 	 0.010752688172043012
thus require 	 0.1
estimate , 	 0.25
on OCR 	 0.0047169811320754715
field comes 	 0.037037037037037035
NLP using 	 0.02127659574468085
characters \* 	 0.0625
or discourse 	 0.009009009009009009
Another popular 	 0.07692307692307693
for results 	 0.0036101083032490976
concerned in 	 0.2
takes as 	 0.3333333333333333
about each 	 0.025
multiple references 	 0.07692307692307693
contain periods 	 0.08333333333333333
all , 	 0.06976744186046512
Then , 	 0.4
<s> Work 	 0.0015372790161414297
is presented 	 0.0040650406504065045
with pilots 	 0.00546448087431694
useful for 	 0.21428571428571427
next token 	 0.14285714285714285
theory for 	 0.07692307692307693
language parsing 	 0.013513513513513514
Lawrence Rabiner 	 1.0
Talmy Givón 	 1.0
terms such 	 0.07692307692307693
the Baum-Welch 	 0.0006920415224913495
<s> Telephony 	 0.0007686395080707148
high quality 	 0.05555555555555555
and relevance 	 0.001445086705202312
complex formalisms 	 0.041666666666666664
a major 	 0.006134969325153374
being a 	 0.1111111111111111
spoken sentences 	 0.07142857142857142
some linguistic 	 0.012048192771084338
person , 	 0.21052631578947367
weak , 	 1.0
Why it 	 0.14285714285714285
resulting classifier 	 0.25
future . 	 0.3333333333333333
exigencies of 	 1.0
gestures , 	 0.5
video , 	 0.2
vibrates per 	 1.0
opportunities and 	 1.0
a sound 	 0.008588957055214725
phone , 	 0.5
in conjunction 	 0.003745318352059925
tests . 	 0.25
concepts . 	 0.4
In other 	 0.01904761904761905
unexpected features 	 1.0
of papers 	 0.00089126559714795
SpeechTEK Europe 	 0.5
uttered one 	 0.3333333333333333
this book 	 0.01098901098901099
more accurate 	 0.031578947368421054
heard or 	 1.0
, reading 	 0.0005614823133071309
the inter-word 	 0.0006920415224913495
the parsing 	 0.001384083044982699
<s> Stages 	 0.0007686395080707148
open problem 	 0.25
significant increases 	 0.1111111111111111
in overall 	 0.0018726591760299626
<s> Multilingual 	 0.0007686395080707148
psycholinguistics , 	 0.5
query relevant 	 0.6666666666666666
performance , 	 0.1111111111111111
Kittredge , 	 0.5
London -RRB- 	 1.0
and statistical 	 0.004335260115606936
Malcolm Coulthard 	 1.0
even though 	 0.07407407407407407
knowledge base 	 0.14814814814814814
that state 	 0.0035460992907801418
grammar Rhetoric 	 0.02702702702702703
as shallow-transfer 	 0.003484320557491289
translates to 	 1.0
typically undirected 	 0.05555555555555555
amount of 	 1.0
tagging words 	 0.04
-LRB- Lehnert 	 0.005420054200542005
input to 	 0.07317073170731707
adding citations 	 0.5
have developed 	 0.009615384615384616
EVALITA campaign 	 0.5
and Pang 	 0.001445086705202312
programmed by 	 0.5
Because ROUGE 	 0.5
validated and 	 1.0
broader and 	 1.0
into rule-based 	 0.01282051282051282
transform -LRB- 	 0.2
sent in 	 1.0
often make 	 0.022727272727272728
their products 	 0.029411764705882353
similarity scores 	 0.1
different related 	 0.02040816326530612
comparison uses 	 0.3333333333333333
features characterize 	 0.038461538461538464
-LRB- see 	 0.032520325203252036
Aided Human 	 0.3333333333333333
a science 	 0.00245398773006135
computational power 	 0.2
air -LRB- 	 0.2
-RRB- case 	 0.0027100271002710027
objectives of 	 0.5
limited number 	 0.2
Petrov , 	 1.0
and Romanseval 	 0.001445086705202312
application for 	 0.07142857142857142
question-answering engines 	 0.5
article is 	 0.034482758620689655
keep the 	 0.3333333333333333
being followed 	 0.05555555555555555
classifier that 	 0.14285714285714285
BORIS system 	 1.0
analyze a 	 0.25
a lunar 	 0.001226993865030675
objective True\/False 	 0.2
power resulting 	 0.25
meaning . 	 0.08695652173913043
assign targets 	 0.2
Type = 	 1.0
and French 	 0.001445086705202312
intermediary representation 	 0.6666666666666666
-LRB- now 	 0.008130081300813009
answer , 	 0.03333333333333333
entire words 	 0.3333333333333333
system to 	 0.053763440860215055
standard is 	 0.07142857142857142
is dependency 	 0.0020325203252032522
Understudy for 	 1.0
requirements . 	 0.5
after the 	 0.16666666666666666
each time 	 0.022222222222222223
and form 	 0.001445086705202312
approaches based 	 0.03571428571428571
went on 	 0.2
a superset 	 0.001226993865030675
all four 	 0.023255813953488372
<s> Shallow 	 0.0015372790161414297
most common 	 0.10344827586206896
that are 	 0.05319148936170213
EHR . 	 0.3333333333333333
spoken -RRB- 	 0.07142857142857142
might select 	 0.038461538461538464
product reviews 	 0.14285714285714285
widely applied 	 0.125
, facts 	 0.0005614823133071309
Every acoustic 	 1.0
how they 	 0.10344827586206896
divider , 	 1.0
area are 	 0.09090909090909091
they belong 	 0.025
Red is 	 1.0
amounts of 	 1.0
, 1952 	 0.0005614823133071309
chosen . 	 0.2
nasality , 	 1.0
linguistics , 	 0.4
, clean 	 0.0005614823133071309
Environment Canada 	 1.0
, archiving 	 0.0005614823133071309
unigrams and 	 0.08333333333333333
, addressed 	 0.0005614823133071309
in information 	 0.0018726591760299626
and bought 	 0.001445086705202312
the largest 	 0.0006920415224913495
low precision 	 0.3333333333333333
which make 	 0.014492753623188406
long research 	 0.5
among the 	 0.125
create both 	 0.058823529411764705
of whether 	 0.00089126559714795
examples for 	 0.041666666666666664
hypothetical , 	 1.0
, ASR 	 0.0005614823133071309
to ignore 	 0.0013280212483399733
which creates 	 0.007246376811594203
accuracy under 	 0.03225806451612903
taxonomies . 	 1.0
field with 	 0.037037037037037035
for his 	 0.007220216606498195
and wrote 	 0.001445086705202312
as discussions 	 0.003484320557491289
Rajman M. 	 1.0
STT '' 	 1.0
for testing 	 0.0036101083032490976
financial and 	 0.25
`` case 	 0.005291005291005291
QA Before 	 0.047619047619047616
extracting and 	 0.2
like Page\/Lex\/TextRank 	 0.03571428571428571
recognized words 	 0.16666666666666666
rule-based , 	 0.14285714285714285
sentence must 	 0.020833333333333332
automotive maintenance 	 1.0
of translating 	 0.00089126559714795
than computers 	 0.022222222222222223
reported accuracy 	 0.2
about which 	 0.025
If there 	 0.1
<s> Topic 	 0.0007686395080707148
not to 	 0.008928571428571428
clear that 	 0.25
template . 	 0.25
apparent from 	 1.0
<s> One 	 0.009223674096848577
about it 	 0.025
increasingly complex 	 0.3333333333333333
region in 	 1.0
computer , 	 0.022727272727272728
from machine 	 0.019230769230769232
available . 	 0.17647058823529413
adjective -LRB- 	 0.14285714285714285
to their 	 0.0026560424966799467
most later 	 0.017241379310344827
However sentence 	 0.02702702702702703
material may 	 0.5
accuracy can 	 0.03225806451612903
and there 	 0.005780346820809248
of 1956 	 0.00089126559714795
possibly others 	 0.5
style , 	 0.5
relations among 	 0.16666666666666666
marks and 	 0.25
search corpus 	 0.09090909090909091
resort to 	 1.0
accepted , 	 1.0
milliseconds -RRB- 	 0.5
for approximating 	 0.0036101083032490976
One task 	 0.07692307692307693
acoustic noise 	 0.3333333333333333
n't end 	 0.25
parser generators 	 0.0625
Vietnamese , 	 1.0
-LRB- in 	 0.005420054200542005
diversity as 	 0.25
a document 	 0.008588957055214725
NLG researchers 	 0.047619047619047616
be different 	 0.004219409282700422
<s> Recognizing 	 0.0007686395080707148
it should 	 0.008547008547008548
segment text 	 0.2222222222222222
from natural 	 0.009615384615384616
the emotional 	 0.001384083044982699
if documents 	 0.03571428571428571
large amount 	 0.043478260869565216
of Hidden 	 0.00089126559714795
overall system 	 0.16666666666666666
instance text 	 0.07142857142857142
and comprehension 	 0.001445086705202312
probabilistic rules 	 0.14285714285714285
otherwise achieves 	 0.5
references , 	 0.25
Amharic and 	 1.0
applications . 	 0.16
larger sequences 	 0.0625
not pursued 	 0.008928571428571428
features involve 	 0.038461538461538464
a sentiment 	 0.00245398773006135
'' text 	 0.005154639175257732
, anthropology 	 0.0005614823133071309
deal of 	 0.25
system used 	 0.010752688172043012
errors -LRB- 	 0.4
source sentence 	 0.08333333333333333
text lacks 	 0.006289308176100629
require minimal 	 0.045454545454545456
away unlikely 	 0.5
200 billion 	 0.5
contain words 	 0.08333333333333333
Advanced reasoning 	 0.2
Margaret Wetherell 	 1.0
hand-written rules 	 0.8571428571428571
Carmen Rosa 	 1.0
produce one 	 0.045454545454545456
learning procedures 	 0.046511627906976744
avoid confusion 	 1.0
of news 	 0.0017825311942959
have multiple 	 0.009615384615384616
Search to 	 0.5
then run 	 0.02857142857142857
Bell Telephone 	 1.0
engines to 	 0.3333333333333333
are generally 	 0.016597510373443983
be confused 	 0.004219409282700422
technology Sensory 	 0.045454545454545456
a trillion-word 	 0.001226993865030675
user-specified fraction 	 0.5
typically grouped 	 0.05555555555555555
bill payment 	 0.5
conversion . 	 0.3333333333333333
typical machine-learning-based 	 0.1111111111111111
<s> Substantial 	 0.0007686395080707148
insufficient . 	 1.0
several teams 	 0.045454545454545456
, SAM 	 0.0005614823133071309
' at 	 0.05263157894736842
best model 	 0.05555555555555555
Janet Holmes 	 0.5
entire content 	 0.3333333333333333
a syntactic 	 0.001226993865030675
Recognition -LRB- 	 0.125
successfully for 	 0.3333333333333333
collections of 	 0.25
other than 	 0.014285714285714285
words and 	 0.06422018348623854
coreference resolution 	 1.0
groups : 	 0.2
the additional 	 0.0006920415224913495
clarify a 	 1.0
ongoing issue 	 0.5
was conducted 	 0.025974025974025976
understanding also 	 0.030303030303030304
procedure still 	 0.3333333333333333
type validity 	 0.07142857142857142
but the 	 0.04411764705882353
input can 	 0.024390243902439025
which simply 	 0.007246376811594203
particular feature 	 0.07692307692307693
grammatical parts 	 0.09090909090909091
evaluations such 	 0.16666666666666666
, Why 	 0.0005614823133071309
to classify 	 0.0013280212483399733
would need 	 0.018867924528301886
December 2010 	 1.0
and language 	 0.004335260115606936
language to 	 0.02702702702702703
and previously-written 	 0.001445086705202312
Leading software 	 1.0
much of 	 0.09090909090909091
approached in 	 0.5
, video 	 0.0011229646266142617
rate the 	 0.09090909090909091
captured by 	 1.0
beings , 	 1.0
are informative 	 0.004149377593360996
He took 	 0.125
word error 	 0.016666666666666666
a meaning 	 0.00245398773006135
As with 	 0.05555555555555555
France , 	 0.5
by further 	 0.005714285714285714
trigram , 	 0.6666666666666666
`` correct 	 0.005291005291005291
funding continued 	 0.125
is working 	 0.0040650406504065045
the statistical 	 0.001384083044982699
a text-understanding 	 0.001226993865030675
By having 	 0.3333333333333333
the strings 	 0.0006920415224913495
psycholinguistics when 	 0.5
components that 	 0.2
first stage 	 0.030303030303030304
the frequency 	 0.0006920415224913495
algorithm essentially 	 0.03571428571428571
navigation systems 	 0.5
found no 	 0.07142857142857142
, punctuation 	 0.0005614823133071309
compare generated 	 0.14285714285714285
annotation process 	 0.25
Current difficulties 	 0.2
personnel . 	 1.0
training systems 	 0.03571428571428571
associated score 	 0.25
<s> Users 	 0.0007686395080707148
machine -RRB- 	 0.012658227848101266
exactly how 	 0.3333333333333333
knowledge but 	 0.037037037037037035
advantage that 	 0.2
automatic summarization 	 0.08695652173913043
recognition instead 	 0.008264462809917356
sometimes be 	 0.07692307692307693
got about 	 1.0
an active 	 0.007575757575757576
, whereas 	 0.0011229646266142617
this understanding 	 0.01098901098901099
, determine 	 0.003368893879842785
is as 	 0.0020325203252032522
one language 	 0.03076923076923077
by describing 	 0.005714285714285714
to represent 	 0.0013280212483399733
`` be 	 0.010582010582010581
only some 	 0.05263157894736842
applications Robotics 	 0.04
an RCA 	 0.015151515151515152
involve grammar 	 0.16666666666666666
relevant . 	 0.14285714285714285
permuted automatically 	 1.0
IMR during 	 0.5
mentioned above 	 0.16666666666666666
enables several 	 1.0
or negative 	 0.009009009009009009
Russian sentences 	 1.0
expert that 	 1.0
artificial languages 	 0.09090909090909091
The state-of-the-art 	 0.005208333333333333
robustness in 	 0.25
has fueled 	 0.011904761904761904
Development Activity 	 1.0
a fancy 	 0.001226993865030675
specific strengths 	 0.047619047619047616
Pang and 	 0.3333333333333333
specialised document 	 0.5
allow spoken 	 0.2
-RRB- that 	 0.005420054200542005
many keyphrases 	 0.019230769230769232
trends of 	 1.0
weapon critical 	 0.5
so the 	 0.23333333333333334
to an 	 0.0013280212483399733
some measure 	 0.012048192771084338
an automatic 	 0.015151515151515152
an approximation 	 0.015151515151515152
observe patterns 	 1.0
zero '' 	 1.0
followed . 	 0.25
classifier , 	 0.14285714285714285
of building 	 0.00089126559714795
Imagine you 	 1.0
when reading 	 0.02857142857142857
author of 	 0.3333333333333333
a cryptanalyst 	 0.001226993865030675
to place 	 0.0026560424966799467
McDonald -LRB- 	 1.0
even in 	 0.07407407407407407
plain text 	 1.0
; the 	 0.0851063829787234
Chomskyan theories 	 1.0
not invented 	 0.008928571428571428
going over 	 0.25
See chart 	 0.16666666666666666
Vocabulary Size 	 0.3333333333333333
EMR -LRB- 	 0.3333333333333333
or automatically 	 0.009009009009009009
Church independently 	 0.3333333333333333
vectors , 	 0.3333333333333333
estate advertisements 	 1.0
of London 	 0.00089126559714795
into more 	 0.02564102564102564
example-generation strategy 	 1.0
important parts 	 0.0625
Content determination 	 1.0
get ranked 	 0.14285714285714285
abruptly at 	 1.0
textbook is 	 0.5
words were 	 0.01834862385321101
discourse analysis 	 0.2222222222222222
a natural 	 0.006134969325153374
be reached 	 0.004219409282700422
together the 	 0.125
noun '' 	 0.07142857142857142
media , 	 0.5
searching for 	 0.3333333333333333
the history 	 0.0006920415224913495
Henry Widdowson 	 0.5
named IEEE 	 0.14285714285714285
and English 	 0.001445086705202312
human beings 	 0.021739130434782608
translation simultaneously 	 0.013513513513513514
red . 	 1.0
at emotional 	 0.014705882352941176
and captioned 	 0.001445086705202312
short time-scales 	 0.125
to other 	 0.0013280212483399733
features -LRB- 	 0.038461538461538464
user interface 	 0.07142857142857142
% to 	 0.05128205128205128
in system 	 0.0018726591760299626
interest was 	 0.09090909090909091
request can 	 1.0
bases , 	 1.0
algorithms for 	 0.11428571428571428
logical , 	 0.16666666666666666
, 5000 	 0.0005614823133071309
well the 	 0.03571428571428571
on bilingual 	 0.0047169811320754715
doctors make 	 0.3333333333333333
would contain 	 0.018867924528301886
speech -RRB- 	 0.02631578947368421
and recall 	 0.002890173410404624
rich languages 	 0.2
keyboard . 	 0.3333333333333333
sixty Russian 	 1.0
sentences that 	 0.06578947368421052
find an 	 0.15384615384615385
Force for 	 0.5
about machine 	 0.025
ontology requires 	 0.5
these heuristics 	 0.023809523809523808
having ` 	 0.2
ROUGE -LRB- 	 0.2
character is 	 0.09090909090909091
3 +4 	 0.2
seem completely 	 0.5
The poor 	 0.005208333333333333
address this 	 0.25
In a 	 0.01904761904761905
syntactic and 	 0.15384615384615385
evaluations are 	 0.3333333333333333
Tauschek had 	 0.5
showing evidence 	 0.5
Thai and 	 0.5
decisions about 	 0.2
10msec section 	 0.5
optimal match 	 1.0
source - 	 0.041666666666666664
must also 	 0.07142857142857142
<s> Lexical 	 0.0015372790161414297
between text 	 0.02564102564102564
, commanding 	 0.0005614823133071309
for NLP 	 0.0036101083032490976
are ranked 	 0.004149377593360996
Viterbi algorithm 	 1.0
nature of 	 1.0
systems read 	 0.008928571428571428
A good 	 0.02
arise because 	 1.0
as semiotics 	 0.003484320557491289
limited . 	 0.2
; that 	 0.02127659574468085
textual weather 	 0.2
a filter 	 0.001226993865030675
became part 	 0.2
of accent 	 0.00089126559714795
by looking 	 0.005714285714285714
human-written texts 	 0.5
occurrence of 	 0.5
The management 	 0.005208333333333333
modeling approach 	 0.14285714285714285
will be 	 0.2571428571428571
and similar 	 0.001445086705202312
to its 	 0.0013280212483399733
on smaller 	 0.0047169811320754715
, there 	 0.006176305446378439
must make 	 0.07142857142857142
e.g. vocabulary 	 0.017857142857142856
databases -RRB- 	 0.125
safety critical 	 1.0
and\/or aural 	 0.3333333333333333
context in 	 0.030303030303030304
might learn 	 0.038461538461538464
Energy -LRB- 	 1.0
-RRB- involved 	 0.0027100271002710027
be correct 	 0.004219409282700422
neutral or 	 0.5
as each 	 0.003484320557491289
restricted `` 	 0.25
from large 	 0.009615384615384616
summary 's 	 0.047619047619047616
local document 	 0.3333333333333333
contain the 	 0.16666666666666666
approaches and 	 0.03571428571428571
Understanding Conferences 	 0.5
typically a 	 0.05555555555555555
<s> Hence 	 0.0015372790161414297
is slow 	 0.0020325203252032522
<s> n 	 0.0007686395080707148
is one 	 0.012195121951219513
principles which 	 1.0
repetitive . 	 0.5
construction of 	 0.6666666666666666
's idea 	 0.0196078431372549
context have 	 0.030303030303030304
a meaningful 	 0.00245398773006135
used through 	 0.008849557522123894
-LRB- grammatical 	 0.0027100271002710027
conjunction , 	 0.3333333333333333
improved . 	 0.25
have resulted 	 0.009615384615384616
human languages 	 0.021739130434782608
<s> Winograd 	 0.0007686395080707148
are certain 	 0.004149377593360996
centrality '' 	 0.5
are obtained 	 0.008298755186721992
evaluations have 	 0.16666666666666666
if indeed 	 0.03571428571428571
reliable results 	 0.5
It converted 	 0.02631578947368421
integrated into 	 0.3333333333333333
may happen 	 0.019230769230769232
would still 	 0.018867924528301886
-LRB- CSR 	 0.005420054200542005
least partly 	 0.2
expressed like 	 0.16666666666666666
two conflicting 	 0.034482758620689655
applies a 	 0.2857142857142857
J. , 	 0.6666666666666666
-LRB- Harris 	 0.005420054200542005
data redundancy 	 0.012987012987012988
the sampling 	 0.0006920415224913495
to correct 	 0.0013280212483399733
dividing written 	 0.3333333333333333
supervised keyphrase 	 0.125
method based 	 0.125
include overt 	 0.037037037037037035
really was 	 1.0
states -RRB- 	 0.25
of January 	 0.00089126559714795
has published 	 0.011904761904761904
I would 	 1.0
it takes 	 0.008547008547008548
extractive methods 	 0.14285714285714285
worse if 	 1.0
characterize keyphrases 	 0.5
to normalize 	 0.0013280212483399733
points of 	 0.5
run on 	 0.2
Martin , 	 0.5
intelligence systems 	 0.125
speaker deliberately 	 0.05555555555555555
sentence structure 	 0.020833333333333332
that depend 	 0.0035460992907801418
they rely 	 0.025
by Junqua 	 0.005714285714285714
step , 	 0.13333333333333333
for each 	 0.02527075812274368
parameter estimation 	 1.0
system such 	 0.010752688172043012
system and 	 0.03225806451612903
the unigram 	 0.0006920415224913495
machine '' 	 0.012658227848101266
be generated 	 0.004219409282700422
act as 	 0.75
a parser 	 0.0036809815950920245
complicated backgrounds 	 0.3333333333333333
only to 	 0.02631578947368421
information usually 	 0.021739130434782608
deferred speech 	 1.0
is an 	 0.02032520325203252
refined score 	 1.0
There has 	 0.18181818181818182
perspective in 	 0.25
thanks to 	 1.0
are usually 	 0.012448132780082987
can do 	 0.011049723756906077
from an 	 0.009615384615384616
such statistical 	 0.008130081300813009
and generate 	 0.001445086705202312
requirements of 	 0.5
linguistic informational 	 0.0625
requires in-depth 	 0.0625
properties as 	 0.25
into one 	 0.02564102564102564
monetary value 	 1.0
question domain 	 0.023809523809523808
ultimately want 	 1.0
and Nelson 	 0.001445086705202312
Holmes , 	 1.0
unsupervised approach 	 0.125
Early versions 	 0.5
are those 	 0.004149377593360996
concept -LRB- 	 0.25
modeling and 	 0.14285714285714285
summarization based 	 0.02
improving output 	 1.0
new token 	 0.041666666666666664
the EHR 	 0.0006920415224913495
extractor might 	 0.5
an article 	 0.015151515151515152
The same 	 0.010416666666666666
agree -RRB- 	 0.3333333333333333
and knowledge 	 0.001445086705202312
for inclusion 	 0.0036101083032490976
say ` 	 0.2857142857142857
methods presented 	 0.022727272727272728
Summarization -RRB- 	 0.5
random surfer 	 0.14285714285714285
them , 	 0.21052631578947367
do n't 	 0.07692307692307693
city . 	 1.0
requires specialized 	 0.0625
many debates 	 0.019230769230769232
-LRB- p. 	 0.0027100271002710027
from advertisements 	 0.009615384615384616
, preposition 	 0.0005614823133071309
<s> Hulth 	 0.0023059185242121443
yesterday with 	 0.6666666666666666
the reduction 	 0.0006920415224913495
algebra . 	 0.5
in 1953 	 0.0018726591760299626
complex NLP 	 0.08333333333333333
machine processes 	 0.012658227848101266
Page , 	 1.0
to enumerate 	 0.0013280212483399733
developed . 	 0.07692307692307693
Further restricted-domain 	 0.3333333333333333
, will 	 0.0011229646266142617
Correct answers 	 1.0
`` understanding 	 0.005291005291005291
an English-like 	 0.007575757575757576
with Swedish 	 0.00546448087431694
Extracted sentences 	 1.0
will mention 	 0.02857142857142857
script . 	 0.5
Patent 2,663,758 	 0.3333333333333333
automated , 	 0.14285714285714285
written texts 	 0.07692307692307693
concurrently with 	 1.0
demonstration that 	 0.2
OCR systems 	 0.08163265306122448
the norm 	 0.0006920415224913495
after applying 	 0.08333333333333333
overlap should 	 0.25
of multimedia 	 0.00089126559714795
variance on 	 1.0
; a 	 0.02127659574468085
complex question 	 0.041666666666666664
strengths and 	 0.5
, turns 	 0.0005614823133071309
to remain 	 0.0013280212483399733
as described 	 0.006968641114982578
of individual 	 0.0017825311942959
omitted -RRB- 	 1.0
produce consonants 	 0.045454545454545456
potentially unlimited 	 0.3333333333333333
, with 	 0.004491858506457047
some states 	 0.012048192771084338
schemata . 	 1.0
were rare 	 0.024390243902439025
NLG applications 	 0.047619047619047616
The following 	 0.020833333333333332
the particular 	 0.001384083044982699
-LRB- perhaps 	 0.0027100271002710027
the user 	 0.0034602076124567475
promising line 	 1.0
`` processing 	 0.010582010582010581
began to 	 0.5714285714285714
<s> People 	 0.0007686395080707148
would also 	 0.018867924528301886
hour or 	 1.0
, 1979 	 0.0005614823133071309
are accepted 	 0.004149377593360996
emerge that 	 1.0
is considered 	 0.0040650406504065045
Envelopes may 	 1.0
semantic equivalence 	 0.047619047619047616
encourage systems 	 1.0
interactions between 	 1.0
and do 	 0.001445086705202312
application domain 	 0.07142857142857142
Creating referring 	 0.5
the only 	 0.0006920415224913495
Statistics are 	 0.3333333333333333
must appear 	 0.07142857142857142
often as 	 0.045454545454545456
'' by 	 0.02577319587628866
, headlines 	 0.0005614823133071309
words as 	 0.009174311926605505
of estimating 	 0.00089126559714795
summaries available 	 0.023255813953488372
Use of 	 0.5
were `` 	 0.024390243902439025
including , 	 0.07142857142857142
are created 	 0.012448132780082987
very early 	 0.024390243902439025
problem of 	 0.18181818181818182
optimized for 	 1.0
Some examples 	 0.047619047619047616
edit distances 	 1.0
is because 	 0.0020325203252032522
areas , 	 0.16666666666666666
the KEA 	 0.0006920415224913495
match each 	 0.16666666666666666
mention in 	 0.3333333333333333
Voice Control 	 0.2
languages with 	 0.02
Another resource 	 0.07692307692307693
Granada Different 	 0.5
triples or 	 0.3333333333333333
between those 	 0.02564102564102564
and right-most 	 0.001445086705202312
's GenEx 	 0.0196078431372549
, spoken 	 0.0005614823133071309
of training 	 0.0035650623885918
methods when 	 0.022727272727272728
segments -LRB- 	 0.2
<s> History 	 0.0015372790161414297
the abbreviation 	 0.0006920415224913495
and linguistic 	 0.001445086705202312
low pollen 	 0.3333333333333333
is meaningful 	 0.0020325203252032522
in other 	 0.009363295880149813
extract subjective 	 0.25
- based 	 0.1875
& Lehrberger 	 0.125
difficult than 	 0.10714285714285714
had to 	 0.07142857142857142
was drawn 	 0.012987012987012988
vertices in 	 0.1111111111111111
, LinguaSys 	 0.0005614823133071309
a similar 	 0.001226993865030675
generated summaries 	 0.06666666666666667
represented by 	 0.3333333333333333
select the 	 0.3333333333333333
repetitive stress 	 0.5
-RRB- ; 	 0.02168021680216802
ongoing as 	 0.5
reliability , 	 0.5
Designing a 	 1.0
polarity '' 	 0.25
stochastic , 	 0.25
evident that 	 0.5
expression generation 	 0.1
interlingua . 	 1.0
usually operate 	 0.03125
and semi-supervised 	 0.001445086705202312
against a 	 0.2
metrics correlate 	 0.1111111111111111
accuracy in 	 0.03225806451612903
, Deirdre 	 0.0005614823133071309
dealing with 	 1.0
that detected 	 0.0035460992907801418
Meaningful Use 	 1.0
be challenged 	 0.004219409282700422
best single 	 0.05555555555555555
the -LRB- 	 0.0006920415224913495
from other 	 0.009615384615384616
the Apollo 	 0.0006920415224913495
see Inter-rater 	 0.05
expression , 	 0.2
hours . 	 0.5
to wreck 	 0.0013280212483399733
summarization methods 	 0.02
software vendors 	 0.037037037037037035
co-occurring neighbors 	 1.0
of oral 	 0.00089126559714795
so that 	 0.2
the acoustic 	 0.0006920415224913495
, depends 	 0.0005614823133071309
in 2006 	 0.0018726591760299626
dictator is 	 1.0
; By 	 0.02127659574468085
confusable words 	 1.0
intrinsic properties 	 0.25
differing contexts 	 1.0
features that 	 0.07692307692307693
part that 	 0.037037037037037035
<s> Turney 	 0.0007686395080707148
have specific 	 0.009615384615384616
include Single 	 0.037037037037037035
the augmented 	 0.0006920415224913495
data available 	 0.03896103896103896
the inherent 	 0.0006920415224913495
and thus 	 0.004335260115606936
Cognitive psychology 	 0.3333333333333333
the distinctive 	 0.0006920415224913495
the Parseval\/GEIG 	 0.0006920415224913495
several summarization 	 0.045454545454545456
controlling flight 	 1.0
and address 	 0.001445086705202312
essentially perfectly 	 0.125
and exclamation 	 0.001445086705202312
involves `` 	 0.1
to foster 	 0.0013280212483399733
these actions 	 0.023809523809523808
system output 	 0.010752688172043012
an interest 	 0.007575757575757576
`` a 	 0.005291005291005291
semi - 	 1.0
understand why 	 0.14285714285714285
Longacre , 	 1.0
Charniak points 	 1.0
1964 to 	 1.0
summaries . 	 0.13953488372093023
Such models 	 0.25
Recognizing the 	 1.0
the computer 	 0.001384083044982699
, Stef 	 0.0005614823133071309
algorithms that 	 0.05714285714285714
particular part 	 0.07692307692307693
actual NLP 	 0.2
the generated 	 0.001384083044982699
was much 	 0.025974025974025976
reasoning . 	 0.14285714285714285
single source 	 0.14285714285714285
, stochastic 	 0.0005614823133071309
to build 	 0.0013280212483399733
efforts based 	 0.14285714285714285
good results 	 0.07692307692307693
In contrast 	 0.047619047619047616
` caught 	 0.0625
Who is 	 0.5
been carried 	 0.014705882352941176
the Reader 	 0.0006920415224913495
document may 	 0.05555555555555555
to process 	 0.00398406374501992
program to 	 0.09090909090909091
-RRB- Acoustical 	 0.0027100271002710027
nouns -LRB- 	 0.1111111111111111
speech single 	 0.006578947368421052
these tasks 	 0.047619047619047616
augmented transition 	 1.0
can condense 	 0.0055248618784530384
than others 	 0.044444444444444446
the speech-enabled 	 0.0006920415224913495
returned from 	 0.5
six -LRB- 	 0.5
in languages 	 0.0018726591760299626
sciences , 	 0.5
-LRB- on 	 0.005420054200542005
a known 	 0.00245398773006135
corresponding to 	 0.3333333333333333
same in-depth 	 0.04
at phrasing 	 0.014705882352941176
variety of 	 1.0
in an 	 0.0149812734082397
included a 	 0.125
vs. manual 	 0.08333333333333333
discussion groups 	 0.5
<s> Competing 	 0.0007686395080707148
approach , 	 0.05714285714285714
Apollo moon 	 1.0
more such 	 0.010526315789473684
using neural 	 0.01694915254237288
a solved 	 0.00245398773006135
simple substitution 	 0.038461538461538464
cases , 	 0.3888888888888889
V.J. , 	 1.0
1990s there 	 0.3333333333333333
semantic analysis 	 0.09523809523809523
large portion 	 0.043478260869565216
pronouns with 	 0.5
draws on 	 1.0
improvements in 	 0.5
then used 	 0.02857142857142857
an expression 	 0.007575757575757576
implicitly determines 	 1.0
keyphrase extractor 	 0.05263157894736842
Spanish do 	 0.5
fancy statistics 	 1.0
the subjectivity 	 0.0006920415224913495
characters to 	 0.0625
computers , 	 0.2222222222222222
words . 	 0.14678899082568808
word being 	 0.03333333333333333
with 17 	 0.00546448087431694
trainee controller 	 1.0
real-world applications 	 0.16666666666666666
even , 	 0.037037037037037035
integrated part 	 0.3333333333333333
translating . 	 0.25
in psycholinguistics 	 0.0018726591760299626
become repetitive 	 0.25
these , 	 0.047619047619047616
-RRB- Video 	 0.0027100271002710027
as individual 	 0.003484320557491289
Processor is 	 1.0
POS categories 	 0.07692307692307693
different similarity 	 0.02040816326530612
machine-learning approach 	 0.25
Fairclough , 	 1.0
documents might 	 0.02631578947368421
we create 	 0.044444444444444446
as sounds 	 0.003484320557491289
spaces , 	 0.2
of entities 	 0.00089126559714795
use vocal 	 0.013888888888888888
a naval 	 0.001226993865030675
by induction 	 0.005714285714285714
systems must 	 0.008928571428571428
<s> Its 	 0.0015372790161414297
a fair 	 0.001226993865030675
analyze all 	 0.25
text normally 	 0.006289308176100629
on contrastive 	 0.0047169811320754715
physicians who 	 1.0
rule-based and 	 0.14285714285714285
contains the 	 0.2
the automatic 	 0.001384083044982699
applications it 	 0.04
are actually 	 0.004149377593360996
pairs , 	 1.0
languages words 	 0.02
languages can 	 0.04
of scholars 	 0.00089126559714795
segmentation : 	 0.09090909090909091
Voice Command 	 0.2
a learning 	 0.00245398773006135
manually assigned 	 0.25
being said 	 0.05555555555555555
large collections 	 0.043478260869565216
contexts and 	 0.14285714285714285
be studied 	 0.004219409282700422
LexRank uses 	 0.08333333333333333
<s> Big 	 0.0007686395080707148
for heavily 	 0.0036101083032490976
disambiguate the 	 0.3333333333333333
the opportunity 	 0.0006920415224913495
not able 	 0.008928571428571428
training '' 	 0.03571428571428571
eat '' 	 1.0
and encouraged 	 0.001445086705202312
detection and 	 0.5
area includes 	 0.09090909090909091
been developed 	 0.014705882352941176
answering methods 	 0.08333333333333333
assertions in 	 0.5
while some 	 0.05
Sandra Thompson 	 1.0
they differ 	 0.025
to encourage 	 0.0013280212483399733
e.g. The 	 0.03571428571428571
matrix , 	 1.0
still contains 	 0.06666666666666667
many person-years 	 0.019230769230769232
parser with 	 0.0625
apply to 	 0.4
, follow-the-bouncing-ball 	 0.0005614823133071309
combination with 	 0.4
course , 	 0.3333333333333333
measures try 	 0.16666666666666666
several years 	 0.09090909090909091
perceptions are 	 1.0
the geological 	 0.0006920415224913495
cases where 	 0.16666666666666666
high noise 	 0.05555555555555555
sequences of 	 0.3333333333333333
could thus 	 0.0625
products , 	 0.25
word context 	 0.016666666666666666
by both 	 0.005714285714285714
Why do 	 0.14285714285714285
forms than 	 0.16666666666666666
Church 's 	 0.3333333333333333
greatly . 	 0.2857142857142857
wreck a 	 1.0
analysis which 	 0.015384615384615385
whom -RRB- 	 0.5
distinctive groups 	 0.5
is semantic 	 0.0020325203252032522
where formal 	 0.02857142857142857
assumed that 	 1.0
the ability 	 0.001384083044982699
and synthesis 	 0.001445086705202312
Turney and 	 0.2222222222222222
: Instead 	 0.00980392156862745
Deferred speech 	 1.0
years later 	 0.14285714285714285
? -RRB- 	 0.125
text summaries 	 0.006289308176100629
of comprehensive 	 0.00089126559714795
construct an 	 0.3333333333333333
document retrieval 	 0.027777777777777776
: Why 	 0.00980392156862745
networks can 	 0.07142857142857142
`` Dogged 	 0.005291005291005291
the conceptual 	 0.0006920415224913495
The machine-learning 	 0.005208333333333333
consumption -RRB- 	 1.0
+4 -RRB- 	 1.0
dispense with 	 1.0
problem . 	 0.22727272727272727
itself as 	 0.2
environment , 	 0.16666666666666666
assessed mainly 	 1.0
see it 	 0.05
little interference 	 0.3333333333333333
cases are 	 0.05555555555555555
consistent terminology 	 1.0
per page 	 0.25
be representative 	 0.004219409282700422
moved across 	 1.0
embedded . 	 0.25
nuggets of 	 1.0
simple data 	 0.038461538461538464
text unit 	 0.006289308176100629
included in 	 0.125
methods , 	 0.09090909090909091
generally more 	 0.18181818181818182
the class 	 0.0006920415224913495
applying some 	 0.25
most suitable 	 0.017241379310344827
be '' 	 0.008438818565400843
these every 	 0.023809523809523808
within another 	 0.05555555555555555
statistically-based speech 	 1.0
advanced -LRB- 	 0.2
CyberEmotions project 	 1.0
formalisms are 	 0.5
document -LRB- 	 0.05555555555555555
whether each 	 0.07692307692307693
p. 32 	 1.0
the affect 	 0.0006920415224913495
\/ F-16 	 0.3333333333333333
`` corpora 	 0.005291005291005291
be included 	 0.004219409282700422
techniques from 	 0.043478260869565216
imagery , 	 1.0
rate still 	 0.09090909090909091
<s> Are 	 0.0007686395080707148
lower levels 	 0.4
ambiguity in 	 0.125
associated word 	 0.25
<s> HMMs 	 0.0023059185242121443
made WebOCR 	 0.0625
connects to 	 1.0
proposed what 	 0.1111111111111111
spun it 	 1.0
: extraction 	 0.00980392156862745
Higher rates 	 1.0
other punctuation 	 0.014285714285714285
models derived 	 0.038461538461538464
different grammatical 	 0.02040816326530612
compared syntactic 	 0.14285714285714285
in summary 	 0.003745318352059925
theory to 	 0.07692307692307693
United Nations 	 0.2222222222222222
bill stub 	 0.5
meets the 	 0.5
language of 	 0.006756756756756757
the record 	 0.0006920415224913495
emphasize different 	 1.0
difficult , 	 0.03571428571428571
texts or 	 0.058823529411764705
its communicative 	 0.02857142857142857
perform automated 	 0.09090909090909091
automatic evaluation 	 0.13043478260869565
produce models 	 0.045454545454545456
The loss 	 0.005208333333333333
some training 	 0.012048192771084338
comparative depths 	 1.0
semantic model 	 0.047619047619047616
unsupervised , 	 0.125
: syntax 	 0.00980392156862745
attaching real-valued 	 1.0
degraded-images , 	 1.0
, evaluation 	 0.0005614823133071309
bills returned 	 1.0
maximum mutual 	 0.16666666666666666
looks for 	 0.25
sequences -LRB- 	 0.1111111111111111
linguistic competence 	 0.0625
SHRDLU provided 	 0.16666666666666666
task requiring 	 0.023809523809523808
Re-encoding this 	 1.0
each observed 	 0.022222222222222223
from non 	 0.009615384615384616
computer science 	 0.09090909090909091
example The 	 0.012345679012345678
end . 	 0.125
of named 	 0.00089126559714795
produce textual 	 0.045454545454545456
algorithms have 	 0.08571428571428572
networks . 	 0.21428571428571427
