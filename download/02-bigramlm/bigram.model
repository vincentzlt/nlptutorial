, maybe 	 0.0005614823133071309
free beer 	 0.25
with references 	 0.00546448087431694
for plural 	 0.0036101083032490976
Dyer developed 	 1.0
processing problems 	 0.018518518518518517
'' for 	 0.005154639175257732
statistical natural 	 0.030303030303030304
together in 	 0.125
My head 	 1.0
been devised 	 0.014705882352941176
of features 	 0.00089126559714795
accommodate left 	 0.2
common plural 	 0.04
where much 	 0.02857142857142857
of physics 	 0.00089126559714795
English sentences 	 0.02702702702702703
-RRB- can 	 0.008130081300813009
phrase that 	 0.1
<s> CLAWS 	 0.0015372790161414297
Other taggers 	 0.14285714285714285
procedures can 	 0.5
10,000 to 	 1.0
the National 	 0.001384083044982699
service , 	 0.4
discouraged the 	 1.0
relationships can 	 0.16666666666666666
system needs 	 0.043010752688172046
Semantic analysis 	 0.3333333333333333
& Online 	 0.125
specialised expertise 	 0.5
dependent system 	 0.3333333333333333
the difficulty 	 0.0006920415224913495
that Piron 	 0.0035460992907801418
n-gram ROUGE 	 0.5
term language 	 0.05555555555555555
especially if 	 0.06666666666666667
that depend 	 0.0035460992907801418
coherent sequences 	 0.2
while the 	 0.05
cursive characters 	 0.2
is included 	 0.0020325203252032522
, during 	 0.0005614823133071309
Vulcan later 	 0.5
in 2006 	 0.0018726591760299626
deferred speech 	 1.0
made indifferent 	 0.0625
metrics are 	 0.1111111111111111
be modified 	 0.004219409282700422
which generate 	 0.021739130434782608
life ? 	 0.25
beyond the 	 0.5
same in 	 0.04
recognize equivalent 	 0.1111111111111111
a system 	 0.012269938650306749
Voice Input 	 0.2
procedures used 	 0.25
depth of 	 0.3333333333333333
interface commercially 	 0.25
for gestures 	 0.0036101083032490976
geological analysis 	 1.0
presentation of 	 1.0
this paper 	 0.01098901098901099
, Court 	 0.0005614823133071309
Wordnet lexicon 	 1.0
, sets 	 0.0005614823133071309
even for 	 0.037037037037037035
anomalies . 	 1.0
extraction can 	 0.03225806451612903
selected based 	 0.5
Extraction techniques 	 0.3333333333333333
difference is 	 0.25
a strength 	 0.001226993865030675
a useful 	 0.001226993865030675
Microsoft Corporation 	 0.5
the ambiguous 	 0.001384083044982699
sentences from 	 0.02631578947368421
conversations such 	 0.3333333333333333
understanding '' 	 0.06060606060606061
as just 	 0.003484320557491289
was one 	 0.012987012987012988
garden-path sentences 	 1.0
summarization Machine 	 0.02
vowels . 	 0.3333333333333333
Algorithms which 	 0.5
when given 	 0.05714285714285714
that alone 	 0.0035460992907801418
spend much 	 1.0
real-valued vectors 	 0.3333333333333333
wave and 	 0.1111111111111111
Studies -RRB- 	 1.0
sophisticated understanding 	 0.14285714285714285
, meanings 	 0.0005614823133071309
The product 	 0.010416666666666666
original scanned 	 0.07692307692307693
, PAM 	 0.0005614823133071309
convey meaning 	 0.3333333333333333
or paragraphs 	 0.009009009009009009
Man bites 	 0.5
the ALPAC 	 0.001384083044982699
of sentiment 	 0.004456327985739751
If `` 	 0.1
classification error 	 0.058823529411764705
Evaluation As 	 0.1111111111111111
is expected 	 0.0020325203252032522
successful . 	 0.1111111111111111
error -LRB- 	 0.16666666666666666
upgrade a 	 1.0
slow speech 	 0.5
generate examples 	 0.05555555555555555
line as 	 0.3333333333333333
of 200 	 0.00089126559714795
a neural 	 0.001226993865030675
speaking , 	 0.625
recognizing difficult 	 0.2
context and 	 0.12121212121212122
, rather 	 0.0011229646266142617
caps -RRB- 	 1.0
distorted by 	 0.5
the EVALITA 	 0.0006920415224913495
ontologies ' 	 0.16666666666666666
and common-sense 	 0.001445086705202312
some nice 	 0.012048192771084338
<s> Applications 	 0.0015372790161414297
names , 	 0.2857142857142857
Lifeline as 	 1.0
mental processes 	 0.6666666666666666
In both 	 0.01904761904761905
is where 	 0.0040650406504065045
summary '' 	 0.047619047619047616
generation . 	 0.2222222222222222
are simple 	 0.004149377593360996
Health Organization 	 0.5
recognition such 	 0.008264462809917356
-LRB- F-16 	 0.0027100271002710027
natural language 	 0.76
and simpler 	 0.001445086705202312
40 % 	 1.0
B. , 	 1.0
casual speech 	 1.0
clarification . 	 0.3333333333333333
error-prone and 	 1.0
job , 	 0.5
in solving 	 0.0018726591760299626
would recognize 	 0.018867924528301886
the morphology 	 0.0006920415224913495
all four 	 0.023255813953488372
descriptor in 	 1.0
than extraction 	 0.022222222222222223
with Tom 	 0.00546448087431694
-LRB- end 	 0.0027100271002710027
focuses on 	 1.0
AI systems 	 0.6666666666666666
states -RRB- 	 0.25
Michael Stubbs 	 0.25
would not 	 0.018867924528301886
most prior 	 0.017241379310344827
really was 	 1.0
this are 	 0.02197802197802198
and linguistic 	 0.001445086705202312
be available 	 0.004219409282700422
; a 	 0.02127659574468085
phrases . 	 0.3125
using shallow 	 0.01694915254237288
involves `` 	 0.1
very specific 	 0.024390243902439025
items in 	 0.5
needs to 	 0.4
different approaches 	 0.02040816326530612
recorded speech 	 0.5
lot of 	 0.3333333333333333
data that 	 0.025974025974025976
-5 to 	 1.0
-- whole 	 0.04
task depends 	 0.023809523809523808
relatively simple 	 1.0
be kept 	 0.004219409282700422
helicopters , 	 0.5
features characterize 	 0.038461538461538464
sidestepped the 	 1.0
Consortium has 	 1.0
only be 	 0.02631578947368421
problem because 	 0.022727272727272728
see wide 	 0.05
on unsupervised 	 0.0047169811320754715
success and 	 0.2
new entrants 	 0.041666666666666664
summary ' 	 0.023809523809523808
STT '' 	 1.0
alone usually 	 0.25
techniques could 	 0.043478260869565216
most importantly 	 0.017241379310344827
and NLP 	 0.002890173410404624
to interface 	 0.0013280212483399733
reasoning mechanisms 	 0.14285714285714285
2 , 	 0.2
, speech 	 0.004491858506457047
addressed the 	 0.5
measures can 	 0.3333333333333333
, therefore 	 0.0016844469399213925
be ? 	 0.004219409282700422
, heavy-noise 	 0.0005614823133071309
ones in 	 0.1
adjacent unigrams 	 0.16666666666666666
derivation or 	 0.25
a matter 	 0.001226993865030675
Stanford , 	 0.5
using elements 	 0.01694915254237288
similarity to 	 0.1
of automatically 	 0.0017825311942959
commonly serve 	 0.125
of simple 	 0.0017825311942959
document set 	 0.027777777777777776
common to 	 0.04
sorting center 	 1.0
division of 	 0.5
will need 	 0.02857142857142857
it into 	 0.042735042735042736
we learn 	 0.022222222222222223
be considered 	 0.008438818565400843
syntactic , 	 0.07692307692307693
promise to 	 1.0
Syphon -LRB- 	 1.0
<s> An 	 0.008455034588777863
better measure 	 0.1111111111111111
they differ 	 0.025
photos against 	 1.0
also manual 	 0.014492753623188406
as smart 	 0.003484320557491289
the noun 	 0.0006920415224913495
not also 	 0.008928571428571428
, glossary 	 0.0011229646266142617
the trainee 	 0.0006920415224913495
machine-learning algorithms 	 0.25
The machine-learning 	 0.005208333333333333
<s> Recently 	 0.0007686395080707148
translation of 	 0.14864864864864866
stages of 	 0.5
evaluation workshops 	 0.018518518518518517
read the 	 0.14285714285714285
computer speech 	 0.022727272727272728
and experience 	 0.001445086705202312
by extracting 	 0.005714285714285714
combining decisions 	 0.25
general personal 	 0.045454545454545456
the final 	 0.002768166089965398
, Liberman 	 0.0005614823133071309
way as 	 0.041666666666666664
nearly anything 	 0.5
language constructs 	 0.006756756756756757
always , 	 0.3333333333333333
1989 ? 	 0.5
the JAS-39 	 0.0006920415224913495
, error-prone 	 0.0005614823133071309
received considerable 	 0.5
The user 	 0.005208333333333333
A.C. Nielsen 	 1.0
canned phrases 	 0.5
parsing community 	 0.03571428571428571
area include 	 0.09090909090909091
similarities to 	 0.5
improvements in 	 0.5
etc. '' 	 0.045454545454545456
evaluated , 	 0.14285714285714285
1,915,993 -RRB- 	 1.0
follows a 	 0.5
abstractive method 	 0.16666666666666666
newspaper articles 	 0.3333333333333333
pertain strongly 	 1.0
features would 	 0.038461538461538464
Once the 	 0.2
-LRB- AFTI 	 0.0027100271002710027
sequences , 	 0.1111111111111111
and 1980s 	 0.002890173410404624
like supervised 	 0.03571428571428571
of stochastic 	 0.00089126559714795
attention to 	 0.5
are not 	 0.02074688796680498
evaluation of 	 0.07407407407407407
, responding 	 0.0005614823133071309
William Labov 	 0.5
and composing 	 0.001445086705202312
They simply 	 0.3333333333333333
outputs of 	 1.0
recently there 	 0.3333333333333333
initial sounds 	 0.3333333333333333
determine if 	 0.21739130434782608
shallow methods 	 0.16666666666666666
1965 it 	 0.25
support question 	 0.25
'' versus 	 0.005154639175257732
is 7 	 0.0020325203252032522
being a 	 0.1111111111111111
-LRB- See 	 0.01084010840108401
soon developed 	 0.3333333333333333
require advanced 	 0.045454545454545456
visible Markov 	 0.3333333333333333
1998 The 	 0.25
include a 	 0.1111111111111111
basic elements 	 0.07692307692307693
generated text 	 0.13333333333333333
nodes based 	 0.14285714285714285
the recognizer 	 0.0006920415224913495
must interpret 	 0.07142857142857142
`` pseudo-pilot 	 0.005291005291005291
are averaged 	 0.004149377593360996
These systems 	 0.23529411764705882
on my 	 0.0047169811320754715
are surprisingly 	 0.004149377593360996
set '' 	 0.05128205128205128
Research in 	 0.125
is actually 	 0.0040650406504065045
corpus contains 	 0.03225806451612903
collections , 	 0.5
out '' 	 0.07142857142857142
SpeechTEK and 	 0.5
e.g. Constraints 	 0.017857142857142856
reduced amount 	 0.25
of work 	 0.00089126559714795
be able 	 0.02109704641350211
grammars often 	 0.07142857142857142
makes use 	 0.125
initial capital 	 0.3333333333333333
translation , 	 0.10810810810810811
by expanding 	 0.005714285714285714
software vendors 	 0.037037037037037035
is substantial 	 0.0020325203252032522
things . 	 0.3333333333333333
<s> Potentially 	 0.0007686395080707148
nature of 	 1.0
`` hub 	 0.005291005291005291
still the 	 0.06666666666666667
2006 and 	 0.3333333333333333
by analyzing 	 0.011428571428571429
stress injuries 	 0.5
gender , 	 1.0
system whose 	 0.010752688172043012
-RRB- Court 	 0.0027100271002710027
formalisms such 	 0.5
market for 	 0.3333333333333333
ontologies and 	 0.16666666666666666
These two 	 0.058823529411764705
cohesion '' 	 1.0
less likely 	 0.16666666666666666
giving these 	 0.5
Generally , 	 0.6
word to 	 0.016666666666666666
1982 , 	 0.3333333333333333
on machine-learning 	 0.0047169811320754715
Court reporting 	 1.0
may then 	 0.019230769230769232
, several 	 0.0005614823133071309
spaces of 	 0.2
Harold Garfinkel 	 1.0
his wingmen 	 0.08333333333333333
<s> So 	 0.0023059185242121443
speaker . 	 0.16666666666666666
in itself 	 0.0018726591760299626
from it 	 0.009615384615384616
English of 	 0.02702702702702703
how strong 	 0.034482758620689655
person\/persons rather 	 1.0
first solid 	 0.030303030303030304
-LRB- DARPA 	 0.0027100271002710027
users to 	 0.2222222222222222
SHRDLU could 	 0.16666666666666666
direct comparison 	 0.16666666666666666
system or 	 0.021505376344086023
c -RRB- 	 1.0
levels for 	 0.13636363636363635
article accuracy 	 0.034482758620689655
predicted pollen 	 0.5
well be 	 0.03571428571428571
other in 	 0.014285714285714285
, Phrases 	 0.0005614823133071309
these terms 	 0.023809523809523808
for continuous 	 0.0036101083032490976
, strategies 	 0.0005614823133071309
Most of 	 0.5
complicating the 	 1.0
for reading 	 0.007220216606498195
to automatically 	 0.00796812749003984
the mechanical 	 0.0006920415224913495
attaching real-valued 	 1.0
or after 	 0.0045045045045045045
is like 	 0.0040650406504065045
, showing 	 0.0005614823133071309
in trying 	 0.0018726591760299626
searching for 	 0.3333333333333333
techniques , 	 0.08695652173913043
somewhat recent 	 0.5
better data 	 0.1111111111111111
applications there 	 0.04
'' where 	 0.005154639175257732
not contain 	 0.008928571428571428
: Separate 	 0.0196078431372549
several years 	 0.09090909090909091
the walk 	 0.0006920415224913495
error rates 	 0.25
last example 	 0.2
some tasks 	 0.012048192771084338
Machines Research 	 1.0
actually playing 	 0.3333333333333333
Phonemes , 	 1.0
early systems 	 0.1
linguistic competence 	 0.0625
interpretation capabilities 	 0.5
been trained 	 0.014705882352941176
questioner might 	 0.25
proper nouns 	 0.14285714285714285
PageRank\/TextRank on 	 1.0
as people 	 0.003484320557491289
In general 	 0.02857142857142857
state-of-the-art in 	 0.5
but to 	 0.014705882352941176
Summarizers -LRB- 	 1.0
equivalent questions 	 0.2
illustrates how 	 0.5
and it 	 0.0072254335260115606
several teams 	 0.045454545454545456
disparate fields 	 1.0
<s> Multilingual 	 0.0007686395080707148
ignore this 	 1.0
assign a 	 0.4
keyword matching 	 1.0
-LRB- NLG 	 0.008130081300813009
the media 	 0.0006920415224913495
state transducer 	 0.14285714285714285
into smaller 	 0.01282051282051282
from large 	 0.009615384615384616
creating an 	 0.2857142857142857
occurred in 	 1.0
-LRB- more 	 0.005420054200542005
This hierarchy 	 0.015873015873015872
although there 	 0.16666666666666666
it can 	 0.05128205128205128
Transform , 	 1.0
decoding is 	 1.0
translation and 	 0.04054054054054054
an earlier 	 0.007575757575757576
they would 	 0.025
sizes generally 	 0.3333333333333333
-RRB- hours 	 0.0027100271002710027
significant complexity 	 0.1111111111111111
machine recognition 	 0.012658227848101266
correct . 	 0.2
facts about 	 1.0
each segment 	 0.044444444444444446
that time 	 0.0035460992907801418
identified . 	 0.2
<s> Statistics 	 0.0023059185242121443
sentences correct 	 0.013157894736842105
creates a 	 0.5
features . 	 0.07692307692307693
<s> Dynamic 	 0.0023059185242121443
libraries GRM 	 0.5
, Sandra 	 0.0005614823133071309
in Statistical 	 0.0018726591760299626
all official 	 0.023255813953488372
on casual 	 0.0047169811320754715
networks . 	 0.21428571428571427
many types 	 0.019230769230769232
tasks is 	 0.03125
contains errors 	 0.2
by Zellig 	 0.005714285714285714
`` look 	 0.005291005291005291
One task 	 0.07692307692307693
shapes of 	 0.6666666666666666
corpus is 	 0.03225806451612903
may contain 	 0.038461538461538464
n't vice 	 0.25
consisted of 	 1.0
assembling output 	 1.0
makes sense 	 0.125
or nonexistent 	 0.0045045045045045045
OCR , 	 0.14285714285714285
2007 . 	 0.4
correct summary 	 0.06666666666666667
-LRB- some 	 0.0027100271002710027
common nouns 	 0.08
the vocabulary 	 0.0006920415224913495
parsing refers 	 0.03571428571428571
English are 	 0.02702702702702703
than what 	 0.022222222222222223
not obvious 	 0.008928571428571428
Software -LRB- 	 0.5
types including 	 0.07142857142857142
<s> Little 	 0.0007686395080707148
as noun 	 0.003484320557491289
systems . 	 0.08928571428571429
discussed below 	 0.42857142857142855
a member 	 0.001226993865030675
improve the 	 0.07692307692307693
developed RSI 	 0.038461538461538464
for abbreviations 	 0.0036101083032490976
metric -LRB- 	 0.3333333333333333
was a 	 0.03896103896103896
dependency parsers 	 0.2
model Modern 	 0.03333333333333333
newswire reports 	 1.0
art for 	 0.5
the Canadian 	 0.001384083044982699
post-processing by 	 0.3333333333333333
is called 	 0.012195121951219513
often as 	 0.045454545454545456
systems applications 	 0.008928571428571428
humans would 	 0.08333333333333333
after the 	 0.16666666666666666
mentioned above 	 0.16666666666666666
different parts 	 0.04081632653061224
Annex on 	 1.0
States Air 	 0.14285714285714285
be useful 	 0.012658227848101266
impossible . 	 0.5
-- discourse 	 0.04
shed light 	 1.0
<s> Basic 	 0.0007686395080707148
design the 	 0.25
the Latin 	 0.0006920415224913495
its history 	 0.02857142857142857
thousands or 	 0.3333333333333333
resort to 	 1.0
SIGGEN portion 	 1.0
before it 	 0.16666666666666666
sometimes preferred 	 0.07692307692307693
many words 	 0.038461538461538464
et al. 	 1.0
form the 	 0.05
Input -RRB- 	 0.5
be translated 	 0.012658227848101266
debates , 	 1.0
together to 	 0.125
if-then rules 	 1.0
vocabulary . 	 0.25
- processing 	 0.0625
integration with 	 1.0
In one 	 0.009523809523809525
while ratings 	 0.05
, Mariani 	 0.0005614823133071309
are divided 	 0.004149377593360996
or uncertainties 	 0.0045045045045045045
this makes 	 0.01098901098901099
of omni-font 	 0.00089126559714795
recognizes the 	 1.0
common feature 	 0.04
outside world 	 0.5
Google . 	 0.5
image , 	 0.3333333333333333
Goldberg continued 	 0.5
systems can 	 0.026785714285714284
relying on 	 1.0
types , 	 0.07142857142857142
not mark 	 0.008928571428571428
years various 	 0.047619047619047616
deep approach 	 0.14285714285714285
clip of 	 1.0
Eurofighter Typhoon 	 1.0
-RRB- an 	 0.0027100271002710027
by programs 	 0.005714285714285714
quality of 	 0.5
svg This 	 1.0
handling such 	 0.5
Veterans Administration 	 1.0
right '' 	 0.1
containing four 	 0.125
appear . 	 0.0625
dynamically create 	 0.5
<s> Two 	 0.005380476556495004
it statically 	 0.008547008547008548
in recent 	 0.003745318352059925
effective decision-support 	 0.16666666666666666
'' could 	 0.005154639175257732
content alone 	 0.08333333333333333
Standard Annex 	 0.5
dividing speech 	 0.3333333333333333
for innumerable 	 0.0036101083032490976
Network classifies 	 1.0
of entities 	 0.00089126559714795
any significant 	 0.03225806451612903
-LRB- one 	 0.0027100271002710027
for Larry 	 0.0036101083032490976
a podcast 	 0.001226993865030675
Statistical NLP 	 0.2222222222222222
relative position 	 0.3333333333333333
An 8 	 0.0625
, facts 	 0.0005614823133071309
input sales 	 0.024390243902439025
trained hidden 	 0.3333333333333333
structure . 	 0.16666666666666666
per minute 	 0.25
east . 	 1.0
many others 	 0.019230769230769232
into one 	 0.02564102564102564
painstakingly `` 	 1.0
it proved 	 0.008547008547008548
text on 	 0.006289308176100629
make them 	 0.05
in an 	 0.0149812734082397
than one 	 0.06666666666666667
1928 the 	 1.0
many significant 	 0.019230769230769232
meaningful symbol 	 0.125
made explicit 	 0.0625
hard and 	 0.16666666666666666
book '' 	 0.125
-LRB- Adda 	 0.0027100271002710027
genres of 	 1.0
normal speech 	 0.5
other aspects 	 0.014285714285714285
greatly . 	 0.2857142857142857
contexts . 	 0.2857142857142857
filter preselects 	 0.5
management applications 	 0.14285714285714285
typically evaluated 	 0.05555555555555555
Interspeech -RRB- 	 1.0
feeling that 	 1.0
languages in 	 0.02
human thought 	 0.021739130434782608
not have 	 0.017857142857142856
or dimensions 	 0.0045045045045045045
world applications 	 0.06666666666666667
Summarization systems 	 0.25
of sounds 	 0.00089126559714795
For telephone 	 0.01639344262295082
by Yehoshua 	 0.005714285714285714
the metrics 	 0.0006920415224913495
successful for 	 0.1111111111111111
`` Man 	 0.010582010582010581
usability . 	 1.0
accuracy was 	 0.03225806451612903
human-readable address 	 1.0
only to 	 0.02631578947368421
SVM , 	 1.0
Challenges shared-task 	 1.0
data set 	 0.012987012987012988
the introduction 	 0.0006920415224913495
weights to 	 0.4
as shallow-transfer 	 0.003484320557491289
simple substitution 	 0.038461538461538464
surprisingly difficult 	 0.3333333333333333
performance mainly 	 0.05555555555555555
are informative 	 0.004149377593360996
and removed 	 0.001445086705202312
During this 	 0.5
authors provide 	 0.2
certain region 	 0.14285714285714285
-RRB- had 	 0.0027100271002710027
only automate 	 0.02631578947368421
best model 	 0.05555555555555555
tasks typically 	 0.03125
to arrive 	 0.0013280212483399733
systems include 	 0.008928571428571428
handwriting . 	 0.5
Systems corp. 	 0.08333333333333333
extracted summaries 	 1.0
some fundamental 	 0.012048192771084338
voice dialog 	 0.07692307692307693
reranking in 	 1.0
classification decisions 	 0.058823529411764705
his work 	 0.08333333333333333
, adjectives 	 0.0005614823133071309
cases where 	 0.16666666666666666
turns . 	 0.3333333333333333
context-free grammars 	 0.36363636363636365
structured into 	 0.16666666666666666
disciplines , 	 1.0
model avoids 	 0.03333333333333333
two measures 	 0.034482758620689655
is because 	 0.0020325203252032522
RCA engineers 	 0.2
example : 	 0.024691358024691357
as 1946 	 0.003484320557491289
been seen 	 0.04411764705882353
turn simplified 	 0.16666666666666666
the 1960s 	 0.001384083044982699
examples to 	 0.041666666666666664
semantic ; 	 0.047619047619047616
highest probability 	 0.3333333333333333
who is 	 0.2
1954 on 	 0.3333333333333333
1976 the 	 0.5
Forces Security 	 1.0
answer candidate 	 0.03333333333333333
Sentence boundary 	 0.2
in parametric 	 0.0018726591760299626
basic and 	 0.07692307692307693
impossible when 	 0.5
under the 	 0.2
accommodate ambiguity 	 0.4
as the 	 0.09407665505226481
texts of 	 0.11764705882352941
is particularly 	 0.0040650406504065045
of Ethnomethodology 	 0.00089126559714795
multiple references 	 0.07692307692307693
refer to 	 1.0
that underlies 	 0.0035460992907801418
research devoted 	 0.023809523809523808
sentiment of 	 0.04
describing each 	 0.25
and Snyder 	 0.001445086705202312
translated , 	 0.25
Problem Solving 	 1.0
is steered 	 0.0020325203252032522
a growing 	 0.001226993865030675
text that 	 0.025157232704402517
Then , 	 0.4
to normalize 	 0.0013280212483399733
Separate words 	 0.5
text used 	 0.006289308176100629
probabilistically at 	 1.0
step that 	 0.13333333333333333
was LILOG 	 0.012987012987012988
distinct sentences 	 0.14285714285714285
translation at 	 0.013513513513513514
a writer 	 0.001226993865030675
classification-related measure 	 1.0
the barmaid 	 0.0006920415224913495
pyramid showing 	 1.0
improve . 	 0.07692307692307693
, etc. 	 0.011229646266142616
disturbed by 	 1.0
'' can 	 0.02577319587628866
tokens like 	 0.14285714285714285
, thereby 	 0.0005614823133071309
answered about 	 0.2
Patent 1,915,993 	 0.3333333333333333
same in-depth 	 0.04
project and 	 0.07692307692307693
updated textbook 	 1.0
problem overlaps 	 0.022727272727272728
be filtered 	 0.012658227848101266
how phrases 	 0.034482758620689655
securely ; 	 1.0
Political discourse 	 1.0
chosen domains 	 0.2
expression , 	 0.2
during handwriting 	 0.1
include SpeechTEK 	 0.037037037037037035
, 4 	 0.0005614823133071309
candidate , 	 0.3333333333333333
first word 	 0.030303030303030304
several ways 	 0.045454545454545456
-LRB- correct 	 0.0027100271002710027
appear , 	 0.0625
require rapid 	 0.045454545454545456
blocks world 	 0.25
<s> Since 	 0.0030745580322828594
documents or 	 0.02631578947368421
produce both 	 0.045454545454545456
employed within 	 1.0
financial benefits 	 0.25
or just 	 0.0045045045045045045
MorphoChallenge Semi-supervised 	 1.0
of analyses 	 0.00089126559714795
a probability 	 0.001226993865030675
not worked 	 0.008928571428571428
: e.g. 	 0.0196078431372549
Digitized Sound 	 1.0
, using 	 0.005614823133071308
movie reviews 	 0.3333333333333333
vol-2 Black 	 1.0
Ge'ez script 	 1.0
overlaps to 	 0.5
declaration of 	 1.0
-RRB- up 	 0.0027100271002710027
techniques merely 	 0.043478260869565216
produce a 	 0.13636363636363635
deploy machine 	 1.0
<s> Sometimes 	 0.0007686395080707148
and atmosphere 	 0.001445086705202312
then common 	 0.02857142857142857
technique in 	 0.14285714285714285
and English 	 0.001445086705202312
entire banking 	 0.3333333333333333
`` Dogged 	 0.005291005291005291
Lamb , 	 1.0
be phrased 	 0.004219409282700422
terms such 	 0.07692307692307693
require subjects 	 0.045454545454545456
data redundancy 	 0.012987012987012988
part -LRB- 	 0.037037037037037035
text unit 	 0.006289308176100629
be thresholded 	 0.004219409282700422
-LRB- monetary 	 0.0027100271002710027
multi-document summarization 	 0.75
of symbols 	 0.00089126559714795
of optical 	 0.00089126559714795
address this 	 0.25
and above 	 0.001445086705202312
-RRB- with 	 0.008130081300813009
markup like 	 1.0
prisoners ? 	 0.5
Smartphones . 	 1.0
SPHINX toolkit 	 1.0
second important 	 0.1
that scans 	 0.0035460992907801418
NLP programs 	 0.02127659574468085
generated out 	 0.06666666666666667
underlying formal 	 0.3333333333333333
and Sentences 	 0.001445086705202312
approach allows 	 0.02857142857142857
an excellent 	 0.007575757575757576
An increasing 	 0.0625
where particular 	 0.02857142857142857
than that 	 0.044444444444444446
combined in 	 0.5
The learning 	 0.005208333333333333
% of 	 0.20512820512820512
explicit word 	 0.2
or aspects 	 0.0045045045045045045
this field 	 0.02197802197802198
<s> Searches 	 0.0007686395080707148
; in 	 0.02127659574468085
as focusing 	 0.003484320557491289
primitive computer-type 	 1.0
to message 	 0.0013280212483399733
most can 	 0.017241379310344827
very hard 	 0.024390243902439025
Corpus Research 	 0.0625
segmentation : 	 0.09090909090909091
-- in 	 0.04
that describe 	 0.0035460992907801418
maintenance -RRB- 	 1.0
captioning , 	 1.0
the Reader 	 0.0006920415224913495
strengths and 	 0.5
environment . 	 0.3333333333333333
imaging is 	 1.0
Broadly , 	 1.0
; nor 	 0.02127659574468085
includes making 	 0.14285714285714285
neural networks 	 0.5333333333333333
corpora on 	 0.09090909090909091
for errors 	 0.0036101083032490976
Lehrberger 1982 	 1.0
is similar 	 0.0040650406504065045
This task 	 0.031746031746031744
representations of 	 0.25
, they 	 0.004491858506457047
domain-specific knowledge 	 0.5
use context-free 	 0.013888888888888888
most parts 	 0.05172413793103448
, how 	 0.0005614823133071309
and split 	 0.001445086705202312
From these 	 1.0
in using 	 0.0056179775280898875
a page 	 0.001226993865030675
Deirdre Wilson 	 1.0
keyphrases assigned 	 0.02857142857142857
edition published 	 1.0
Use '' 	 0.5
, yielding 	 0.0005614823133071309
speech processing 	 0.006578947368421052
, up 	 0.0005614823133071309
produce output 	 0.09090909090909091
work at 	 0.041666666666666664
while some 	 0.05
HMMs are 	 0.25
-LRB- rather 	 0.0027100271002710027
Words in 	 0.25
tasks from 	 0.03125
natural speech 	 0.02666666666666667
using statistical 	 0.03389830508474576
their suitability 	 0.029411764705882353
learned on 	 0.2
accuracy for 	 0.06451612903225806
NNS for 	 1.0
varied from 	 1.0
not be 	 0.10714285714285714
code . 	 0.42857142857142855
-RRB- dogs 	 0.0027100271002710027
learning the 	 0.023255813953488372
, Santoni 	 0.0005614823133071309
of documents 	 0.004456327985739751
together into 	 0.125
as corresponding 	 0.003484320557491289
the schematic 	 0.0006920415224913495
semantic relationship 	 0.047619047619047616
tense of 	 0.5
disambiguation , 	 0.1
to doctors 	 0.0013280212483399733
word breaks 	 0.016666666666666666
distinct vowels 	 0.14285714285714285
fail during 	 0.3333333333333333
, results 	 0.0005614823133071309
a valid 	 0.001226993865030675
unambiguous sentence-ending 	 0.5
source documents 	 0.125
of which 	 0.008912655971479501
a sound 	 0.008588957055214725
speech choice 	 0.006578947368421052
, Norman 	 0.0005614823133071309
accuracy over 	 0.03225806451612903
of estimating 	 0.00089126559714795
: complicated 	 0.00980392156862745
and etc. 	 0.001445086705202312
a list 	 0.008588957055214725
the ' 	 0.0006920415224913495
make no 	 0.05
direct a 	 0.16666666666666666
a filter 	 0.001226993865030675
usually done 	 0.0625
and Italian 	 0.001445086705202312
+ Web-based 	 0.16666666666666666
proposed a 	 0.2222222222222222
James Paul 	 0.25
been hand-annotated 	 0.029411764705882353
express sentiment 	 0.2
research attempts 	 0.047619047619047616
become well 	 0.25
this character 	 0.01098901098901099
Deciding what 	 1.0
domains and 	 0.25
Discourse analysis 	 1.0
and Command 	 0.001445086705202312
to answer 	 0.00398406374501992
listens for 	 1.0
, article 	 0.0016844469399213925
and persuasion 	 0.001445086705202312
you ' 	 0.07692307692307693
tagging program 	 0.04
also considerable 	 0.014492753623188406
Hard of 	 0.5
the comprehension 	 0.0006920415224913495
`` black 	 0.005291005291005291
reduce acoustic 	 1.0
into each 	 0.01282051282051282
informativeness . 	 0.3333333333333333
skew , 	 1.0
Often natural 	 0.3333333333333333
After training 	 0.3333333333333333
standard table 	 0.07142857142857142
genre . 	 0.5
Agency -LRB- 	 0.5
severe in 	 1.0
sold his 	 0.3333333333333333
from 10,000 	 0.009615384615384616
have been 	 0.25
of one 	 0.0035650623885918
the earliest-used 	 0.001384083044982699
if in 	 0.07142857142857142
sentiment analysis 	 0.52
benefit from 	 1.0
technique which 	 0.14285714285714285
aims at 	 0.3333333333333333
noise but 	 0.125
off on 	 0.5
dogs '' 	 0.5714285714285714
other cockpit 	 0.014285714285714285
vertex would 	 0.3333333333333333
way : 	 0.041666666666666664
new sentences 	 0.041666666666666664
resorting to 	 1.0
where it 	 0.02857142857142857
of an 	 0.011586452762923352
eigenvector centrality 	 0.5
a system-generated 	 0.001226993865030675
the meaning 	 0.006920415224913495
terms that 	 0.07692307692307693
tagger , 	 0.4444444444444444
a potentially 	 0.001226993865030675
first . 	 0.030303030303030304
the sense 	 0.0006920415224913495
Corpus was 	 0.125
number should 	 0.023255813953488372
of producing 	 0.00089126559714795
rightmost derivation 	 1.0
first step 	 0.06060606060606061
especially of 	 0.06666666666666667
, means 	 0.0005614823133071309
Laclau , 	 1.0
the types 	 0.001384083044982699
operating on 	 0.5
mid-90s . 	 1.0
Recognition '' 	 0.375
another -RRB- 	 0.07692307692307693
of TextRank 	 0.00089126559714795
and\/or religious 	 0.3333333333333333
a standard 	 0.00245398773006135
example by 	 0.024691358024691357
that character-by-character 	 0.0035460992907801418
discussions in 	 0.3333333333333333
IBM and 	 0.3333333333333333
reliable results 	 0.5
went on 	 0.2
left to 	 0.16666666666666666
have corpus 	 0.009615384615384616
initially clear 	 1.0
-RRB- as 	 0.008130081300813009
-- often 	 0.04
abstractive summarization 	 0.3333333333333333
of TF-IDF 	 0.00089126559714795
barmaid -RRB- 	 0.5
heuristics with 	 0.5
the English 	 0.0020761245674740486
Q&A systems 	 1.0
always a 	 0.3333333333333333
of idioms 	 0.00089126559714795
domains where 	 0.125
leftmost derivation 	 1.0
POS tags 	 0.15384615384615385
of stock 	 0.00089126559714795
into basic 	 0.01282051282051282
models that 	 0.11538461538461539
available and 	 0.058823529411764705
expected answer 	 0.14285714285714285
, Paroubek 	 0.0005614823133071309
Short history 	 1.0
the core 	 0.0006920415224913495
concept into 	 0.25
including linguistics 	 0.14285714285714285
compiled newswire 	 1.0
Human Summarization 	 0.2
our learned 	 0.2
POS-taggers , 	 1.0
of perspective 	 0.00089126559714795
level provides 	 0.05
learning from 	 0.046511627906976744
programs in 	 0.09090909090909091
of context-free 	 0.00089126559714795
the Web 	 0.001384083044982699
several phases 	 0.045454545454545456
converted the 	 0.3333333333333333
professor at 	 1.0
each word 	 0.1111111111111111
take as 	 0.1
derivations of 	 1.0
as named 	 0.003484320557491289
semantics or 	 0.14285714285714285
assumed that 	 1.0
subscription department 	 1.0
evaluating the 	 0.2
by searching 	 0.005714285714285714
large quantity 	 0.043478260869565216
directly comparable 	 0.2
individual speaker 	 0.08333333333333333
went to 	 0.4
routing -LRB- 	 0.3333333333333333
detailed background 	 0.5
letters : 	 0.1
Kolodner . 	 1.0
advantages over 	 1.0
Desktop & 	 1.0
sophisticated measures 	 0.14285714285714285
improving output 	 1.0
parsing input 	 0.03571428571428571
semantic theories 	 0.047619047619047616
constraints are 	 0.25
relationships among 	 0.16666666666666666
probability to 	 0.14285714285714285
political forums 	 0.3333333333333333
, according 	 0.0005614823133071309
Hence -LRB- 	 0.5
comparison of 	 0.3333333333333333
was not 	 0.025974025974025976
the delta 	 0.0006920415224913495
vs. `` 	 0.16666666666666666
forward-backward algorithm 	 1.0
contains only 	 0.1
statistical language 	 0.030303030303030304
minute , 	 1.0
of AI 	 0.00089126559714795
were the 	 0.024390243902439025
known for 	 0.038461538461538464
Of particular 	 1.0
given amount 	 0.041666666666666664
words relate 	 0.009174311926605505
simple bar 	 0.038461538461538464
to accurately 	 0.0013280212483399733
used an 	 0.008849557522123894
fonts are 	 0.3333333333333333
shipment of 	 1.0
Another approach 	 0.15384615384615385
proposed what 	 0.1111111111111111
systems in 	 0.008928571428571428
get bunch 	 0.14285714285714285
Carbonell , 	 1.0
`` sailor 	 0.010582010582010581
possible -LRB- 	 0.041666666666666664
medium levels 	 0.3333333333333333
to work 	 0.0026560424966799467
, complicating 	 0.0005614823133071309
other levels 	 0.014285714285714285
combine the 	 0.3333333333333333
humans possess 	 0.08333333333333333
easier for 	 0.125
rules ATNs 	 0.023255813953488372
approaches differ 	 0.03571428571428571
models upon 	 0.038461538461538464
rate is 	 0.18181818181818182
the ability 	 0.001384083044982699
temporal and 	 0.5
committed into 	 1.0
possibility of 	 0.75
'' vertex 	 0.005154639175257732
Google published 	 0.25
current QA 	 0.14285714285714285
significant increases 	 0.1111111111111111
However even 	 0.02702702702702703
sense that 	 0.125
them attractive 	 0.05263157894736842
first pass 	 0.030303030303030304
an algorithm 	 0.022727272727272728
versus `` 	 1.0
<s> Importance 	 0.0007686395080707148
corresponds to 	 1.0
dialogue in 	 0.5
Frederick Jelinek 	 1.0
journal abstracts 	 0.3333333333333333
, Richard 	 0.0005614823133071309
methods when 	 0.022727272727272728
as mentioned 	 0.003484320557491289
feature which 	 0.07692307692307693
, notably 	 0.0005614823133071309
diverse '' 	 0.5
any of 	 0.06451612903225806
where syllables 	 0.02857142857142857
achieved only 	 0.1
be viewed 	 0.016877637130801686
` the 	 0.0625
distinguished . 	 1.0
be parsed 	 0.004219409282700422
Japanese and 	 0.25
which is 	 0.09420289855072464
information appear 	 0.021739130434782608
common ones 	 0.04
, Battle 	 0.0005614823133071309
a degree 	 0.001226993865030675
formalization . 	 0.5
which generates 	 0.007246376811594203
projection followed 	 1.0
in 2007 	 0.0018726591760299626
source document 	 0.08333333333333333
Querying application 	 1.0
CSR have 	 0.3333333333333333
social contexts 	 0.07142857142857142
procedural information 	 1.0
For the 	 0.01639344262295082
<s> because 	 0.0007686395080707148
-LRB- MLLR 	 0.0027100271002710027
levels , 	 0.045454545454545456
organization of 	 0.4
the documents 	 0.0020761245674740486
simpler sounds 	 0.6666666666666666
, Sept. 	 0.0005614823133071309
NLG the 	 0.047619047619047616
difficult problems 	 0.10714285714285714
we do 	 0.022222222222222223
made feasible 	 0.0625
state transducers 	 0.07142857142857142
and generating 	 0.001445086705202312
simple extraction 	 0.038461538461538464
techniques similar 	 0.043478260869565216
delimited . 	 0.25
-LRB- SBD 	 0.0027100271002710027
demonstration in 	 0.2
highly-specialized natural 	 1.0
verbs are 	 0.2
same words 	 0.04
did cause 	 0.2
of code 	 0.00089126559714795
people from 	 0.0625
as overall 	 0.003484320557491289
such representation 	 0.008130081300813009
current text 	 0.14285714285714285
many NLP 	 0.019230769230769232
evaluation programs 	 0.018518518518518517
NLP An 	 0.02127659574468085
people who 	 0.125
for results 	 0.0036101083032490976
of databases 	 0.00089126559714795
Kurzweil and 	 0.14285714285714285
summary and 	 0.047619047619047616
accurate transcription 	 0.14285714285714285
a context-free 	 0.0036809815950920245
involved the 	 0.16666666666666666
and speech 	 0.001445086705202312
broad agreement 	 0.25
-LRB- DTW 	 0.0027100271002710027
US Navy 	 0.14285714285714285
rates on 	 0.125
be an 	 0.004219409282700422
be derived 	 0.008438818565400843
the Information 	 0.0006920415224913495
a given 	 0.014723926380368098
term meaning 	 0.05555555555555555
card OCR 	 0.25
phone error 	 0.25
processes used 	 0.4
many authors 	 0.019230769230769232
on complex 	 0.0047169811320754715
classification of 	 0.058823529411764705
Integration -LRB- 	 1.0
best , 	 0.05555555555555555
, searching 	 0.0005614823133071309
normal human 	 0.5
use techniques 	 0.013888888888888888
, cursive 	 0.0005614823133071309
voice has 	 0.07692307692307693
In all 	 0.009523809523809525
positive and 	 0.2857142857142857
performance improvements 	 0.05555555555555555
Telephone Company 	 1.0
approach described 	 0.02857142857142857
a case 	 0.001226993865030675
contexts make 	 0.14285714285714285
`` understand 	 0.005291005291005291
to market 	 0.0013280212483399733
possible sentences 	 0.041666666666666664
-LRB- NLP 	 0.008130081300813009
more deeply 	 0.010526315789473684
estimate , 	 0.25
Dependence vs. 	 1.0
as in 	 0.027874564459930314
a company 	 0.001226993865030675
in free 	 0.003745318352059925
generated summaries 	 0.06666666666666667
tasks like 	 0.0625
, William 	 0.0011229646266142617
95 % 	 1.0
quantities of 	 0.6666666666666666
on hand-written 	 0.0047169811320754715
<s> Competing 	 0.0007686395080707148
in Northern 	 0.0018726591760299626
entry has 	 0.25
and encouraged 	 0.001445086705202312
also characterized 	 0.014492753623188406
and delta-delta 	 0.002890173410404624
larger source 	 0.0625
on post-processing 	 0.0047169811320754715
correlation is 	 0.5
requirements . 	 0.5
greatly with 	 0.14285714285714285
earliest-used machine 	 0.5
1993 . 	 0.3333333333333333
the periods 	 0.0006920415224913495
within another 	 0.05555555555555555
evaluated to 	 0.2857142857142857
to input 	 0.0026560424966799467
<s> Also 	 0.0023059185242121443
usually perform 	 0.03125
trees using 	 0.16666666666666666
good results 	 0.07692307692307693
Unsupervised Morpheme 	 0.16666666666666666
US Army 	 0.14285714285714285
Discontinuous or 	 1.0
but may 	 0.029411764705882353
simplify the 	 1.0
for What 	 0.0036101083032490976
been closely 	 0.014705882352941176
Such inflection 	 0.125
writing style 	 0.1111111111111111
or probabilities 	 0.0045045045045045045
Throughout the 	 1.0
approach applies 	 0.02857142857142857
accomplish with 	 1.0
: noun 	 0.00980392156862745
device converted 	 0.5
multilingual textual 	 0.3333333333333333
most successful 	 0.034482758620689655
computer program 	 0.11363636363636363
1981 -RRB- 	 1.0
for measuring 	 0.0036101083032490976
are sometimes 	 0.004149377593360996
when we 	 0.05714285714285714
evaluation technique 	 0.018518518518518517
, sufficiently 	 0.0005614823133071309
about human 	 0.025
is more 	 0.006097560975609756
of to 	 0.00089126559714795
generate keyphrases 	 0.05555555555555555
were walking 	 0.024390243902439025
more , 	 0.010526315789473684
any pauses 	 0.03225806451612903
capitalizes all 	 1.0
with no 	 0.01092896174863388
page , 	 0.42857142857142855
alternative courses 	 0.3333333333333333
A very 	 0.02
, Shipibo 	 0.0005614823133071309
their corresponding 	 0.029411764705882353
that overlap 	 0.0035460992907801418
and translation 	 0.004335260115606936
by providing 	 0.005714285714285714
A subtask 	 0.02
machine -RRB- 	 0.012658227848101266
to consistently 	 0.0013280212483399733
-LRB- F 	 0.0027100271002710027
may generate 	 0.019230769230769232
through sentiment 	 0.125
picture on 	 0.25
densely connected 	 1.0
traditional linguistics 	 1.0
becomes easier 	 0.5
beginning in 	 0.5
idea is 	 0.14285714285714285
sentence boundaries 	 0.08333333333333333
able to 	 1.0
given , 	 0.041666666666666664
with which 	 0.00546448087431694
-RRB- ; 	 0.02168021680216802
with them 	 0.01639344262295082
'' has 	 0.010309278350515464
candidates so 	 0.2
processing ; 	 0.018518518518518517
conducted in 	 0.4
UK . 	 0.5
Bernard Vauquois 	 1.0
commercial products 	 0.09090909090909091
methods to 	 0.09090909090909091
debated much 	 1.0
help speakers 	 0.1111111111111111
sophisticated methods 	 0.14285714285714285
commonly defined 	 0.125
fields of 	 0.3333333333333333
recognition applications 	 0.008264462809917356
blend into 	 0.3333333333333333
model summaries 	 0.06666666666666667
-LRB- SWER 	 0.0027100271002710027
, Politics 	 0.0005614823133071309
proposes some 	 1.0
<s> Progress 	 0.0007686395080707148
be a 	 0.05485232067510549
's speech 	 0.0196078431372549
like Page\/Lex\/TextRank 	 0.03571428571428571
case that 	 0.058823529411764705
basic techniques 	 0.07692307692307693
ARRA -RRB- 	 1.0
probability is 	 0.14285714285714285
<s> Approaches 	 0.0023059185242121443
extremely expensive 	 0.25
, USMC 	 0.0005614823133071309
strategies , 	 0.5
entry -LRB- 	 0.25
discourse turns 	 0.027777777777777776
noun '' 	 0.07142857142857142
going over 	 0.25
1930s . 	 1.0
multi-way scale 	 1.0
in 1982 	 0.0018726591760299626
; Each 	 0.02127659574468085
identifiers . 	 1.0
whether they 	 0.07692307692307693
PARRY , 	 1.0
, possessive 	 0.0005614823133071309
machine would 	 0.02531645569620253
Helicopters The 	 1.0
morphemes -LRB- 	 0.3333333333333333
: Determine 	 0.00980392156862745
not readily 	 0.008928571428571428
effective in 	 0.3333333333333333
thesis at 	 1.0
used in 	 0.20353982300884957
, Digital 	 0.0005614823133071309
yet been 	 0.5
step towards 	 0.06666666666666667
supervised machine 	 0.0625
is also 	 0.02032520325203252
70 % 	 0.75
qualities of 	 0.5
these summaries 	 0.047619047619047616
from has 	 0.009615384615384616
, translation 	 0.0011229646266142617
The construction 	 0.005208333333333333
final keyphrase 	 0.1111111111111111
of Natural 	 0.00089126559714795
HMM-based approach 	 0.6666666666666666
the Lander 	 0.0006920415224913495
hand-printed documents 	 0.25
The choice 	 0.005208333333333333
evaluation considers 	 0.018518518518518517
include all 	 0.037037037037037035
al. 1989 	 1.0
speech dynamics 	 0.006578947368421052
generating the 	 0.2
shared tasks 	 0.5
computers , 	 0.2222222222222222
Intelligence and 	 0.3333333333333333
word . 	 0.13333333333333333
sophisticated questioners 	 0.14285714285714285
partly statistical 	 1.0
analysis -- 	 0.015384615384615385
either explicit 	 0.1
far is 	 0.125
in ontologies 	 0.0018726591760299626
our everyday 	 0.2
length cutoff 	 0.125
features\/aspects , 	 1.0
1980s the 	 0.1111111111111111
limited amounts 	 0.1
correlation between 	 0.5
lunar science 	 1.0
utility and 	 0.5
scanner to 	 0.3333333333333333
Amount line 	 1.0
speeches , 	 1.0
Brill 's 	 0.3333333333333333
it does 	 0.008547008547008548
processed incorrectly 	 0.16666666666666666
post - 	 1.0
-LRB- University 	 0.0027100271002710027
academic research 	 1.0
In normal 	 0.009523809523809525
similar in 	 0.037037037037037035
hidden Markov 	 0.875
of rocks 	 0.00089126559714795
legal and 	 0.3333333333333333
each of 	 0.1111111111111111
to specify 	 0.0013280212483399733
easily portable 	 0.2222222222222222
other related 	 0.014285714285714285
a classification 	 0.001226993865030675
navigation , 	 0.5
typically the 	 0.05555555555555555
' could 	 0.05263157894736842
properties . 	 0.25
networks as 	 0.07142857142857142
Romance languages 	 1.0
sentence position 	 0.041666666666666664
even if 	 0.1111111111111111
algorithm exploits 	 0.03571428571428571
a credit 	 0.00245398773006135
choice of 	 0.25
term is 	 0.05555555555555555
upload paper 	 1.0
when annotating 	 0.02857142857142857
reads it 	 0.5
<s> Goldberg 	 0.0007686395080707148
pollen example 	 0.07692307692307693
of potential 	 0.0017825311942959
as paraphrase 	 0.003484320557491289
thought of 	 0.6666666666666666
fixed schemata 	 0.5
trend to 	 1.0
implied challenges 	 1.0
government sponsored 	 0.3333333333333333
verbs -LRB- 	 0.2
directly to 	 0.4
selection is 	 1.0
from , 	 0.009615384615384616
thus speech 	 0.1
it that 	 0.008547008547008548
data for 	 0.012987012987012988
Electronic Health 	 0.5
accuracy -RRB- 	 0.06451612903225806
the article 	 0.0006920415224913495
possibilities , 	 0.2
country , 	 0.25
techniques . 	 0.043478260869565216
textual representation 	 0.2
the choices 	 0.0006920415224913495
DTW has 	 0.3333333333333333
for any 	 0.0036101083032490976
onto its 	 1.0
problem of 	 0.18181818181818182
-RRB- leverages 	 0.0027100271002710027
the opportunity 	 0.0006920415224913495
A morphosyntactic 	 0.02
relies on 	 1.0
user-provided number 	 1.0
Frost , 	 1.0
British English 	 0.3333333333333333
evaluation purposes 	 0.018518518518518517
by human 	 0.017142857142857144
number -RRB- 	 0.046511627906976744
English prose 	 0.02702702702702703
that attempt 	 0.0035460992907801418
in the 	 0.2602996254681648
important -RRB- 	 0.0625
a distinction 	 0.001226993865030675
selects important 	 0.5
many as 	 0.019230769230769232
second system 	 0.1
content -LRB- 	 0.08333333333333333
1 -LRB- 	 0.25
<s> Voice 	 0.0007686395080707148
signal . 	 0.16666666666666666
basically a 	 1.0
clearly many 	 0.3333333333333333
quantitative evaluation 	 0.5
Spontaneous Speech 	 1.0
's job 	 0.0392156862745098
commercializing paper-to-computer 	 1.0
that statistical 	 0.0035460992907801418
and align 	 0.001445086705202312
on-line character 	 0.3333333333333333
common-sense reasoning 	 1.0
the correct 	 0.004152249134948097
on Hidden 	 0.0047169811320754715
the special 	 0.0006920415224913495
Hardy , 	 1.0
its own 	 0.14285714285714285
by keyphrase 	 0.005714285714285714
or might 	 0.0045045045045045045
table is 	 0.14285714285714285
questions . 	 0.07692307692307693
name -LRB- 	 0.2
and sentences 	 0.002890173410404624
The task 	 0.020833333333333332
their solutions 	 0.029411764705882353
Computer Products 	 0.3333333333333333
Aided Human 	 0.3333333333333333
, Why 	 0.0005614823133071309
stock . 	 0.6666666666666666
recognition research 	 0.008264462809917356
also refer 	 0.014492753623188406
problem , 	 0.09090909090909091
agree on 	 0.3333333333333333
being used 	 0.05555555555555555
translate text 	 0.3333333333333333
English on 	 0.02702702702702703
regular expressions 	 1.0
Intra-texual methods 	 1.0
little inflectional 	 0.3333333333333333
Text simplification 	 0.16666666666666666
into all 	 0.01282051282051282
F-score , 	 1.0
aircraft , 	 0.2857142857142857
Wallace Chafe 	 1.0
likely uttered 	 0.0625
-LRB- IE 	 0.005420054200542005
other systems 	 0.02857142857142857
perfectly , 	 1.0
<s> Question 	 0.003843197540353574
appear often 	 0.0625
Black E. 	 0.5
Hands-free computing 	 1.0
be representative 	 0.004219409282700422
, Ann 	 0.0005614823133071309
real-world applications 	 0.16666666666666666
existing summaries 	 0.2
-- 10 	 0.04
summarization is 	 0.12
the president 	 0.001384083044982699
might also 	 0.038461538461538464
forms than 	 0.16666666666666666
be difficult 	 0.004219409282700422
the words 	 0.004152249134948097
<s> Unfortunately 	 0.0007686395080707148
feature is 	 0.07692307692307693
approach ; 	 0.02857142857142857
distinctions are 	 0.5
would output 	 0.018867924528301886
replicated his 	 1.0
light on 	 0.3333333333333333
bag of 	 1.0
one used 	 0.015384615384615385
consecutive years 	 0.5
understood only 	 1.0
sentiment -LRB- 	 0.04
-RRB- question 	 0.0027100271002710027
recognition : 	 0.01652892561983471
card imprints 	 0.25
a greater 	 0.001226993865030675
gold standards 	 0.16666666666666666
see Inter-rater 	 0.05
definition of 	 0.6
quality standards 	 0.1
various constructions 	 0.05555555555555555
in agglutinative 	 0.0018726591760299626
new token 	 0.041666666666666664
of written 	 0.0035650623885918
digits `` 	 1.0
caused problems 	 1.0
limited number 	 0.2
has the 	 0.023809523809523808
metrics : 	 0.1111111111111111
US patent 	 0.2857142857142857
sub-categories . 	 1.0
which may 	 0.007246376811594203
around Documents 	 0.125
trained on 	 0.3333333333333333
`` beyond 	 0.005291005291005291
so has 	 0.03333333333333333
of `` 	 0.0071301247771836
the area 	 0.001384083044982699
: task-based 	 0.00980392156862745
usually takes 	 0.03125
human might 	 0.021739130434782608
also stems 	 0.014492753623188406
in abstractive 	 0.0018726591760299626
has yet 	 0.011904761904761904
make the 	 0.05
be identified 	 0.008438818565400843
processing systems 	 0.05555555555555555
It consists 	 0.02631578947368421
turn a 	 0.16666666666666666
automates the 	 1.0
one observation 	 0.015384615384615385
example Mr. 	 0.012345679012345678
very useful 	 0.04878048780487805
Once these 	 0.2
were then 	 0.024390243902439025
the input 	 0.005536332179930796
Recall-Oriented Understudy 	 1.0
when a 	 0.11428571428571428
whole words 	 0.1111111111111111
on statistical 	 0.009433962264150943
generates summaries 	 0.3333333333333333
distinction with 	 0.2
MLLT -RRB- 	 1.0
-LRB- Lehnert 	 0.005420054200542005
français . 	 1.0
<s> Therefore 	 0.0015372790161414297
read musical 	 0.14285714285714285
chatterbots such 	 0.5
rule should 	 0.3333333333333333
at binary 	 0.014705882352941176
ACL Anthology 	 0.5
1971 Terry 	 0.3333333333333333
analytical approaches 	 0.5
for mental 	 0.0036101083032490976
of around 	 0.0017825311942959
readable summary 	 0.3333333333333333
in another 	 0.00749063670411985
as possible 	 0.017421602787456445
word recognition 	 0.016666666666666666
pilot effectiveness 	 0.2
-LRB- now 	 0.008130081300813009
paper . 	 0.09090909090909091
in Canada 	 0.0018726591760299626
six numbers 	 0.5
with isolated 	 0.00546448087431694
theories in 	 0.2
recommending '' 	 1.0
that identify 	 0.0035460992907801418
On a 	 0.16666666666666666
even , 	 0.037037037037037035
<s> require 	 0.0007686395080707148
written including 	 0.038461538461538464
on one 	 0.009433962264150943
lexical analysis 	 0.07692307692307693
statistical decision-making 	 0.030303030303030304
and confusability 	 0.001445086705202312
applying a 	 0.25
`` blocks 	 0.010582010582010581
boundaries and 	 0.09090909090909091
subset of 	 1.0
, gestures 	 0.0005614823133071309
The two 	 0.010416666666666666
sounds : 	 0.06666666666666667
learning techniques 	 0.023255813953488372
is identifying 	 0.0020325203252032522
to see 	 0.0026560424966799467
analyser to 	 1.0
general learning 	 0.045454545454545456
a method 	 0.0049079754601227
likely following 	 0.0625
popular journals 	 0.1111111111111111
made more 	 0.125
an opinion 	 0.007575757575757576
semi-supervised learning 	 0.5
controller tasks 	 0.25
on new 	 0.0047169811320754715
say your 	 0.14285714285714285
were performed 	 0.024390243902439025
<s> TextRank 	 0.0023059185242121443
details the 	 0.5
M. 1999 	 0.25
results show 	 0.047619047619047616
then used 	 0.02857142857142857
take into 	 0.3
Web or 	 0.1111111111111111
Records -LRB- 	 1.0
and multitude 	 0.001445086705202312
polarity classification 	 0.125
transcended the 	 1.0
you say 	 0.07692307692307693
second edition 	 0.1
a fashion 	 0.001226993865030675
such large 	 0.008130081300813009
of guessing 	 0.00089126559714795
Bayes classifier 	 0.3333333333333333
speech With 	 0.006578947368421052
statistical output 	 0.030303030303030304
likely part 	 0.0625
achieved by 	 0.2
several modules 	 0.045454545454545456
a hierarchy 	 0.001226993865030675
and 2009 	 0.001445086705202312
, e.g. 	 0.005614823133071308
World War 	 0.14285714285714285
algorithms . 	 0.11428571428571428
-- makes 	 0.04
texts , 	 0.11764705882352941
character -RRB- 	 0.045454545454545456
- based 	 0.1875
representations are 	 0.25
-RRB- coefficients 	 0.0027100271002710027
more often 	 0.010526315789473684
speech . 	 0.06578947368421052
and copying 	 0.001445086705202312
now called 	 0.07692307692307693
These findings 	 0.058823529411764705
themselves as 	 0.25
so it 	 0.06666666666666667
groups within 	 0.2
speakers . 	 0.25
analysis is 	 0.015384615384615385
for several 	 0.007220216606498195
all perform 	 0.023255813953488372
through its 	 0.125
be nested 	 0.004219409282700422
which had 	 0.007246376811594203
encoding world 	 1.0
<s> Prior 	 0.0007686395080707148
cognition and 	 1.0
University introduced 	 0.1111111111111111
automatically and 	 0.09523809523809523
the so-called 	 0.0006920415224913495
Canada Post 	 0.16666666666666666
, containing 	 0.0011229646266142617
algorithms are 	 0.05714285714285714
test environment 	 0.1
attitude may 	 0.5
things , 	 0.3333333333333333
the Parliament 	 0.0006920415224913495
NLG is 	 0.09523809523809523
create odd 	 0.058823529411764705
principled way 	 1.0
document\/text genre 	 0.5
computers for 	 0.1111111111111111
e.g. transformational 	 0.017857142857142856
<s> Programming 	 0.0015372790161414297
→ <verb> 	 0.3333333333333333
<s> Aggregation 	 0.0007686395080707148
and Language 	 0.005780346820809248
of 1,000 	 0.00089126559714795
proposed . 	 0.1111111111111111
portions . 	 1.0
of assertions 	 0.00089126559714795
taken up 	 0.3333333333333333
applications it 	 0.04
handover system 	 1.0
speech tools 	 0.006578947368421052
and non-linear 	 0.001445086705202312
users with 	 0.1111111111111111
linguistic formalism 	 0.0625
intelligence . 	 0.125
and extrinsic 	 0.001445086705202312
parse garden-path 	 0.1111111111111111
of whole 	 0.0017825311942959
an important 	 0.015151515151515152
second -RRB- 	 0.1
Research Institute 	 0.125
with medium 	 0.00546448087431694
and questions 	 0.001445086705202312
true keyphrases 	 0.5
there is 	 0.35
easier than 	 0.25
for breathing 	 0.0036101083032490976
words coming 	 0.009174311926605505
the size 	 0.001384083044982699
a translation 	 0.00245398773006135
that person 	 0.0035460992907801418
hand , 	 0.5
produce useful 	 0.045454545454545456
van Dijk 	 0.5
proper evaluation 	 0.14285714285714285
the simple 	 0.0006920415224913495
S. , 	 1.0
exception , 	 1.0
's EndWar 	 0.0196078431372549
over vertices 	 0.08333333333333333
currently focus 	 0.14285714285714285
the record 	 0.0006920415224913495
call routing 	 0.3333333333333333
judges can 	 0.5
an entire 	 0.007575757575757576
<s> Morphological 	 0.0007686395080707148
basic knowledge 	 0.07692307692307693
'' corpora 	 0.005154639175257732
valid summary 	 1.0
attached to 	 0.5
sound should 	 0.1
predict keyphrases 	 0.16666666666666666
vendors began 	 0.25
prior work 	 0.3333333333333333
is unable 	 0.0020325203252032522
leverages the 	 1.0
Intrinsic evaluations 	 0.3333333333333333
his PhD 	 0.08333333333333333
in politics 	 0.0018726591760299626
which structured 	 0.007246376811594203
interaction The 	 0.125
are broken 	 0.004149377593360996
in multiple 	 0.0018726591760299626
about this 	 0.025
changed direction 	 0.5
an in-depth 	 0.007575757575757576
tagger is 	 0.1111111111111111
of question 	 0.00267379679144385
receivers . 	 1.0
five different 	 0.2
ambiguity in 	 0.125
Correct answers 	 1.0
to current 	 0.0013280212483399733
the probabilities 	 0.002768166089965398
utterance can 	 0.3333333333333333
web page 	 0.125
mapping the 	 0.5
marketing . 	 1.0
vs. extrinsic 	 0.08333333333333333
1994 , 	 1.0
input -RRB- 	 0.024390243902439025
source - 	 0.041666666666666664
them with 	 0.05263157894736842
automatically evaluating 	 0.047619047619047616
written , 	 0.038461538461538464
sufficient iteration 	 0.2
are useful 	 0.004149377593360996
edge if 	 0.3333333333333333
Battle management 	 0.5
and noise 	 0.001445086705202312
trainee controller 	 1.0
a consideration 	 0.001226993865030675
lower levels 	 0.4
night . 	 1.0
in full 	 0.0018726591760299626
Example-based machine 	 0.6666666666666666
termed `` 	 0.5
the part 	 0.001384083044982699
religious services 	 1.0
error rate 	 0.4166666666666667
<s> Head-driven 	 0.0007686395080707148
Patent 2,663,758 	 0.3333333333333333
classifier that 	 0.14285714285714285
Prolog generally 	 1.0
The hidden 	 0.005208333333333333
on word 	 0.0047169811320754715
Other issues 	 0.14285714285714285
neural network 	 0.3333333333333333
out that 	 0.07142857142857142
the presentation 	 0.0006920415224913495
end . 	 0.125
also obtained 	 0.014492753623188406
standard can 	 0.07142857142857142
better scoring 	 0.1111111111111111
, Ruth 	 0.0005614823133071309
Weaver wrote 	 1.0
by precision 	 0.005714285714285714
stationary signal 	 0.2857142857142857
segments -LRB- 	 0.2
the form 	 0.0006920415224913495
Designing a 	 1.0
, Barbara 	 0.0005614823133071309
been tried 	 0.029411764705882353
can perform 	 0.0055248618784530384
Answer formulation 	 0.3333333333333333
data be 	 0.012987012987012988
bank . 	 1.0
: Vocabulary 	 0.00980392156862745
popularity as 	 1.0
a category 	 0.001226993865030675
and counter 	 0.001445086705202312
phoneme , 	 0.5
J. Phillips 	 0.3333333333333333
parser that 	 0.0625
tag for 	 0.0625
learning approaches 	 0.023255813953488372
part that 	 0.037037037037037035
probabilistic modeling 	 0.14285714285714285
or desired 	 0.0045045045045045045
strokes for 	 1.0
that adjectives 	 0.0035460992907801418
punctuation , 	 0.2857142857142857
experience is 	 0.5
of 15-20 	 0.00089126559714795
recognition scores 	 0.008264462809917356
usually termed 	 0.03125
forecasts in 	 0.2
See chart 	 0.16666666666666666
a process 	 0.0049079754601227
stochastic semantic 	 0.125
later part-of-speech 	 0.1
English alphabet 	 0.05405405405405406
for testing 	 0.0036101083032490976
factor -LRB- 	 0.5
to remain 	 0.0013280212483399733
representation and 	 0.10526315789473684
is easy 	 0.0020325203252032522
contains additional 	 0.1
is some 	 0.0020325203252032522
or EHR 	 0.0045045045045045045
tongues sharing 	 1.0
all possible 	 0.06976744186046512
'' text 	 0.005154639175257732
systems that 	 0.0625
, Michael 	 0.0022459292532285235
sets from 	 0.09090909090909091
Unicode Consortium 	 1.0
the APEXC 	 0.0006920415224913495
way they 	 0.041666666666666664
or length 	 0.0045045045045045045
semitied covariance 	 1.0
form a 	 0.15
The rise 	 0.005208333333333333
Translation '' 	 0.3333333333333333
research direction 	 0.023809523809523808
Schroeder , 	 1.0
solved first 	 0.2
his famous 	 0.08333333333333333
be output 	 0.004219409282700422
additional constraints 	 0.16666666666666666
without much 	 0.07692307692307693
, analyze 	 0.0005614823133071309
source sentence 	 0.08333333333333333
grammar which 	 0.05405405405405406
video -RRB- 	 0.2
step , 	 0.13333333333333333
defined only 	 0.16666666666666666
studying the 	 1.0
distribution of 	 0.25
9 parts 	 1.0
represent natural 	 0.1111111111111111
textual summary 	 0.2
the harder 	 0.0020761245674740486
two levels 	 0.034482758620689655
think of 	 0.3333333333333333
not used 	 0.017857142857142856
rarely have 	 0.3333333333333333
In 1983 	 0.009523809523809525
simple as 	 0.07692307692307693
characteristics of 	 0.5
both use 	 0.03225806451612903
or a 	 0.08558558558558559
was connected 	 0.012987012987012988
will generate 	 0.08571428571428572
process broken 	 0.027777777777777776
-LRB- sailor 	 0.005420054200542005
extraction , 	 0.1935483870967742
those running 	 0.045454545454545456
-LRB- a 	 0.013550135501355014
, Paul 	 0.0016844469399213925
Structure , 	 1.0
of possibilities 	 0.00089126559714795
= Noun 	 0.1111111111111111
a 70 	 0.001226993865030675
If there 	 0.1
, Ingria 	 0.0005614823133071309
by hand 	 0.03428571428571429
background material 	 0.3333333333333333
, large 	 0.0005614823133071309
coherent and 	 0.2
basic approach 	 0.07692307692307693
with Japanese 	 0.00546448087431694
HLDA -RRB- 	 1.0
before or 	 0.16666666666666666
paragraph boundaries 	 0.3333333333333333
controversy is 	 1.0
names . 	 0.2857142857142857
and knowledge 	 0.001445086705202312
produce vowels 	 0.045454545454545456
mechanisms of 	 0.5
, 13 	 0.0005614823133071309
the set 	 0.001384083044982699
theory Functional 	 0.07692307692307693
in which 	 0.0149812734082397
for Gisting 	 0.007220216606498195
an instance 	 0.007575757575757576
aircraft platforms 	 0.14285714285714285
widely-reported news 	 1.0
human translation 	 0.043478260869565216
processing , 	 0.16666666666666666
theory , 	 0.3076923076923077
located anywhere 	 1.0
on his 	 0.0047169811320754715
speaker dependent 	 0.05555555555555555
relevant content 	 0.14285714285714285
most notably 	 0.017241379310344827
<s> Efficient 	 0.0007686395080707148
d'évaluation de 	 1.0
disassembling and 	 1.0
than sixty 	 0.022222222222222223
naturally spoken 	 0.5
appear in 	 0.4375
meaning and 	 0.043478260869565216
is typical 	 0.0020325203252032522
separate out 	 0.1
-RRB- are 	 0.008130081300813009
a relic 	 0.001226993865030675
league over 	 1.0
without a 	 0.07692307692307693
cope with 	 1.0
by metrics 	 0.005714285714285714
search corpus 	 0.09090909090909091
typically around 	 0.05555555555555555
NC '' 	 1.0
Ratliff originally 	 1.0
the nature 	 0.0034602076124567475
using edges 	 0.01694915254237288
felt -RRB- 	 1.0
, relatively 	 0.0005614823133071309
that approximate 	 0.0035460992907801418
compute various 	 0.5
The fact 	 0.005208333333333333
evaluation data 	 0.018518518518518517
algorithms , 	 0.14285714285714285
power resulting 	 0.25
simply do 	 0.08333333333333333
analysis model 	 0.015384615384615385
about it 	 0.025
echoes , 	 1.0
be turned 	 0.004219409282700422
documents is 	 0.02631578947368421
, discontinuous 	 0.0005614823133071309
in one 	 0.00749063670411985
Adverse conditions 	 1.0
Blind -LRB- 	 0.5
of valuable 	 0.00089126559714795
the U.S. 	 0.002768166089965398
tagger to 	 0.1111111111111111
actual data 	 0.2
very broad 	 0.04878048780487805
typical machine-learning-based 	 0.1111111111111111
specific right 	 0.047619047619047616
in healthcare 	 0.0018726591760299626
second ; 	 0.1
summarization based 	 0.02
course , 	 0.3333333333333333
neighbors , 	 0.3333333333333333
and attempt 	 0.001445086705202312
walking more 	 0.3333333333333333
and Civil 	 0.001445086705202312
e.g. the 	 0.05357142857142857
good for 	 0.07692307692307693
been devoted 	 0.014705882352941176
and voice 	 0.001445086705202312
male-female normalization 	 1.0
a relaxed 	 0.001226993865030675
organised by 	 1.0
one to 	 0.03076923076923077
redundancy in 	 0.6666666666666666
Part-of-speech Tagging 	 0.5
certain cases 	 0.14285714285714285
satisfactory in 	 1.0
associated score 	 0.25
in multiscript 	 0.0018726591760299626
the acoustic 	 0.0006920415224913495
a five-star 	 0.001226993865030675
The phenomenon 	 0.005208333333333333
for large-vocabulary 	 0.0036101083032490976
represented as 	 0.3333333333333333
-RRB- : 	 0.024390243902439025
by mapping 	 0.005714285714285714
corpus -LRB- 	 0.03225806451612903
attempts at 	 0.3333333333333333
Training air 	 0.5
language by 	 0.006756756756756757
dictionary entries 	 0.14285714285714285
of extracted 	 0.00089126559714795
context data 	 0.030303030303030304
by limiting 	 0.005714285714285714
fall in 	 0.25
, Emanuel 	 0.0011229646266142617
problems Processes 	 0.058823529411764705
classification '' 	 0.29411764705882354
nuggets of 	 1.0
platform for 	 0.5
course of 	 0.3333333333333333
by highlighting 	 0.005714285714285714
adviser for 	 1.0
most authoritative 	 0.017241379310344827
some invalid 	 0.012048192771084338
Linguistics -LRB- 	 0.3333333333333333
versions needed 	 0.3333333333333333
freely available 	 1.0
from systems 	 0.009615384615384616
i.e. determining 	 0.05263157894736842
measure based 	 0.09090909090909091
like summarization 	 0.03571428571428571
for health 	 0.0036101083032490976
on absorbing 	 0.0047169811320754715
expansion . 	 0.3333333333333333
Canada . 	 0.16666666666666666
called query-biased 	 0.05555555555555555
both affine 	 0.03225806451612903
<s> Described 	 0.0007686395080707148
of nodes 	 0.0035650623885918
what kinds 	 0.03125
articulation , 	 1.0
if the 	 0.35714285714285715
voice response 	 0.07692307692307693
morphologically rich 	 1.0
absorbing states 	 0.3333333333333333
save time 	 1.0
Many machine 	 0.08333333333333333
the letters 	 0.0006920415224913495
whereas in 	 0.3333333333333333
the 1970s 	 0.0006920415224913495
labor involved 	 0.5
register by 	 1.0
shortened version 	 1.0
Natural Language 	 0.25
entire content 	 0.3333333333333333
adverbs , 	 1.0
integrated part 	 0.3333333333333333
trade speed 	 0.5
CWA -RRB- 	 1.0
harder it 	 0.14285714285714285
talk-in-interaction . 	 1.0
telegraph code 	 1.0
-LRB- though 	 0.0027100271002710027
which bore 	 0.007246376811594203
<s> Perspectives 	 0.0007686395080707148
in reference 	 0.003745318352059925
their features\/aspects 	 0.029411764705882353
and rich 	 0.001445086705202312
filtered by 	 0.3333333333333333
by many 	 0.005714285714285714
manipulate the 	 0.3333333333333333
to overfitting 	 0.0013280212483399733
Graesser , 	 1.0
large percentage 	 0.043478260869565216
to resolve 	 0.00398406374501992
to coherent 	 0.0013280212483399733
subjective information 	 0.3333333333333333
words . 	 0.14678899082568808
something similar 	 1.0
sound clip 	 0.1
The nodes 	 0.005208333333333333
is still 	 0.008130081300813009
discourse structure 	 0.027777777777777776
corresponding increase 	 0.16666666666666666
HMMs -RRB- 	 0.25
Web 2.0 	 0.1111111111111111
complexity of 	 0.6666666666666666
is worth 	 0.0040650406504065045
relevant information 	 0.14285714285714285
A procedure 	 0.02
symbol in 	 0.25
which have 	 0.014492753623188406
language might 	 0.006756756756756757
between words 	 0.05128205128205128
much about 	 0.045454545454545456
One way 	 0.07692307692307693
PCFGs -LRB- 	 1.0
phrase , 	 0.1
others they 	 0.08333333333333333
say that 	 0.14285714285714285
ground established 	 1.0
user needs 	 0.07142857142857142
each dictionary 	 0.022222222222222223
more quickly 	 0.010526315789473684
Models are 	 0.3333333333333333
fine tune 	 0.5
only on 	 0.05263157894736842
deals with 	 1.0
any markup 	 0.03225806451612903
a speaker-dependent 	 0.001226993865030675
them which 	 0.05263157894736842
analysis to 	 0.015384615384615385
TextRank , 	 0.14285714285714285
example of 	 0.08641975308641975
some heuristic 	 0.012048192771084338
match each 	 0.16666666666666666
<s> Collection 	 0.0007686395080707148
notably by 	 0.3333333333333333
, Cleave 	 0.0005614823133071309
if and 	 0.03571428571428571
RCA collaborated 	 0.2
devised to 	 0.5
dog bites 	 0.3333333333333333
abstraction can 	 0.25
the founder 	 0.0006920415224913495
terminate a 	 1.0
especially inflectional 	 0.06666666666666667
analyzed with 	 0.2
meaningful units 	 0.125
's subscription 	 0.0196078431372549
Edmund Fournier 	 1.0
integrated information 	 0.3333333333333333
, international 	 0.0005614823133071309
classifier for 	 0.14285714285714285
between posts 	 0.02564102564102564
inherent expressivity 	 1.0
from those 	 0.019230769230769232
world assumption 	 0.13333333333333333
and ontology 	 0.001445086705202312
personal computers 	 0.25
opinions or 	 0.5
pauses . 	 0.25
NLG researchers 	 0.047619047619047616
that read 	 0.0035460992907801418
those using 	 0.045454545454545456
flight '' 	 0.5
and occurs 	 0.001445086705202312
: Transfer-based 	 0.00980392156862745
constraints ; 	 0.25
New Orleans 	 1.0
OCR is 	 0.061224489795918366
, 5000 	 0.0005614823133071309
findings were 	 1.0
Grace project 	 1.0
or structures 	 0.0045045045045045045
1 % 	 0.5
using all 	 0.01694915254237288
difficult problem 	 0.03571428571428571
here , 	 0.5
is recognizing 	 0.0020325203252032522
discourse in 	 0.05555555555555555
Named entity 	 1.0
applications , 	 0.16
Relevance -LRB- 	 1.0
company for 	 0.3333333333333333
precision - 	 0.2
the peak 	 0.0006920415224913495
signal or 	 0.16666666666666666
distances for 	 0.5
a large 	 0.0098159509202454
to think 	 0.0013280212483399733
-LRB- Style 	 0.0027100271002710027
direction is 	 0.3333333333333333
well enough 	 0.03571428571428571
the computer 	 0.001384083044982699
automatically focus 	 0.047619047619047616
operations . 	 1.0
continued development 	 0.1111111111111111
CKY algorithm 	 1.0
an average 	 0.007575757575757576
working from 	 0.14285714285714285
'' all 	 0.005154639175257732
, produced 	 0.0016844469399213925
Liberman M. 	 1.0
, vehicle 	 0.0005614823133071309
sentence -LRB- 	 0.020833333333333332
<s> Introduction 	 0.0007686395080707148
that domain 	 0.0035460992907801418
distribution ; 	 0.25
Conversation analysis 	 1.0
LMF -RRB- 	 1.0
we may 	 0.022222222222222223
and you 	 0.004335260115606936
proposed by 	 0.1111111111111111
had plateaued 	 0.07142857142857142
vagueness of 	 1.0
upper-case letter 	 1.0
large-vocabulary speech 	 0.6666666666666666
Process . 	 1.0
are discussed 	 0.004149377593360996
features into 	 0.038461538461538464
whole word 	 0.1111111111111111
a handheld 	 0.001226993865030675
clean hand-printed 	 0.5
the Royal 	 0.001384083044982699
Among these 	 1.0
voice applications 	 0.07692307692307693
to compare 	 0.005312084993359893
of Hearing 	 0.00089126559714795
to acquire 	 0.0013280212483399733
versions of 	 0.3333333333333333
only succeeding 	 0.02631578947368421
solutions . 	 0.5
SR systems 	 0.3333333333333333
that act 	 0.0035460992907801418
algorithm could 	 0.03571428571428571
McCarthy coined 	 1.0
conditions in 	 0.2
more generally 	 0.010526315789473684
Corpus . 	 0.0625
`` angry 	 0.005291005291005291
databases . 	 0.5
in other 	 0.009363295880149813
confusable words 	 1.0
to ease 	 0.0013280212483399733
are capitalized 	 0.004149377593360996
compare the 	 0.2857142857142857
segment , 	 0.2222222222222222
ELIZA . 	 0.1111111111111111
to place 	 0.0026560424966799467
Natural '' 	 0.08333333333333333
field , 	 0.037037037037037035
at Birkbeck 	 0.029411764705882353
faster but 	 0.3333333333333333
spirit to 	 1.0
speaker normalization 	 0.05555555555555555
stability in 	 1.0
global ' 	 0.3333333333333333
one may 	 0.015384615384615385
? '' 	 0.375
represent . 	 0.2222222222222222
existing words 	 0.2
of navigation 	 0.00089126559714795
Shepard 's 	 0.3333333333333333
by air 	 0.011428571428571429
connected text 	 0.2
answer , 	 0.03333333333333333
the cost 	 0.0006920415224913495
examples for 	 0.041666666666666664
the fields 	 0.0006920415224913495
a sample 	 0.001226993865030675
classifier module 	 0.14285714285714285
for sound 	 0.0036101083032490976
a universal 	 0.001226993865030675
the translator 	 0.0006920415224913495
the identification 	 0.001384083044982699
the can 	 0.0006920415224913495
and ushered 	 0.001445086705202312
in conjunction 	 0.003745318352059925
concerned with 	 0.8
the difficulties 	 0.0006920415224913495
sentiment classification 	 0.04
i.e. so 	 0.05263157894736842
at Cognitive 	 0.014705882352941176
quite high 	 0.125
these natural 	 0.023809523809523808
, Eurospeech\/ICSLP 	 0.0005614823133071309
representing printed 	 0.5
a smaller 	 0.001226993865030675
tuned weights 	 1.0
Brown Corpus 	 0.8571428571428571
in artificial 	 0.0018726591760299626
without intervening 	 0.07692307692307693
comes into 	 0.2
B , 	 1.0
two conflicting 	 0.034482758620689655
eigenvalue 1 	 1.0
of such 	 0.004456327985739751
held each 	 1.0
pioneered this 	 0.3333333333333333
intelligence that 	 0.25
quantitative one 	 0.25
domain tends 	 0.05
how a 	 0.06896551724137931
FST , 	 1.0
context-free approximation 	 0.09090909090909091
in context 	 0.00749063670411985
summaries , 	 0.06976744186046512
-LRB- JSF 	 0.0027100271002710027
some summarization 	 0.012048192771084338
given corpus 	 0.041666666666666664
a program 	 0.0049079754601227
of n-dimensional 	 0.00089126559714795
T unigrams 	 0.3333333333333333
30 years 	 0.6666666666666666
as EAGLi 	 0.003484320557491289
graph using 	 0.07692307692307693
book -LRB- 	 0.25
Hulth uses 	 0.3333333333333333
structured real-world 	 0.16666666666666666
Automatic vs. 	 0.1111111111111111
article is 	 0.034482758620689655
one can 	 0.015384615384615385
, distinct 	 0.0005614823133071309
evaluation might 	 0.018518518518518517
how often 	 0.034482758620689655
past thirty 	 0.3333333333333333
a highly 	 0.001226993865030675
him perform 	 0.5
converting printed 	 0.5
waves . 	 0.14285714285714285
improve machine 	 0.07692307692307693
four steps 	 0.14285714285714285
The technology 	 0.005208333333333333
whole of 	 0.1111111111111111
have questioned 	 0.009615384615384616
we produce 	 0.044444444444444446
interference between 	 1.0
in CSR 	 0.0018726591760299626
apple the 	 0.3333333333333333
form of 	 0.35
accurate even 	 0.14285714285714285
cost related 	 0.5
thus be 	 0.1
as each 	 0.003484320557491289
applying PageRank 	 0.5
and cognition 	 0.001445086705202312
performs simple 	 1.0
also referred 	 0.014492753623188406
Books like 	 1.0
influence in 	 1.0
As this 	 0.05555555555555555
IE -RRB- 	 0.6666666666666666
ca n't 	 1.0
stop character 	 1.0
cursive script 	 0.4
`` tag 	 0.010582010582010581
sentiment is 	 0.04
The unsupervised 	 0.005208333333333333
and when 	 0.001445086705202312
closer to 	 1.0
higher level 	 0.14285714285714285
action . 	 0.2
training step 	 0.07142857142857142
construct lightweight 	 0.3333333333333333
suffix '' 	 1.0
me the 	 1.0
perform a 	 0.2727272727272727
identifying relevant 	 0.16666666666666666
Statistical Methods 	 0.1111111111111111
rudimentary translation 	 0.5
dBase system 	 1.0
advanced scanning 	 0.4
similar `` 	 0.037037037037037035
He taught 	 0.125
<s> Recognition 	 0.0015372790161414297
released speech 	 0.5
the CCD 	 0.0006920415224913495
guesses ' 	 1.0
, Harvey 	 0.0005614823133071309
program to 	 0.09090909090909091
currency for 	 1.0
planning an 	 0.5
acoustics -RRB- 	 1.0
are based 	 0.02074688796680498
US ports 	 0.14285714285714285
-LRB- so 	 0.005420054200542005
discourse studies 	 0.027777777777777776
the Armed 	 0.0006920415224913495
, D 	 0.0005614823133071309
separately from 	 1.0
some common 	 0.012048192771084338
-RRB- . 	 0.27371273712737126
ability to 	 0.75
to this 	 0.00796812749003984
QA performance 	 0.047619047619047616
necessary for 	 0.3
allow blind 	 0.2
letter that 	 0.16666666666666666
but has 	 0.014705882352941176
pars -LRB- 	 1.0
measured in 	 0.16666666666666666
was extensively 	 0.012987012987012988
vibrates per 	 1.0
judgments . 	 1.0
moderate with 	 0.2
transfer-based machine 	 0.6666666666666666
Another good 	 0.07692307692307693
Europe began 	 0.2
Issues In 	 0.5
we say 	 0.044444444444444446
walking slowly 	 0.3333333333333333
Paragraph Structure 	 1.0
for more 	 0.007220216606498195
broken into 	 0.6
most sentiment 	 0.017241379310344827
Mutual Information 	 1.0
prepared , 	 1.0
by Frost 	 0.005714285714285714
storing , 	 1.0
clarify a 	 1.0
a problem 	 0.0036809815950920245
views as 	 1.0
as text 	 0.003484320557491289
analysis depends 	 0.015384615384615385
represent as 	 0.1111111111111111
that part-of-speech 	 0.0035460992907801418
sold by 	 0.3333333333333333
So , 	 0.3333333333333333
not context-free 	 0.008928571428571428
Keyphrase extraction 	 0.5
many speech 	 0.019230769230769232
systems must 	 0.008928571428571428
standard is 	 0.07142857142857142
connected by 	 0.2
an automatic 	 0.015151515151515152
depending on 	 0.75
vibration , 	 1.0
, NAACL 	 0.0005614823133071309
one within 	 0.015384615384615385
devoted in 	 0.2
a lexicon 	 0.00245398773006135
same string 	 0.04
difficult for 	 0.03571428571428571
an isolated 	 0.007575757575757576
while Carnegie 	 0.05
previously-written human 	 1.0
phrases , 	 0.125
in our 	 0.0018726591760299626
of hyphenation 	 0.00089126559714795
For instance 	 0.11475409836065574
e.g. from 	 0.017857142857142856
larger corpus 	 0.125
, can 	 0.003368893879842785
compiler or 	 0.3333333333333333
confused with 	 1.0
technique used 	 0.14285714285714285
produce interpretable 	 0.045454545454545456
prefer the 	 0.5
lexical resources 	 0.07692307692307693
somewhat shallow 	 0.5
left , 	 0.16666666666666666
ambiguous when 	 0.08333333333333333
is lessened 	 0.0020325203252032522
largely similar 	 0.2
discriminant analysis 	 1.0
Arabic , 	 0.25
that are 	 0.05319148936170213
assignment of 	 0.5
is fairly 	 0.0020325203252032522
Amplitude -LRB- 	 1.0
the IEEE 	 0.001384083044982699
Since the 	 0.2
the objectives 	 0.0006920415224913495
was nearly 	 0.012987012987012988
from an 	 0.009615384615384616
thus has 	 0.1
methods . 	 0.022727272727272728
hear sound 	 0.5
did Joe 	 0.2
is closer 	 0.0020325203252032522
words in 	 0.09174311926605505
, abstracts 	 0.0005614823133071309
these good 	 0.023809523809523808
pollen levels 	 0.6923076923076923
particular case 	 0.07692307692307693
strong feeling 	 0.25
factors , 	 0.3333333333333333
representation system 	 0.05263157894736842
its best 	 0.02857142857142857
translation Example-based 	 0.013513513513513514
, especially 	 0.0050533408197641775
sentiment based 	 0.04
methods achieved 	 0.022727272727272728
generated by 	 0.06666666666666667
About 90 	 0.5
now looking 	 0.07692307692307693
collection . 	 0.2
entertaining can 	 0.5
formalisms are 	 0.5
that TextRank 	 0.0070921985815602835
may explore 	 0.019230769230769232
new part 	 0.041666666666666664
2002 a 	 0.5
in 1954 	 0.003745318352059925
Gaussians , 	 1.0
inflection is 	 1.0
symbol . 	 0.5
message boards 	 0.5
but steadily 	 0.014705882352941176
strength score 	 0.2
Processing -RRB- 	 0.25
during World 	 0.1
to provide 	 0.005312084993359893
portable . 	 0.3333333333333333
When a 	 0.14285714285714285
expressions that 	 0.3333333333333333
period may 	 0.5
distorted , 	 0.5
not pre 	 0.008928571428571428
answer may 	 0.03333333333333333
speech , 	 0.07236842105263158
page including 	 0.14285714285714285
conversation with 	 0.5
grammatical contexts 	 0.09090909090909091
By having 	 0.3333333333333333
stages are 	 0.5
recognizing and 	 0.2
conversion . 	 0.3333333333333333
In 1971 	 0.009523809523809525
be any 	 0.004219409282700422
90 % 	 1.0
1950s included 	 0.25
most linguists 	 0.017241379310344827
to high 	 0.00398406374501992
generally refers 	 0.09090909090909091
resolve ambiguities 	 0.5
task entirely 	 0.023809523809523808
by question 	 0.005714285714285714
include straightforward 	 0.037037037037037035
knowledge comes 	 0.037037037037037035
2,000 words 	 0.5
images environment 	 0.16666666666666666
levels is 	 0.045454545454545456
What you 	 0.09090909090909091
are maximum 	 0.004149377593360996
EAGLi for 	 1.0
defines components 	 0.5
1970 -RRB- 	 0.3333333333333333
Wireless World 	 1.0
the years 	 0.0006920415224913495
consumption -RRB- 	 1.0
EMR according 	 0.3333333333333333
Consultant -LRB- 	 1.0
star scale 	 0.5
The simplest 	 0.005208333333333333
already been 	 0.4
very early 	 0.024390243902439025
size , 	 0.16666666666666666
, word 	 0.0005614823133071309
they think 	 0.025
document can 	 0.027777777777777776
and artificial 	 0.001445086705202312
's SPHINX 	 0.0196078431372549
questioner , 	 0.5
available , 	 0.23529411764705882
singular common 	 0.25
can use 	 0.016574585635359115
in information 	 0.0018726591760299626
name of 	 0.2
`` automatic 	 0.005291005291005291
History The 	 0.5
computer databases 	 0.022727272727272728
`` unsupervised 	 0.005291005291005291
sub-sounds , 	 1.0
any feedback 	 0.03225806451612903
The reader 	 0.005208333333333333
a person\/persons 	 0.001226993865030675
translation method 	 0.013513513513513514
or five 	 0.0045045045045045045
five-star scale 	 1.0
If probabilities 	 0.1
Popular speech 	 1.0
using sentence 	 0.01694915254237288
is leading 	 0.0020325203252032522
positives by 	 1.0
not absolutely 	 0.008928571428571428
the complex 	 0.0006920415224913495
Methods for 	 0.5
% , 	 0.05128205128205128
translation is 	 0.013513513513513514
The interpretation 	 0.005208333333333333
development of 	 0.5833333333333334
It proved 	 0.02631578947368421
computer-aided analysis 	 0.3333333333333333
rate . 	 0.09090909090909091
no incorrect 	 0.07692307692307693
, speech-to-text 	 0.0005614823133071309
E-set : 	 1.0
dogs the 	 0.14285714285714285
to digitize 	 0.0013280212483399733
text conversion 	 0.006289308176100629
Like keyphrase 	 0.5
grammar : 	 0.02702702702702703
was higher 	 0.012987012987012988
radio frequencies 	 1.0
meanings according 	 0.25
domain and 	 0.05
text segmentation 	 0.0440251572327044
images of 	 0.3333333333333333
-LRB- Hirschman 	 0.0027100271002710027
known to 	 0.07692307692307693
occurrence , 	 0.5
restrictions . 	 1.0
you see 	 0.07692307692307693
interoperability between 	 1.0
summaries for 	 0.023255813953488372
Jurafsky and 	 1.0
or continuous 	 0.0045045045045045045
systems -RRB- 	 0.008928571428571428
affect OCR 	 0.3333333333333333
these other 	 0.023809523809523808
handwritten , 	 0.5
communication Pragmatics 	 0.2
Finally , 	 1.0
by their 	 0.005714285714285714
linear regression 	 0.14285714285714285
logic , 	 0.25
In 1929 	 0.009523809523809525
binary judgement 	 0.25
main verb 	 0.125
free speech 	 0.25
of POS 	 0.0017825311942959
taggers . 	 0.14285714285714285
have used 	 0.019230769230769232
simulation is 	 0.3333333333333333
word is 	 0.06666666666666667
mentions '' 	 0.3333333333333333
in systems 	 0.003745318352059925
to , 	 0.0026560424966799467
relic of 	 1.0
as voicemail 	 0.003484320557491289
Evaluation An 	 0.1111111111111111
the contents 	 0.0006920415224913495
native speaker 	 1.0
singular forms 	 0.25
distinct ideas 	 0.14285714285714285
was also 	 0.025974025974025976
question and 	 0.023809523809523808
can identify 	 0.0055248618784530384
evaluation In 	 0.018518518518518517
make ; 	 0.05
different issue 	 0.02040816326530612
generators of 	 0.5
as subject 	 0.003484320557491289
greatly on 	 0.14285714285714285
language of 	 0.006756756756756757
pour le 	 1.0
machine translation 	 0.4936708860759494
document is 	 0.027777777777777776
classifying the 	 0.2
wreck a 	 1.0
Department of 	 1.0
syntax are 	 0.09090909090909091
that the 	 0.08156028368794327
characterized in 	 0.25
work well 	 0.041666666666666664
or generated 	 0.0045045045045045045
beforehand -LRB- 	 1.0
to narrative 	 0.0013280212483399733
of Quechua 	 0.00089126559714795
large text 	 0.043478260869565216
systems , 	 0.05357142857142857
several quality 	 0.045454545454545456
-RRB- do 	 0.0027100271002710027
check how 	 0.5
Santorini gives 	 1.0
to much 	 0.0013280212483399733
for language 	 0.0036101083032490976
ideal deep 	 1.0
the `` 	 0.0034602076124567475
a specific 	 0.006134969325153374
instance , 	 0.6428571428571429
open-access journal 	 1.0
Ask.com . 	 1.0
concept , 	 0.25
topics , 	 0.14285714285714285
spoken language 	 0.14285714285714285
perhaps , 	 0.16666666666666666
's 1990 	 0.0196078431372549
achieving high 	 0.5
tasks has 	 0.03125
CFG -LRB- 	 1.0
require the 	 0.18181818181818182
to millions 	 0.0013280212483399733
, stemming 	 0.0005614823133071309
get ranked 	 0.14285714285714285
to improve 	 0.01195219123505976
of machine 	 0.0071301247771836
relations among 	 0.16666666666666666
fly have 	 1.0
or grammatical 	 0.0045045045045045045
can decide 	 0.0055248618784530384
NLP with 	 0.02127659574468085
article such 	 0.034482758620689655
Web as 	 0.1111111111111111
directly from 	 0.2
the literature 	 0.0006920415224913495
column of 	 1.0
Use of 	 0.5
thus beyond 	 0.1
is red 	 0.0020325203252032522
acoustic modeling 	 0.3333333333333333
the proliferation 	 0.0006920415224913495
b -RRB- 	 1.0
rare -- 	 0.25
perspective . 	 0.25
semiotics , 	 1.0
translating . 	 0.25
input with 	 0.024390243902439025
structures of 	 0.2
on recognition 	 0.0047169811320754715
notoriously , 	 1.0
in 1993 	 0.0018726591760299626
, D. 	 0.0005614823133071309
where sentences 	 0.05714285714285714
does , 	 0.1
The Future 	 0.005208333333333333
-RRB- refer 	 0.0027100271002710027
can '' 	 0.011049723756906077
segmentation in 	 0.030303030303030304
simplified the 	 0.5
nuances and 	 1.0
parsers for 	 0.15384615384615385
Michael Schober 	 0.25
prone to 	 1.0
can then 	 0.0055248618784530384
are Deaf 	 0.004149377593360996
Swales , 	 1.0
mentioned the 	 0.16666666666666666
the mid 	 0.0006920415224913495
of applications 	 0.00267379679144385
commands . 	 0.4
himself translated 	 0.5
to `` 	 0.005312084993359893
complex sets 	 0.041666666666666664
as doctors 	 0.003484320557491289
issues relating 	 0.2
Solving System 	 0.5
Adda 1999 	 0.5
are known 	 0.012448132780082987
How to 	 0.2857142857142857
Automated Summarizers 	 0.5
some statistical 	 0.012048192771084338
minimum phone 	 0.5
a core 	 0.001226993865030675
, English-like 	 0.0005614823133071309
about basic 	 0.025
plant OCR 	 1.0
developing text 	 0.25
been shown 	 0.014705882352941176
is orthogonal 	 0.0020325203252032522
submit them 	 0.5
categorization , 	 1.0
words into 	 0.03669724770642202
a short 	 0.006134969325153374
country into 	 0.25
given formal 	 0.041666666666666664
Acoustical signals 	 0.5
<s> , 	 0.0007686395080707148
are generally 	 0.016597510373443983
weapon release 	 0.5
they consider 	 0.025
such ambiguity 	 0.024390243902439025
of distinct 	 0.00089126559714795
encouraged researchers 	 1.0
Foucault himself 	 0.3333333333333333
-LRB- digital 	 0.0027100271002710027
Future research 	 0.5
the effectiveness 	 0.0006920415224913495
evaluations , 	 0.16666666666666666
the complexity 	 0.005536332179930796
using random 	 0.01694915254237288
from social 	 0.009615384615384616
complex endeavors 	 0.041666666666666664
Richard Kittredge 	 1.0
to write 	 0.0013280212483399733
the vowel 	 0.001384083044982699
The combination 	 0.005208333333333333
first , 	 0.030303030303030304
Telephony and 	 1.0
not appear 	 0.008928571428571428
theory of 	 0.07692307692307693
be expected 	 0.012658227848101266
1,000 words 	 0.5
, German 	 0.0011229646266142617
, list 	 0.0005614823133071309
often , 	 0.022727272727272728
rephrase sentences 	 1.0
an inseparable 	 0.007575757575757576
adequately solved 	 1.0
useful summary 	 0.07142857142857142
of over 	 0.00089126559714795
's many 	 0.0196078431372549
Language , 	 0.08333333333333333
tagging will 	 0.04
, Margaret 	 0.0005614823133071309
warped '' 	 1.0
is publicly 	 0.0020325203252032522
But recognition 	 0.16666666666666666
weighted finite 	 0.3333333333333333
categories and 	 0.2222222222222222
every 10 	 0.3333333333333333
in evaluating 	 0.0018726591760299626
past decade 	 0.3333333333333333
related data 	 0.06666666666666667
or automotive 	 0.0045045045045045045
at MIT 	 0.029411764705882353
closed world 	 1.0
concatenated text 	 1.0
programs that 	 0.09090909090909091
programs . 	 0.2727272727272727
then run 	 0.02857142857142857
elementary sounds 	 1.0
job ; 	 0.5
of some 	 0.004456327985739751
parsing - 	 0.07142857142857142
In Europe 	 0.01904761904761905
... . 	 0.5
with DTW 	 0.00546448087431694
words and 	 0.06422018348623854
with boundary 	 0.00546448087431694
world of 	 0.06666666666666667
-LRB- 95 	 0.0027100271002710027
characterizes its 	 1.0
informative enough 	 0.5
is largely 	 0.0020325203252032522
an embedded 	 0.007575757575757576
increasing number 	 0.3333333333333333
proper syntax 	 0.14285714285714285
subjects with 	 1.0
for data 	 0.007220216606498195
campaign is 	 0.2
custom speech 	 0.5
sentences with 	 0.013157894736842105
slang . 	 1.0
averaged . 	 1.0
pronunciations or 	 1.0
semantic lexicons 	 0.047619047619047616
or its 	 0.0045045045045045045
off-line character 	 1.0
and count 	 0.001445086705202312
person uses 	 0.05263157894736842
predicted value 	 0.5
declared during 	 0.5
two classes 	 0.034482758620689655
started to 	 0.25
-RRB- -RRB- 	 0.005420054200542005
rules by 	 0.023255813953488372
requiring knowledge 	 0.5
, regardless 	 0.0016844469399213925
rather can 	 0.0625
order to 	 0.5714285714285714
applies a 	 0.2857142857142857
are efficient 	 0.004149377593360996
metrics in 	 0.1111111111111111
selecting examples 	 0.2
associated word 	 0.25
500 samples 	 0.5
investigation performed 	 1.0
following years 	 0.06666666666666667
Fighter Technology 	 1.0
parsers which 	 0.07692307692307693
realized on 	 1.0
US baseball 	 0.14285714285714285
Ohio Bell 	 1.0
complex sounds 	 0.041666666666666664
this centroid 	 0.01098901098901099
simple demonstrations 	 0.038461538461538464
conjunction , 	 0.3333333333333333
singular , 	 0.25
1997 , 	 0.5
Stylistics -LRB- 	 1.0
toy world 	 0.5
how to 	 0.10344827586206896
of retail 	 0.00089126559714795
boundaries may 	 0.09090909090909091
rates even 	 0.125
approaches emphasize 	 0.03571428571428571
be moderate 	 0.004219409282700422
from machine 	 0.019230769230769232
or meets 	 0.0045045045045045045
itself while 	 0.2
use so-called 	 0.013888888888888888
rate -LRB- 	 0.09090909090909091
light -RRB- 	 0.3333333333333333
an epidemic 	 0.007575757575757576
is difficult 	 0.008130081300813009
we take 	 0.022222222222222223
ROUGE is 	 0.4
ongoing issue 	 0.5
to both 	 0.00398406374501992
time . 	 0.12121212121212122
displayed as 	 0.5
between `` 	 0.02564102564102564
or speed 	 0.0045045045045045045
machine learning 	 0.24050632911392406
influenced by 	 1.0
Corpus and 	 0.125
of unigrams 	 0.0017825311942959
On others 	 0.16666666666666666
experiment which 	 0.2
of low 	 0.00089126559714795
in generating 	 0.0018726591760299626
sub-titling , 	 1.0
sequence alignment 	 0.125
parsing . 	 0.10714285714285714
TextRank results 	 0.07142857142857142
getting into 	 0.25
those used 	 0.13636363636363635
more forms 	 0.010526315789473684
that these 	 0.010638297872340425
data source 	 0.012987012987012988
`` do 	 0.005291005291005291
Acoustical distortions 	 0.5
segment the 	 0.1111111111111111
letters of 	 0.2
the Optophone 	 0.0006920415224913495
Types of 	 1.0
speech commands 	 0.006578947368421052
the Sociologist 	 0.0006920415224913495
POST -RRB- 	 1.0
creates new 	 0.5
detection and 	 0.5
would fail 	 0.018867924528301886
Decoding of 	 0.5
Vocabulary is 	 0.3333333333333333
degradation in 	 1.0
a simulation 	 0.001226993865030675
a statistical 	 0.0036809815950920245
hurricane season 	 1.0
lattices represented 	 1.0
required the 	 0.14285714285714285
A shallow 	 0.04
and checked 	 0.001445086705202312
training document 	 0.03571428571428571
, 1976 	 0.0011229646266142617
and FAA 	 0.001445086705202312
at conclusions 	 0.014705882352941176
3 % 	 0.2
MEAD -RRB- 	 1.0
grammars for 	 0.07142857142857142
of government 	 0.0017825311942959
natural summaries 	 0.013333333333333334
Foucault , 	 0.3333333333333333
, Englund 	 0.0005614823133071309
state . 	 0.07142857142857142
The earliest 	 0.005208333333333333
that closely 	 0.0035460992907801418
Although it 	 0.125
the fact 	 0.002768166089965398
underlying knowledge 	 0.3333333333333333
an input 	 0.022727272727272728
algorithms that 	 0.05714285714285714
only into 	 0.02631578947368421
or legal 	 0.0045045045045045045
foreign '' 	 0.5
may appear 	 0.019230769230769232
that revolutionized 	 0.0035460992907801418
in smaller 	 0.0018726591760299626
text ; 	 0.006289308176100629
a word 	 0.013496932515337423
removing objective 	 0.5
groups at 	 0.2
<s> Attribute 	 0.0007686395080707148
and TextRank 	 0.001445086705202312
at A.C. 	 0.014705882352941176
telephony is 	 0.3333333333333333
and labor 	 0.001445086705202312
Two vertices 	 0.14285714285714285
A simplified 	 0.02
whose usage 	 0.3333333333333333
, Joseph 	 0.0005614823133071309
and discuss 	 0.001445086705202312
Given an 	 0.07142857142857142
article and 	 0.06896551724137931
independently developed 	 1.0
1969 Roger 	 0.5
like ca 	 0.03571428571428571
reading comprehension 	 0.25
time-consuming part 	 0.3333333333333333
grammars can 	 0.07142857142857142
of immunology 	 0.00089126559714795
growing interest 	 0.5
produced , 	 0.1111111111111111
also an 	 0.014492753623188406
ports . 	 1.0
all of 	 0.09302325581395349
Turney paper 	 0.2222222222222222
the result 	 0.002768166089965398
resolve some 	 0.25
, whereas 	 0.0011229646266142617
relationships . 	 0.16666666666666666
answered questions 	 0.6
, headlines 	 0.0005614823133071309
where each 	 0.02857142857142857
it runs 	 0.008547008547008548
approaches Automatic 	 0.03571428571428571
highly interactive 	 0.1111111111111111
& lines 	 0.125
campaign on 	 0.2
Artificial Intelligence 	 0.5
and Pang 	 0.001445086705202312
task also 	 0.023809523809523808
Strzalkowski T. 	 1.0
see Tablet 	 0.05
For this 	 0.04918032786885246
standard written 	 0.07142857142857142
arbitrary piece 	 0.3333333333333333
Typhoon currently 	 1.0
rank `` 	 0.16666666666666666
into individual 	 0.01282051282051282
the editor 	 0.0006920415224913495
precise function 	 0.3333333333333333
<s> Sentiment 	 0.003843197540353574
we need 	 0.13333333333333333
each year 	 0.022222222222222223
has never 	 0.023809523809523808
for sentence 	 0.0036101083032490976
valuable detailed 	 0.5
version of 	 0.6666666666666666
columns and 	 1.0
is one 	 0.012195121951219513
content of 	 0.08333333333333333
an Electronic 	 0.007575757575757576
Smith went 	 1.0
<s> During 	 0.0030745580322828594
-RRB- Cohesion 	 0.0027100271002710027
answer temporal 	 0.03333333333333333
more qualities 	 0.010526315789473684
restricted vocabularies 	 0.25
form filling 	 0.05
benefits to 	 0.5
verbalization . 	 1.0
Naive Bayes 	 1.0
much useful 	 0.045454545454545456
most significant 	 0.017241379310344827
, domotic 	 0.0005614823133071309
decelerations during 	 1.0
color images 	 1.0
a good 	 0.0049079754601227
on dictionary 	 0.0047169811320754715
tokens 12 	 0.14285714285714285
English speaking 	 0.02702702702702703
weapon critical 	 0.5
features , 	 0.038461538461538464
, Google 	 0.0005614823133071309
provide additional 	 0.16666666666666666
conditions . 	 0.2
Development Activity 	 1.0
discourse relationships 	 0.027777777777777776
of anaphora 	 0.00089126559714795
unsupervised , 	 0.125
that about 	 0.0035460992907801418
a rapidly 	 0.001226993865030675
Working with 	 1.0
with computer-aided 	 0.00546448087431694
Federation of 	 1.0
language representation 	 0.006756756756756757
, Screenshot 	 0.0005614823133071309
incorrectly causing 	 1.0
applications in 	 0.08
13 , 	 0.5
POS-tagging algorithms 	 1.0
comparable . 	 1.0
question is 	 0.09523809523809523
summary that 	 0.047619047619047616
R. Howarth 	 0.16666666666666666
`` classification 	 0.015873015873015872
by deep 	 0.005714285714285714
parsed efficiently 	 0.25
a basis 	 0.00245398773006135
systems since 	 0.008928571428571428
20th-century newspaper 	 1.0
In terms 	 0.009523809523809525
and semantics 	 0.004335260115606936
`` Speaker 	 0.010582010582010581
Perceptron , 	 1.0
at SRI 	 0.014705882352941176
an LDA-based 	 0.007575757575757576
easier to 	 0.25
how close 	 0.034482758620689655
quickly , 	 1.0
of non-annotated 	 0.00089126559714795
morphosyntactic descriptor 	 1.0
heuristic to 	 0.3333333333333333
therefore help 	 0.2
current state 	 0.14285714285714285
English is 	 0.02702702702702703
global semitied 	 0.3333333333333333
or adjective 	 0.0045045045045045045
dedicated OCR 	 0.3333333333333333
immunology , 	 1.0
<s> Often 	 0.0023059185242121443
several seconds 	 0.045454545454545456
SemEval -RRB- 	 1.0
E , 	 1.0
numbers represent 	 0.14285714285714285
easily retrieved 	 0.1111111111111111
, ASR 	 0.0005614823133071309
weather data 	 0.14285714285714285
lexicon of 	 0.2222222222222222
is how 	 0.0040650406504065045
Creating referring 	 0.5
semantic information 	 0.09523809523809523
sounds it 	 0.06666666666666667
if we 	 0.07142857142857142
e.g. vocabulary 	 0.017857142857142856
picture above 	 0.25
from text 	 0.019230769230769232
advanced '' 	 0.2
more such 	 0.010526315789473684
decades -LRB- 	 1.0
edges build 	 0.14285714285714285
what sound 	 0.03125
sentences -LRB- 	 0.02631578947368421
up or 	 0.045454545454545456
and Lao 	 0.001445086705202312
hard to 	 0.3333333333333333
to threshold 	 0.0013280212483399733
physics that 	 1.0
grammars that 	 0.07142857142857142
Hirschman L. 	 0.5
others , 	 0.08333333333333333
the generation 	 0.0006920415224913495
the interim 	 0.0006920415224913495
; otherwise 	 0.02127659574468085
the question 	 0.011072664359861591
could understand 	 0.0625
College at 	 0.5
still somewhat 	 0.06666666666666667
be checked 	 0.004219409282700422
e.g. containing 	 0.017857142857142856
by transfer-based 	 0.005714285714285714
to customize 	 0.0026560424966799467
coherence . 	 0.3333333333333333
stationary process 	 0.14285714285714285
perhaps by 	 0.16666666666666666
notable early 	 1.0
and inspired 	 0.001445086705202312
improved . 	 0.25
naturally occurring 	 0.5
a name 	 0.00245398773006135
important . 	 0.0625
many higher 	 0.019230769230769232
words must 	 0.009174311926605505
intervening punctuation 	 1.0
the Advanced 	 0.0006920415224913495
of implementing 	 0.00089126559714795
output in 	 0.038461538461538464
its lexicon 	 0.02857142857142857
official languages 	 1.0
The next 	 0.005208333333333333
rules of 	 0.09302325581395349
the state-of-the-art 	 0.0006920415224913495
which undertook 	 0.007246376811594203
, comprehend 	 0.0005614823133071309
Basic sound 	 1.0
an objective 	 0.007575757575757576
post-secondary look 	 1.0
maintains that 	 1.0
properties and 	 0.25
Produce a 	 1.0
tasks returning 	 0.03125
compactly , 	 1.0
an enormous 	 0.007575757575757576
begin and 	 0.3333333333333333
, '' 	 0.0016844469399213925
as machine 	 0.003484320557491289
translation paradigm 	 0.013513513513513514
can benefit 	 0.011049723756906077
would also 	 0.018867924528301886
linear combination 	 0.14285714285714285
periods in 	 0.3333333333333333
wishes to 	 1.0
Page , 	 1.0
see it 	 0.05
Further information 	 0.3333333333333333
questions have 	 0.038461538461538464
in Scotland 	 0.0018726591760299626
one , 	 0.06153846153846154
installed at 	 0.6666666666666666
One might 	 0.07692307692307693
task it 	 0.023809523809523808
critical tasks 	 0.25
a concept 	 0.001226993865030675
queries , 	 0.3333333333333333
complex setting 	 0.041666666666666664
systems become 	 0.008928571428571428
performed in 	 0.2
in fact 	 0.0018726591760299626
the author 	 0.0020761245674740486
& Server 	 0.125
particular method 	 0.07692307692307693
, NLG 	 0.0005614823133071309
coreference resolution 	 1.0
summary by 	 0.023809523809523808
of prisoner-of-war 	 0.00089126559714795
from United 	 0.009615384615384616
degrees of 	 0.5
will seem 	 0.02857142857142857
questions , 	 0.3076923076923077
by adding 	 0.011428571428571429
Angenot , 	 1.0
easier part 	 0.125
them -LRB- 	 0.05263157894736842
medical professionals 	 0.16666666666666666
known summaries 	 0.038461538461538464
be measured 	 0.004219409282700422
interpreter , 	 0.5
'' do 	 0.005154639175257732
transform of 	 0.2
speaking -RRB- 	 0.125
summaries or 	 0.023255813953488372
sounds . 	 0.06666666666666667
followed . 	 0.25
A. Woods 	 0.2
converted it 	 0.3333333333333333
in NLG 	 0.0056179775280898875
, Deborah 	 0.0011229646266142617
of unknown 	 0.00089126559714795
It follows 	 0.02631578947368421
statistical properties 	 0.030303030303030304
overfitting the 	 0.5
RDF . 	 1.0
and manage 	 0.001445086705202312
were similar 	 0.024390243902439025
to detect 	 0.0013280212483399733
author when 	 0.3333333333333333
be weighted 	 0.004219409282700422
kind , 	 0.09090909090909091
Who invented 	 0.5
-RRB- examples 	 0.0027100271002710027
, Walter 	 0.0005614823133071309
part-of-speech tag 	 0.06666666666666667
extensively used 	 1.0
engineers worked 	 1.0
perspectives and 	 1.0
than procedural 	 0.022222222222222223
of planning 	 0.00089126559714795
into general 	 0.01282051282051282
unambiguously identified 	 1.0
simple terms 	 0.038461538461538464
this in 	 0.01098901098901099
commercial interest 	 0.09090909090909091
posed by 	 0.3333333333333333
We apply 	 0.14285714285714285
is provided 	 0.0020325203252032522
systems of 	 0.05357142857142857
classification indicates 	 0.058823529411764705
with his 	 0.00546448087431694
to by 	 0.0026560424966799467
revealing socio-psychological 	 1.0
unfamiliar input 	 1.0
somehow internalize 	 1.0
to change 	 0.0013280212483399733
've just 	 0.5
often used 	 0.022727272727272728
near each 	 1.0
Gene Ontology 	 1.0
features might 	 0.038461538461538464
on each 	 0.0047169811320754715
multiply . 	 1.0
was tested 	 0.012987012987012988
state of 	 0.35714285714285715
character recognition 	 0.5
students , 	 0.3333333333333333
Gripen cockpit 	 1.0
impact on 	 0.5
answers , 	 0.08333333333333333
problems for 	 0.058823529411764705
flow together 	 1.0
finished product 	 0.5
person 's 	 0.21052631578947367
to express 	 0.0013280212483399733
minimizes the 	 1.0
specify precisely 	 1.0
create some 	 0.058823529411764705
HMMs learn 	 0.125
classifies features 	 1.0
some written 	 0.024096385542168676
for medical 	 0.0036101083032490976
character alone 	 0.045454545454545456
Gustav Tauschek 	 1.0
Semantic Evaluation 	 0.3333333333333333
recognition benchmark 	 0.008264462809917356
convey . 	 0.3333333333333333
devices . 	 0.5
reported there 	 0.2
group of 	 0.5
speech that 	 0.006578947368421052
, Facebook 	 0.0005614823133071309
input to 	 0.07317073170731707
others -RRB- 	 0.08333333333333333
Realisation : 	 1.0
way , 	 0.041666666666666664
which showed 	 0.007246376811594203
random walks 	 0.2857142857142857
increased so 	 0.2
Response based 	 1.0
! <s/> 	 1.0
heuristic post-processing 	 0.3333333333333333
<s> Keyphrases 	 0.0007686395080707148
are consumed 	 0.004149377593360996
: give 	 0.0196078431372549
vertices\/unigrams are 	 1.0
or Web-based 	 0.0045045045045045045
each for 	 0.022222222222222223
An explicit 	 0.0625
or negative 	 0.009009009009009009
expertise , 	 1.0
are language-specific 	 0.004149377593360996
Cook , 	 1.0
the approach 	 0.0006920415224913495
linguistics is 	 0.15
proceeds in 	 1.0
system selects 	 0.010752688172043012
, addressed 	 0.0005614823133071309
email Multimodal 	 0.5
last night 	 0.2
human meteorologist 	 0.021739130434782608
appear more 	 0.0625
tasks implemented 	 0.03125
this procedure 	 0.01098901098901099
must appear 	 0.07142857142857142
<s> Although 	 0.005380476556495004
question understanding 	 0.023809523809523808
robust against 	 0.25
management environments 	 0.14285714285714285
the theoretical 	 0.0006920415224913495
especially input 	 0.13333333333333333
punched cards 	 1.0
because while 	 0.03333333333333333
dominance of 	 1.0
credit card 	 1.0
of problems 	 0.0017825311942959
dog '' 	 0.3333333333333333
time question 	 0.030303030303030304
after applying 	 0.08333333333333333
which recognized 	 0.007246376811594203
is having 	 0.0020325203252032522
be quite 	 0.004219409282700422
of arbitrary 	 0.00089126559714795
might include 	 0.038461538461538464
to explicitly 	 0.0026560424966799467
some work 	 0.012048192771084338
to one 	 0.0026560424966799467
stationary probabilities 	 0.14285714285714285
sociolinguistics Ethnography 	 0.5
classification : 	 0.058823529411764705
e.g. person 	 0.017857142857142856
to benefit 	 0.0013280212483399733
to data 	 0.0026560424966799467
Sound waves 	 0.3333333333333333
, modern 	 0.0005614823133071309
arrive at 	 1.0
comes mainly 	 0.2
hand . 	 0.07142857142857142
Guidelines see 	 0.5
five years 	 0.4
topics discussed 	 0.14285714285714285
inter-texual ones 	 0.5
disruptive to 	 1.0
classification , 	 0.058823529411764705
which has 	 0.050724637681159424
the affect 	 0.0006920415224913495
lexer would 	 1.0
effect the 	 0.5
the successful 	 0.0006920415224913495
database or 	 0.2
substitutions . 	 1.0
translates to 	 1.0
, affective 	 0.0005614823133071309
section called 	 0.16666666666666666
smart keyboard 	 1.0
PC + 	 0.25
de l'assignation 	 0.5
's annual 	 0.0196078431372549
morphology , 	 0.7142857142857143
call to 	 0.3333333333333333
first mechanized 	 0.030303030303030304
answer highlighted 	 0.03333333333333333
`` polarity 	 0.005291005291005291
Jonathan Potter 	 1.0
use a 	 0.05555555555555555
is testing 	 0.0020325203252032522
SBD -RRB- 	 1.0
goals of 	 1.0
automatic lip-synch 	 0.043478260869565216
a fast-evolving 	 0.001226993865030675
trivial word 	 0.25
`` Intelligent 	 0.005291005291005291
best option 	 0.05555555555555555
example is 	 0.012345679012345678
Bottom-up parsing 	 1.0
Some current 	 0.09523809523809523
orthography . 	 0.5
document gives 	 0.027777777777777776
led by 	 0.3333333333333333
to multi-platforms 	 0.0013280212483399733
not all 	 0.017857142857142856
home '' 	 1.0
translated it 	 0.25
on lexicon 	 0.0047169811320754715
sailor ! 	 0.2
speech -RRB- 	 0.02631578947368421
then include 	 0.02857142857142857
contained in 	 1.0
<s> Other 	 0.005380476556495004
cluster of 	 1.0
N is 	 0.3333333333333333
undertake harder 	 1.0
certain assumptions 	 0.14285714285714285
a stochastic 	 0.001226993865030675
accurately if 	 0.5
sublanguage analysis 	 0.3333333333333333
either as 	 0.3
the Ohio 	 0.0006920415224913495
articles , 	 0.125
preliminary , 	 0.3333333333333333
would difficult 	 0.018867924528301886
covers speech 	 0.25
`` nine 	 0.005291005291005291
like Chinese 	 0.07142857142857142
known key 	 0.038461538461538464
's idea 	 0.0196078431372549
a demonstration 	 0.0036809815950920245
data and 	 0.025974025974025976
Military High-performance 	 1.0
implemented by 	 0.2
mobile processor 	 0.5
study based 	 0.25
earlier in 	 0.25
separate field 	 0.1
those organised 	 0.045454545454545456
: The 	 0.0392156862745098
IT technology 	 1.0
Ensemble methods 	 1.0
of evaluation 	 0.004456327985739751
morphology of 	 0.14285714285714285
researchers have 	 0.3
there are 	 0.375
were written 	 0.024390243902439025
these two 	 0.023809523809523808
Products began 	 0.5
the results 	 0.002768166089965398
this . 	 0.01098901098901099
humans transcribe 	 0.08333333333333333
from false 	 0.009615384615384616
difficult words 	 0.03571428571428571
and methodologies 	 0.001445086705202312
repetitive . 	 0.5
Garfinkel who 	 1.0
above -- 	 0.07692307692307693
1966 , 	 0.3333333333333333
untrained on 	 1.0
to discover 	 0.0013280212483399733
no matter 	 0.07692307692307693
distinctive groups 	 0.5
BLEU . 	 0.3333333333333333
of very 	 0.0017825311942959
a language 	 0.007361963190184049
-LRB- An 	 0.005420054200542005
solve the 	 0.25
into its 	 0.01282051282051282
an email 	 0.007575757575757576
47 % 	 1.0
unwanted constructs 	 1.0
discussed above 	 0.14285714285714285
meaning -LRB- 	 0.043478260869565216
of newspaper 	 0.00089126559714795
entropy classifier 	 0.2
, Carnegie 	 0.0005614823133071309
Server OCR 	 1.0
, The 	 0.0005614823133071309
A Universal 	 0.02
varying degrees 	 1.0
suitability as 	 0.5
of human 	 0.004456327985739751
reuse , 	 1.0
an online 	 0.007575757575757576
the speech-enabled 	 0.0006920415224913495
choice between 	 0.125
characterized DARPA 	 0.25
even where 	 0.037037037037037035
while LexRank 	 0.1
which recursively 	 0.007246376811594203
advantage of 	 0.8
co-occurrence graph 	 0.6666666666666666
different contexts 	 0.02040816326530612
to select 	 0.005312084993359893
and early 	 0.001445086705202312
may chose 	 0.019230769230769232
at phrasing 	 0.014705882352941176
merely copy 	 0.5
, medial 	 0.0005614823133071309
Corporation -LRB- 	 0.5
-RRB- The 	 0.005420054200542005
similar kind 	 0.037037037037037035
vectors -LRB- 	 0.3333333333333333
meaning to 	 0.08695652173913043
not 100 	 0.008928571428571428
gained surprising 	 0.5
's Digest 	 0.058823529411764705
huge handmade 	 1.0
recognition '' 	 0.01652892561983471
avoiding linguistic 	 0.5
In this 	 0.047619047619047616
PageRank on 	 0.16666666666666666
the broadcast 	 0.0006920415224913495
section of 	 0.16666666666666666
the features 	 0.0034602076124567475
well captured 	 0.03571428571428571
system also 	 0.010752688172043012
Although the 	 0.375
number of 	 0.8372093023255814
discussed between 	 0.14285714285714285
linguistics concerned 	 0.05
keyphrases to 	 0.02857142857142857
of top-down 	 0.0017825311942959
good search 	 0.07692307692307693
speech for 	 0.02631578947368421
an expression 	 0.007575757575757576
maybe to 	 1.0
<s> Google 	 0.0007686395080707148
problem from 	 0.022727272727272728
, idioms 	 0.0005614823133071309
oriented systems 	 1.0
text '' 	 0.006289308176100629
or shifted 	 0.0045045045045045045
the 70 	 0.0006920415224913495
of possible 	 0.00267379679144385
human-made model 	 0.5
removed . 	 1.0
in source 	 0.0018726591760299626
he had 	 0.14285714285714285
bridging relationship 	 1.0
voice than 	 0.07692307692307693
predict performance 	 0.16666666666666666
75 % 	 1.0
systems to 	 0.008928571428571428
to each 	 0.006640106241699867
must take 	 0.07142857142857142
got about 	 1.0
techniques in 	 0.043478260869565216
helps doctors 	 0.5
language output 	 0.006756756756756757
as weights 	 0.003484320557491289
archiving and 	 1.0
i.e. relationship 	 0.05263157894736842
technology is 	 0.13636363636363635
Commercial applications 	 0.5
phrases with 	 0.0625
Referring expression 	 1.0
and without 	 0.002890173410404624
it generalizes 	 0.008547008547008548
Subsequently a 	 1.0
machine and 	 0.012658227848101266
Understudy for 	 1.0
signals are 	 1.0
enough data 	 0.4
since 2000 	 0.1
the art 	 0.001384083044982699
fidelity of 	 1.0
systems is 	 0.026785714285714284
documents more 	 0.02631578947368421
output than 	 0.038461538461538464
Subjectivity '' 	 1.0
and Intelligent 	 0.001445086705202312
, would 	 0.0016844469399213925
other hand 	 0.07142857142857142
automatic machine 	 0.043478260869565216
Church 's 	 0.3333333333333333
the unsupervised 	 0.0006920415224913495
be seen 	 0.012658227848101266
those human 	 0.045454545454545456
in automatic 	 0.003745318352059925
have more 	 0.028846153846153848
rate the 	 0.09090909090909091
, Larry 	 0.0005614823133071309
Harris beginning 	 0.1111111111111111
meant that 	 0.5
, has 	 0.0005614823133071309
's Law 	 0.0196078431372549
a training 	 0.001226993865030675
the algorithm 	 0.0006920415224913495
text corpus 	 0.012578616352201259
thresholded to 	 1.0
`` foreign 	 0.005291005291005291
The fonts 	 0.005208333333333333
management Question 	 0.14285714285714285
and compare 	 0.002890173410404624
explore what 	 0.25
It essentially 	 0.02631578947368421
language is 	 0.033783783783783786
actioning it 	 1.0
a scale 	 0.001226993865030675
<s> Schools 	 0.0007686395080707148
returned by 	 0.25
, due 	 0.0005614823133071309
remains a 	 0.25
technology This 	 0.045454545454545456
formatted output 	 1.0
pre-process data 	 1.0
and answered 	 0.001445086705202312
comprehend Morse 	 1.0
centres require 	 1.0
significant increase 	 0.1111111111111111
application where 	 0.07142857142857142
replace the 	 1.0
robot questions 	 0.5
retrieval results 	 0.14285714285714285
same meaning 	 0.04
the different 	 0.0006920415224913495
independence Isolated 	 1.0
we hear 	 0.022222222222222223
Approaches One 	 0.3333333333333333
have already 	 0.009615384615384616
extractive keyphrase 	 0.14285714285714285
normalize for 	 1.0
models for 	 0.23076923076923078
graph will 	 0.15384615384615385
Web . 	 0.2222222222222222
LUNAR was 	 0.3333333333333333
it accepts 	 0.008547008547008548
harder 75 	 0.14285714285714285
compiler due 	 0.3333333333333333
such rules 	 0.008130081300813009
flight displays 	 0.5
range from 	 0.2857142857142857
recognition vary 	 0.008264462809917356
-LRB- this 	 0.0027100271002710027
than by 	 0.022222222222222223
input gracefully 	 0.024390243902439025
been developed 	 0.014705882352941176
sometimes provided 	 0.07692307692307693
similarity . 	 0.1
commercial . 	 0.09090909090909091
divider , 	 1.0
and Martin 	 0.001445086705202312
There is 	 0.2727272727272727
disambiguation Word-sense 	 0.1
goal of 	 0.2857142857142857
spelling . 	 1.0
ranking sentences 	 0.14285714285714285
use following 	 0.013888888888888888
modules that 	 0.5
Sometimes it 	 1.0
for without 	 0.0036101083032490976
, potentially 	 0.0005614823133071309
count for 	 0.2
vertex for 	 0.6666666666666666
and most 	 0.001445086705202312
his `` 	 0.08333333333333333
most summarization 	 0.017241379310344827
ambiguities one 	 0.25
<s> IBM 	 0.0007686395080707148
measurement of 	 0.5
put the 	 0.25
voice recognition 	 0.07692307692307693
'' summary 	 0.005154639175257732
computer interaction 	 0.022727272727272728
rate still 	 0.09090909090909091
millions of 	 1.0
with using 	 0.00546448087431694
representing successive 	 0.5
If it 	 0.1
allow discriminative 	 0.2
tags used 	 0.3333333333333333
step is 	 0.06666666666666667
tags . 	 0.3333333333333333
are the 	 0.04564315352697095
sense of 	 0.125
or weapon 	 0.0045045045045045045
considerable success 	 0.2
certainty of 	 1.0
Sentence breaking 	 0.2
<s> In 	 0.07455803228285934
especially interested 	 0.06666666666666667
coordinates and 	 1.0
the possibility 	 0.002768166089965398
10msec section 	 0.5
speech recognition-related 	 0.006578947368421052
the second 	 0.001384083044982699
containing the 	 0.375
research necessary 	 0.023809523809523808
summary used 	 0.023809523809523808
is intended 	 0.0040650406504065045
= no. 	 0.1111111111111111
deciding where 	 0.16666666666666666
reducing the 	 0.5
the approximate 	 0.0006920415224913495
the one 	 0.0006920415224913495
2011 campaign 	 0.5
understanding programs 	 0.030303030303030304
rank , 	 0.16666666666666666
on specific 	 0.0047169811320754715
person . 	 0.05263157894736842
` hit 	 0.0625
tend to 	 1.0
precision . 	 0.2
unlabeled data 	 1.0
and Ken 	 0.001445086705202312
, sales 	 0.0005614823133071309
to have 	 0.013280212483399735
sentence in 	 0.041666666666666664
illustrates some 	 0.5
the lexicon 	 0.0006920415224913495
small range 	 0.1111111111111111
general approaches 	 0.045454545454545456
of word-frequency 	 0.00089126559714795
fundamentally different 	 1.0
larger text 	 0.0625
of selecting 	 0.00089126559714795
extract sentences 	 0.25
the meeting 	 0.0006920415224913495
experiment in 	 0.2
? <s/> 	 0.5
correct output 	 0.06666666666666667
real-world knowledge 	 0.16666666666666666
helicopter . 	 0.25
and built 	 0.001445086705202312
keep a 	 0.3333333333333333
whether a 	 0.15384615384615385
a cheque 	 0.001226993865030675
the data 	 0.002768166089965398
pre-processing e.g. 	 1.0
be ranked 	 0.004219409282700422
grouped with 	 0.5
to summarization 	 0.0026560424966799467
, cognitive 	 0.0005614823133071309
to bootstrap 	 0.0013280212483399733
, business 	 0.0005614823133071309
When several 	 0.14285714285714285
acoustic and 	 0.16666666666666666
here there 	 0.5
The lexer 	 0.005208333333333333
each choice 	 0.022222222222222223
Georgetown experiment 	 1.0
reverse process 	 0.5
and Weizenbaum 	 0.001445086705202312
as 1 	 0.003484320557491289
, + 	 0.0011229646266142617
right answer 	 0.1
purpose at 	 0.2
input -LRB- 	 0.04878048780487805
procedure lies 	 0.3333333333333333
accuracy . 	 0.22580645161290322
on smaller 	 0.0047169811320754715
, wrote 	 0.0005614823133071309
will usually 	 0.02857142857142857
<s> Langues 	 0.0007686395080707148
system determine 	 0.010752688172043012
Critical discourse 	 0.5
be linked 	 0.008438818565400843
Penpoint OS 	 1.0
Oklahoma , 	 1.0
failed to 	 1.0
these apply 	 0.023809523809523808
and practice 	 0.001445086705202312
shallow '' 	 0.16666666666666666
Universal Part-of-Speech 	 1.0
the foreign 	 0.0006920415224913495
the pollen 	 0.0006920415224913495
being based 	 0.05555555555555555
mine the 	 1.0
to its 	 0.0013280212483399733
George Lakoff 	 1.0
ambiguity by 	 0.125
by voice 	 0.011428571428571429
the blind 	 0.0006920415224913495
-LRB- CWA 	 0.0027100271002710027
syntactic coverage 	 0.07692307692307693
the linguistic 	 0.0006920415224913495
in Europe 	 0.003745318352059925
rushing to 	 1.0
data on 	 0.012987012987012988
emotions in 	 1.0
rarity and 	 1.0
in both 	 0.0018726591760299626
Ray Kurzweil 	 1.0
by highly 	 0.005714285714285714
, Bobrow 	 0.0005614823133071309
12 categories 	 0.2
between an 	 0.02564102564102564
for a 	 0.10469314079422383
where successively 	 0.02857142857142857
form ? 	 0.05
ones . 	 0.2
: Manual 	 0.00980392156862745
1970 -LRB- 	 0.3333333333333333
sentences presented 	 0.013157894736842105
such template-matching 	 0.008130081300813009
pioneered at 	 0.3333333333333333
a flight 	 0.001226993865030675
as interlingual 	 0.003484320557491289
sequences -LRB- 	 0.1111111111111111
and not 	 0.011560693641618497
Research on 	 0.125
`` set 	 0.005291005291005291
e.g. The 	 0.03571428571428571
their estimated 	 0.029411764705882353
for most 	 0.007220216606498195
, dimensionality 	 0.0005614823133071309
HMT -RRB- 	 1.0
sentence -RRB- 	 0.041666666666666664
Markov Models 	 0.16666666666666666
is seen 	 0.0020325203252032522
are actually 	 0.004149377593360996
method of 	 0.125
or subjective 	 0.009009009009009009
basic task 	 0.07692307692307693
one meaning 	 0.03076923076923077
: What 	 0.00980392156862745
it agrees 	 0.008547008547008548
with Fourier 	 0.00546448087431694
work based 	 0.041666666666666664
GRACE d'évaluation 	 1.0
headed by 	 1.0
person may 	 0.05263157894736842
-- between 	 0.04
are typically 	 0.012448132780082987
her to 	 0.5
more effective 	 0.010526315789473684
<s> ... 	 0.0007686395080707148
recognition instead 	 0.008264462809917356
grammar-based methods 	 1.0
utility with 	 0.5
speed for 	 0.14285714285714285
Guidelines for 	 0.5
successive letters 	 0.5
The USAF 	 0.005208333333333333
you must 	 0.07692307692307693
4 . 	 0.4
automatic keyphrase 	 0.043478260869565216
which parts 	 0.007246376811594203
-LRB- 1993 	 0.0027100271002710027
-LRB- also 	 0.01084010840108401
another problem 	 0.15384615384615385
Shallow parsing 	 0.5
explored . 	 0.5
include an 	 0.07407407407407407
preliminary approach 	 0.3333333333333333
a major 	 0.006134969325153374
aims to 	 0.6666666666666666
greatly reduced 	 0.14285714285714285
supervised methods 	 0.125
aircraft . 	 0.14285714285714285
The best 	 0.005208333333333333
returned with 	 0.25
, definition 	 0.0005614823133071309
needed -RRB- 	 0.6190476190476191
exploring the 	 1.0
triples that 	 0.3333333333333333
to foster 	 0.0013280212483399733
information into 	 0.043478260869565216
member of 	 1.0
various features 	 0.05555555555555555
as latent 	 0.003484320557491289
sentiment about 	 0.04
than metrics 	 0.022222222222222223
John Heritage 	 0.125
to learn 	 0.00796812749003984
incorrect ones 	 0.3333333333333333
a fancy 	 0.001226993865030675
particular part 	 0.07692307692307693
with increasing 	 0.00546448087431694
Ontology -RRB- 	 1.0
: extraction 	 0.00980392156862745
sounds are 	 0.13333333333333333
classifiers -RRB- 	 0.5
advantage that 	 0.2
the tokens 	 0.001384083044982699
and SVOX 	 0.001445086705202312
consist of 	 1.0
computer language 	 0.022727272727272728
construction of 	 0.6666666666666666
they still 	 0.025
In 1949 	 0.009523809523809525
maximum entropy 	 0.5
as could 	 0.003484320557491289
whether documents 	 0.07692307692307693
setting of 	 0.2
differ , 	 0.3333333333333333
stage using 	 0.2
Janet Holmes 	 0.5
developed a 	 0.11538461538461539
NASA 's 	 1.0
following the 	 0.06666666666666667
<s> Political 	 0.0007686395080707148
those concerning 	 0.045454545454545456
LDA-based projection 	 1.0
next four 	 0.14285714285714285
intrinsic properties 	 0.25
then compute 	 0.02857142857142857
there was 	 0.075
cognitive psychology 	 0.5
words -RRB- 	 0.027522935779816515
cover . 	 1.0
large data 	 0.043478260869565216
journals include 	 0.5
lexicon . 	 0.1111111111111111
exploration , 	 1.0
of assembling 	 0.00089126559714795
-LRB- EMR 	 0.0027100271002710027
more software 	 0.010526315789473684
denote an 	 0.5
with pilots 	 0.00546448087431694
the relevant 	 0.0006920415224913495
number 20 	 0.023255813953488372
and metrics 	 0.001445086705202312
NLP -RRB- 	 0.0851063829787234
expect ; 	 0.3333333333333333
which words 	 0.014492753623188406
particularly the 	 0.2
technology . 	 0.09090909090909091
the only 	 0.0006920415224913495
<s> Large-scale 	 0.0007686395080707148
that of 	 0.028368794326241134
of case-based 	 0.00089126559714795
text map 	 0.006289308176100629
variability , 	 1.0
translation components 	 0.013513513513513514
text categorization 	 0.006289308176100629
type as 	 0.07142857142857142
was of 	 0.012987012987012988
language interface 	 0.006756756756756757
the feature\/aspect-based 	 0.0006920415224913495
final language 	 0.1111111111111111
manually created 	 0.25
of proper 	 0.00089126559714795
<s> consider 	 0.0007686395080707148
more deterministic 	 0.021052631578947368
Thus the 	 0.08333333333333333
of inflected 	 0.00089126559714795
\* , 	 0.5
<s> Topics 	 0.0007686395080707148
, engaging 	 0.0005614823133071309
data rather 	 0.012987012987012988
-LRB- VITO 	 0.0027100271002710027
measurement is 	 0.5
and entered 	 0.001445086705202312
bootstrap using 	 1.0
closely approximates 	 0.2
best one 	 0.05555555555555555
a journal 	 0.001226993865030675
size of 	 0.16666666666666666
With continuous 	 0.14285714285714285
At the 	 0.3333333333333333
Attribute grammars 	 1.0
answering methods 	 0.08333333333333333
then direct 	 0.02857142857142857
who applied 	 0.1
for customisation 	 0.0036101083032490976
have complex 	 0.009615384615384616
of integration 	 0.00089126559714795
networks are 	 0.07142857142857142
you have 	 0.15384615384615385
19th - 	 1.0
of similar 	 0.0017825311942959
an active 	 0.007575757575757576
forms . 	 0.16666666666666666
computer to 	 0.045454545454545456
for substantial 	 0.0036101083032490976
negative to 	 0.125
<s> LUNAR 	 0.0007686395080707148
possible on 	 0.041666666666666664
<s> Much 	 0.0023059185242121443
grammars , 	 0.14285714285714285
algorithm is 	 0.17857142857142858
, real 	 0.0005614823133071309
quantitative approaches 	 0.25
as weather 	 0.003484320557491289
200 billion 	 0.5
available . 	 0.17647058823529413
the shipment 	 0.0006920415224913495
best guesses 	 0.05555555555555555
mention in 	 0.3333333333333333
such name 	 0.008130081300813009
syntax , 	 0.45454545454545453
funding continued 	 0.125
they observe 	 0.025
LexRank and 	 0.08333333333333333
clues are 	 0.3333333333333333
both to 	 0.12903225806451613
criterion depends 	 0.5
answers are 	 0.08333333333333333
Behind this 	 1.0
with realistic 	 0.00546448087431694
Recently , 	 1.0
Eugene Charniak 	 1.0
proposed keyphrases 	 0.2222222222222222
south east 	 1.0
correctly-developed summaries 	 1.0
as candidates 	 0.003484320557491289
the large 	 0.0006920415224913495
agreement about 	 0.3333333333333333
meaning into 	 0.043478260869565216
still used 	 0.06666666666666667
parser for 	 0.0625
presented . 	 0.16666666666666666
then appear 	 0.02857142857142857
assigning the 	 1.0
summarization exactly 	 0.02
less -RRB- 	 0.08333333333333333
and efficient 	 0.002890173410404624
than precision 	 0.022222222222222223
when integrated 	 0.02857142857142857
will likely 	 0.05714285714285714
<s> Deferred 	 0.0007686395080707148
were accelerations 	 0.024390243902439025
one according 	 0.015384615384615385
fairly trivial 	 0.25
Several papers 	 0.3333333333333333
the sentence 	 0.004152249134948097
understand simple 	 0.2857142857142857
explicit formalization 	 0.2
though the 	 0.1
strongly than 	 0.5
components . 	 0.2
neutral or 	 0.5
discuss the 	 1.0
where clear 	 0.02857142857142857
each pilot 	 0.022222222222222223
the ten-year-long 	 0.0006920415224913495
systems depended 	 0.008928571428571428
is checking 	 0.0020325203252032522
recognized with 	 0.16666666666666666
standards are 	 0.2
way we 	 0.08333333333333333
embedded lists 	 0.25
and recursive-descent 	 0.001445086705202312
the search 	 0.0006920415224913495
roughly proportional 	 0.3333333333333333
both very 	 0.03225806451612903
spoken text 	 0.07142857142857142
a feasibility 	 0.001226993865030675
The second 	 0.005208333333333333
summarization technology 	 0.02
Technolangue\/Easy Text 	 0.5
challenge in 	 1.0
, adjective 	 0.0005614823133071309
following example 	 0.13333333333333333
that humans 	 0.0070921985815602835
Stemming Text 	 1.0
various natural 	 0.05555555555555555
A more 	 0.02
helps him 	 0.5
tokens -LRB- 	 0.14285714285714285
can improve 	 0.0055248618784530384
MARGIE -LRB- 	 1.0
Specifically , 	 1.0
lead to 	 1.0
of communication 	 0.0017825311942959
extracting meaningful 	 0.2
Rule-based The 	 0.5
both isloated 	 0.03225806451612903
seem to 	 0.5
made of 	 0.1875
, Klavans 	 0.0005614823133071309
rules , 	 0.11627906976744186
speech tagger 	 0.006578947368421052
Jump to 	 1.0
prove impossible 	 1.0
finds many 	 1.0
Text segmentation 	 0.16666666666666666
the serial 	 0.0006920415224913495
by Schank 	 0.005714285714285714
key clauses 	 0.16666666666666666
's gonna 	 0.0196078431372549
leading to 	 1.0
vary in 	 0.5
, large-vocabulary 	 0.0005614823133071309
SHRDLU for 	 0.16666666666666666
performance , 	 0.1111111111111111
In these 	 0.009523809523809525
Campaigns -RRB- 	 1.0
comparison uses 	 0.3333333333333333
applies to 	 0.14285714285714285
Some text 	 0.047619047619047616
to OCR 	 0.0013280212483399733
, reasoning 	 0.0005614823133071309
published at 	 0.14285714285714285
Another possible 	 0.07692307692307693
, rule-based 	 0.0005614823133071309
information need 	 0.021739130434782608
<s> Major 	 0.0015372790161414297
can increase 	 0.0055248618784530384
a higher 	 0.00245398773006135
knowledge specific 	 0.037037037037037035
be evaluated 	 0.008438818565400843
and smaller 	 0.001445086705202312
of disambiguation 	 0.00089126559714795
variables such 	 1.0
simultaneously with 	 0.5
by The 	 0.005714285714285714
problem is 	 0.11363636363636363
on attaching 	 0.009433962264150943
specific strengths 	 0.047619047619047616
or text 	 0.009009009009009009
devices for 	 0.25
and applications 	 0.001445086705202312
historically used 	 0.5
implicitly determines 	 1.0
is preferable 	 0.0020325203252032522
part-of-speech , 	 0.06666666666666667
but have 	 0.029411764705882353
transformations . 	 0.5
graph small 	 0.07692307692307693
Army Corps 	 0.5
occur together 	 0.2
, roughness 	 0.0005614823133071309
often the 	 0.045454545454545456
more widespread 	 0.010526315789473684
project ongoing 	 0.07692307692307693
-LRB- HLDA 	 0.0027100271002710027
same input 	 0.08
<s> Data 	 0.0007686395080707148
word-forms are 	 1.0
: syntax 	 0.00980392156862745
, answer 	 0.0005614823133071309
However , 	 0.8648648648648649
ellipsis , 	 1.0
not able 	 0.008928571428571428
Lauriault\/Loriot , 	 1.0
expressed by 	 0.16666666666666666
neat , 	 1.0
<s> Extraction 	 0.0015372790161414297
the important 	 0.0006920415224913495
parties du 	 1.0
it belongs 	 0.008547008547008548
right , 	 0.1
, Dr. 	 0.0005614823133071309
fail to 	 0.3333333333333333
, particularly 	 0.0011229646266142617
According to 	 1.0
opportunities and 	 1.0
and synthesis 	 0.001445086705202312
<s> increases 	 0.0007686395080707148
problem with 	 0.022727272727272728
linguistics . 	 0.05
In 1974 	 0.009523809523809525
as blogs 	 0.003484320557491289
the perception 	 0.0006920415224913495
condense a 	 1.0
Tell me 	 1.0
excess of 	 1.0
phrase How 	 0.1
e.g. marking 	 0.017857142857142856
with attribute 	 0.00546448087431694
be digitalized 	 0.004219409282700422
-LRB- MPE 	 0.0027100271002710027
, if 	 0.005614823133071308
subfields of 	 1.0
recognition from 	 0.01652892561983471
typically uses 	 0.05555555555555555
superimpose one 	 1.0
steadily , 	 1.0
to translate 	 0.00398406374501992
French in 	 0.125
Theo van 	 1.0
stub reader 	 1.0
explicit models 	 0.2
across most 	 0.6
recognition deteriorated 	 0.008264462809917356
purpose of 	 0.2
condensation operations 	 1.0
must also 	 0.07142857142857142
practice , 	 0.5
`` the 	 0.026455026455026454
efforts based 	 0.14285714285714285
then with 	 0.02857142857142857
Cloud Computing 	 1.0
the 1990s 	 0.001384083044982699
limit the 	 0.5
favor a 	 0.5
corpus has 	 0.03225806451612903
harder when 	 0.14285714285714285
the dominance 	 0.0006920415224913495
WordNet . 	 0.5
usually abbreviated 	 0.03125
useful to 	 0.14285714285714285
translation software 	 0.04054054054054054
substitution of 	 1.0
-RRB- '' 	 0.005420054200542005
especially useful 	 0.06666666666666667
contain rules 	 0.08333333333333333
became an 	 0.2
summaries with 	 0.046511627906976744
clearly visible 	 0.3333333333333333
Handwriting recognition 	 1.0
subject of 	 0.25
noun , 	 0.42857142857142855
only do 	 0.02631578947368421
he developed 	 0.14285714285714285
that apply 	 0.0035460992907801418
substantial financial 	 0.2
still have 	 0.06666666666666667
and syntactic 	 0.002890173410404624
; `` 	 0.02127659574468085
increasing G-loads 	 0.3333333333333333
Language Workshop 	 0.08333333333333333
machine processes 	 0.012658227848101266
help blind 	 0.1111111111111111
learning automatically 	 0.023255813953488372
attribute or 	 0.5
costs -LRB- 	 1.0
another , 	 0.07692307692307693
correct according 	 0.06666666666666667
A first 	 0.02
promising line 	 1.0
, Charles 	 0.0005614823133071309
, written 	 0.0005614823133071309
F = 	 1.0
of hard 	 0.0017825311942959
desired language 	 0.2
features like 	 0.038461538461538464
or interpreter 	 0.009009009009009009
defined to 	 0.16666666666666666
role as 	 0.25
tests the 	 0.5
without having 	 0.07692307692307693
recognition but 	 0.008264462809917356
as individual 	 0.003484320557491289
The technique 	 0.005208333333333333
seen as 	 0.3
covers tasks 	 0.25
We assume 	 0.14285714285714285
be required 	 0.004219409282700422
language -LRB- 	 0.013513513513513514
work of 	 0.08333333333333333
universal '' 	 0.3333333333333333
Future of 	 0.5
23 letters 	 1.0
<s> Due 	 0.0007686395080707148
has meant 	 0.011904761904761904
Black 1991 	 0.5
generated -LRB- 	 0.06666666666666667
perform an 	 0.09090909090909091
or phrases 	 0.009009009009009009
Office -LRB- 	 1.0
demonstration that 	 0.2
readily conveyed 	 0.3333333333333333
system proposed 	 0.010752688172043012
usually involves 	 0.03125
blocks worlds 	 0.25
structures in 	 0.2
grammar having 	 0.02702702702702703
not seen 	 0.008928571428571428
rare or 	 0.25
large corpora 	 0.043478260869565216
unweighted edges 	 1.0
identical to 	 1.0
other 10 	 0.014285714285714285
help to 	 0.1111111111111111
successfully in 	 0.3333333333333333
that some 	 0.014184397163120567
published in 	 0.14285714285714285
retrieved . 	 1.0
during a 	 0.2
of syntax 	 0.0017825311942959
have human-made 	 0.009615384615384616
tag of 	 0.0625
provider dictates 	 1.0
data sets 	 0.03896103896103896
summaries must 	 0.023255813953488372
learning and 	 0.023255813953488372
<s> Relationship 	 0.0007686395080707148
small local 	 0.1111111111111111
P + 	 0.5
licensed on 	 1.0
Rosa Caldas-Coulthard 	 1.0
languages using 	 0.02
emails -RRB- 	 0.5
is precision 	 0.0020325203252032522
, room 	 0.0005614823133071309
going backward 	 0.25
due to 	 0.4
co-occurring neighbors 	 1.0
are extractive 	 0.004149377593360996
i.e. source 	 0.05263157894736842
whether -LRB- 	 0.07692307692307693
-RRB- mark 	 0.0027100271002710027
make a 	 0.2
is commercially 	 0.0020325203252032522
so most 	 0.03333333333333333
as part 	 0.006968641114982578
of parser 	 0.0017825311942959
been extended 	 0.014705882352941176
hand -RRB- 	 0.07142857142857142
numbers , 	 0.2857142857142857
exponential number 	 0.5
much like 	 0.045454545454545456
used that 	 0.008849557522123894
, contractions 	 0.0005614823133071309
explicitly promoting 	 0.25
understanding system 	 0.030303030303030304
nodes that 	 0.14285714285714285
questions from 	 0.038461538461538464
Extraction Algorithm 	 0.3333333333333333
many years 	 0.019230769230769232
, identifying 	 0.0016844469399213925
VOLSUNGA . 	 1.0
distinguish between 	 0.4
signal can 	 0.3333333333333333
coupons returned 	 1.0
of emails 	 0.00089126559714795
'' to 	 0.015463917525773196
be known 	 0.004219409282700422
method used 	 0.0625
a conversation 	 0.001226993865030675
blind , 	 0.25
hurts '' 	 0.5
translation Automotive 	 0.013513513513513514
than has 	 0.022222222222222223
probabilities . 	 0.18181818181818182
or -LRB- 	 0.0045045045045045045
uncertainties at 	 1.0
data sources 	 0.025974025974025976
system recognizes 	 0.010752688172043012
, generating 	 0.0005614823133071309
recognized normal 	 0.16666666666666666
Language Generation 	 0.08333333333333333
a document 	 0.008588957055214725
ōrātiōnis -RRB- 	 1.0
meteorologist -RRB- 	 1.0
lower than 	 0.2
the sounds 	 0.001384083044982699
include Chinese 	 0.037037037037037035
one word 	 0.03076923076923077
, no 	 0.0016844469399213925
generation , 	 0.1111111111111111
Another popular 	 0.07692307692307693
output . 	 0.15384615384615385
which creates 	 0.007246376811594203
of 3 	 0.00089126559714795
the SPOTLIGHT 	 0.0006920415224913495
mentions -LRB- 	 0.3333333333333333
Chinese is 	 0.14285714285714285
marks , 	 0.25
Latin pars 	 0.25
segmentation In 	 0.030303030303030304
voice dialing 	 0.07692307692307693
while Church 	 0.05
<s> Front-End 	 0.0007686395080707148
devoted exclusively 	 0.2
Dependent '' 	 1.0
will not 	 0.11428571428571428
sometimes suggest 	 0.07692307692307693
computer applications 	 0.022727272727272728
, relay 	 0.0005614823133071309
spaces or 	 0.2
space exploration 	 0.2
English in 	 0.02702702702702703
Some examples 	 0.047619047619047616
Using almost 	 0.5
gradual lessening 	 1.0
Fowler , 	 1.0
DARPA funding 	 0.25
: for 	 0.00980392156862745
patterns rather 	 0.2
can simplify 	 0.0055248618784530384
structure of 	 0.3333333333333333
Corpus of 	 0.0625
Newton pioneered 	 1.0
tagging . 	 0.08
capture the 	 0.5
structure grammar 	 0.08333333333333333
has dried 	 0.011904761904761904
organization documents 	 0.2
for detecting 	 0.0036101083032490976
tagging , 	 0.08
corpora of 	 0.09090909090909091
software started 	 0.037037037037037035
Digitize the 	 1.0
is affected 	 0.0020325203252032522
as researchers 	 0.003484320557491289
see and 	 0.05
much additional 	 0.045454545454545456
generate textual 	 0.05555555555555555
sequence of 	 0.875
are three 	 0.004149377593360996
network -LRB- 	 0.16666666666666666
: Stemming 	 0.00980392156862745
most negative 	 0.017241379310344827
a Computer 	 0.001226993865030675
our alphabetic 	 0.2
below 50 	 0.2
were developed 	 0.12195121951219512
XML documents 	 1.0
defined by 	 0.16666666666666666
a sophisticated 	 0.001226993865030675
achieve performance 	 0.5
has published 	 0.011904761904761904
own right 	 0.16666666666666666
1949 . 	 0.5
corpora that 	 0.09090909090909091
over 1,000 	 0.08333333333333333
D. , 	 0.4
characters from 	 0.0625
be retrained 	 0.004219409282700422
smaller ones 	 0.14285714285714285
'' from 	 0.005154639175257732
achieves 98.5 	 0.5
useful NLG 	 0.07142857142857142
without reference 	 0.07692307692307693
construct an 	 0.3333333333333333
conditions Accuracy 	 0.2
using their 	 0.01694915254237288
recording conditions 	 1.0
in popular 	 0.0018726591760299626
Transcription -LRB- 	 1.0
in 1966 	 0.0018726591760299626
machine printed 	 0.012658227848101266
Nikolas Rose 	 1.0
On January 	 0.16666666666666666
tag to 	 0.0625
Advanced applications 	 0.2
they use 	 0.025
mostly as 	 0.5
Harris had 	 0.1111111111111111
of Speech 	 0.00089126559714795
, TNO 	 0.0005614823133071309
assertions in 	 0.5
to recognize 	 0.00796812749003984
autopilot system 	 1.0
called `` 	 0.2777777777777778
part-of-speech markers 	 0.06666666666666667
Transactions on 	 1.0
a rudimentary 	 0.001226993865030675
-LRB- U.S. 	 0.005420054200542005
languages have 	 0.04
output of 	 0.15384615384615385
quantities . 	 0.3333333333333333
segmentation process 	 0.030303030303030304
skilled linguist 	 1.0
study of 	 0.25
labels to 	 1.0
online expression 	 0.125
web site 	 0.25
supported by 	 1.0
low pollen 	 0.3333333333333333
the harmonic 	 0.0006920415224913495
nodes for 	 0.14285714285714285
reported was 	 0.2
and interjection 	 0.001445086705202312
fundamental errors 	 0.5
sentences weighted 	 0.013157894736842105
adjective or 	 0.42857142857142855
and printed 	 0.001445086705202312
which make 	 0.014492753623188406
Determine the 	 1.0
print a 	 1.0
of British 	 0.00089126559714795
CANDIDE from 	 1.0
SourceForge . 	 1.0
, rapidly 	 0.0005614823133071309
-LRB- basically 	 0.0027100271002710027
can not 	 0.08287292817679558
the effort 	 0.0006920415224913495
of interaction 	 0.00089126559714795
the last 	 0.0020761245674740486
feasible to 	 0.5
Another important 	 0.07692307692307693
came from 	 0.5
two senses 	 0.034482758620689655
controlled by 	 1.0
judges , 	 0.5
often not 	 0.045454545454545456
when multiple 	 0.02857142857142857
is extremely 	 0.0040650406504065045
materials to 	 0.5
<s> We 	 0.005380476556495004
Test of 	 1.0
taggers The 	 0.14285714285714285
emotion , 	 1.0
The sailor 	 0.005208333333333333
spoken natural 	 0.07142857142857142
data category 	 0.012987012987012988
and Roger 	 0.001445086705202312
marking up 	 0.5
impossibility of 	 1.0
at lower 	 0.014705882352941176
this reason 	 0.02197802197802198
Additional aspects 	 1.0
outside the 	 0.5
merging of 	 0.5
can discriminate 	 0.0055248618784530384
a meaning 	 0.00245398773006135
is apple 	 0.0020325203252032522
of translating 	 0.00089126559714795
page . 	 0.14285714285714285
geospatial questions 	 1.0
now done 	 0.07692307692307693
knowledge but 	 0.037037037037037035
for them 	 0.007220216606498195
even with 	 0.037037037037037035
or positive 	 0.0045045045045045045
are beginning 	 0.004149377593360996
Elinor Ochs 	 1.0
list approach 	 0.09090909090909091
and curves 	 0.001445086705202312
for computer 	 0.010830324909747292
parse individual 	 0.1111111111111111
categories , 	 0.1111111111111111
data table 	 0.012987012987012988
pages , 	 0.42857142857142855
routed along 	 0.5
specific to 	 0.047619047619047616
of adaptive 	 0.00089126559714795
with keyphrases 	 0.00546448087431694
of conversations 	 0.00089126559714795
document summarization 	 0.1388888888888889
while verbs 	 0.05
incorporates a 	 1.0
In common 	 0.009523809523809525
as ` 	 0.003484320557491289
typically given 	 0.05555555555555555
after 2,000 	 0.08333333333333333
translation would 	 0.02702702702702703
following . 	 0.13333333333333333
includes Turney 	 0.14285714285714285
some measure 	 0.012048192771084338
it formed 	 0.008547008547008548
recognition equipment 	 0.008264462809917356
transcription . 	 0.5
<s> Our 	 0.0023059185242121443
by a 	 0.10285714285714286
and perspective 	 0.001445086705202312
The goal 	 0.005208333333333333
Thai and 	 0.5
do '' 	 0.038461538461538464
Corporation originally 	 0.25
once you 	 1.0
`` um 	 0.005291005291005291
also been 	 0.057971014492753624
nonsensical to 	 1.0
needs . 	 0.1
Lexical segmentation 	 0.5
are claiming 	 0.004149377593360996
reproducing formatted 	 1.0
sentence to 	 0.020833333333333332
whole sentences 	 0.2222222222222222
at NYU 	 0.014705882352941176
context of 	 0.15151515151515152
constituents , 	 0.5
of Lichtenstein 	 0.00089126559714795
written-out number 	 1.0
text lacks 	 0.006289308176100629
Hirschman 1998 	 0.5
these same 	 0.023809523809523808
Interactional sociolinguistics 	 1.0
This criterion 	 0.015873015873015872
followed perhaps 	 0.25
transducer , 	 0.5
by simple 	 0.005714285714285714
of sentence-level 	 0.00089126559714795
president of 	 1.0
and DCD 	 0.001445086705202312
ratings for 	 0.1111111111111111
many strokes 	 0.019230769230769232
with low 	 0.00546448087431694
to ones 	 0.0013280212483399733
extraction depends 	 0.03225806451612903
front-end SR 	 1.0
and , 	 0.004335260115606936
text by 	 0.006289308176100629
Advanced reasoning 	 0.2
workshops dedicated 	 0.5
perhaps surprisingly 	 0.16666666666666666
between dynamically 	 0.02564102564102564
often a 	 0.022727272727272728
evaluation has 	 0.018518518518518517
join different 	 1.0
it up 	 0.008547008547008548
sounds representing 	 0.06666666666666667
or component 	 0.0045045045045045045
and wrote 	 0.001445086705202312
input such 	 0.024390243902439025
Machinery and 	 1.0
of handwritten 	 0.00089126559714795
valuable new 	 0.5
Important journals 	 1.0
sentence is 	 0.041666666666666664
variance on 	 1.0
a particular 	 0.0049079754601227
teams to 	 0.5
software had 	 0.037037037037037035
language document 	 0.006756756756756757
criteria . 	 0.25
data within 	 0.012987012987012988
and generic 	 0.001445086705202312
analyses , 	 0.2
OnlineOCR With 	 0.3333333333333333
help prevent 	 0.1111111111111111
reader installed 	 0.1
multimedia -LRB- 	 0.5
relevant . 	 0.14285714285714285
of pairs 	 0.0017825311942959
or characters 	 0.0045045045045045045
adjective 40 	 0.14285714285714285
simulated a 	 0.5
applied to 	 0.7333333333333333
amenable to 	 1.0
perspective in 	 0.25
visible light 	 0.3333333333333333
result , 	 0.2727272727272727
architecture uses 	 0.5
Machine learning 	 0.1111111111111111
act theory 	 0.25
Sentence segmentation 	 0.4
produce such 	 0.045454545454545456
-LRB- actual 	 0.0027100271002710027
feature dependencies 	 0.07692307692307693
date -LRB- 	 0.6666666666666666
corpus as 	 0.03225806451612903
text -LRB- 	 0.03773584905660377
each ambiguity 	 0.022222222222222223
linguistics and 	 0.05
drawn right-to-left 	 1.0
Verschueren , 	 1.0
their chosen 	 0.029411764705882353
so for 	 0.03333333333333333
Again , 	 1.0
of anomalies 	 0.00089126559714795
parsing a 	 0.03571428571428571
, researchers 	 0.0011229646266142617
as weighted 	 0.003484320557491289
the EARS 	 0.0006920415224913495
Nagao in 	 1.0
Modern general-purpose 	 0.3333333333333333
and decorrelating 	 0.001445086705202312
of marketing 	 0.00089126559714795
subsystem for 	 1.0
both left-most 	 0.03225806451612903
Booth and 	 1.0
relationship between 	 0.16666666666666666
, he 	 0.0011229646266142617
much more 	 0.18181818181818182
might not 	 0.07692307692307693
of grammatical 	 0.00089126559714795
often contains 	 0.022727272727272728
volume of 	 0.5
`` semi-supervised 	 0.005291005291005291
a linguistic 	 0.00245398773006135
work for 	 0.041666666666666664
reporting -RRB- 	 0.3333333333333333
Was he 	 1.0
generally used 	 0.09090909090909091
Science Research 	 0.5
Friday have 	 1.0
entropy has 	 0.2
earlier some 	 0.25
<s> Incorporating 	 0.0007686395080707148
an abbreviation 	 0.007575757575757576
an urgent 	 0.007575757575757576
first use 	 0.030303030303030304
is best 	 0.0020325203252032522
software varied 	 0.037037037037037035
never been 	 0.4
commonly taught 	 0.125
records . 	 0.5
analyzed , 	 0.2
`` features 	 0.005291005291005291
that pollen 	 0.0035460992907801418
Security Agency 	 1.0
trillion-word corpus 	 1.0
internalize the 	 1.0
commercial system 	 0.09090909090909091
may blend 	 0.019230769230769232
initial , 	 0.3333333333333333
linguists decided 	 0.3333333333333333
importantly , 	 1.0
language models 	 0.013513513513513514
Bell Telephone 	 1.0
author of 	 0.3333333333333333
for up-to-date 	 0.0036101083032490976
also quite 	 0.014492753623188406
citations for 	 0.3333333333333333
lexical and 	 0.15384615384615385
vendors speech 	 0.25
them , 	 0.21052631578947367
into standard 	 0.01282051282051282
polarity on 	 0.125
accurately -LRB- 	 0.5
machines or 	 0.25
assigned , 	 0.5
are commonly 	 0.004149377593360996
the issue 	 0.002768166089965398
Introduction and 	 1.0
training in 	 0.03571428571428571
How should 	 0.14285714285714285
is being 	 0.006097560975609756
star ratings 	 0.5
much larger 	 0.09090909090909091
and paragraphs 	 0.001445086705202312
the quality 	 0.0034602076124567475
With IT 	 0.14285714285714285
When the 	 0.14285714285714285
stochastic . 	 0.125
CSIS , 	 0.5
Optophone , 	 1.0
Master lead-in 	 1.0
observed vector 	 1.0
plural common 	 0.2
canned text 	 0.5
world knowledge 	 0.13333333333333333
words of 	 0.027522935779816515
now commonplace 	 0.07692307692307693
: lexical 	 0.00980392156862745
progress was 	 0.2857142857142857
human intervention 	 0.021739130434782608
'' might 	 0.010309278350515464
There has 	 0.18181818181818182
sentences begin 	 0.013157894736842105
evaluation approach 	 0.018518518518518517
in different 	 0.0056179775280898875
children 's 	 0.5
German city 	 0.25
in supervised 	 0.0018726591760299626
more power 	 0.010526315789473684
may or 	 0.019230769230769232
the computerization 	 0.0006920415224913495
form . 	 0.1
depends greatly 	 0.125
be achieved 	 0.02109704641350211
desktop OCR 	 1.0
for machine 	 0.010830324909747292
the burden 	 0.0006920415224913495
Ticket stock 	 1.0
<s> Sentence 	 0.0023059185242121443
comes to 	 0.2
or '' 	 0.0045045045045045045
to a 	 0.03718459495351926
+ , 	 0.3333333333333333
of hand 	 0.00089126559714795
or movies 	 0.0045045045045045045
controllers Training 	 0.3333333333333333
out other 	 0.07142857142857142
this method 	 0.01098901098901099
complicated backgrounds 	 0.3333333333333333
J. , 	 0.6666666666666666
93-95 % 	 1.0
1954 -RRB- 	 0.3333333333333333
Orleans '' 	 0.5
Christmas fall 	 1.0
relevant text 	 0.14285714285714285
the LOB 	 0.0006920415224913495
Intelligence '' 	 0.3333333333333333
use `` 	 0.013888888888888888
-- including 	 0.04
be explained 	 0.004219409282700422
much time 	 0.045454545454545456
rewrite it 	 1.0
are clearly 	 0.004149377593360996
tonal language 	 1.0
going to 	 0.25
progress and 	 0.14285714285714285
is strong 	 0.0020325203252032522
introduced the 	 1.0
Whether a 	 0.5
ones , 	 0.3
using neural 	 0.01694915254237288
entirely and 	 0.5
theories of 	 0.6
that integrated 	 0.0035460992907801418
which its 	 0.007246376811594203
augmented transition 	 1.0
each time 	 0.022222222222222223
fields . 	 0.16666666666666666
having the 	 0.2
of Engineers 	 0.0017825311942959
<s> Beginning 	 0.0007686395080707148
usually can 	 0.03125
titles , 	 0.5
other features 	 0.014285714285714285
that dispense 	 0.0035460992907801418
In 2006 	 0.009523809523809525
pages . 	 0.2857142857142857
Lightning II 	 1.0
a graph 	 0.0036809815950920245
research in 	 0.14285714285714285
the Standard 	 0.0006920415224913495
various algorithms 	 0.05555555555555555
Advanced Fighter 	 0.2
texts so 	 0.058823529411764705
a quantity 	 0.001226993865030675
original training 	 0.07692307692307693
proven useful 	 1.0
evaluated using 	 0.14285714285714285
context that 	 0.030303030303030304
; while 	 0.02127659574468085
lists that 	 1.0
in time 	 0.0018726591760299626
hand coding 	 0.07142857142857142
general speech 	 0.045454545454545456
as content 	 0.003484320557491289
spoken sentence 	 0.07142857142857142
interest in 	 0.6363636363636364
among humans 	 0.125
should predict 	 0.05263157894736842
filtering out 	 1.0
Increasingly , 	 1.0
uses only 	 0.07142857142857142
are needed 	 0.004149377593360996
as objective 	 0.003484320557491289
entropy , 	 0.2
local document 	 0.3333333333333333
List of 	 1.0
the earliest 	 0.0006920415224913495
to determine 	 0.014608233731739707
real difference 	 0.1111111111111111
not initially 	 0.008928571428571428
simple pro 	 0.038461538461538464
-RRB- or 	 0.01084010840108401
probabilistic decisions 	 0.2857142857142857
data mining 	 0.025974025974025976
People with 	 1.0
effectively learning 	 0.3333333333333333
text-to-speech and 	 0.5
models Main 	 0.038461538461538464
annotated -LRB- 	 0.5
are in-principle 	 0.004149377593360996
that by 	 0.0035460992907801418
embedded quotations 	 0.25
a native 	 0.00245398773006135
: Dynamic 	 0.00980392156862745
TF-IDF vectors 	 1.0
the chosen 	 0.0006920415224913495
One can 	 0.07692307692307693
The edges 	 0.005208333333333333
to ensure 	 0.0013280212483399733
paraphrasing sections 	 1.0
complexity . 	 0.08333333333333333
a 3 	 0.001226993865030675
2007 is 	 0.2
partially influenced 	 1.0
at input 	 0.014705882352941176
involves the 	 0.2
of data 	 0.006238859180035651
other forms 	 0.014285714285714285
person-years of 	 1.0
appropriately spelled 	 0.5
it would 	 0.03418803418803419
Internet financial 	 0.5
not only 	 0.0625
McDonald -LRB- 	 1.0
the essence 	 0.001384083044982699
systems do 	 0.008928571428571428
, within 	 0.0005614823133071309
often be 	 0.022727272727272728
, stochastic 	 0.0005614823133071309
be semantic 	 0.004219409282700422
extent -RRB- 	 0.25
assigned keywords 	 0.5
software technology 	 0.037037037037037035
meaning then 	 0.043478260869565216
semantic or 	 0.047619047619047616
special challenges 	 0.2
operating system 	 0.5
of cepstral 	 0.00089126559714795
larger corpora 	 0.0625
mostly work 	 0.5
sounds very 	 0.06666666666666667
mail since 	 0.5
be given 	 0.004219409282700422
data will 	 0.012987012987012988
and LUNAR 	 0.001445086705202312
with equipment 	 0.00546448087431694
ranks , 	 0.5
currently require 	 0.14285714285714285
pitch , 	 1.0
corpus in 	 0.06451612903225806
space complexity 	 0.2
have so-called 	 0.009615384615384616
the inferior 	 0.0006920415224913495
the wave 	 0.0006920415224913495
should correspond 	 0.05263157894736842
string of 	 1.0
especially common 	 0.13333333333333333
that specific 	 0.0035460992907801418
-RRB- measure 	 0.0027100271002710027
tense , 	 0.5
tool to 	 0.5
fundamental , 	 0.5
Its main 	 0.5
after going 	 0.08333333333333333
Semi-supervised and 	 1.0
the gradual 	 0.0006920415224913495
major algorithms 	 0.08333333333333333
Such strategy 	 0.125
tract length 	 1.0
instances of 	 0.6666666666666666
<s> Recognizing 	 0.0007686395080707148
the statistics 	 0.0006920415224913495
command centres 	 0.5
word can 	 0.03333333333333333
appropriate action 	 0.25
abstracts , 	 0.5
an abstract 	 0.007575757575757576
in visible 	 0.0018726591760299626
-LRB- how 	 0.008130081300813009
recognition efforts 	 0.008264462809917356
the 1980s 	 0.001384083044982699
The Georgetown 	 0.015625
no subtypes 	 0.07692307692307693
social sciences 	 0.14285714285714285
decided that 	 0.3333333333333333
Another area 	 0.07692307692307693
recognition products 	 0.008264462809917356
products . 	 0.25
within an 	 0.05555555555555555
analyses of 	 0.2
apple is 	 0.6666666666666666
realm of 	 1.0
probabilities to 	 0.09090909090909091
though ROUGE-1 	 0.1
to act 	 0.0026560424966799467
side effect 	 1.0
undertaken to 	 0.5
massive collections 	 1.0
measure of 	 0.18181818181818182
mark the 	 0.3333333333333333
automotive maintenance 	 1.0
substantial funding 	 0.2
together the 	 0.125
or words 	 0.0045045045045045045
databases -RRB- 	 0.125
came into 	 0.5
best -RRB- 	 0.05555555555555555
Machine translation 	 0.5555555555555556
a new 	 0.007361963190184049
samples from 	 0.5
sad , 	 1.0
The more 	 0.005208333333333333
sentence breaking 	 0.020833333333333332
Analysis & 	 0.2
series of 	 0.875
ANR-Passage project 	 1.0
LREC Granada 	 1.0
deduction to 	 1.0
total accuracy 	 0.5
as commercial 	 0.003484320557491289
entities employed 	 0.14285714285714285
or they 	 0.0045045045045045045
card spending 	 0.25
, human 	 0.003368893879842785
Therein lies 	 1.0
than speech 	 0.022222222222222223
English this 	 0.02702702702702703
Puma helicopter 	 1.0
part-of-speech categories 	 0.06666666666666667
worked , 	 0.2
vocabulary of 	 0.125
applications have 	 0.08
involves deciding 	 0.1
, resource 	 0.0005614823133071309
Main article 	 1.0
with edit 	 0.00546448087431694
a collect 	 0.001226993865030675
already discussed 	 0.2
<s> Many 	 0.008455034588777863
approximates that 	 0.5
personal digital 	 0.25
the collection 	 0.0006920415224913495
German capitalizes 	 0.25
results are 	 0.19047619047619047
PC platform 	 0.25
Spanish , 	 0.5
continue to 	 1.0
environment where 	 0.16666666666666666
Corporation and 	 0.25
to overcome 	 0.0013280212483399733
Anaphor resolution 	 1.0
are grounded 	 0.008298755186721992
prisoner-of-war camp 	 1.0
milliseconds -RRB- 	 0.5
comprehension and 	 0.14285714285714285
The algorithm 	 0.005208333333333333
, annotation 	 0.0005614823133071309
1960s and 	 0.3333333333333333
no means 	 0.07692307692307693
ATC simulators 	 0.2
clear imaging 	 0.25
and know 	 0.001445086705202312
applications Robotics 	 0.04
as discussions 	 0.003484320557491289
mid-1960s . 	 1.0
and include 	 0.002890173410404624
campaign dedicated 	 0.2
holes '' 	 1.0
been parse 	 0.014705882352941176
and what 	 0.001445086705202312
social media 	 0.2857142857142857
of parse 	 0.00089126559714795
and Nelson 	 0.001445086705202312
In a 	 0.01904761904761905
collect call 	 1.0
coverage , 	 0.3333333333333333
`` Translation 	 0.005291005291005291
system takes 	 0.010752688172043012
written by 	 0.23076923076923078
generally evaluated 	 0.09090909090909091
agreement is 	 0.3333333333333333
particular NLP 	 0.07692307692307693
discourses and 	 0.5
long input 	 0.5
See also 	 0.5
as Google 	 0.003484320557491289
UK dealing 	 0.25
as that 	 0.003484320557491289
like Ncmsan 	 0.03571428571428571
targets to 	 1.0
large set 	 0.043478260869565216
's methodology 	 0.0196078431372549
completely nonsensical 	 1.0
top-down expansion 	 0.25
courses of 	 1.0
this book 	 0.01098901098901099
databases and 	 0.125
has increased 	 0.011904761904761904
; A 	 0.02127659574468085
distinctions -LRB- 	 0.5
levels even 	 0.045454545454545456
parse a 	 0.1111111111111111
the Puma 	 0.0006920415224913495
automatically as 	 0.047619047619047616
NLP evaluation 	 0.06382978723404255
by other 	 0.005714285714285714
only the 	 0.10526315789473684
that assigns 	 0.0035460992907801418
in recognizing 	 0.003745318352059925
examples . 	 0.16666666666666666
some machine 	 0.012048192771084338
was hand-written 	 0.012987012987012988
descriptive rather 	 0.3333333333333333
even `` 	 0.037037037037037035
warping -LRB- 	 0.25
issues Disambiguation 	 0.2
speech act 	 0.006578947368421052
create more 	 0.058823529411764705
source software 	 0.041666666666666664
<s> Improved 	 0.0007686395080707148
sentence such 	 0.020833333333333332
positive , 	 0.14285714285714285
Rabiner can 	 1.0
perfect -LRB- 	 1.0
speech into 	 0.013157894736842105
indiscriminate . 	 1.0
eigenvector corresponding 	 0.5
to paper-intensive 	 0.0013280212483399733
it listens 	 0.008547008547008548
concepts between 	 0.2
for an 	 0.0036101083032490976
speaker reads 	 0.05555555555555555
it conducted 	 0.008547008547008548
a reduced 	 0.001226993865030675
translating texts 	 0.25
topic of 	 0.125
segments at 	 0.2
other places 	 0.014285714285714285
years researchers 	 0.047619047619047616
concepts . 	 0.4
theoretical aspects 	 0.3333333333333333
identification . 	 0.2
parsers can 	 0.07692307692307693
clauses , 	 1.0
http:\/\/haydn.isi.edu\/ROUGE\/ -RRB- 	 1.0
Peter Turney 	 1.0
-RRB- may 	 0.0027100271002710027
is split 	 0.0040650406504065045
more than 	 0.042105263157894736
million word 	 0.3333333333333333
` naturally 	 0.0625
Activity -LRB- 	 1.0
put a 	 0.25
voicemail to 	 1.0
Re-encoding this 	 1.0
algorithm implicitly 	 0.03571428571428571
aircraft -LRB- 	 0.2857142857142857
and makes 	 0.001445086705202312
sophisticated NLG 	 0.14285714285714285
a German 	 0.001226993865030675
example for 	 0.024691358024691357
used together 	 0.008849557522123894
understand the 	 0.2857142857142857
unsupervised keyphrase 	 0.125
-- the 	 0.12
imprints for 	 1.0
words will 	 0.01834862385321101
most widely 	 0.017241379310344827
and text 	 0.005780346820809248
`` On 	 0.005291005291005291
Haton -LRB- 	 1.0
compounded by 	 1.0
case in 	 0.058823529411764705
consideration the 	 0.3333333333333333
the person 	 0.002768166089965398
not lead 	 0.008928571428571428
computer programming 	 0.022727272727272728
In order 	 0.01904761904761905
dynamic study 	 0.2
functions such 	 0.5
real human 	 0.1111111111111111
from Moore 	 0.009615384615384616
including , 	 0.07142857142857142
1987 -LRB- 	 0.3333333333333333
commonly teach 	 0.125
<s> -LRB- 	 0.014604150653343582
report -RRB- 	 0.25
thus it 	 0.1
proved similarly 	 0.3333333333333333
larger context 	 0.0625
ELIZA worked 	 0.1111111111111111
started the 	 0.25
G , 	 1.0
and analyze 	 0.001445086705202312
too many 	 0.3333333333333333
Nearest-neighbor have 	 1.0
Bush 's 	 0.5
, coughing 	 0.0005614823133071309
learning applications 	 0.023255813953488372
for Speech 	 0.0036101083032490976
wave is 	 0.2222222222222222
way to 	 0.4166666666666667
dictionary can 	 0.14285714285714285
rates greatly 	 0.125
which focused 	 0.007246376811594203
an NLP 	 0.022727272727272728
require to 	 0.045454545454545456
In 1935 	 0.009523809523809525
appropriate perspective 	 0.25
Graph This 	 1.0
scale -LRB- 	 0.16666666666666666
from improved 	 0.009615384615384616
published his 	 0.14285714285714285
, neutral 	 0.0005614823133071309
text into 	 0.0440251572327044
data maintained 	 0.012987012987012988
be generated 	 0.004219409282700422
interaction with 	 0.125
case '' 	 0.058823529411764705
; total 	 0.02127659574468085
Text-proofing Natural 	 1.0
multilingual corpus 	 0.3333333333333333
undertaken , 	 0.5
an early 	 0.007575757575757576
digital dictation 	 0.14285714285714285
on training 	 0.0047169811320754715
integer , 	 1.0
moved across 	 1.0
years development 	 0.09523809523809523
, paper 	 0.0005614823133071309
parsed by 	 0.75
contextual or 	 0.5
general use 	 0.045454545454545456
action of 	 0.2
General Post 	 1.0
Neural networks 	 0.75
capitalize names 	 1.0
ATC training 	 0.4
similarity or 	 0.1
since the 	 0.1
in languages 	 0.0018726591760299626
6 over 	 0.25
: Category 	 0.00980392156862745
concern . 	 1.0
conducted with 	 0.2
against which 	 0.2
why Vice 	 0.14285714285714285
and converted 	 0.001445086705202312
2.0 was 	 0.5
though such 	 0.1
move items 	 1.0
Parsers are 	 0.5
potentially unlimited 	 0.3333333333333333
give the 	 0.5
unmanageable . 	 1.0
Modern speech 	 0.3333333333333333
containing several 	 0.125
, Speech 	 0.0016844469399213925
characterised by 	 1.0
can express 	 0.022099447513812154
<s> Beatrice 	 0.0007686395080707148
in contrast 	 0.0018726591760299626
often . 	 0.022727272727272728
formal or 	 0.1111111111111111
the food 	 0.0006920415224913495
probabilities returned 	 0.09090909090909091
simple parsing 	 0.038461538461538464
turns -RRB- 	 0.3333333333333333
function either 	 0.125
summaries automatically 	 0.023255813953488372
, also 	 0.002807411566535654
when deployed 	 0.02857142857142857
others were 	 0.08333333333333333
published a 	 0.2857142857142857
sound impressive 	 0.05
art . 	 0.5
syntactic parsing 	 0.07692307692307693
extract a 	 0.25
enormous amount 	 1.0
1960s were 	 0.3333333333333333
the character 	 0.0006920415224913495
additionally requires 	 1.0
answer corpus 	 0.03333333333333333
-RRB- Bhatia 	 0.0027100271002710027
`` recommending 	 0.005291005291005291
particular feature 	 0.07692307692307693
overall polarity 	 0.16666666666666666
of 5 	 0.00089126559714795
curves , 	 1.0
Once performed 	 0.4
the morphemes 	 0.0006920415224913495
approximate meaning 	 0.5
similarity between 	 0.2
personal computing 	 0.25
selected as 	 0.5
further research 	 0.125
thus require 	 0.1
card number 	 0.25
accuracy rate 	 0.06451612903225806
morphological , 	 0.3333333333333333
a tool 	 0.00245398773006135
service . 	 0.2
requires fairly 	 0.0625
task remains 	 0.023809523809523808
to unfamiliar 	 0.0013280212483399733
-LRB- MAHS 	 0.0027100271002710027
traffic controllers 	 1.0
may dismiss 	 0.019230769230769232
Both methods 	 0.3333333333333333
, 2 	 0.0005614823133071309
distinguishes two 	 0.5
translate five 	 0.16666666666666666
Penn -RRB- 	 0.1111111111111111
the problem 	 0.006228373702422145
Authorities in 	 1.0
to various 	 0.0013280212483399733
A. van 	 0.2
post-processed by 	 1.0
Formal equivalence 	 1.0
Nations and 	 0.5
the above 	 0.001384083044982699
completion of 	 1.0
or left-to-right 	 0.0045045045045045045
Speaker Dependent 	 0.16666666666666666
, semantics 	 0.0016844469399213925
<s> Eight 	 0.0007686395080707148
prisoners or 	 0.5
transform -LRB- 	 0.2
restaurant , 	 0.5
Category = 	 0.5
growing field 	 0.5
and paste 	 0.001445086705202312
right kind 	 0.1
-- Pointwise 	 0.04
lookup algorithms 	 1.0
Optical Character 	 0.3333333333333333
is going 	 0.0020325203252032522
statistical translation 	 0.030303030303030304
co-occur at 	 0.5
computational power 	 0.2
sentences at 	 0.013157894736842105
-LRB- 1966 	 0.0027100271002710027
Reukos S. 	 1.0
more -RRB- 	 0.010526315789473684
most practical 	 0.017241379310344827
Little further 	 1.0
the Parseval\/GEIG 	 0.0006920415224913495
process termed 	 0.027777777777777776
that operated 	 0.0035460992907801418
have on 	 0.009615384615384616
they had 	 0.025
years -LRB- 	 0.047619047619047616
easy to 	 1.0
; rejecting 	 0.0425531914893617
points out 	 0.5
article then 	 0.034482758620689655
on work 	 0.0047169811320754715
of summarization 	 0.0071301247771836
find answers 	 0.07692307692307693
, imagery 	 0.0005614823133071309
These are 	 0.11764705882352941
, moves 	 0.0005614823133071309
reports , 	 0.2
of oral 	 0.00089126559714795
objects of 	 0.2
where using 	 0.02857142857142857
with Nuance 	 0.00546448087431694
slide represent 	 1.0
makes the 	 0.25
capabilities and 	 0.2
<s> BASEBALL 	 0.0007686395080707148
attractive method 	 0.3333333333333333
additional evidence 	 0.16666666666666666
often span 	 0.022727272727272728
have in 	 0.019230769230769232
in 1971 	 0.0018726591760299626
, negative 	 0.0005614823133071309
corpus such 	 0.03225806451612903
It approaches 	 0.02631578947368421
also similar 	 0.014492753623188406
record of 	 1.0
Levenshtein distance 	 1.0
translation when 	 0.013513513513513514
be approached 	 0.004219409282700422
largely been 	 0.2
A direct 	 0.02
extraction removes 	 0.03225806451612903
complex formalisms 	 0.041666666666666664
of campaigns 	 0.00089126559714795
from which 	 0.028846153846153848
of comprehensive 	 0.00089126559714795
corp. . 	 1.0
Sydney Lamb 	 1.0
by paying 	 0.005714285714285714
one has 	 0.015384615384615385
units such 	 0.14285714285714285
summarizing multiple 	 1.0
of recognition 	 0.0017825311942959
<s> Was 	 0.0007686395080707148
making them 	 0.14285714285714285
trigram , 	 0.6666666666666666
with human-made 	 0.00546448087431694
we are 	 0.044444444444444446
that an 	 0.0035460992907801418
allowing us 	 0.3333333333333333
methodologies . 	 1.0
necessarily match 	 0.5
any kind 	 0.03225806451612903
AI-complete problem 	 0.3333333333333333
for determining 	 0.007220216606498195
with corpus 	 0.00546448087431694
adjacent and 	 0.16666666666666666
connected directly 	 0.2
: Convert 	 0.0196078431372549
printed in 	 0.08333333333333333
best candidate 	 0.05555555555555555
To mine 	 0.1111111111111111
, these 	 0.0011229646266142617
professional translator 	 1.0
went past 	 0.2
aids for 	 1.0
would enable 	 0.018867924528301886
ontology requires 	 0.5
optimization methods 	 1.0
create features 	 0.058823529411764705
bilingual text 	 0.5
generation : 	 0.2222222222222222
a collection 	 0.00245398773006135
given sequences 	 0.041666666666666664
An extrinsic 	 0.0625
Apart from 	 1.0
each with 	 0.022222222222222223
useful keyphrases 	 0.07142857142857142
text comprehension 	 0.006289308176100629
Recognition is 	 0.125
to reformulate 	 0.0013280212483399733
as computational 	 0.003484320557491289
dimensions For 	 0.3333333333333333
following `` 	 0.06666666666666667
, NNS 	 0.0005614823133071309
The recently 	 0.005208333333333333
or an 	 0.009009009009009009
; we 	 0.02127659574468085
summaries of 	 0.09302325581395349
are instructed 	 0.004149377593360996
what sense 	 0.03125
not been 	 0.017857142857142856
whom -RRB- 	 0.5
power , 	 0.25
probabilistic division 	 0.14285714285714285
sublanguage domains 	 0.3333333333333333
OCR-A font 	 1.0
extracting their 	 0.2
continued to 	 0.3333333333333333
from speech 	 0.009615384615384616
<s> Statistical 	 0.0023059185242121443
parameter estimation 	 1.0
DeRose 's 	 0.4
cases , 	 0.3888888888888889
for some 	 0.018050541516245487
cases -RRB- 	 0.05555555555555555
was demonstrated 	 0.012987012987012988
, reading 	 0.0005614823133071309
the SR 	 0.0006920415224913495
`` good 	 0.005291005291005291
applied the 	 0.06666666666666667
template-matching OCR 	 1.0
<s> Text 	 0.0015372790161414297
headlines , 	 1.0
goal . 	 0.14285714285714285
determining sentiment 	 0.16666666666666666
Speaking '' 	 1.0
of telephony 	 0.00089126559714795
a watertight 	 0.001226993865030675
a whole 	 0.00245398773006135
extensive knowledge 	 0.3333333333333333
direct real-world 	 0.16666666666666666
via the 	 1.0
a single 	 0.011042944785276074
reached 20,000 	 0.5
MMR -RRB- 	 1.0
and Janet 	 0.001445086705202312
Turkish , 	 1.0
define several 	 0.5
produce the 	 0.13636363636363635
controllers . 	 0.3333333333333333
the learning 	 0.0006920415224913495
Real time 	 0.5
APEXC machine 	 1.0
model mechanisms 	 0.03333333333333333
without significant 	 0.07692307692307693
restricted vocabulary 	 0.25
as one 	 0.006968641114982578
Digest and 	 0.3333333333333333
generating examples 	 0.2
bar code 	 1.0
building a 	 1.0
applications including 	 0.04
distinguish reliably 	 0.2
themselves sometimes 	 0.25
extractors are 	 1.0
recognition since 	 0.008264462809917356
semantic theory 	 0.09523809523809523
, Roger 	 0.0005614823133071309
but when 	 0.014705882352941176
in advanced 	 0.0018726591760299626
<s> Generally 	 0.003843197540353574
<s> Higher 	 0.0007686395080707148
NLP that 	 0.02127659574468085
During the 	 0.5
analog wave 	 0.5
1954 involved 	 0.3333333333333333
is semantic 	 0.0020325203252032522
of a 	 0.08199643493761141
What is 	 0.2727272727272727
news , 	 0.07692307692307693
specifically developed 	 0.5
water . 	 1.0
to appear 	 0.0026560424966799467
communication -LRB- 	 0.4
including images 	 0.07142857142857142
state computers 	 0.07142857142857142
dramatically reduced 	 1.0
use in 	 0.013888888888888888
lattice -RRB- 	 1.0
, automates 	 0.0005614823133071309
Approaches Bernard 	 0.3333333333333333
, David 	 0.0016844469399213925
and require 	 0.004335260115606936
sold to 	 0.3333333333333333
processors or 	 1.0
the realm 	 0.0006920415224913495
be further 	 0.004219409282700422
only of 	 0.02631578947368421
vocabulary size 	 0.125
resulting from 	 0.25
be effective 	 0.008438818565400843
target handover 	 0.09090909090909091
and language 	 0.004335260115606936
2010 and 	 0.3333333333333333
out loud 	 0.07142857142857142
teach that 	 1.0
obtained by 	 0.5714285714285714
in differing 	 0.0018726591760299626
decisions probabilistically 	 0.1
summaries formed 	 0.023255813953488372
other word 	 0.014285714285714285
Some writing 	 0.047619047619047616
with fixed 	 0.00546448087431694
Other areas 	 0.14285714285714285
to incorporate 	 0.0013280212483399733
operational settings 	 1.0
term for 	 0.16666666666666666
Although Harris 	 0.125
returns text 	 1.0
reading credit 	 0.125
A machine 	 0.02
of democracy 	 0.00089126559714795
Word segmentation 	 0.14285714285714285
as an 	 0.04529616724738676
paragraphs , 	 0.25
unknowns , 	 1.0
tagging for 	 0.04
proliferation of 	 1.0
categories can 	 0.1111111111111111
standard telegraph 	 0.07142857142857142
plus some 	 1.0
judgement or 	 0.3333333333333333
some labeled 	 0.012048192771084338
politics , 	 1.0
appear near 	 0.0625
she were 	 1.0
being processed 	 0.05555555555555555
the easier 	 0.0006920415224913495
Individuals with 	 1.0
<s> Information 	 0.0007686395080707148
raised in 	 1.0
used as 	 0.04424778761061947
the mental 	 0.0006920415224913495
'' or 	 0.02577319587628866
= Human 	 0.1111111111111111
or speech 	 0.0045045045045045045
delta and 	 1.0
unrealistically high 	 1.0
but not 	 0.058823529411764705
be written 	 0.004219409282700422
the TextRank 	 0.001384083044982699
by Homayoon 	 0.005714285714285714
is commonly 	 0.0040650406504065045
are spoken 	 0.004149377593360996
English phrase 	 0.02702702702702703
utilize large 	 0.5
to reflect 	 0.0013280212483399733
Steven DeRose 	 1.0
Depending on 	 1.0
: navigation 	 0.00980392156862745
graph . 	 0.15384615384615385
parse trees 	 0.2222222222222222
to backup 	 0.0013280212483399733
underlying idea 	 0.3333333333333333
Rule-based machine 	 0.5
created by 	 0.2857142857142857
very slow 	 0.024390243902439025
the front 	 0.0020761245674740486
grammar -LRB- 	 0.02702702702702703
genetic algorithm 	 1.0
for continued 	 0.0036101083032490976
lexicon representation 	 0.1111111111111111
greater accuracy 	 0.3333333333333333
conclusions . 	 1.0
Running PageRank\/TextRank 	 1.0
as Scansoft 	 0.003484320557491289
At Stanford 	 0.3333333333333333
way and 	 0.041666666666666664
, current 	 0.0005614823133071309
analysis . 	 0.09230769230769231
sentences , 	 0.10526315789473684
accommodate various 	 0.2
Starting in 	 1.0
offer the 	 1.0
, going 	 0.0005614823133071309
consistent terminology 	 1.0
<s> Constraints 	 0.0007686395080707148
gaming and 	 1.0
domains ASR 	 0.125
i.e. it 	 0.05263157894736842
decision-support aids 	 1.0
template slot 	 0.25
Language modeling 	 0.08333333333333333
articles from 	 0.125
D , 	 1.0
reference that 	 0.125
a general 	 0.0036809815950920245
n't for 	 0.25
<s> Intra-texual 	 0.0007686395080707148
and Haton 	 0.001445086705202312
splitting may 	 0.5
Call home 	 1.0
it began 	 0.008547008547008548
ranked by 	 0.2
combining it 	 0.25
While supervised 	 0.2
Speech understanding 	 0.03225806451612903
FoG , 	 0.5
of elementary 	 0.00089126559714795
Intuitively , 	 1.0
they belong 	 0.025
would probably 	 0.018867924528301886
and create 	 0.002890173410404624
different tongues 	 0.02040816326530612
recognition task 	 0.01652892561983471
Answer extraction 	 0.6666666666666666
Sager at 	 0.5
forth . 	 1.0
, probabilities 	 0.0005614823133071309
Beaugrande , 	 1.0
schemes frequently 	 0.5
contain punctuation 	 0.08333333333333333
inadequate protection 	 1.0
by experts 	 0.005714285714285714
measure than 	 0.09090909090909091
the publication 	 0.001384083044982699
to query 	 0.0013280212483399733
same method 	 0.04
not use 	 0.017857142857142856
about to 	 0.025
, flexibility 	 0.0005614823133071309
label discourse 	 1.0
those proved 	 0.045454545454545456
human linguists 	 0.021739130434782608
and 2 	 0.001445086705202312
flatbed scanner 	 1.0
use cepstral 	 0.013888888888888888
GenEx algorithm 	 1.0
the details 	 0.0006920415224913495
Major tasks 	 0.5
large , 	 0.043478260869565216
tests . 	 0.25
integrating speech 	 1.0
degraded-images , 	 1.0
encourage systems 	 1.0
categories -LRB- 	 0.1111111111111111
`` Computer 	 0.005291005291005291
for summarization 	 0.0036101083032490976
, any 	 0.0005614823133071309
QUALM -LRB- 	 1.0
Increase as 	 1.0
would run 	 0.03773584905660377
market to 	 0.3333333333333333
measures try 	 0.16666666666666666
informational content 	 0.5
to video 	 0.0013280212483399733
possess -LRB- 	 1.0
Home automation 	 1.0
gonna do 	 1.0
keyphrase extractor 	 0.05263157894736842
or con 	 0.0045045045045045045
, verb 	 0.0011229646266142617
envelope based 	 1.0
on extractive 	 0.0047169811320754715
meaningful . 	 0.125
image representing 	 0.3333333333333333
metrics correlate 	 0.1111111111111111
tag sets 	 0.25
be gained 	 0.004219409282700422
that merges 	 0.0035460992907801418
some grammar 	 0.012048192771084338
type , 	 0.07142857142857142
and 2007 	 0.001445086705202312
potential of 	 0.2857142857142857
do something 	 0.038461538461538464
assess the 	 0.3333333333333333
two distinctive 	 0.034482758620689655
, installed 	 0.0005614823133071309
next token 	 0.14285714285714285
searching and 	 0.3333333333333333
Shift-Reduce parsing 	 1.0
NIST 's 	 0.5
this right 	 0.01098901098901099
an Inuit 	 0.007575757575757576
-RRB- at 	 0.0027100271002710027
translators and 	 1.0
natural languages 	 0.12
blogs and 	 0.5
images , 	 0.3333333333333333
<s> But 	 0.004611837048424289
expansion Automated 	 0.3333333333333333
and check 	 0.001445086705202312
, because 	 0.004491858506457047
robots , 	 1.0
as SVM 	 0.003484320557491289
, developed 	 0.0005614823133071309
each lexical 	 0.022222222222222223
system that 	 0.03225806451612903
example demonstrates 	 0.012345679012345678
For a 	 0.03278688524590164
The Archaeology 	 0.005208333333333333
running publication 	 0.3333333333333333
adjacent instances 	 0.16666666666666666
knowledge of 	 0.14814814814814814
OCR and 	 0.02040816326530612
of science 	 0.00089126559714795
Given a 	 0.7142857142857143
Many of 	 0.16666666666666666
in further 	 0.0018726591760299626
strings of 	 1.0
partial answers 	 1.0
affect is 	 0.3333333333333333
very rare 	 0.024390243902439025
terms to 	 0.07692307692307693
are time-consuming 	 0.004149377593360996
Archaeology of 	 1.0
'' systems 	 0.015463917525773196
languages The 	 0.02
language '' 	 0.013513513513513514
recent book 	 0.125
improved computer 	 0.25
, it 	 0.01347557551937114
summarization research 	 0.02
can sometimes 	 0.0055248618784530384
translation Machine 	 0.013513513513513514
to accommodate 	 0.0026560424966799467
. . 	 0.00078003120124805
We then 	 0.14285714285714285
Languages with 	 0.3333333333333333
CoNLL shared 	 1.0
ways . 	 0.125
the relevance 	 0.0006920415224913495
candidate can 	 0.3333333333333333
it -RRB- 	 0.008547008547008548
production when 	 0.3333333333333333
thus avoiding 	 0.1
computer gaming 	 0.022727272727272728
The vertices 	 0.005208333333333333
of Knowledge 	 0.00089126559714795
apply statistical 	 0.2
helping people 	 1.0
to convey 	 0.00398406374501992
in recognition 	 0.003745318352059925
including morphemes 	 0.07142857142857142
text summaries 	 0.006289308176100629
to say 	 0.00398406374501992
version ; 	 0.3333333333333333
linked because 	 0.3333333333333333
separated by 	 0.6666666666666666
interactions between 	 1.0
if one 	 0.03571428571428571
generation techniques 	 0.1111111111111111
GRASSHOPPER algorithm 	 0.3333333333333333
While this 	 0.2
democratizing publishing 	 0.5
inseparable part 	 1.0
, setting 	 0.0011229646266142617
use training 	 0.027777777777777776
sentences or 	 0.013157894736842105
used through 	 0.008849557522123894
it affects 	 0.008547008547008548
Canadian Hansard 	 0.5
With isolated 	 0.14285714285714285
<s> Before 	 0.0007686395080707148
typical features 	 0.1111111111111111
walks . 	 0.5
toy project 	 0.5
part-of-speech tagging 	 0.4666666666666667
The notion 	 0.010416666666666666
would produce 	 0.03773584905660377
such as 	 0.7317073170731707
is that 	 0.024390243902439025
was considerable 	 0.012987012987012988
the direct 	 0.0006920415224913495
about NLP 	 0.025
the Baum-Welch 	 0.0006920415224913495
is what 	 0.0040650406504065045
many -LRB- 	 0.019230769230769232
a sentiment 	 0.00245398773006135
discors pour 	 1.0
that ROUGE 	 0.0035460992907801418
associate discrete 	 0.5
sampling rate 	 1.0
Deep approaches 	 1.0
, Aletta 	 0.0005614823133071309
<s> Vocalizations 	 0.0007686395080707148
existing so 	 0.2
pre-existing corpus 	 0.5
solutions to 	 0.5
method . 	 0.125
and placement 	 0.001445086705202312
the picture 	 0.001384083044982699
often uses 	 0.022727272727272728
Recovery and 	 1.0
adding sentences 	 0.5
to sort 	 0.0013280212483399733
or query 	 0.0045045045045045045
<s> Named 	 0.0007686395080707148
the suffix 	 0.0006920415224913495
lot and 	 0.3333333333333333
while abstraction 	 0.05
in walking 	 0.0018726591760299626
Read vs. 	 1.0
Winograd finished 	 0.3333333333333333
from its 	 0.009615384615384616
demonstrations , 	 1.0
possible word 	 0.041666666666666664
or syntactic 	 0.0045045045045045045
actual measurement 	 0.2
Questions are 	 1.0
unsupervised approach 	 0.125
as within 	 0.003484320557491289
preceding token 	 1.0
human speakers 	 0.021739130434782608
results of 	 0.047619047619047616
view of 	 0.3333333333333333
biographical questions 	 1.0
identify the 	 0.5
into play 	 0.01282051282051282
deteriorated with 	 1.0
capitalization can 	 0.3333333333333333
techniques is 	 0.043478260869565216
statistical ; 	 0.030303030303030304
top level 	 0.2
two summaries 	 0.034482758620689655
<s> Black-box 	 0.0007686395080707148
; applies 	 0.02127659574468085
tends to 	 1.0
The same 	 0.010416666666666666
ability of 	 0.25
wear a 	 1.0
concepts are 	 0.4
position of 	 0.25
that might 	 0.0070921985815602835
a field 	 0.0036809815950920245
Understanding Conferences 	 0.5
are reported 	 0.004149377593360996
language such 	 0.006756756756756757
is contrast 	 0.0020325203252032522
representation -LRB- 	 0.10526315789473684
most text 	 0.017241379310344827
applying some 	 0.25
a factory 	 0.001226993865030675
trivial due 	 0.25
books a 	 1.0
, as 	 0.01291409320606401
evaluated . 	 0.14285714285714285
LR parsers 	 1.0
while Snyder 	 0.05
new data 	 0.041666666666666664
not wear 	 0.008928571428571428
semi - 	 1.0
in most 	 0.00749063670411985
above . 	 0.07692307692307693
perform functions 	 0.09090909090909091
, archiving 	 0.0005614823133071309
manual annotation 	 0.5
, slowly 	 0.0005614823133071309
left recursion 	 0.16666666666666666
, was 	 0.0022459292532285235
, NN 	 0.0005614823133071309
parser are 	 0.0625
postal code 	 1.0
final post-processing 	 0.1111111111111111
Perhaps the 	 1.0
scaling system 	 1.0
the 100 	 0.0006920415224913495
answer a 	 0.03333333333333333
word in 	 0.06666666666666667
the questions 	 0.0006920415224913495
as horoscope 	 0.003484320557491289
time series 	 0.030303030303030304
NLP algorithms 	 0.0425531914893617
Section , 	 1.0
and memory 	 0.001445086705202312
the information 	 0.0034602076124567475
in these 	 0.0056179775280898875
: Produce 	 0.00980392156862745
candidates for 	 0.2
In 1955 	 0.009523809523809525
Engineers , 	 0.5
, despite 	 0.0005614823133071309
efforts were 	 0.14285714285714285
and so 	 0.008670520231213872
cross-lingual questions 	 0.5
modern statistical 	 0.2
would correspond 	 0.018867924528301886
knowledge about 	 0.1111111111111111
most popular 	 0.05172413793103448
<s> Commercial 	 0.0015372790161414297
employs rule-based 	 0.5
words being 	 0.009174311926605505
matching and 	 0.2
: how 	 0.00980392156862745
at short 	 0.014705882352941176
tagging by 	 0.04
Kurzweil 's 	 0.14285714285714285
The first 	 0.036458333333333336
available on 	 0.058823529411764705
repeated relations 	 0.5
possibility to 	 0.25
offs in 	 1.0
recognition programs 	 0.008264462809917356
, Louise 	 0.0005614823133071309
and for 	 0.001445086705202312
bills returned 	 1.0
name must 	 0.2
up a 	 0.09090909090909091
to all 	 0.00398406374501992
transformational grammar 	 1.0
typically produces 	 0.05555555555555555
word of 	 0.016666666666666666
-RRB- automatically 	 0.0027100271002710027
rendered view 	 1.0
that `` 	 0.02127659574468085
evaluating NLG 	 0.2
ambiguity of 	 0.125
of social 	 0.0017825311942959
above all 	 0.07692307692307693
% error 	 0.02564102564102564
United Nations 	 0.2222222222222222
`` Turn 	 0.005291005291005291
, previously 	 0.0005614823133071309
June 1990 	 1.0
where word 	 0.02857142857142857
media has 	 0.16666666666666666
include voice 	 0.037037037037037035
typewritten reports 	 0.2
in all 	 0.0056179775280898875
a negative 	 0.001226993865030675
`` tagged 	 0.010582010582010581
to predicting 	 0.0013280212483399733
, Kurzweil 	 0.0016844469399213925
Obama , 	 1.0
requires citations 	 0.0625
will cover 	 0.02857142857142857
into punched 	 0.01282051282051282
mining refers 	 0.2
<s> Optical 	 0.0015372790161414297
function of 	 0.125
is meaningful 	 0.0020325203252032522
use natural 	 0.013888888888888888
human knowledge 	 0.021739130434782608
controversial . 	 1.0
attractive recognition 	 0.3333333333333333
the initial 	 0.0006920415224913495
of printed 	 0.0017825311942959
semantics -RRB- 	 0.07142857142857142
characters . 	 0.125
subsequent application 	 0.5
finished writing 	 0.5
NLP The 	 0.0425531914893617
decisions or 	 0.1
inflected languages 	 1.0
spoken version 	 0.07142857142857142
known word 	 0.038461538461538464
and coverage 	 0.001445086705202312
the vagueness 	 0.0006920415224913495
Malcolm Coulthard 	 1.0
one there 	 0.015384615384615385
, probabilistic 	 0.0016844469399213925
from NLP 	 0.009615384615384616
open letter 	 0.25
interactivity -LRB- 	 1.0
compensate for 	 1.0
1976 -RRB- 	 0.5
`` in 	 0.010582010582010581
`` Apparatus 	 0.005291005291005291
of tokens 	 0.0017825311942959
even produce 	 0.037037037037037035
action should 	 0.2
the isolation 	 0.0006920415224913495
do predict 	 0.038461538461538464
annual Loebner 	 0.5
at that 	 0.014705882352941176
learn from 	 0.07692307692307693
tagging is 	 0.08
before . 	 0.16666666666666666
Parliament of 	 0.5
to bridge 	 0.0013280212483399733
be selected 	 0.004219409282700422
of dependency 	 0.00089126559714795
example the 	 0.012345679012345678
final stage 	 0.1111111111111111
these topics 	 0.023809523809523808
getting ranked 	 0.25
by interactive 	 0.005714285714285714
predict what 	 0.16666666666666666
Germany . 	 0.5
<s> That 	 0.0023059185242121443
2010 -RRB- 	 0.3333333333333333
objects listed 	 0.2
Army , 	 0.25
combination with 	 0.4
Interface , 	 1.0
EMR -RRB- 	 0.3333333333333333
typical to 	 0.1111111111111111
<s> Artificial 	 0.0007686395080707148
Speech can 	 0.03225806451612903
typically finds 	 0.05555555555555555
patterns of 	 0.2
hand-written rules 	 0.8571428571428571
, mainly 	 0.0005614823133071309
mission to 	 1.0
kind of 	 0.7272727272727273
product line 	 0.14285714285714285
Web is 	 0.1111111111111111
automatically generated 	 0.14285714285714285
the . 	 0.0006920415224913495
speech from 	 0.006578947368421052
learn a 	 0.23076923076923078
Dictionary-based Main 	 0.5
have produced 	 0.009615384615384616
odd looking 	 1.0
Part-of-speech tagging 	 0.5
PAM -LRB- 	 1.0
assumption , 	 0.5
methods for 	 0.045454545454545456
with either 	 0.00546448087431694
Deaf or 	 1.0
human in 	 0.021739130434782608
search Query 	 0.09090909090909091
DARPA -RRB- 	 0.25
speech as 	 0.013157894736842105
produce keyphrases 	 0.045454545454545456
mainly the 	 0.16666666666666666
of algorithms 	 0.00089126559714795
learner . 	 0.5
unknown and 	 1.0
real-world examples 	 0.16666666666666666
which often 	 0.007246376811594203
of filtering 	 0.00089126559714795
`` Customized 	 0.005291005291005291
understanding or 	 0.030303030303030304
assertion , 	 1.0
generally lend 	 0.09090909090909091
of just 	 0.00089126559714795
transducer verifying 	 0.5
reasoning for 	 0.14285714285714285
in French 	 0.0018726591760299626
set , 	 0.05128205128205128
parsing concatenated 	 0.03571428571428571
spectral-domain of 	 1.0
these techniques 	 0.023809523809523808
Goldberg developed 	 0.5
that making 	 0.0035460992907801418
, cultural 	 0.0005614823133071309
Evaluating summaries 	 1.0
graph can 	 0.07692307692307693
by rules 	 0.005714285714285714
thus makes 	 0.1
select `` 	 0.16666666666666666
text from 	 0.012578616352201259
code of 	 0.14285714285714285
However such 	 0.02702702702702703
digital computers 	 0.14285714285714285
, thus 	 0.0011229646266142617
which use 	 0.007246376811594203
Vito Technology 	 1.0
a maximum 	 0.00245398773006135
one symbol 	 0.015384615384615385
propositions , 	 1.0
which kind 	 0.007246376811594203
accompanying HTK 	 1.0
Deborah Tannen 	 0.5
with capitalization 	 0.00546448087431694
students at 	 0.6666666666666666
classifying its 	 0.2
moon missions 	 1.0
language sentences 	 0.006756756756756757
includes both 	 0.14285714285714285
progress is 	 0.14285714285714285
Parseval\/GEIG project 	 1.0
think that 	 0.3333333333333333
-LRB- 2004 	 0.0027100271002710027
the relationships 	 0.0006920415224913495
make ` 	 0.05
a trillion-word 	 0.001226993865030675
multi-platforms such 	 1.0
, Carmen 	 0.0005614823133071309
tried to 	 0.3333333333333333
data can 	 0.025974025974025976
engines such 	 0.3333333333333333
, only 	 0.0011229646266142617
pilot , 	 0.2
as their 	 0.006968641114982578
use simple 	 0.013888888888888888
the microphone 	 0.0006920415224913495
textual corpora 	 0.2
with applications 	 0.00546448087431694
Aided Machine 	 0.3333333333333333
in English 	 0.009363295880149813
transformed into 	 1.0
dynamics of 	 0.5
The extractor 	 0.005208333333333333
is slow 	 0.0020325203252032522
required to 	 0.14285714285714285
detection of 	 0.5
use vocal 	 0.013888888888888888
highlighting candidate 	 1.0
least to 	 0.2
<s> Commanders 	 0.0007686395080707148
allow for 	 0.2
each document 	 0.022222222222222223
gained by 	 0.5
Leading software 	 1.0
<s> Reading 	 0.0007686395080707148
although capitalization 	 0.16666666666666666
both cases 	 0.03225806451612903
has been 	 0.3333333333333333
been especially 	 0.014705882352941176
LinguaSys , 	 1.0
unified mathematical 	 1.0
a class 	 0.001226993865030675
became part 	 0.2
is copied 	 0.0020325203252032522
by Pang 	 0.005714285714285714
, recall 	 0.0005614823133071309
time step 	 0.030303030303030304
complexity while 	 0.08333333333333333
, closed-domain 	 0.0005614823133071309
Search collections 	 0.5
appropriate syntactic 	 0.25
good summary 	 0.15384615384615385
.5 decision 	 1.0
is widely 	 0.0040650406504065045
wave which 	 0.1111111111111111
model . 	 0.06666666666666667
We have 	 0.14285714285714285
a likelihood 	 0.001226993865030675
in isolation 	 0.0018726591760299626
, read 	 0.0011229646266142617
used being 	 0.008849557522123894
accordingly . 	 1.0
Ethnography of 	 1.0
use\/mention distinction 	 1.0
a wave 	 0.0049079754601227
letters blend 	 0.1
translation Main 	 0.013513513513513514
new utterance 	 0.041666666666666664
site . 	 1.0
a binary 	 0.00245398773006135
also be 	 0.10144927536231885
Truecasing Statistical 	 1.0
to encode 	 0.0013280212483399733
can have 	 0.011049723756906077
it uses 	 0.017094017094017096
, articulation 	 0.0005614823133071309
shops in 	 1.0
of NLP 	 0.004456327985739751
learning . 	 0.09302325581395349
contrast , 	 0.625
which research 	 0.007246376811594203
accuracy will 	 0.03225806451612903
others . 	 0.25
<s> Comparing 	 0.0007686395080707148
between text 	 0.02564102564102564
dogs → 	 0.14285714285714285
to put 	 0.0026560424966799467
it extremely 	 0.008547008547008548
system usability 	 0.010752688172043012
became one 	 0.2
human-ratings and 	 1.0
, simulated 	 0.0005614823133071309
exactly this 	 0.3333333333333333
often it 	 0.022727272727272728
period of 	 0.5
to locate 	 0.0013280212483399733
topics or 	 0.14285714285714285
A comprehensive 	 0.02
, leading 	 0.0005614823133071309
traditionally written 	 0.5
from all 	 0.009615384615384616
to return 	 0.0026560424966799467
text units 	 0.018867924528301886
over a 	 0.08333333333333333
adjective , 	 0.14285714285714285
Computationally , 	 1.0
automatic metric 	 0.043478260869565216
within documents 	 0.05555555555555555
or more 	 0.018018018018018018
experience , 	 0.5
<s> Increasingly 	 0.0015372790161414297
the helicopter 	 0.0020761245674740486
-RRB- showed 	 0.0027100271002710027
include stages 	 0.037037037037037035
determine keyphrases 	 0.043478260869565216
32 -RRB- 	 1.0
Objectives The 	 1.0
anymore , 	 1.0
but also 	 0.07352941176470588
two sequences 	 0.034482758620689655
would require 	 0.05660377358490566
rule-based translation 	 0.14285714285714285
use only 	 0.027777777777777776
speed , 	 0.2857142857142857
programs sponsored 	 0.09090909090909091
rooms , 	 1.0
-LRB- intonation 	 0.0027100271002710027
graphic user 	 1.0
are able 	 0.012448132780082987
on their 	 0.009433962264150943
, culminating 	 0.0005614823133071309
noun can 	 0.07142857142857142
without limits 	 0.07692307692307693
customize OCR 	 0.5
nor even 	 1.0
`` correct 	 0.005291005291005291
is unclear 	 0.0020325203252032522
speech can 	 0.013157894736842105
to date 	 0.0026560424966799467
of course 	 0.0017825311942959
against noise 	 0.2
Content determination 	 1.0
`` zero 	 0.005291005291005291
ones are 	 0.1
of giving 	 0.00089126559714795
purposes , 	 0.25
data annotation 	 0.012987012987012988
in spite 	 0.0018726591760299626
to suggest 	 0.0013280212483399733
help understand 	 0.1111111111111111
Voice commands 	 0.2
style , 	 0.5
distinguish from 	 0.2
use an 	 0.013888888888888888
approach used 	 0.02857142857142857
of any 	 0.00267379679144385
'' aimed 	 0.005154639175257732
vulnerable to 	 1.0
fluent native 	 1.0
expected , 	 0.14285714285714285
algorithms fall 	 0.02857142857142857
an extractive 	 0.007575757575757576
to Australia 	 0.0013280212483399733
system , 	 0.10752688172043011
hence reducing 	 0.5
e-communities die 	 0.5
only real 	 0.02631578947368421
Because progress 	 0.5
theories on 	 0.2
several other 	 0.045454545454545456
that interact 	 0.0035460992907801418
times throughout 	 0.2
short , 	 0.125
segmentation between 	 0.030303030303030304
of spectral-domain 	 0.00089126559714795
it about 	 0.008547008547008548
Performing grammatical 	 1.0
the tagging 	 0.001384083044982699
Guzman , 	 1.0
standard techniques 	 0.07142857142857142
on tasks 	 0.009433962264150943
evaluation Intrinsic 	 0.018518518518518517
the informal 	 0.0006920415224913495
closely associate 	 0.2
or Hard 	 0.0045045045045045045
triggered other 	 1.0
documents -LRB- 	 0.13157894736842105
the position 	 0.0006920415224913495
people 's 	 0.0625
to find 	 0.010624169986719787
Inclusive choice 	 1.0
records is 	 0.25
already published 	 0.2
, typically 	 0.0016844469399213925
The topic 	 0.005208333333333333
confirmed by 	 1.0
Journal . 	 0.3333333333333333
concluded that 	 1.0
try to 	 1.0
standard metric 	 0.07142857142857142
languages tend 	 0.02
a separate 	 0.00245398773006135
in Japanese 	 0.0018726591760299626
document 's 	 0.027777777777777776
the forward-backward 	 0.0006920415224913495
found Intelligent 	 0.07142857142857142
issued to 	 1.0
or other 	 0.009009009009009009
and LOB 	 0.001445086705202312
and parsing 	 0.001445086705202312
patterns would 	 0.2
to save 	 0.0013280212483399733
adjectives and 	 0.3333333333333333
some methods 	 0.024096385542168676
<s> Every 	 0.0007686395080707148
often involve 	 0.022727272727272728
The genetic 	 0.005208333333333333
-RRB- it 	 0.0027100271002710027
of knowledge 	 0.0017825311942959
in e-communities 	 0.0018726591760299626
to dry 	 0.0013280212483399733
segment . 	 0.1111111111111111
computer-generated weather 	 1.0
a human-language 	 0.001226993865030675
Many systems 	 0.08333333333333333
interactive program 	 0.25
break hyphenated 	 0.5
speakers were 	 0.25
not rare 	 0.008928571428571428
, partially 	 0.0005614823133071309
one more 	 0.015384615384615385
technique chosen 	 0.14285714285714285
: List 	 0.00980392156862745
keyphrases attached 	 0.02857142857142857
appear `` 	 0.0625
more general 	 0.031578947368421054
Lancaster-Oslo-Bergen Corpus 	 1.0
of Michigan 	 0.00089126559714795
computers -RRB- 	 0.1111111111111111
answering deals 	 0.16666666666666666
It is 	 0.6052631578947368
input and\/or 	 0.024390243902439025
and limited 	 0.001445086705202312
better QA 	 0.1111111111111111
paper used 	 0.09090909090909091
All the 	 1.0
concatenating the 	 1.0
task-effectiveness at 	 0.5
things -RRB- 	 0.3333333333333333
statistical and 	 0.030303030303030304
an extension 	 0.007575757575757576
, Rajman 	 0.0005614823133071309
the strength 	 0.001384083044982699
focus to 	 0.14285714285714285
empirical solutions 	 1.0
kinds of 	 1.0
high probability 	 0.05555555555555555
fluency to 	 1.0
of SHRDLU 	 0.00089126559714795
sentences to 	 0.07894736842105263
six -LRB- 	 0.5
or poetry 	 0.0045045045045045045
characteristics . 	 0.5
from technology 	 0.009615384615384616
new . 	 0.041666666666666664
: fact 	 0.00980392156862745
scanned can 	 0.3333333333333333
hours . 	 0.5
some major 	 0.012048192771084338
at hand 	 0.014705882352941176
text , 	 0.18867924528301888
can understand 	 0.0055248618784530384
single words 	 0.07142857142857142
tune the 	 1.0
memory Political 	 0.5
Spitzer 's 	 1.0
and movie 	 0.001445086705202312
, factors 	 0.0005614823133071309
which was 	 0.036231884057971016
NAACL , 	 1.0
and laughter 	 0.001445086705202312
are probably 	 0.004149377593360996
Reiter and 	 1.0
we want 	 0.044444444444444446
although not 	 0.16666666666666666
, Marcus 	 0.0005614823133071309
have specific 	 0.009615384615384616
Palm OS 	 1.0
left and 	 0.3333333333333333
products ' 	 0.25
could often 	 0.0625
text documents 	 0.006289308176100629
and Cary 	 0.001445086705202312
by PageRank 	 0.005714285714285714
mention that 	 0.3333333333333333
phonetic segmentation 	 0.5
<s> Adda 	 0.0007686395080707148
read characters 	 0.14285714285714285
that PageRank 	 0.0035460992907801418
decide : 	 0.25
same objects 	 0.04
expression generation 	 0.1
task requiring 	 0.023809523809523808
area includes 	 0.09090909090909091
applications can 	 0.04
about 1965 	 0.025
become repetitive 	 0.25
campaigns were 	 0.5
its decomposition 	 0.02857142857142857
-- or 	 0.04
These results 	 0.058823529411764705
JAS-39 Gripen 	 1.0
series -RRB- 	 0.125
uses cosine 	 0.07142857142857142
machine-learning-based implementation 	 1.0
raters typically 	 1.0
vendors . 	 0.25
how it 	 0.06896551724137931
, such 	 0.019651880965749578
\* '' 	 0.25
parser generators 	 0.0625
: Top-down 	 0.00980392156862745
far northeast 	 0.25
and Romanseval 	 0.001445086705202312
The problems 	 0.005208333333333333
that transcended 	 0.0035460992907801418
of spoken 	 0.0017825311942959
multiple times 	 0.07692307692307693
1953 U.S. 	 1.0
The result 	 0.005208333333333333
a reference 	 0.00245398773006135
answering systems 	 0.08333333333333333
that larger 	 0.0035460992907801418
names that 	 0.2857142857142857
to NLP 	 0.0013280212483399733
, often 	 0.0016844469399213925
report in 	 0.25
room acoustics 	 1.0
John McCarthy 	 0.125
segments each 	 0.2
Winograd was 	 0.3333333333333333
of costly 	 0.00089126559714795
shallowest , 	 1.0
in medical 	 0.0018726591760299626
Japanese , 	 0.125
rules are 	 0.023255813953488372
ATC situation 	 0.2
difficulties in 	 0.5
case-based reasoning 	 1.0
the methods 	 0.0006920415224913495
Often used 	 0.3333333333333333
non-whitespace character 	 1.0
vowels depends 	 0.3333333333333333
that consider 	 0.0035460992907801418
summarization often 	 0.02
to derive 	 0.0013280212483399733
possible improvement 	 0.041666666666666664
; Michel 	 0.02127659574468085
multi-word phrases 	 1.0
domains , 	 0.125
other types 	 0.014285714285714285
Some critics 	 0.047619047619047616
that form 	 0.0035460992907801418
highly ambiguous 	 0.1111111111111111
progress - 	 0.14285714285714285
matching -RRB- 	 0.2
an era 	 0.007575757575757576
counting cases 	 1.0
-RRB- <s/> 	 0.037940379403794036
of medical 	 0.0017825311942959
Input for 	 0.5
some but 	 0.012048192771084338
the text-to-speech 	 0.0006920415224913495
other disciplines 	 0.014285714285714285
expensive since 	 0.14285714285714285
QA More 	 0.047619047619047616
contrastive analysis 	 1.0
years after 	 0.047619047619047616
any data 	 0.03225806451612903
centers of 	 1.0
the grammatical 	 0.0020761245674740486
where metrics 	 0.02857142857142857
printing , 	 1.0
in order 	 0.013108614232209739
A precise 	 0.02
<s> If 	 0.006149116064565719
source can 	 0.041666666666666664
help automatic 	 0.1111111111111111
Wendy Lehnert 	 1.0
per page 	 0.25
encode in 	 1.0
speaker-dependent system 	 1.0
with little 	 0.00546448087431694
is embedded 	 0.0020325203252032522
doing extensive 	 0.5
as short 	 0.003484320557491289
and taking 	 0.001445086705202312
between ` 	 0.02564102564102564
agree -RRB- 	 0.3333333333333333
input and 	 0.04878048780487805
Ken Church 	 1.0
provide any 	 0.16666666666666666
function for 	 0.125
and similar 	 0.001445086705202312
While LexRank 	 0.2
perform by 	 0.09090909090909091
starting to 	 1.0
a tag 	 0.001226993865030675
an intermediary 	 0.007575757575757576
For other 	 0.01639344262295082
percentage of 	 1.0
various types 	 0.1111111111111111
news conference 	 0.07692307692307693
largely dependent 	 0.2
difficult tasks 	 0.07142857142857142
Collection of 	 1.0
Eight years 	 1.0
even human 	 0.037037037037037035
QA system 	 0.23809523809523808
discussing how 	 0.5
that carried 	 0.0035460992907801418
separate it 	 0.2
system whereby 	 0.010752688172043012
a robot 	 0.001226993865030675
and does 	 0.001445086705202312
, Jay 	 0.0005614823133071309
Leeuwen , 	 1.0
warping is 	 0.5
and more 	 0.0072254335260115606
learn the 	 0.07692307692307693
programmed by 	 0.5
deal primarily 	 0.25
other pieces 	 0.014285714285714285
, LexRank 	 0.0005614823133071309
candidates , 	 0.2
and found 	 0.001445086705202312
`` Who 	 0.010582010582010581
Gismo '' 	 0.5
and combining 	 0.001445086705202312
social networks 	 0.21428571428571427
recognizers into 	 0.5
controller , 	 0.5
, other 	 0.0005614823133071309
weather forecasts 	 0.5714285714285714
as English 	 0.010452961672473868
a reliance 	 0.001226993865030675
sublanguage of 	 0.3333333333333333
single speaker 	 0.07142857142857142
The improvement 	 0.005208333333333333
end abruptly 	 0.125
of English 	 0.00267379679144385
plural , 	 0.4
matching the 	 0.2
Japanese prisoner 	 0.125
This parses 	 0.015873015873015872
survey of 	 1.0
with the 	 0.15300546448087432
average text 	 0.5
` global 	 0.0625
on Audio 	 0.0047169811320754715
R -RRB- 	 1.0
Conferences in 	 0.5
analyzing human-written 	 0.2
'' occur 	 0.005154639175257732
internal representation 	 0.6
languages See 	 0.02
has 4 	 0.011904761904761904
Orientation -- 	 1.0
approach to 	 0.17142857142857143
is parsing 	 0.0020325203252032522
tables of 	 0.3333333333333333
researchers must 	 0.1
video the 	 0.2
expect that 	 0.3333333333333333
Video games 	 1.0
Snyder performed 	 0.5
high degree 	 0.05555555555555555
to corpus 	 0.0013280212483399733
a canonical 	 0.001226993865030675
machines to 	 0.25
works These 	 0.5
when working 	 0.02857142857142857
the natural 	 0.001384083044982699
Hidden Markov 	 1.0
the mission 	 0.0006920415224913495
2 descriptions 	 0.2
eventually spun 	 1.0
disabilities can 	 0.25
will determine 	 0.02857142857142857
networks Main 	 0.07142857142857142
= masculine 	 0.1111111111111111
the reduction 	 0.0006920415224913495
accuracy may 	 0.03225806451612903
to correlate 	 0.0013280212483399733
automation Interactive 	 1.0
are called 	 0.008298755186721992
of fusion 	 0.00089126559714795
fonts , 	 0.3333333333333333
speaker recognition 	 0.05555555555555555
translated in 	 0.25
, leads 	 0.0005614823133071309
an evaluation 	 0.007575757575757576
nine '' 	 1.0
interaction by 	 0.125
common -LRB- 	 0.04
machine-encoded text 	 1.0
learning model 	 0.023255813953488372
, key 	 0.0005614823133071309
a security 	 0.001226993865030675
whole phrases 	 0.1111111111111111
including : 	 0.14285714285714285
affective state 	 1.0
the evaluators 	 0.0006920415224913495
these algorithms 	 0.023809523809523808
not Afghanistan 	 0.008928571428571428
Understanding Conference 	 0.5
into computer-understandable 	 0.01282051282051282
At this 	 0.3333333333333333
<s> Speaker 	 0.0015372790161414297
chosen is 	 0.2
Widdowson , 	 1.0
and after 	 0.004335260115606936
full progress 	 0.2
Overall organization 	 1.0
Anthology . 	 1.0
instance of 	 0.14285714285714285
bottom-up parsers 	 1.0
of traditional 	 0.00089126559714795
. '' 	 0.00546021840873635
punctuation and 	 0.2857142857142857
led to 	 0.6666666666666666
a more 	 0.0049079754601227
WER -RRB- 	 1.0
real-world data 	 0.3333333333333333
concerned in 	 0.2
weights . 	 0.4
October 2007 	 1.0
peak , 	 1.0
written English 	 0.038461538461538464
ambiguous word 	 0.08333333333333333
phonemes of 	 0.16666666666666666
uh '' 	 1.0
, machine 	 0.0016844469399213925
restricted `` 	 0.25
`` eat 	 0.005291005291005291
cepstral normalization 	 0.5
engine page 	 0.16666666666666666
keyphrases that 	 0.08571428571428572
is technology 	 0.0020325203252032522
there 's 	 0.025
extract the 	 0.25
% ; 	 0.02564102564102564
the entity 	 0.0006920415224913495
Analysis Standardization 	 0.2
of chatterbots 	 0.00089126559714795
<s> Real 	 0.0015372790161414297
modeling has 	 0.14285714285714285
% -LRB- 	 0.07692307692307693
the wife 	 0.0006920415224913495
text-to-speech technology 	 0.25
Scotland to 	 0.2
some extent 	 0.012048192771084338
language have 	 0.006756756756756757
Kucera and 	 1.0
methods is 	 0.022727272727272728
training documents 	 0.10714285714285714
testing . 	 0.2
popular `` 	 0.1111111111111111
never be 	 0.4
common way 	 0.08
Inter-rater reliability 	 1.0
Ford Sync 	 1.0
used SYSTRAN 	 0.008849557522123894
attention , 	 0.5
Joe Biden 	 1.0
system can 	 0.010752688172043012
user-specified fraction 	 0.5
the open-access 	 0.0006920415224913495
solve properly 	 0.25
within three 	 0.1111111111111111
or what 	 0.009009009009009009
roughness , 	 1.0
future tense 	 0.3333333333333333
for heavily 	 0.0036101083032490976
and stochastic 	 0.001445086705202312
summaries is 	 0.06976744186046512
both algorithms 	 0.03225806451612903
and going 	 0.001445086705202312
applications fall 	 0.04
LexRank deals 	 0.08333333333333333
that without 	 0.0035460992907801418
appears that 	 0.2
higher than 	 0.14285714285714285
the word 	 0.005536332179930796
those patterns 	 0.045454545454545456
computer code 	 0.022727272727272728
information can 	 0.021739130434782608
-LRB- Keyphrase 	 0.0027100271002710027
various combinations 	 0.05555555555555555
different ones 	 0.02040816326530612
Up to 	 1.0
Greek -LRB- 	 0.3333333333333333
information in 	 0.043478260869565216
about 1,000,000 	 0.025
contain words 	 0.08333333333333333
be correct 	 0.004219409282700422
The LexRank 	 0.005208333333333333
grammatical tagging 	 0.18181818181818182
to define 	 0.0026560424966799467
and associated 	 0.001445086705202312
their similarity 	 0.029411764705882353
definition , 	 0.4
positions as 	 1.0
event , 	 0.6666666666666666
Progress mainly 	 1.0
distinctive initial 	 0.5
could thus 	 0.0625
been previously 	 0.014705882352941176
to texts 	 0.0013280212483399733
generates a 	 0.6666666666666666
can determine 	 0.011049723756906077
-LRB- such 	 0.02168021680216802
not require 	 0.008928571428571428
Unsupervised taggers 	 0.16666666666666666
numeric scores 	 1.0
of distinctions 	 0.00089126559714795
that answered 	 0.0035460992907801418
was installed 	 0.012987012987012988
within computer 	 0.05555555555555555
an easier 	 0.007575757575757576
specially designed 	 1.0
, Words 	 0.0005614823133071309
1979 -RRB- 	 1.0
movies , 	 1.0
of building 	 0.00089126559714795
Other segmentation 	 0.14285714285714285
3 '' 	 0.2
moves , 	 1.0
treat them 	 0.5
applications such 	 0.04
asking for 	 0.5
systems now 	 0.008928571428571428
<s> Where 	 0.0007686395080707148
ranking algorithm 	 0.2857142857142857
paradigm calls 	 0.3333333333333333
actual forecast 	 0.2
constraint , 	 1.0
in Liu 	 0.0018726591760299626
statistical engine 	 0.030303030303030304
be understood 	 0.004219409282700422
following issues 	 0.06666666666666667
process `` 	 0.027777777777777776
Question classes 	 0.2857142857142857
150 examples 	 0.5
the generated 	 0.001384083044982699
OnlineOCR or 	 0.3333333333333333
Chomskyan theories 	 1.0
difficult task 	 0.03571428571428571
most suitable 	 0.017241379310344827
John Swales 	 0.25
LexRank simply 	 0.08333333333333333
published . 	 0.14285714285714285
SATZ architecture 	 1.0
reasoning . 	 0.14285714285714285
used mostly 	 0.008849557522123894
crucial to 	 1.0
, Jef 	 0.0005614823133071309
, one 	 0.003368893879842785
giving the 	 0.5
the Annual 	 0.0006920415224913495
extraction and 	 0.06451612903225806
of questions 	 0.004456327985739751
sounds '' 	 0.06666666666666667
hyphenated words 	 1.0
How are 	 0.2857142857142857
the history 	 0.0006920415224913495
available -LRB- 	 0.058823529411764705
as easily 	 0.006968641114982578
-- to 	 0.04
saying . 	 1.0
automatic summary 	 0.08695652173913043
paper skew 	 0.09090909090909091
linguist to 	 0.5
them good 	 0.05263157894736842
later licensed 	 0.1
of online 	 0.00089126559714795
and named 	 0.001445086705202312
glossary words 	 0.5
processing in 	 0.037037037037037035
to compiled 	 0.0013280212483399733
Lakoff , 	 1.0
Automatic tagging 	 0.1111111111111111
reader to 	 0.1
Neural Network 	 0.25
Norval , 	 1.0
a criterion 	 0.001226993865030675
processor speeds 	 1.0
Brain has 	 1.0
inventor Jacob 	 1.0
are still 	 0.016597510373443983
at level 	 0.014705882352941176
conceptual dependency 	 0.5
all where 	 0.023255813953488372
a Markov 	 0.001226993865030675
by no 	 0.005714285714285714
to names 	 0.0013280212483399733
dependent on 	 0.6666666666666666
critical new 	 0.25
which they 	 0.014492753623188406
the capital 	 0.001384083044982699
Shepard , 	 0.3333333333333333
stream of 	 0.5
full-text search 	 1.0
which would 	 0.014492753623188406
view language 	 0.3333333333333333
could just 	 0.0625
generate polynomial-size 	 0.05555555555555555
algebra . 	 0.5
last year 	 0.2
be . 	 0.004219409282700422
same problem 	 0.04
prevent incorrect 	 1.0
: Error 	 0.00980392156862745
linguistics -RRB- 	 0.1
summarization approaches 	 0.02
any sentences 	 0.03225806451612903
Carla Willig 	 1.0
-LRB- linguistics 	 0.005420054200542005
Medical Language 	 0.5
abstractive methods 	 0.3333333333333333
copying important 	 1.0
Bible Society 	 1.0
responsible for 	 1.0
were SHRDLU 	 0.024390243902439025
top-down parsing 	 0.5
many aspects 	 0.019230769230769232
systems and 	 0.008928571428571428
guided by 	 1.0
a facemask 	 0.001226993865030675
strong is 	 0.25
much better 	 0.045454545454545456
rule-based algorithms 	 0.14285714285714285
features indicating 	 0.038461538461538464
be challenged 	 0.004219409282700422
content words 	 0.08333333333333333
being considered 	 0.1111111111111111
Corpus contains 	 0.0625
splicing and 	 1.0
with storing 	 0.00546448087431694
linguistics , 	 0.4
noting that 	 1.0
hands , 	 1.0
the filter 	 0.0006920415224913495
by using 	 0.017142857142857144
source , 	 0.041666666666666664
for each 	 0.02527075812274368
word problems 	 0.016666666666666666
two general 	 0.034482758620689655
'' tag 	 0.005154639175257732
others seem 	 0.08333333333333333
up of 	 0.045454545454545456
providing customer 	 0.5
and W. 	 0.001445086705202312
to require 	 0.0013280212483399733
of syntactic 	 0.0017825311942959
but the 	 0.04411764705882353
is now 	 0.006097560975609756
` kick 	 0.0625
speech segmentation 	 0.03289473684210526
Wall Street 	 1.0
overcome this 	 0.5
of robustness 	 0.00089126559714795
into phonetic-based 	 0.01282051282051282
of faster 	 0.00089126559714795
doctors make 	 0.3333333333333333
much smaller 	 0.045454545454545456
know is 	 0.5
part-of-speech possibilities 	 0.06666666666666667
system used 	 0.010752688172043012
Constraint Grammar 	 1.0
been using 	 0.029411764705882353
A different 	 0.04
: TextRank 	 0.0196078431372549
also classify 	 0.014492753623188406
<s> Overview 	 0.0015372790161414297
issue on 	 0.125
accurate recognition 	 0.14285714285714285
Naomi Sager 	 1.0
what happens 	 0.03125
variant of 	 1.0
opening '' 	 1.0
might contain 	 0.038461538461538464
specific example 	 0.047619047619047616
Zacharov -RRB- 	 1.0
to new 	 0.005312084993359893
marked for 	 0.6666666666666666
This technique 	 0.015873015873015872
for ASR 	 0.0036101083032490976
<s> Brain 	 0.0007686395080707148
matter how 	 0.3333333333333333
evaluating automatically 	 0.2
to language 	 0.0013280212483399733
phone , 	 0.5
also used 	 0.028985507246376812
translator for 	 0.14285714285714285
explicitly delimited 	 0.25
, systems 	 0.0011229646266142617
when translating 	 0.02857142857142857
translation by 	 0.013513513513513514
question focus 	 0.023809523809523808
specified in 	 1.0
, creating 	 0.0011229646266142617
words not 	 0.009174311926605505
frequency with 	 0.5
this product 	 0.01098901098901099
weak , 	 1.0
to HMM 	 0.0013280212483399733
deciding to 	 0.3333333333333333
angry , 	 0.5
output distribution 	 0.038461538461538464
parsing or 	 0.07142857142857142
to -RRB- 	 0.0013280212483399733
would then 	 0.018867924528301886
language from 	 0.006756756756756757
computerization of 	 1.0
, shop 	 0.0005614823133071309
rarely successful 	 0.3333333333333333
English : 	 0.02702702702702703
Markov models 	 0.3333333333333333
hand-crafted rules 	 0.5
neural-network output 	 1.0
citation needed 	 1.0
value , 	 0.3333333333333333
complex NLP 	 0.08333333333333333
this refined 	 0.01098901098901099
a phone 	 0.001226993865030675
the annual 	 0.0006920415224913495
the judge 	 0.0006920415224913495
signed language 	 1.0
with equivalent 	 0.01092896174863388
12 , 	 0.2
chose different 	 1.0
positive sentiment 	 0.14285714285714285
Langues vol-2 	 1.0
concerns finding 	 0.5
designed grammars 	 0.14285714285714285
another he 	 0.07692307692307693
about each 	 0.025
cases one 	 0.05555555555555555
classifying a 	 0.4
document such 	 0.027777777777777776
sound on 	 0.05
They can 	 0.3333333333333333
characters to 	 0.0625
is likely 	 0.006097560975609756
person was 	 0.05263157894736842
usually the 	 0.03125
See machine 	 0.16666666666666666
language system 	 0.006756756756756757
Terry Winograd 	 1.0
cache language 	 1.0
voice is 	 0.07692307692307693
sharing one 	 1.0
the segment 	 0.0006920415224913495
diagramming of 	 1.0
words two 	 0.009174311926605505
's dissertation 	 0.0196078431372549
independent system 	 0.5
can say 	 0.0055248618784530384
dependency relations 	 0.2
short-time units 	 0.5
summaries known 	 0.023255813953488372
their linguistic 	 0.029411764705882353
Lehnart . 	 1.0
some training 	 0.012048192771084338
with n 	 0.00546448087431694
QA Questions 	 0.047619047619047616
will indicate 	 0.02857142857142857
for both 	 0.0036101083032490976
the identities 	 0.0006920415224913495
distinct parts 	 0.14285714285714285
meaning of 	 0.30434782608695654
Subsumption -LRB- 	 1.0
to distinguish 	 0.006640106241699867
maximal probability 	 1.0
been data-to-text 	 0.014705882352941176
journals -LRB- 	 0.5
various levels 	 0.05555555555555555
to assess 	 0.0013280212483399733
discussions . 	 0.3333333333333333
normalization -LRB- 	 0.16666666666666666
phrase appears 	 0.1
Lexical choice 	 0.5
summary sentences 	 0.023809523809523808
noise in 	 0.125
Some notably 	 0.047619047619047616
when the 	 0.11428571428571428
both `` 	 0.06451612903225806
speaker , 	 0.05555555555555555
War II 	 1.0
advances in 	 1.0
conditions ; 	 0.2
learned . 	 0.2
not produce 	 0.008928571428571428
he or 	 0.14285714285714285
tag probabilities 	 0.0625
be used 	 0.08016877637130802
have also 	 0.009615384615384616
and Dragon 	 0.001445086705202312
legends and 	 1.0
on speech 	 0.0047169811320754715
in specific 	 0.003745318352059925
performed using 	 0.1
linear transform 	 0.14285714285714285
generated texts 	 0.06666666666666667
referring expression 	 0.5
and larger 	 0.002890173410404624
of mobile 	 0.00089126559714795
control of 	 0.6
processing task 	 0.018518518518518517
some parsing 	 0.012048192771084338
, pronoun 	 0.0005614823133071309
, is 	 0.0072992700729927005
Tablet PC 	 1.0
whether an 	 0.07692307692307693
symbols or 	 0.3333333333333333
disambiguation -LRB- 	 0.1
In 1914 	 0.009523809523809525
'' which 	 0.005154639175257732
to fine 	 0.0013280212483399733
pattern . 	 0.16666666666666666
clean it 	 0.5
de Beaugrande 	 0.5
ME -RRB- 	 0.5
run-time . 	 1.0
articulated theory 	 1.0
syllables , 	 0.5
set that 	 0.02564102564102564
-LRB- Speereo 	 0.0027100271002710027
the 93-95 	 0.0006920415224913495
, Michel 	 0.0011229646266142617
are available 	 0.008298755186721992
of hidden 	 0.00089126559714795
it seems 	 0.008547008547008548
pass . 	 1.0
World , 	 0.14285714285714285
PC history 	 0.25
domains . 	 0.25
over most 	 0.08333333333333333
delimited , 	 0.5
gestures in 	 0.5
on personal 	 0.0047169811320754715
count . 	 0.2
by supplying 	 0.005714285714285714
information contained 	 0.021739130434782608
and insurance 	 0.001445086705202312
Granada Pallet 	 0.5
errors per 	 0.2
statistical NLP 	 0.06060606060606061
the moderate 	 0.0020761245674740486
annotated and 	 0.5
adjacent sounds 	 0.16666666666666666
clues in 	 0.3333333333333333
of digital 	 0.00089126559714795
-RRB- HMMs 	 0.0027100271002710027
invented Penicillin 	 0.5
William A. 	 0.5
<s> History 	 0.0015372790161414297
systems usually 	 0.017857142857142856
-- thus 	 0.04
even appears 	 0.037037037037037035
Are there 	 1.0
forms of 	 0.3333333333333333
be analyzed 	 0.004219409282700422
knowledge to 	 0.037037037037037035
are also 	 0.03319502074688797
errors -LRB- 	 0.4
-LRB- corpus 	 0.0027100271002710027
mild repetitive 	 1.0
levels as 	 0.045454545454545456
by one 	 0.005714285714285714
top ranking 	 0.2
though we 	 0.1
could do 	 0.0625
displays . 	 1.0
is search 	 0.0020325203252032522
European Union 	 0.3333333333333333
an action 	 0.007575757575757576
is usually 	 0.016260162601626018
-LRB- grammatical 	 0.0027100271002710027
an attractive 	 0.007575757575757576
end of 	 0.25
to How 	 0.0013280212483399733
years in 	 0.047619047619047616
for avoiding 	 0.0036101083032490976
L. 1998 	 1.0
text cohesion 	 0.006289308176100629
broadband technologies 	 1.0
visited and 	 1.0
typically agree 	 0.05555555555555555
to clean 	 0.0013280212483399733
which discourse 	 0.007246376811594203
steps , 	 0.5
and became 	 0.001445086705202312
regard . 	 0.2
7 % 	 0.14285714285714285
experimenting . 	 1.0
with . 	 0.00546448087431694
best to 	 0.1111111111111111
a lattice 	 0.001226993865030675
gets about 	 0.5
for modeling 	 0.0036101083032490976
Inc. in 	 0.5
be reached 	 0.004219409282700422
were later 	 0.024390243902439025
This research 	 0.015873015873015872
around the 	 0.375
several sub-problems 	 0.045454545454545456
fully solved 	 0.16666666666666666
of artificial 	 0.00089126559714795
a great 	 0.00245398773006135
not found 	 0.008928571428571428
<s> Moreover 	 0.0030745580322828594
ROUGE metric 	 0.2
In speech 	 0.009523809523809525
, like 	 0.0016844469399213925
at IBM 	 0.014705882352941176
measures of 	 0.16666666666666666
looks natural 	 0.25
If the 	 0.4
far from 	 0.125
frame ; 	 0.5
derive meaning 	 0.5
individual words 	 0.08333333333333333
other task 	 0.014285714285714285
tagger on 	 0.1111111111111111
be recognized 	 0.008438818565400843
word accuracies 	 0.016666666666666666
human -RRB- 	 0.043478260869565216
they also 	 0.025
the work 	 0.001384083044982699
other medium 	 0.014285714285714285
<s> At 	 0.0023059185242121443
all cases 	 0.023255813953488372
called Cross-Sentence 	 0.05555555555555555
commanding an 	 1.0
'' approaches 	 0.010309278350515464
warping Dynamic 	 0.25
Swedish pilots 	 1.0
text classification 	 0.006289308176100629
-LRB- grammar 	 0.0027100271002710027
emotional effect 	 0.25
Annotate the 	 1.0
turn also 	 0.16666666666666666
what new 	 0.03125
at each 	 0.014705882352941176
as WordNet 	 0.003484320557491289
been displaced 	 0.014705882352941176
Quechua , 	 1.0
grammar is 	 0.05405405405405406
-RRB- found 	 0.0027100271002710027
-LRB- OCR 	 0.0027100271002710027
coming between 	 1.0
to deal 	 0.0013280212483399733
on hand-crafted 	 0.0047169811320754715
professionals . 	 1.0
Some SR 	 0.047619047619047616
distribution that 	 0.5
on such 	 0.0047169811320754715
ambiguous context-free 	 0.08333333333333333
solution can 	 1.0
require in 	 0.045454545454545456
suitable -LRB- 	 0.25
's work 	 0.0196078431372549
more formally 	 0.010526315789473684
by the 	 0.15428571428571428
for evaluation 	 0.0036101083032490976
as a 	 0.11846689895470383
by further 	 0.005714285714285714
was unveiled 	 0.012987012987012988
Models . 	 0.3333333333333333
This can 	 0.015873015873015872
or noun 	 0.0045045045045045045
years . 	 0.19047619047619047
, psycholinguistics 	 0.0005614823133071309
release or 	 0.3333333333333333
advertisements . 	 1.0
it appropriately 	 0.008547008547008548
also capitalized 	 0.014492753623188406
fade away 	 1.0
Plot Units 	 1.0
the ROUGE 	 0.0006920415224913495
: By 	 0.00980392156862745
we use 	 0.022222222222222223
Technology Integration 	 0.3333333333333333
book-new . 	 1.0
-LRB- MCE 	 0.0027100271002710027
problem in 	 0.09090909090909091
F-16 VISTA 	 0.5
the intended 	 0.001384083044982699
in reverse 	 0.0018726591760299626
certain e-communities 	 0.14285714285714285
software that 	 0.037037037037037035
clear that 	 0.25
of 2009 	 0.00089126559714795
essay scoring 	 1.0
online news 	 0.125
of Chomskyan 	 0.00089126559714795
language metamodel 	 0.006756756756756757
is required 	 0.0040650406504065045
ranking over 	 0.14285714285714285
the mid-90s 	 0.0006920415224913495
needed , 	 0.047619047619047616
This rubric 	 0.015873015873015872
and Windows 	 0.001445086705202312
accurate by 	 0.14285714285714285
so phonemes 	 0.03333333333333333
a speech 	 0.00245398773006135
<s> Again 	 0.0007686395080707148
weapons release 	 1.0
of smoothing 	 0.00089126559714795
the BORIS 	 0.0006920415224913495
them in 	 0.05263157894736842
virtually any 	 0.5
for commercial 	 0.0036101083032490976
create summaries 	 0.058823529411764705
the south 	 0.001384083044982699
advent of 	 1.0
a robotic 	 0.001226993865030675
examples Performance 	 0.041666666666666664
Brenton D. 	 1.0
made between 	 0.0625
A document 	 0.02
extrinsic -RRB- 	 0.16666666666666666
Airline Ticket 	 1.0
-LRB- 3 	 0.005420054200542005
by Henry 	 0.005714285714285714
This strategy 	 0.015873015873015872
the notable 	 0.0006920415224913495
voice commands 	 0.07692307692307693
highly . 	 0.1111111111111111
and this 	 0.001445086705202312
complex than 	 0.08333333333333333
syntactic features 	 0.07692307692307693
ambiguities and 	 0.25
decisions . 	 0.1
technique referred 	 0.14285714285714285
ATIS . 	 1.0
or service 	 0.0045045045045045045
occur , 	 0.2
compared German 	 0.14285714285714285
letters or 	 0.1
hardly any 	 1.0
as vertices 	 0.003484320557491289
, an 	 0.005614823133071308
which proposed 	 0.007246376811594203
Coreference resolution 	 1.0
Beigi covers 	 1.0
start to 	 0.14285714285714285
lines and 	 0.3333333333333333
use neural 	 0.013888888888888888
than in 	 0.022222222222222223
financial and 	 0.25
the graph 	 0.004152249134948097
Head-driven phrase 	 1.0
production and 	 0.3333333333333333
divided into 	 0.6666666666666666
center , 	 1.0
major influence 	 0.08333333333333333
is written 	 0.0020325203252032522
10 % 	 0.25
the technology 	 0.0006920415224913495
Overview of 	 1.0
input are 	 0.024390243902439025
or generators 	 0.0045045045045045045
and Audio 	 0.001445086705202312
allows the 	 0.375
and world 	 0.001445086705202312
, EMNLP 	 0.0005614823133071309
is becoming 	 0.0020325203252032522
commonly researched 	 0.125
entities in 	 0.14285714285714285
Phillips . 	 1.0
word boundaries 	 0.016666666666666666
collecting a 	 1.0
T , 	 0.16666666666666666
not aid 	 0.008928571428571428
opinion mining 	 0.2
a genetic 	 0.001226993865030675
<s> Both 	 0.0015372790161414297
lightweight ontologies 	 1.0
the task 	 0.004844290657439446
early 1990s 	 0.1
% range 	 0.02564102564102564
correspond to 	 1.0
unigram matching 	 0.2
analysis Sublanguage 	 0.015384615384615385
and dictionary-based 	 0.001445086705202312
1998 Language 	 0.25
shown to 	 0.4
the elements 	 0.0006920415224913495
into machine 	 0.01282051282051282
whole workday 	 0.1111111111111111
The cache 	 0.005208333333333333
understanding : 	 0.030303030303030304
learning disabilities 	 0.023255813953488372
Top-down parsing 	 1.0
date is 	 0.3333333333333333
require extensive 	 0.09090909090909091
Union as 	 1.0
might shed 	 0.038461538461538464
limited . 	 0.2
upon which 	 1.0
unigram , 	 0.6
original paper 	 0.07692307692307693
roadmap of 	 1.0
to encourage 	 0.0013280212483399733
terms of 	 0.5384615384615384
in `` 	 0.0056179775280898875
functions . 	 0.5
comprehension , 	 0.14285714285714285
intra-texual . 	 1.0
the assistance 	 0.0006920415224913495
Web-based + 	 0.3333333333333333
a finite 	 0.00245398773006135
more appropriately 	 0.010526315789473684
isolation of 	 0.5
do so 	 0.038461538461538464
test the 	 0.1
To avoid 	 0.1111111111111111
easily when 	 0.1111111111111111
Army Avionics 	 0.25
requires an 	 0.0625
Harris at 	 0.1111111111111111
why certain 	 0.14285714285714285
Orleans by 	 0.5
word spaces 	 0.016666666666666666
, intelligent 	 0.0005614823133071309
% to 	 0.05128205128205128
Hindle D. 	 1.0
improved the 	 0.25
Natural language 	 0.6666666666666666
criteria , 	 0.25
He tried 	 0.125
semantically constrained 	 1.0
similarities in 	 0.5
results , 	 0.09523809523809523
language with 	 0.006756756756756757
, Gail 	 0.0005614823133071309
Lichtenstein ? 	 1.0
sentiment strength 	 0.04
smaller dictionary 	 0.14285714285714285
narrative text 	 1.0
systems have 	 0.044642857142857144
base of 	 0.25
an ellipsis 	 0.007575757575757576
, false 	 0.0005614823133071309
machine representation 	 0.02531645569620253
not determine 	 0.008928571428571428
University , 	 0.1111111111111111
or to 	 0.009009009009009009
Germany , 	 0.5
systems typically 	 0.008928571428571428
<s> Big 	 0.0007686395080707148
letter to 	 0.16666666666666666
wide use 	 0.25
breaking -LRB- 	 0.5
recognizing entire 	 0.2
was considered 	 0.012987012987012988
issues of 	 0.2
mining , 	 0.2
Described above 	 1.0
belongs to 	 1.0
formulaic language 	 1.0
solely on 	 1.0
shallow . 	 0.16666666666666666
podcast where 	 1.0
to start 	 0.0026560424966799467
role in 	 0.25
programming language 	 0.2
derivation and 	 0.25
these simple 	 0.023809523809523808
piecewise stationary 	 1.0
of system-generated 	 0.00089126559714795
or Arabic 	 0.0045045045045045045
, biographical 	 0.0005614823133071309
to parsers 	 0.0013280212483399733
ultimately want 	 1.0
computers to 	 0.1111111111111111
statistical inference 	 0.06060606060606061
extrapolate that 	 1.0
distinguish names 	 0.2
of automatic 	 0.0017825311942959
improvement by 	 0.25
for extracting 	 0.0036101083032490976
larger collection 	 0.0625
the largest 	 0.0006920415224913495
the industry 	 0.0006920415224913495
in January 	 0.003745318352059925
sounds -LRB- 	 0.13333333333333333
baseball league 	 1.0
high noise 	 0.05555555555555555
of children 	 0.00089126559714795
intuitive sense 	 1.0
translator that 	 0.14285714285714285
's that 	 0.0196078431372549
how people 	 0.034482758620689655
sentence with 	 0.020833333333333332
may result 	 0.019230769230769232
reference to 	 0.25
inferior results 	 1.0
not to 	 0.008928571428571428
produces Grass 	 0.25
words to 	 0.009174311926605505
training on 	 0.03571428571428571
Gail Jefferson 	 1.0
studied more 	 1.0
the term 	 0.0034602076124567475
, object 	 0.0005614823133071309
each character 	 0.022222222222222223
understanding evaluation 	 0.030303030303030304
written texts 	 0.07692307692307693
Jefferson , 	 1.0
, internet 	 0.0005614823133071309
other purposes 	 0.014285714285714285
by different 	 0.005714285714285714
contains all 	 0.1
produce models 	 0.045454545454545456
scholars have 	 0.5
, Jan 	 0.0005614823133071309
solving larger 	 1.0
of Mandarin 	 0.00089126559714795
the past 	 0.001384083044982699
clarification of 	 0.6666666666666666
In natural 	 0.009523809523809525
documents as 	 0.02631578947368421
Current research 	 0.2
Systems released 	 0.16666666666666666
year or 	 0.16666666666666666
Another project 	 0.07692307692307693
Charles Goodwin 	 1.0
not predict 	 0.008928571428571428
over many 	 0.08333333333333333
main knowledge 	 0.125
the emergence 	 0.0006920415224913495
of units 	 0.00089126559714795
Church independently 	 0.3333333333333333
editor , 	 1.0
of major 	 0.00089126559714795
the ranking 	 0.001384083044982699
user has 	 0.07142857142857142
fast and 	 1.0
rather to 	 0.0625
`` Mr. 	 0.005291005291005291
that when 	 0.0035460992907801418
context . 	 0.21212121212121213
Note , 	 0.1111111111111111
suggest a 	 0.3333333333333333
department in 	 0.5
`` prestige 	 0.005291005291005291
Aerospace Establishment 	 0.5
suitable ontology 	 0.25
everyday life 	 1.0
is getting 	 0.0040650406504065045
It had 	 0.02631578947368421
term `` 	 0.1111111111111111
Interlingual Main 	 0.3333333333333333
Single Word 	 1.0
a kind 	 0.001226993865030675
related tasks 	 0.2
: statistical 	 0.00980392156862745
and weapons 	 0.001445086705202312
a naval 	 0.001226993865030675
transcribe such 	 1.0
Extractor -RRB- 	 1.0
similar ideas 	 0.037037037037037035
Both QA 	 0.3333333333333333
poorly defined 	 1.0
the segmentation 	 0.0006920415224913495
guessing wrong 	 1.0
paragraph . 	 0.3333333333333333
vertices in 	 0.1111111111111111
word error 	 0.016666666666666666
to is 	 0.0013280212483399733
, simple 	 0.0011229646266142617
use text 	 0.013888888888888888
decisions about 	 0.2
Evaluation techniques 	 0.1111111111111111
comprising multiple 	 0.5
other similar 	 0.014285714285714285
field with 	 0.037037037037037035
presented in 	 0.5
dialogues between 	 1.0
Walter Kintsch 	 1.0
since one 	 0.1
events . 	 1.0
early precursor 	 0.1
A word 	 0.02
GRASSHOPPER for 	 0.3333333333333333
70 's 	 0.25
of discrete 	 0.00089126559714795
several -RRB- 	 0.045454545454545456
Microsoft Voice 	 0.5
addition to 	 0.5
the key 	 0.0006920415224913495
creating a 	 0.2857142857142857
used for 	 0.13274336283185842
movie review 	 0.3333333333333333
individual trained 	 0.08333333333333333
Accuracy for 	 0.14285714285714285
computer . 	 0.06818181818181818
likely source 	 0.0625
properties , 	 0.25
a correct 	 0.00245398773006135
evaluation can 	 0.037037037037037035
`` who 	 0.005291005291005291
situation . 	 0.5
is searched 	 0.0020325203252032522
more informative 	 0.010526315789473684
as described 	 0.006968641114982578
they can 	 0.15
that removing 	 0.0035460992907801418
if there 	 0.07142857142857142
disabilities who 	 0.25
improve information 	 0.07692307692307693
Automotive speech 	 1.0
good '' 	 0.07692307692307693
considers an 	 0.5
overlaps between 	 0.5
taggers and 	 0.2857142857142857
relevance assessment 	 0.3333333333333333
both research 	 0.03225806451612903
analyze the 	 0.25
automatically answer 	 0.047619047619047616
location , 	 1.0
work . 	 0.08333333333333333
highly complex 	 0.1111111111111111
in ASR 	 0.003745318352059925
itself or 	 0.2
injuries to 	 1.0
p. 32 	 1.0
Voice2Go -RRB- 	 1.0
, bigram 	 0.0016844469399213925
system with 	 0.010752688172043012
dynamic background 	 0.2
: Content 	 0.00980392156862745
automatically answering 	 0.047619047619047616
, weather 	 0.0005614823133071309
evaluation Black-box 	 0.018518518518518517
was much 	 0.025974025974025976
This reader 	 0.015873015873015872
disappear . 	 1.0
voice user 	 0.07692307692307693
and far 	 0.002890173410404624
photocells , 	 1.0
, trigram 	 0.0011229646266142617
\/ F-16 	 0.3333333333333333
speech Task 	 0.006578947368421052
a part 	 0.00245398773006135
for billing 	 0.0036101083032490976
polarity and 	 0.125
possibilities but 	 0.2
John Du 	 0.125
4-gram matching 	 1.0
known as 	 0.38461538461538464
a appropriate 	 0.001226993865030675
text normalization 	 0.006289308176100629
keyphrase . 	 0.05263157894736842
development cost 	 0.08333333333333333
language processing 	 0.22297297297297297
sentences are 	 0.09210526315789473
an integrated 	 0.007575757575757576
included analyses 	 0.125
new cross-discipline 	 0.041666666666666664
translation simultaneously 	 0.013513513513513514
technology for 	 0.09090909090909091
the statistical 	 0.001384083044982699
the phonemes 	 0.001384083044982699
human variability 	 0.021739130434782608
important topics 	 0.0625
a supervised 	 0.00245398773006135
keyphrases by 	 0.02857142857142857
values of 	 0.5
a car 	 0.001226993865030675
Voice Translator 	 0.2
various genres 	 0.05555555555555555
progress in 	 0.2857142857142857
platform to 	 0.5
of repeated 	 0.00089126559714795
Senseval and 	 1.0
, rarity 	 0.0005614823133071309
easily parsed 	 0.1111111111111111
over the 	 0.25
recognize For 	 0.1111111111111111
individual lines 	 0.08333333333333333
the speech 	 0.006920415224913495
keyphrase system 	 0.05263157894736842
man '' 	 1.0
entering the 	 0.5
not . 	 0.017857142857142856
sentence-level syntax 	 1.0
extremely difficult 	 0.75
English-like command 	 0.3333333333333333
extended in 	 1.0
counselling -RRB- 	 1.0
test as 	 0.1
requires humans 	 0.0625
corpora , 	 0.09090909090909091
more commonly 	 0.021052631578947368
or classified 	 0.0045045045045045045
ambiguous . 	 0.25
a reasonable 	 0.00245398773006135
Aggregation : 	 1.0
Paul Gee 	 0.2
places , 	 0.5
have unambiguous 	 0.009615384615384616
Shipibo . 	 0.5
University of 	 0.3333333333333333
resulting classifier 	 0.25
possibly linked 	 0.5
that within 	 0.0070921985815602835
faces a 	 1.0
be produced 	 0.004219409282700422
or it 	 0.0045045045045045045
our knowledge 	 0.2
a US 	 0.00245398773006135
from Turney 	 0.009615384615384616
shift positions 	 1.0
1935 Tauschek 	 1.0
and current 	 0.001445086705202312
<s> Tokens 	 0.0007686395080707148
higher order 	 0.14285714285714285
be distinguished 	 0.004219409282700422
very difficult 	 0.04878048780487805
than computers 	 0.022222222222222223
as artificial 	 0.003484320557491289
Generation -LRB- 	 0.5
the speaker 	 0.001384083044982699
translate more 	 0.16666666666666666
currently used 	 0.14285714285714285
effective . 	 0.3333333333333333
the real 	 0.0020761245674740486
computer processing 	 0.022727272727272728
the full 	 0.0020761245674740486
top T 	 0.4
prestige '' 	 1.0
larger set 	 0.0625
pre-existing keyphrases 	 0.5
n-gram overlaps 	 0.5
values for 	 0.125
popular is 	 0.1111111111111111
be divided 	 0.004219409282700422
triple probabilities 	 1.0
MMI -RRB- 	 1.0
system -LRB- 	 0.021505376344086023
disambiguation -RRB- 	 0.2
regions . 	 0.5
that contains 	 0.010638297872340425
text more 	 0.006289308176100629
disabilities that 	 0.25
recent years 	 0.5
around 2 	 0.125
and ICR 	 0.002890173410404624
the norm 	 0.0006920415224913495
annual Document 	 0.5
reporting -LRB- 	 0.3333333333333333
out . 	 0.07142857142857142
defective flood-control 	 1.0
: `` 	 0.0196078431372549
also the 	 0.028985507246376812
rank order 	 0.16666666666666666
their input 	 0.029411764705882353
France has 	 0.25
Environment Canada 	 1.0
diversity : 	 0.25
corpus for 	 0.03225806451612903
The 10 	 0.005208333333333333
the entities 	 0.0006920415224913495
probabilistic context-free 	 0.14285714285714285
LexRank differences 	 0.08333333333333333
specifically concerned 	 0.5
<s> Topic 	 0.0007686395080707148
-RRB- work 	 0.0027100271002710027
the detection 	 0.0006920415224913495
vs. Independence 	 0.08333333333333333
processing The 	 0.018518518518518517
cepstral coefficients 	 0.5
non - 	 1.0
to travel 	 0.0013280212483399733
function with 	 0.125
from several 	 0.009615384615384616
are capable 	 0.008298755186721992
many written 	 0.019230769230769232
commonly referred 	 0.125
of summaries 	 0.0035650623885918
A summary 	 0.02
tagging -LRB- 	 0.04
and probably 	 0.001445086705202312
algorithms is 	 0.02857142857142857
the proposed 	 0.001384083044982699
single `` 	 0.07142857142857142
translation systems 	 0.02702702702702703
Reinvestment Act 	 1.0
relationship to 	 0.16666666666666666
was later 	 0.012987012987012988
about following 	 0.025
articles on 	 0.125
be thousands 	 0.004219409282700422
included -LRB- 	 0.125
processing and 	 0.018518518518518517
video captioning 	 0.2
are ambiguous 	 0.004149377593360996
counter examples 	 1.0
con sentiment 	 1.0
that a 	 0.010638297872340425
their lack 	 0.029411764705882353
Research into 	 0.125
recently identified 	 0.3333333333333333
HMM-based part 	 0.3333333333333333
to and 	 0.0013280212483399733
, second 	 0.0005614823133071309
to found 	 0.0013280212483399733
suitable department 	 0.25
Genre Analysis 	 1.0
identified which 	 0.2
hopefully better 	 1.0
It converted 	 0.02631578947368421
all handwritten 	 0.023255813953488372
Topics of 	 1.0
and human-generated 	 0.001445086705202312
up with 	 0.13636363636363635
non-linear transformations 	 1.0
a classifier 	 0.001226993865030675
texts from 	 0.058823529411764705
of organized 	 0.00089126559714795
attained . 	 1.0
P. , 	 1.0
environment , 	 0.16666666666666666
2002 evaluation 	 0.5
always the 	 0.3333333333333333
or serving 	 0.0045045045045045045
one reference 	 0.015384615384615385
-LRB- rescoring 	 0.0027100271002710027
language as 	 0.006756756756756757
of active 	 0.00089126559714795
Note also 	 0.1111111111111111
approach involves 	 0.02857142857142857
sentences can 	 0.039473684210526314
isolated-word recognizers 	 1.0
on understanding 	 0.0047169811320754715
the smaller 	 0.0006920415224913495
low precision 	 0.3333333333333333
because longer 	 0.03333333333333333
Interactive QA 	 0.5
English for 	 0.02702702702702703
of objects 	 0.00089126559714795
with specific 	 0.00546448087431694
be processed 	 0.004219409282700422
of papers 	 0.00089126559714795
derived from 	 0.5
learn it 	 0.07692307692307693
resolution remains 	 0.25
Fully Automated 	 1.0
other fields 	 0.014285714285714285
and signing 	 0.001445086705202312
making a 	 0.14285714285714285
Fairclough , 	 1.0
run an 	 0.2
algorithms -- 	 0.02857142857142857
scans paper 	 1.0
issue of 	 0.125
requires one 	 0.0625
ROUGE-1 values 	 0.2
the role 	 0.001384083044982699
list of 	 0.7272727272727273
effort has 	 0.25
prolific inventor 	 1.0
-LRB- Campaigns 	 0.0027100271002710027
a sentence 	 0.01717791411042945
the field 	 0.011764705882352941
printed page 	 0.08333333333333333
to do 	 0.00398406374501992
-LRB- 2008 	 0.0027100271002710027
began to 	 0.5714285714285714
<s> Up 	 0.0007686395080707148
simple domains 	 0.038461538461538464
To upgrade 	 0.1111111111111111
speed is 	 0.14285714285714285
techniques can 	 0.043478260869565216
a printed 	 0.001226993865030675
<s> Grammatical 	 0.0007686395080707148
even larger 	 0.037037037037037035
writing systems 	 0.2222222222222222
very preliminary 	 0.024390243902439025
substantially . 	 1.0
seen an 	 0.2
In the 	 0.13333333333333333
processing '' 	 0.037037037037037035
be robust 	 0.004219409282700422
is obtained 	 0.0020325203252032522
specific task 	 0.047619047619047616
, people 	 0.0005614823133071309
article : 	 0.4482758620689655
automatically created 	 0.047619047619047616
at varying 	 0.014705882352941176
Research and 	 0.125
readable human 	 0.3333333333333333
language other 	 0.006756756756756757
ASRU . 	 1.0
text image 	 0.006289308176100629
simply model 	 0.08333333333333333
Zellig Harris 	 1.0
to rank 	 0.00398406374501992
simultaneously meets 	 0.5
text summarization 	 0.006289308176100629
plateaued and 	 1.0
a damping 	 0.001226993865030675
simple English 	 0.038461538461538464
called grammatical 	 0.05555555555555555
→ dogs 	 0.3333333333333333
Chantal Mouffe 	 1.0
distinction is 	 0.4
of analyzing 	 0.00089126559714795
interjection . 	 1.0
the construction 	 0.0006920415224913495
pseudo-pilot '' 	 0.5
where semantics 	 0.02857142857142857
difference can 	 0.25
words emerge 	 0.009174311926605505
sentences flow 	 0.013157894736842105
be trained 	 0.004219409282700422
which a 	 0.014492753623188406
formed ? 	 0.4
, \* 	 0.0005614823133071309
fancy statistics 	 1.0
depend on 	 1.0
good source 	 0.07692307692307693
It stands 	 0.02631578947368421
expert that 	 1.0
similar methods 	 0.037037037037037035
an '' 	 0.007575757575757576
information overload 	 0.021739130434782608
word was 	 0.016666666666666666
corpus to 	 0.03225806451612903
Computational Linguistics 	 1.0
domain , 	 0.15
a different 	 0.0036809815950920245
person reads 	 0.05263157894736842
grammatical relationships 	 0.09090909090909091
exhibited good 	 1.0
decisions are 	 0.1
A relationship 	 0.02
publications . 	 1.0
are parsed 	 0.004149377593360996
NLP techniques 	 0.0425531914893617
, largely 	 0.0005614823133071309
this document 	 0.01098901098901099
models were 	 0.038461538461538464
a novel 	 0.001226993865030675
bigrams , 	 1.0
Morse Code 	 1.0
a situation 	 0.001226993865030675
the narrowest 	 0.0006920415224913495
pick the 	 1.0
a deterministic 	 0.00245398773006135
with machine 	 0.00546448087431694
are systems 	 0.012448132780082987
Granada Different 	 0.5
that nuggets 	 0.0035460992907801418
specific letter 	 0.047619047619047616
to direct 	 0.0013280212483399733
primary topics 	 0.5
because punctuation 	 0.03333333333333333
Pierce wrote 	 1.0
verification . 	 1.0
moderate to 	 0.6
or sentiments 	 0.0045045045045045045
are pre-determined 	 0.004149377593360996
user profile 	 0.07142857142857142
that simultaneously 	 0.0035460992907801418
PageRank to 	 0.3333333333333333
Winograd continued 	 0.3333333333333333
5000 or 	 1.0
include distinct 	 0.037037037037037035
Potter , 	 1.0
first layer 	 0.030303030303030304
not explicitly 	 0.008928571428571428
As in 	 0.2222222222222222
1914 , 	 1.0
also pioneered 	 0.014492753623188406
% . 	 0.23076923076923078
or printed 	 0.0045045045045045045
evaluations are 	 0.3333333333333333
<s> Such 	 0.006149116064565719
of accuracy 	 0.0017825311942959
into account 	 0.038461538461538464
adaptive document\/text 	 0.3333333333333333
data has 	 0.012987012987012988
often considered 	 0.022727272727272728
costly training 	 1.0
task , 	 0.09523809523809523
functional grammar 	 0.5
Statistical natural-language 	 0.1111111111111111
BioCreative Message 	 1.0
; Amplitude 	 0.02127659574468085
combination and 	 0.2
; Analysis 	 0.02127659574468085
is broken 	 0.0020325203252032522
probabilistic rules 	 0.14285714285714285
two illustrates 	 0.034482758620689655
by Manfred 	 0.005714285714285714
domain of 	 0.1
to manipulate 	 0.0026560424966799467
compare automatic 	 0.14285714285714285
as classifying 	 0.003484320557491289
Martin presents 	 0.5
needed for 	 0.09523809523809523
present in 	 0.8333333333333334
filter returns 	 0.5
operate on 	 1.0
but evaluation 	 0.014705882352941176
-LRB- closer 	 0.0027100271002710027
The context 	 0.005208333333333333
the 1950s 	 0.0020761245674740486
Human judgement 	 0.2
at different 	 0.014705882352941176
primary output 	 0.5
<s> Using 	 0.0015372790161414297
Although , 	 0.125
be broken 	 0.004219409282700422
parser attempts 	 0.0625
-LRB- sometimes 	 0.0027100271002710027
BLEU is 	 0.3333333333333333
comprising context 	 0.5
draws on 	 1.0
are numerous 	 0.004149377593360996
makes tagging 	 0.125
paragraph summary 	 0.3333333333333333
an NLG 	 0.007575757575757576
sort mail 	 0.3333333333333333
a final 	 0.001226993865030675
including mobile 	 0.07142857142857142
evaluation -LRB- 	 0.018518518518518517
original text 	 0.46153846153846156
more like 	 0.010526315789473684
sound into 	 0.05
weights equal 	 0.2
interpreter . 	 0.5
a purpose 	 0.001226993865030675
intrinsic evaluation 	 0.5
vertices should 	 0.1111111111111111
apply to 	 0.4
are robust 	 0.004149377593360996
Sensory , 	 1.0
first -LRB- 	 0.030303030303030304
etc. , 	 0.045454545454545456
will contain 	 0.02857142857142857
TNO developed 	 1.0
need is 	 0.047619047619047616
Maximum entropy 	 0.6666666666666666
PangeaMT , 	 1.0
formal representations 	 0.2222222222222222
content that 	 0.08333333333333333
of pragmatics 	 0.00089126559714795
-LRB- k 	 0.0027100271002710027
on Semantic 	 0.0047169811320754715
mention how 	 0.3333333333333333
the ARCHILES 	 0.0006920415224913495
LL parsers 	 1.0
turn . 	 0.16666666666666666
finite set 	 0.2
, needed 	 0.0005614823133071309
the translation 	 0.004152249134948097
In fact 	 0.0380952380952381
P , 	 0.5
misspelled words 	 1.0
the spectrum 	 0.0006920415224913495
machine to 	 0.012658227848101266
comprehensive knowledge 	 0.2
semantic representation 	 0.047619047619047616
to those 	 0.0026560424966799467
program and 	 0.045454545454545456
surfer model 	 1.0
and get 	 0.001445086705202312
multiple parts 	 0.07692307692307693
1974 Ray 	 1.0
system output 	 0.010752688172043012
determine its 	 0.08695652173913043
original sound 	 0.07692307692307693
allowable expression 	 0.5
part-of-speech tagger 	 0.06666666666666667
was applied 	 0.012987012987012988
computer analysis 	 0.022727272727272728
accuracy above 	 0.03225806451612903
an opportunity 	 0.007575757575757576
is routed 	 0.0040650406504065045
a time 	 0.00245398773006135
can do 	 0.011049723756906077
that capture 	 0.0035460992907801418
especially those 	 0.2
It thus 	 0.02631578947368421
are concerned 	 0.004149377593360996
use is 	 0.013888888888888888
semantics which 	 0.14285714285714285
other things 	 0.04285714285714286
was by 	 0.025974025974025976
, PangeaMT 	 0.0005614823133071309
meant and 	 0.5
of Roger 	 0.00089126559714795
<s> Mention 	 0.0007686395080707148
recognition -LRB- 	 0.049586776859504134
where phrases 	 0.02857142857142857
gap between 	 1.0
features that 	 0.07692307692307693
to prune 	 0.0013280212483399733
to automatizing 	 0.0013280212483399733
1970 , 	 0.3333333333333333
to most 	 0.0013280212483399733
multiple languages 	 0.07692307692307693
blogs , 	 0.5
linguistics that 	 0.1
Kintsch , 	 1.0
US Veterans 	 0.14285714285714285
Naturally Speaking 	 1.0
, instead 	 0.0005614823133071309
implications of 	 1.0
been undertaken 	 0.014705882352941176
still just 	 0.06666666666666667
that looks 	 0.0035460992907801418
2007 and 	 0.2
`` machine 	 0.005291005291005291
Substantial test 	 0.5
say whether 	 0.14285714285714285
often marked 	 0.022727272727272728
levels but 	 0.045454545454545456
humans can 	 0.08333333333333333
seize the 	 1.0
as semantic 	 0.003484320557491289
; for 	 0.0425531914893617
, TextRank 	 0.0011229646266142617
the Unix 	 0.001384083044982699
similar clues 	 0.037037037037037035
, unverified 	 0.0005614823133071309
profile captures 	 0.3333333333333333
Agency in 	 0.5
-LRB- the 	 0.02168021680216802
two -LRB- 	 0.034482758620689655
phrase structure 	 0.2
slowly and 	 0.5
from both 	 0.009615384615384616
analyzing written 	 0.2
this would 	 0.01098901098901099
that appear 	 0.014184397163120567
recent news 	 0.125
to conduct 	 0.0013280212483399733
omni-font OCR 	 1.0
journal `` 	 0.3333333333333333
features or 	 0.038461538461538464
focus is 	 0.14285714285714285
cosine transform 	 0.3333333333333333
also growing 	 0.014492753623188406
for triples 	 0.0036101083032490976
be apparent 	 0.004219409282700422
were workshops 	 0.024390243902439025
determine the 	 0.391304347826087
bigram , 	 1.0
included : 	 0.125
the state 	 0.001384083044982699
and continued 	 0.001445086705202312
overall speech 	 0.16666666666666666
a complex 	 0.006134969325153374
's polarity 	 0.0196078431372549
parameters related 	 0.25
unigrams in 	 0.25
human summaries 	 0.021739130434782608
possible analyses 	 0.08333333333333333
and volume 	 0.001445086705202312
higher levels 	 0.2857142857142857
judge is 	 0.25
is distorted 	 0.0040650406504065045
recognition algorithms 	 0.008264462809917356
waves can 	 0.14285714285714285
disambiguation on 	 0.1
an advanced 	 0.007575757575757576
five commands 	 0.2
-LRB- Pallet 	 0.0027100271002710027
graphics -- 	 1.0
dedicated to 	 0.6666666666666666
scores for 	 0.2
settings . 	 1.0
coarse-grained relations 	 1.0
typically a 	 0.05555555555555555
using , 	 0.01694915254237288
-RRB- amongst 	 0.0027100271002710027
problem setting 	 0.022727272727272728
line of 	 0.3333333333333333
the domain 	 0.001384083044982699
intent . 	 1.0
including probabilistic 	 0.07142857142857142
Incorporating diversity 	 1.0
cryptanalyst at 	 1.0
a full-text 	 0.001226993865030675
: the 	 0.0196078431372549
letter shapes 	 0.3333333333333333
some topic 	 0.012048192771084338
Street . 	 0.3333333333333333
algorithm . 	 0.14285714285714285
which range 	 0.007246376811594203
perspective so 	 0.25
success in 	 0.2
was the 	 0.05194805194805195
' at 	 0.05263157894736842
Linguistics '' 	 0.3333333333333333
up to 	 0.22727272727272727
methods can 	 0.022727272727272728
is considered 	 0.0040650406504065045
parsing for 	 0.03571428571428571
Sparkle campaign 	 1.0
into a 	 0.21794871794871795
norm . 	 1.0
sentence-end after 	 1.0
as OnlineOCR 	 0.003484320557491289
these programs 	 0.023809523809523808
at . 	 0.014705882352941176
constraints . 	 0.25
-LRB- discourse 	 0.0027100271002710027
readable English 	 0.3333333333333333
'' will 	 0.005154639175257732
<s> Recent 	 0.0023059185242121443
may suffer 	 0.019230769230769232
methods work 	 0.045454545454545456
, though 	 0.003368893879842785
measured with 	 0.16666666666666666
limited application 	 0.1
classes are 	 0.2
<s> Internet 	 0.0007686395080707148
one year 	 0.015384615384615385
heuristics to 	 0.5
the computer-aided 	 0.0006920415224913495
Penn Treebank 	 0.6666666666666666
small , 	 0.2222222222222222
about 70 	 0.05
deep systems 	 0.14285714285714285
KEA -LRB- 	 1.0
choice is 	 0.25
achieve accuracy 	 0.5
Solving this 	 0.5
language documents 	 0.006756756756756757
ears , 	 1.0
hierarchically in 	 1.0
have many 	 0.04807692307692308
ontologies . 	 0.5
by Kurzweil 	 0.005714285714285714
dictates into 	 1.0
documents , 	 0.23684210526315788
of 21 	 0.00089126559714795
methods -LRB- 	 0.045454545454545456
qualities making 	 0.5
indicates that 	 1.0
items . 	 0.5
but machines 	 0.014705882352941176
humans deemed 	 0.08333333333333333
The accuracy 	 0.010416666666666666
Answering QA 	 1.0
related information 	 0.06666666666666667
blend smoothly 	 0.6666666666666666
interaction , 	 0.125
remembering , 	 1.0
multiple part-of-speech 	 0.07692307692307693
have made 	 0.009615384615384616
and Subjectivity 	 0.001445086705202312
we could 	 0.022222222222222223
on which 	 0.014150943396226415
, funding 	 0.0016844469399213925
path , 	 0.5
Language as 	 0.08333333333333333
result is 	 0.18181818181818182
be declared 	 0.004219409282700422
kit ' 	 1.0
in computer-aided 	 0.0018726591760299626
ambiguous English 	 0.08333333333333333
article `` 	 0.034482758620689655
recognition in 	 0.024793388429752067
formalism which 	 1.0
compared syntactic 	 0.14285714285714285
ones focus 	 0.1
to gather 	 0.0013280212483399733
hand or 	 0.14285714285714285
generate some 	 0.05555555555555555
formally , 	 0.5
to rescore 	 0.0013280212483399733
this can 	 0.01098901098901099
, each 	 0.003368893879842785
, generally 	 0.0005614823133071309
text contains 	 0.006289308176100629
an untagged 	 0.007575757575757576
the F35 	 0.0006920415224913495
showed that 	 0.75
proposed as 	 0.1111111111111111
and interaction 	 0.001445086705202312
popular being 	 0.1111111111111111
<s> Parsers 	 0.0015372790161414297
as conveniently 	 0.003484320557491289
, distance 	 0.0005614823133071309
often disagree 	 0.022727272727272728
preposition , 	 1.0
Accuracy is 	 0.14285714285714285
, where 	 0.008422234699606962
determine whether 	 0.043478260869565216
<s> Keyphrase 	 0.0023059185242121443
des parties 	 1.0
phonetically different 	 1.0
But also 	 0.16666666666666666
determines how 	 0.3333333333333333
Automatic translation 	 0.1111111111111111
2011 -RRB- 	 0.5
learning is 	 0.023255813953488372
the major 	 0.001384083044982699
entities often 	 0.14285714285714285
absolutely necessary 	 1.0
does n't 	 0.1
and Latin 	 0.001445086705202312
happen between 	 1.0
languages concepts 	 0.02
2007 -RRB- 	 0.2
top-down parsers 	 0.25
their context 	 0.029411764705882353
broad tags 	 0.25
vertices is 	 0.1111111111111111
specialized output 	 0.5
and used 	 0.001445086705202312
than whole 	 0.022222222222222223
rescore lattices 	 1.0
and was 	 0.001445086705202312
this way 	 0.02197802197802198
Systran , 	 1.0
on sentence 	 0.0047169811320754715
on summarization 	 0.0047169811320754715
surrounding vowels 	 0.2
class of 	 0.75
duplicate typewritten 	 0.5
improve performance 	 0.07692307692307693
using natural 	 0.01694915254237288
businesses look 	 0.5
first choice 	 0.030303030303030304
their `` 	 0.029411764705882353
probabilities would 	 0.09090909090909091
: rule-based 	 0.00980392156862745
the feature 	 0.0006920415224913495
minimal complexity 	 1.0
as early 	 0.003484320557491289
more dynamic 	 0.010526315789473684
etc. -- 	 0.045454545454545456
Hulth showed 	 0.3333333333333333
a way 	 0.006134969325153374
<s> From 	 0.0007686395080707148
- \/ 	 0.0625
use various 	 0.013888888888888888
appears to 	 0.2
address of 	 0.25
, and\/or 	 0.0005614823133071309
mentioned in 	 0.16666666666666666
a text 	 0.01717791411042945
typically involve 	 0.05555555555555555
Chinese or 	 0.14285714285714285
neural approaches 	 0.06666666666666667
reasoning to 	 0.14285714285714285
several choices 	 0.045454545454545456
increased from 	 0.6
use the 	 0.013888888888888888
a task 	 0.0036809815950920245
other punctuation 	 0.014285714285714285
Post Office 	 0.5
Modern NLP 	 0.3333333333333333
interface . 	 0.25
risk of 	 0.5
is shown 	 0.0020325203252032522
starts with 	 0.5
online databases 	 0.125
storm , 	 1.0
qualitative automatic 	 0.5
values correlate 	 0.125
all natural 	 0.023255813953488372
Each word 	 0.16666666666666666
be assumed 	 0.004219409282700422
that corresponded 	 0.0035460992907801418
1971 -LRB- 	 0.3333333333333333
<s> Interactive 	 0.0007686395080707148
are being 	 0.008298755186721992
Intelligent Machines 	 0.3333333333333333
Measuring progress 	 1.0
understanding , 	 0.09090909090909091
\* -LRB- 	 0.25
fulfill the 	 0.5
very limited 	 0.04878048780487805
first approximation 	 0.030303030303030304
a Cognitive 	 0.001226993865030675
Both acoustic 	 0.3333333333333333
-LRB- 1954 	 0.0027100271002710027
programmed with 	 0.5
describe informal 	 0.16666666666666666
; This 	 0.02127659574468085
of ISO\/TC37 	 0.00089126559714795
maintain tractability 	 1.0
more human-generated 	 0.010526315789473684
impressive results 	 0.5
reasoning components 	 0.14285714285714285
in excess 	 0.003745318352059925
tag set 	 0.3125
three basic 	 0.3333333333333333
may all 	 0.019230769230769232
Service has 	 1.0
ARCHILES technique 	 1.0
sentence boundary 	 0.0625
for Computational 	 0.0036101083032490976
many advantages 	 0.019230769230769232
<s> About 	 0.0007686395080707148
image consisting 	 0.3333333333333333
reverse -RRB- 	 0.5
probabilities of 	 0.2727272727272727
Johnstone , 	 1.0
Speech Communication 	 0.03225806451612903
recognition Main 	 0.008264462809917356
emigre Leo 	 1.0
translation or 	 0.013513513513513514
acts or 	 0.3333333333333333
is generated 	 0.006097560975609756
usually thought 	 0.03125
of Sydney 	 0.00089126559714795
camera . 	 0.5
As access 	 0.05555555555555555
database industry 	 0.1
edges are 	 0.14285714285714285
speech attached 	 0.006578947368421052
is often 	 0.022357723577235773
particularly prone 	 0.2
improved their 	 0.25
aural feedback 	 1.0
co-articulation of 	 1.0
the content 	 0.0020761245674740486
successful NLG 	 0.1111111111111111
statistical techniques 	 0.06060606060606061
scale , 	 0.3333333333333333
messages . 	 0.5
transform -RRB- 	 0.2
school-age children 	 1.0
handheld scanner 	 1.0
UC -RRB- 	 0.5
Levinsohn , 	 1.0
evaluation systems 	 0.018518518518518517
per second 	 0.5
Other systems 	 0.14285714285714285
food and 	 1.0
tell us 	 0.3333333333333333
text databases 	 0.006289308176100629
structured speech 	 0.16666666666666666
normally do 	 0.5
, modules 	 0.0005614823133071309
by Frederick 	 0.005714285714285714
classified into 	 1.0
such application 	 0.008130081300813009
recognized words 	 0.16666666666666666
hopes to 	 1.0
and human 	 0.001445086705202312
to such 	 0.0026560424966799467
-LRB- based 	 0.005420054200542005
of concern 	 0.00089126559714795
the given 	 0.001384083044982699
exploit domain-specific 	 1.0
-LRB- Computational 	 0.0027100271002710027
computer database 	 0.022727272727272728
browsing by 	 1.0
like English 	 0.07142857142857142
and print 	 0.001445086705202312
or lowering 	 0.0045045045045045045
publish a 	 1.0
the emotional 	 0.001384083044982699
a noun 	 0.007361963190184049
from randomly 	 0.009615384615384616
canonical form 	 1.0
-LRB- various 	 0.0027100271002710027
agreement among 	 0.3333333333333333
than conversation 	 0.022222222222222223
and character 	 0.001445086705202312
play in 	 1.0
operation . 	 0.5
the invention 	 0.0006920415224913495
key area 	 0.3333333333333333
theorists of 	 1.0
from first 	 0.009615384615384616
HMM states 	 0.3333333333333333
long research 	 0.5
True\/False is 	 1.0
or Spanish 	 0.0045045045045045045
semiotic event 	 1.0
different features 	 0.02040816326530612
libraries for 	 0.5
funding to 	 0.125
10 . 	 0.125
EMNLP , 	 1.0
claimed that 	 1.0
technologies have 	 0.25
for input 	 0.0036101083032490976
benefits : 	 0.5
need a 	 0.19047619047619047
determine both 	 0.043478260869565216
systems currently 	 0.008928571428571428
^ 2 	 0.3333333333333333
with thought-to-paper 	 0.00546448087431694
metric for 	 0.3333333333333333
-LRB- 2001 	 0.0027100271002710027
systems remains 	 0.008928571428571428
resolve . 	 0.25
techniques offer 	 0.043478260869565216
Just which 	 1.0
Dr. Kenneth 	 1.0
- passage 	 0.0625
follows that 	 0.5
assertive -LRB- 	 1.0
itself is 	 0.2
values to 	 0.125
should indicate 	 0.05263157894736842
dictionary-based machine 	 1.0
people for 	 0.0625
the attitude 	 0.0006920415224913495
no. . 	 1.0
supervised classification 	 0.125
in polynomial 	 0.0018726591760299626
Part-of-Speech Tagset 	 1.0
methodology was 	 0.5
with adjacent 	 0.00546448087431694
implemented on 	 0.2
unlimited range 	 1.0
Audio Processing 	 0.5
with manually 	 0.00546448087431694
In languages 	 0.009523809523809525
<s> Instead 	 0.0015372790161414297
video sub-titling 	 0.2
of parsing 	 0.0017825311942959
In 1970 	 0.009523809523809525
meets the 	 0.5
factory -RRB- 	 1.0
further subdivided 	 0.125
have limited 	 0.009615384615384616
, common 	 0.0005614823133071309
pictures or 	 1.0
explore the 	 0.25
Yale University 	 0.5
Schank and 	 0.4
time and 	 0.09090909090909091
rate these 	 0.09090909090909091
major constituents 	 0.08333333333333333
The target 	 0.005208333333333333
relevant to 	 0.14285714285714285
from section 	 0.009615384615384616
be referred 	 0.004219409282700422
to set 	 0.00398406374501992
be combined 	 0.004219409282700422
proposal for 	 1.0
addressee at 	 1.0
Speech Writing 	 0.03225806451612903
lexicon required 	 0.1111111111111111
count of 	 0.4
not functioning 	 0.008928571428571428
edges with 	 0.14285714285714285
or names 	 0.0045045045045045045
verb -LRB- 	 0.15384615384615385
the factors 	 0.0006920415224913495
that now 	 0.0035460992907801418
many keyphrases 	 0.019230769230769232
easily copied 	 0.1111111111111111
trained automatically 	 0.3333333333333333
understanding systems 	 0.030303030303030304
create a 	 0.4117647058823529
a logical 	 0.001226993865030675
-LRB- speed 	 0.0027100271002710027
Many real 	 0.08333333333333333
still to 	 0.06666666666666667
a learner 	 0.001226993865030675
with 12 	 0.00546448087431694
more likely 	 0.010526315789473684
inference algorithm 	 0.25
probabilities not 	 0.09090909090909091
immediately to 	 1.0
... About 	 0.5
<s> Hidden 	 0.0023059185242121443
like syntax 	 0.03571428571428571
is then 	 0.01016260162601626
Givón , 	 1.0
verify the 	 1.0
DUC 2001 	 1.0
in 1949 	 0.0018726591760299626
levels . 	 0.045454545454545456
a past-tense 	 0.001226993865030675
you would 	 0.07692307692307693
in massive 	 0.0018726591760299626
Slembrouck , 	 1.0
IE additionally 	 0.3333333333333333
<s> Last 	 0.0007686395080707148
space . 	 0.2
understand '' 	 0.14285714285714285
of Hidden 	 0.00089126559714795
are usually 	 0.012448132780082987
the typical 	 0.0006920415224913495
is essentially 	 0.006097560975609756
and comprehension 	 0.001445086705202312
d'Albe developed 	 1.0
compared - 	 0.14285714285714285
much through 	 0.045454545454545456
: This 	 0.0392156862745098
: Decoding 	 0.00980392156862745
as they 	 0.010452961672473868
one alternative 	 0.015384615384615385
solved problem 	 0.4
text as 	 0.006289308176100629
walk to 	 0.2
include the 	 0.18518518518518517
recognition because 	 0.008264462809917356
and to 	 0.015895953757225433
a context 	 0.00245398773006135
Handel also 	 1.0
, contrast 	 0.0005614823133071309
less uninterrupted 	 0.08333333333333333
entities '' 	 0.14285714285714285
<s> Stages 	 0.0007686395080707148
under a 	 0.2
in performance 	 0.0018726591760299626
rated with 	 1.0
same characters 	 0.04
several ambiguous 	 0.045454545454545456
By combining 	 0.3333333333333333
and also 	 0.001445086705202312
so simply 	 0.03333333333333333
is broad 	 0.0020325203252032522
than 150 	 0.022222222222222223
to settle 	 0.0013280212483399733
based speech 	 0.018518518518518517
Task description 	 0.3333333333333333
program can 	 0.045454545454545456
extremes , 	 1.0
interlingual , 	 0.25
process automatic 	 0.027777777777777776
just `` 	 0.1111111111111111
<s> Chinese 	 0.0007686395080707148
web blogs 	 0.125
-RRB- Marc 	 0.0027100271002710027
Later , 	 1.0
<s> Initial 	 0.0007686395080707148
the simulation 	 0.0006920415224913495
: Rules 	 0.0196078431372549
omitted -RRB- 	 1.0
are largely 	 0.004149377593360996
of parameters 	 0.00089126559714795
-LRB- like 	 0.0027100271002710027
and Spanish 	 0.001445086705202312
may also 	 0.019230769230769232
by applying 	 0.005714285714285714
, made 	 0.0011229646266142617
Emanuel Goldberg 	 0.5
text about 	 0.012578616352201259
task . 	 0.23809523809523808
bunch of 	 1.0
technique uses 	 0.14285714285714285
markers . 	 0.3333333333333333
-RRB- classifier 	 0.0027100271002710027
represent only 	 0.1111111111111111
coherent or 	 0.2
more widely 	 0.010526315789473684
interest was 	 0.09090909090909091
150,000 words 	 1.0
Robert E. 	 0.5
to represent 	 0.0013280212483399733
levels in 	 0.045454545454545456
germane to 	 1.0
Marginal Relevance 	 1.0
systems with 	 0.017857142857142856
the learner 	 0.0006920415224913495
within different 	 0.05555555555555555
notations , 	 0.5
linguists can 	 0.3333333333333333
be NP-complete 	 0.004219409282700422
informatics . 	 1.0
given unfamiliar 	 0.08333333333333333
Vulcan program 	 0.5
the user 	 0.0034602076124567475
the Eagles 	 0.0006920415224913495
<s> Turney 	 0.0007686395080707148
<s> As 	 0.010760953112990008
asking why 	 0.5
real-time written 	 0.5
of constraints 	 0.00089126559714795
the error 	 0.0006920415224913495
following is 	 0.06666666666666667
Sound is 	 0.3333333333333333
and subsequent 	 0.001445086705202312
a limited 	 0.00245398773006135
Current difficulties 	 0.2
pauses in 	 0.25
More recently 	 0.1111111111111111
's usually 	 0.0196078431372549
international ATC 	 0.5
different profile 	 0.02040816326530612
removal of 	 1.0
on simple 	 0.0047169811320754715
greater risk 	 0.3333333333333333
and related 	 0.004335260115606936
one element 	 0.015384615384615385
of Canada 	 0.0017825311942959
informal behavior 	 0.5
sequences that 	 0.1111111111111111
is better 	 0.0020325203252032522
only at 	 0.02631578947368421
derivation -LRB- 	 0.5
Reading '' 	 0.5
other POS 	 0.014285714285714285
<s> Finally 	 0.0007686395080707148
another linguistic 	 0.07692307692307693
language question-answering 	 0.006756756756756757
performed by 	 0.2
text and 	 0.018867924528301886
NP for 	 1.0
of them 	 0.0017825311942959
, assertion 	 0.0005614823133071309
to words 	 0.0013280212483399733
setting , 	 0.4
of texts 	 0.0017825311942959
domain might 	 0.05
Ernesto Laclau 	 1.0
's voices 	 0.0196078431372549
, content 	 0.0005614823133071309
translations using 	 0.5
senses . 	 0.5
marked by 	 0.3333333333333333
Schank at 	 0.2
the improvement 	 0.0006920415224913495
, at 	 0.0016844469399213925
vary considerably 	 0.16666666666666666
sentence must 	 0.020833333333333332
re-encode the 	 1.0
a comprehensive 	 0.0036809815950920245
toward actions 	 1.0
complex expressions 	 0.041666666666666664
and manipulate 	 0.001445086705202312
methods related 	 0.022727272727272728
In recent 	 0.01904761904761905
standards and 	 0.2
interested in 	 1.0
and Vietnamese 	 0.001445086705202312
rules through 	 0.023255813953488372
in vastly 	 0.0018726591760299626
approach has 	 0.02857142857142857
This model 	 0.031746031746031744
clear . 	 0.25
Carmen Rosa 	 1.0
problem yet 	 0.022727272727272728
recognition computer 	 0.01652892561983471
Solutions have 	 1.0
using digital 	 0.01694915254237288
internet discussion 	 1.0
repeatedly reviewed 	 1.0
and ask 	 0.001445086705202312
Constraints are 	 0.3333333333333333
Japanese . 	 0.125
C , 	 1.0
routing to 	 0.3333333333333333
they refer 	 0.05
to better 	 0.00398406374501992
placement of 	 1.0
mid 1980s 	 1.0
others can 	 0.08333333333333333
exchange of 	 1.0
cross-lingual -RRB- 	 0.5
of phonetic 	 0.00089126559714795
Biden was 	 0.3333333333333333
Consider the 	 1.0
its answer 	 0.02857142857142857
data analysis 	 0.012987012987012988
output , 	 0.038461538461538464
in determining 	 0.0018726591760299626
, need 	 0.0005614823133071309
Cary Grant 	 1.0
the analog 	 0.0006920415224913495
the Vulcan 	 0.0006920415224913495
difficult process 	 0.03571428571428571
, SAM 	 0.0005614823133071309
and extract 	 0.001445086705202312
the Mars 	 0.0006920415224913495
summaries to 	 0.046511627906976744
speaker of 	 0.1111111111111111
a certain 	 0.001226993865030675
a leftmost 	 0.00245398773006135
unigrams placed 	 0.08333333333333333
recognizers have 	 0.5
of systems 	 0.0017825311942959
Machine Summarization 	 0.1111111111111111
﻿Natural language 	 1.0
Latin . 	 0.25
new trend 	 0.041666666666666664
question classification 	 0.023809523809523808
and bought 	 0.001445086705202312
of linguistic 	 0.0017825311942959
to these 	 0.0026560424966799467
it vibrates 	 0.008547008547008548
polynomial-size representations 	 1.0
part usually 	 0.037037037037037035
difficulty using 	 0.14285714285714285
or phonemes 	 0.009009009009009009
such a 	 0.04878048780487805
often inaccurate 	 0.022727272727272728
There are 	 0.5454545454545454
level is 	 0.05
approach that 	 0.05714285714285714
American English 	 0.2
be adequately 	 0.004219409282700422
distinguishes these 	 0.5
different realizations 	 0.02040816326530612
and LILOG 	 0.001445086705202312
a campaign 	 0.001226993865030675
machine-generated summaries 	 1.0
, bigrams 	 0.0011229646266142617
unexpected features 	 1.0
. -RRB- 	 0.0062402496099844
as with 	 0.006968641114982578
closest counterparts 	 0.5
morphemes . 	 0.3333333333333333
transform , 	 0.4
architecture Regardless 	 0.5
produce one 	 0.045454545454545456
% still 	 0.02564102564102564
citations to 	 0.6666666666666666
will `` 	 0.05714285714285714
dialog with 	 0.5
speaking computer 	 0.125
choices that 	 0.2
equal to 	 1.0
accuracy include 	 0.03225806451612903
well-defined problem 	 1.0
characterize a 	 0.5
the shops 	 0.0006920415224913495
fairly often 	 0.25
a closed-captioning 	 0.001226993865030675
search method 	 0.09090909090909091
in taxonomies 	 0.0018726591760299626
dictation system 	 1.0
is generally 	 0.0020325203252032522
Confusable Words 	 1.0
-LRB- predict 	 0.0027100271002710027
impressive . 	 0.5
publicly available 	 1.0
picture distortion 	 0.25
the probable 	 0.0006920415224913495
made in 	 0.125
and we 	 0.001445086705202312
a solved 	 0.00245398773006135
Shipibo Paragraph 	 0.5
of meaningful 	 0.00089126559714795
Recall measures 	 0.3333333333333333
topic or 	 0.125
trends of 	 1.0
funding for 	 0.25
being asked 	 0.05555555555555555
translator needs 	 0.14285714285714285
parsers and 	 0.07692307692307693
differences themselves 	 0.3333333333333333
even though 	 0.07407407407407407
linguistic research 	 0.0625
wrote that 	 0.16666666666666666
printed text 	 0.25
to assign 	 0.00398406374501992
began planning 	 0.14285714285714285
unambiguous . 	 0.5
make it 	 0.1
going thus 	 0.25
expansion of 	 0.3333333333333333
in garden 	 0.0018726591760299626
as well 	 0.04878048780487805
More powerful 	 0.1111111111111111
requires each 	 0.0625
to robots 	 0.0013280212483399733
part because 	 0.037037037037037035
involves several 	 0.1
Unix Consultant 	 0.5
<s> Natural 	 0.004611837048424289
and produce 	 0.002890173410404624
, information 	 0.0011229646266142617
training data 	 0.35714285714285715
made WebOCR 	 0.0625
the outside 	 0.0006920415224913495
This corpus 	 0.031746031746031744
now common 	 0.07692307692307693
made by 	 0.0625
objects -LRB- 	 0.2
, computer 	 0.0011229646266142617
frame has 	 0.5
tools usually 	 0.16666666666666666
the top 	 0.002768166089965398
e.g. with 	 0.017857142857142856
word British 	 0.016666666666666666
of Eastern 	 0.00089126559714795
process The 	 0.027777777777777776
and consonants 	 0.001445086705202312
Translation process 	 0.6666666666666666
graph -RRB- 	 0.07692307692307693
annotating texts 	 1.0
knowledge frequently 	 0.037037037037037035
input devices 	 0.04878048780487805
the issues 	 0.0006920415224913495
input ; 	 0.024390243902439025
for words 	 0.0036101083032490976
highly redundant 	 0.1111111111111111
or keyphrases 	 0.0045045045045045045
, exploring 	 0.0005614823133071309
different methods 	 0.02040816326530612
Weizenbaum sidestepped 	 0.3333333333333333
created based 	 0.14285714285714285
allowable substitutions 	 0.5
, taught 	 0.0005614823133071309
Snyder -LRB- 	 0.5
content-analysis . 	 1.0
capturing data 	 1.0
a syntactic 	 0.001226993865030675
the co-occurrence 	 0.0006920415224913495
generic response 	 0.3333333333333333
display format 	 0.5
which will 	 0.021739130434782608
correct '' 	 0.06666666666666667
<s> Envelopes 	 0.0007686395080707148
often make 	 0.022727272727272728
of 500,000 	 0.00089126559714795
` beyond 	 0.0625
stage is 	 0.4
software has 	 0.037037037037037035
networks has 	 0.07142857142857142
used varies 	 0.008849557522123894
examples in 	 0.041666666666666664
can tell 	 0.0055248618784530384
data where 	 0.012987012987012988
<s> ELIZA 	 0.0023059185242121443
pro or 	 1.0
dividing written 	 0.3333333333333333
the total 	 0.0006920415224913495
discourse grammar 	 0.027777777777777776
is computed 	 0.0020325203252032522
a readable 	 0.001226993865030675
Turney and 	 0.2222222222222222
are in 	 0.012448132780082987
should figure 	 0.05263157894736842
Whether NLP 	 0.5
the Artificial 	 0.0006920415224913495
in quite 	 0.0018726591760299626
taken place 	 0.3333333333333333
discourse processing 	 0.027777777777777776
Royal Aerospace 	 0.5
such accuracy 	 0.008130081300813009
and syntax 	 0.001445086705202312
because the 	 0.13333333333333333
enabling technologies 	 1.0
a communicative 	 0.001226993865030675
thousands of 	 0.6666666666666666
'' that 	 0.020618556701030927
as nouns 	 0.003484320557491289
vs. independence 	 0.08333333333333333
model all 	 0.03333333333333333
possible forms 	 0.041666666666666664
to choose 	 0.0013280212483399733
idea that 	 0.2857142857142857
a parser 	 0.0036809815950920245
level 7 	 0.05
large sets 	 0.08695652173913043
machine-learning paradigm 	 0.25
learning Beginning 	 0.023255813953488372
HTK book 	 0.5
choose different 	 0.5
using photocells 	 0.01694915254237288
UPV -RRB- 	 1.0
: Rule-based 	 0.00980392156862745
computers and 	 0.1111111111111111
disagree that 	 0.3333333333333333
physician , 	 1.0
necessarily portable 	 0.5
and recall 	 0.002890173410404624
with weights 	 0.00546448087431694
of news 	 0.0017825311942959
<s> Essentially 	 0.0007686395080707148
their associated 	 0.029411764705882353
were spoken 	 0.024390243902439025
a gold 	 0.00245398773006135
an easy 	 0.007575757575757576
see LMF 	 0.05
language-specific changes 	 1.0
appliance control 	 1.0
can translate 	 0.0055248618784530384
grammar parsing 	 0.02702702702702703
implemented using 	 0.2
<s> Besides 	 0.0007686395080707148
and Intelligence 	 0.001445086705202312
, training 	 0.0005614823133071309
of at 	 0.00089126559714795
also prefer 	 0.014492753623188406
interface Home 	 0.25
questioners expect 	 1.0
delayed until 	 1.0
dynamic character 	 0.2
the QA 	 0.0006920415224913495
a subtopic 	 0.001226993865030675
references , 	 0.25
term voice 	 0.05555555555555555
languages can 	 0.04
this type 	 0.03296703296703297
from this 	 0.009615384615384616
<s> High-order 	 0.0007686395080707148
cases make 	 0.05555555555555555
only because 	 0.02631578947368421
most likely 	 0.05172413793103448
computed with 	 0.5
the notion 	 0.001384083044982699
is made 	 0.0040650406504065045
what might 	 0.03125
primarily with 	 0.5
so accurate 	 0.03333333333333333
a table 	 0.0036809815950920245
Question book-new 	 0.14285714285714285
dimensions of 	 0.6666666666666666
corpus . 	 0.03225806451612903
systems existing 	 0.008928571428571428
researched tasks 	 1.0
Command -RRB- 	 0.5
systems were 	 0.05357142857142857
for personal 	 0.0036101083032490976
of computerized 	 0.0017825311942959
devised primarily 	 0.5
probability of 	 0.14285714285714285
text . 	 0.16981132075471697
alone may 	 0.25
implementing a 	 1.0
tourism information 	 1.0
parsing systems 	 0.03571428571428571
own ; 	 0.16666666666666666
processing news 	 0.018518518518518517
following are 	 0.06666666666666667
database , 	 0.1
seek to 	 1.0
of small 	 0.00089126559714795
the highest 	 0.0006920415224913495
co-founded Google 	 1.0
article verb 	 0.034482758620689655
people , 	 0.0625
answering There 	 0.08333333333333333
involve counting 	 0.16666666666666666
which can 	 0.036231884057971016
but discards 	 0.014705882352941176
explore and 	 0.25
answer . 	 0.23333333333333334
question or 	 0.023809523809523808
token generation 	 0.25
when inter-annotator 	 0.02857142857142857
example-generation strategy 	 1.0
limited applications 	 0.1
easy-to-use syntax 	 1.0
more common 	 0.010526315789473684
approaches designed 	 0.03571428571428571
-LRB- AVRADA 	 0.0027100271002710027
been attained 	 0.014705882352941176
Politics -LRB- 	 1.0
Shepard went 	 0.3333333333333333
repeated as 	 0.5
the terms 	 0.0006920415224913495
a semantic 	 0.001226993865030675
problems . 	 0.17647058823529413
with larger 	 0.00546448087431694
considered -LRB- 	 0.1111111111111111
Eurospeech\/ICSLP -LRB- 	 1.0
see below 	 0.05
then there 	 0.02857142857142857
categorical . 	 1.0
binary classifier 	 0.25
semantic schemes 	 0.047619047619047616
dependency for 	 0.2
knowledge system 	 0.037037037037037035
of running 	 0.00089126559714795
vectors , 	 0.3333333333333333
produced word 	 0.1111111111111111
one or 	 0.03076923076923077
some rules 	 0.012048192771084338
obtained a 	 0.2857142857142857
One study 	 0.07692307692307693
contents of 	 1.0
TextRank . 	 0.07142857142857142
singular proper 	 0.25
door being 	 0.25
edges between 	 0.14285714285714285
that merely 	 0.0035460992907801418
perhaps because 	 0.16666666666666666
John Pierce 	 0.125
resulting graph 	 0.25
script will 	 0.25
the analysis 	 0.002768166089965398
was most 	 0.012987012987012988
researchers found 	 0.1
likelihood linear 	 0.6666666666666666
of structured 	 0.00089126559714795
the American 	 0.001384083044982699
of efforts 	 0.00089126559714795
University 's 	 0.1111111111111111
systems analyze 	 0.008928571428571428
, call 	 0.0005614823133071309
the coherence 	 0.0006920415224913495
had mentioned 	 0.07142857142857142
, 1952 	 0.0005614823133071309
mainly came 	 0.16666666666666666
text-understanding system 	 1.0
of two 	 0.0017825311942959
just robustness 	 0.1111111111111111
perhaps the 	 0.16666666666666666
Gdaniec C. 	 1.0
would reduce 	 0.018867924528301886
to pre-process 	 0.0013280212483399733
identify new 	 0.08333333333333333
grammar that 	 0.02702702702702703
overall contextual 	 0.16666666666666666
Fourier transform 	 0.6666666666666666
Knowledge on 	 0.5
4 , 	 0.2
late 1950s 	 0.1111111111111111
-LRB- -LRB- 	 0.0027100271002710027
well their 	 0.03571428571428571
RAE -RRB- 	 1.0
unable to 	 1.0
system might 	 0.010752688172043012
MIT wrote 	 0.5
, hopefully 	 0.0005614823133071309
components can 	 0.2
of phrases 	 0.00089126559714795
accuracy using 	 0.03225806451612903
switched to 	 1.0
Wilson , 	 1.0
of restaurant 	 0.00089126559714795
be manually 	 0.004219409282700422
Hafiz , 	 1.0
developed to 	 0.038461538461538464
beyond simple 	 0.16666666666666666
research groups 	 0.023809523809523808
develop OCR 	 0.2
knowledge . 	 0.037037037037037035
case with 	 0.058823529411764705
remains the 	 0.25
until 1970 	 0.5
identification of 	 0.4
feature or 	 0.07692307692307693
vs. open 	 0.08333333333333333
phrases to 	 0.0625
never went 	 0.2
is too 	 0.0020325203252032522
for further 	 0.010830324909747292
in summary 	 0.003745318352059925
on contrastive 	 0.0047169811320754715
to train 	 0.0013280212483399733
for NLP 	 0.0036101083032490976
expect answers 	 0.3333333333333333
the need 	 0.0020761245674740486
either a 	 0.2
throughout a 	 1.0
M. De 	 0.25
stress and 	 0.5
of our 	 0.00089126559714795
normalization . 	 0.3333333333333333
datum is 	 1.0
is especially 	 0.0040650406504065045
was -LRB- 	 0.012987012987012988
it . 	 0.042735042735042736
applies those 	 0.14285714285714285
an adjective 	 0.03787878787878788
languages which 	 0.02
words appear 	 0.009174311926605505
-RRB- Transcription 	 0.0027100271002710027
camera , 	 0.5
assign labels 	 0.2
copy the 	 1.0
formalized in 	 1.0
that affective 	 0.0035460992907801418
to support 	 0.0026560424966799467
followed by 	 0.5
the following 	 0.004844290657439446
with diversity 	 0.00546448087431694
classification . 	 0.11764705882352941
be taken 	 0.004219409282700422
general format 	 0.045454545454545456
the speakers 	 0.0006920415224913495
, merging 	 0.0005614823133071309
used over 	 0.008849557522123894
campaign compared 	 0.2
lexicons -LRB- 	 0.5
into intrinsic 	 0.01282051282051282
provide manually 	 0.16666666666666666
types of 	 0.8571428571428571
increasingly difficult 	 0.3333333333333333
The difficulty 	 0.015625
in political 	 0.0018726591760299626
<s> They 	 0.0023059185242121443
, Verbyx 	 0.0005614823133071309
processing group 	 0.018518518518518517
his method 	 0.08333333333333333
real estate 	 0.1111111111111111
level of 	 0.35
despite the 	 0.3333333333333333
locate the 	 1.0
form words 	 0.05
effectiveness of 	 0.3333333333333333
by visual 	 0.005714285714285714
`` Did 	 0.005291005291005291
The recognition 	 0.005208333333333333
between sentences 	 0.05128205128205128
`` up 	 0.005291005291005291
looks at 	 0.25
So far 	 0.3333333333333333
be done 	 0.02109704641350211
stochastic methods 	 0.125
are both 	 0.008298755186721992
facemask , 	 1.0
character is 	 0.09090909090909091
more time-consuming 	 0.010526315789473684
has many 	 0.023809523809523808
guessed at 	 1.0
<s> However 	 0.02843966179861645
1950s by 	 0.25
OCR patents 	 0.02040816326530612
in statistical 	 0.003745318352059925
speaker and 	 0.05555555555555555
collaborated to 	 1.0
routing bar 	 0.3333333333333333
Similarly , 	 1.0
of reasons 	 0.00089126559714795
and morphology 	 0.001445086705202312
Statistical Main 	 0.1111111111111111
MT companies 	 0.2
which give 	 0.007246376811594203
Noun , 	 1.0
`` blue 	 0.010582010582010581
information to 	 0.08695652173913043
or electronic 	 0.0045045045045045045
and orthography 	 0.001445086705202312
be programmed 	 0.008438818565400843
type provided 	 0.07142857142857142
The Associated 	 0.005208333333333333
'' highly 	 0.005154639175257732
to physicians 	 0.0013280212483399733
still very 	 0.06666666666666667
many forms 	 0.019230769230769232
informativeness of 	 0.6666666666666666
arbitrarily long 	 1.0
list -LRB- 	 0.09090909090909091
or 45 	 0.0045045045045045045
TextRank and 	 0.14285714285714285
`` barmaid 	 0.010582010582010581
The Duchess 	 0.005208333333333333
reader -RRB- 	 0.1
funding has 	 0.125
with highest 	 0.01092896174863388
states such 	 0.25
deciding whether 	 0.3333333333333333
interactive translation 	 0.25
, for 	 0.012352610892756879
Robert Wilensky 	 0.25
be for 	 0.004219409282700422
recall may 	 0.3333333333333333
because fast 	 0.03333333333333333
redundant sentences 	 1.0
rule-based machine-translation 	 0.14285714285714285
focusing on 	 1.0
are content 	 0.004149377593360996
researchers in 	 0.1
usually operate 	 0.03125
of new 	 0.00089126559714795
four together 	 0.14285714285714285
look -LRB- 	 0.2
complex matter 	 0.041666666666666664
<s> Maximum 	 0.0015372790161414297
work in 	 0.125
the single 	 0.0006920415224913495
consisting of 	 1.0
dictator is 	 1.0
meeting summarization 	 1.0
has now 	 0.011904761904761904
specific objects 	 0.047619047619047616
available soon 	 0.058823529411764705
enough information 	 0.2
Nuance Communications 	 0.6666666666666666
world , 	 0.06666666666666667
transmitting by 	 1.0
counterparts in 	 1.0
output quality 	 0.038461538461538464
with questions 	 0.01092896174863388
results in 	 0.047619047619047616
walks and 	 0.5
of statistical 	 0.0017825311942959
have keyphrases 	 0.019230769230769232
the Morpholympics 	 0.0006920415224913495
aloud from 	 1.0
Shallow approaches 	 0.5
want not 	 0.16666666666666666
cross-discipline of 	 1.0
state a 	 0.07142857142857142
difficulty of 	 0.42857142857142855
collection of 	 0.4
era of 	 1.0
Duchess was 	 1.0
information from 	 0.06521739130434782
be described 	 0.004219409282700422
<s> Semantic 	 0.0007686395080707148
often and 	 0.022727272727272728
source and 	 0.041666666666666664
and informativeness 	 0.001445086705202312
dictionary and 	 0.14285714285714285
against a 	 0.2
are two 	 0.008298755186721992
requires the 	 0.1875
average distance 	 0.5
`` Spoken 	 0.005291005291005291
: Many 	 0.00980392156862745
An example 	 0.1875
notably successful 	 0.3333333333333333
extrinsic , 	 0.16666666666666666
financial message 	 0.25
coverage . 	 0.3333333333333333
<s> Perhaps 	 0.0007686395080707148
though automating 	 0.1
Press '' 	 1.0
typical accuracy 	 0.1111111111111111
run on 	 0.2
that deals 	 0.0035460992907801418
is for 	 0.0020325203252032522
Arabic and 	 0.25
developed the 	 0.15384615384615385
people would 	 0.0625
data records 	 0.012987012987012988
such signals 	 0.008130081300813009
<s> Decoding 	 0.0007686395080707148
testing accuracy 	 0.2
find left-most 	 0.07692307692307693
errors or 	 0.2
use considers 	 0.013888888888888888
learn tag 	 0.07692307692307693
author wishes 	 0.3333333333333333
to multi-document 	 0.0013280212483399733
discourse begin 	 0.027777777777777776
topics in 	 0.14285714285714285
looking at 	 0.2
which associate 	 0.007246376811594203
writing -RRB- 	 0.1111111111111111
typically involved 	 0.05555555555555555
a calculator 	 0.00245398773006135
Robotics Speech-to-text 	 1.0
the Grace 	 0.0006920415224913495
spun it 	 1.0
seen before 	 0.2
Systems with 	 0.08333333333333333
Church used 	 0.3333333333333333
working out 	 0.14285714285714285
Tauschek had 	 0.5
disambiguation . 	 0.1
not well 	 0.008928571428571428
divided up 	 0.3333333333333333
, Grishman 	 0.0005614823133071309
and statistical 	 0.004335260115606936
hypothesis `` 	 1.0
the reCAPTCHA 	 0.0006920415224913495
He took 	 0.125
<s> Human 	 0.0023059185242121443
same example-generation 	 0.04
or `` 	 0.018018018018018018
and grammar 	 0.002890173410404624
found by 	 0.07142857142857142
-LRB- i.e. 	 0.02981029810298103
as many 	 0.006968641114982578
the case 	 0.005536332179930796
pragmatics . 	 0.3333333333333333
<s> Few 	 0.0007686395080707148
while logic 	 0.05
him or 	 0.5
lip-synch timing 	 1.0
Piron 's 	 0.3333333333333333
grammar -RRB- 	 0.08108108108108109
roughly , 	 0.6666666666666666
often much 	 0.022727272727272728
task because 	 0.023809523809523808
substantial amount 	 0.2
favor accuracy 	 0.5
known keyphrases 	 0.15384615384615385
the Penn 	 0.005536332179930796
parsing ambiguous 	 0.03571428571428571
phenomenon of 	 0.6
systems use 	 0.05357142857142857
recognition As 	 0.008264462809917356
spelled ` 	 1.0
sentences of 	 0.02631578947368421
the impossibility 	 0.0006920415224913495
texts to 	 0.11764705882352941
segment and 	 0.1111111111111111
expensive to 	 0.14285714285714285
but could 	 0.014705882352941176
intelligence -LRB- 	 0.125
is limited 	 0.0020325203252032522
other are 	 0.014285714285714285
For keyphrase 	 0.01639344262295082
of man-hours 	 0.00089126559714795
Perspectives The 	 1.0
make decisions 	 0.05
to refer 	 0.0013280212483399733
many similar 	 0.019230769230769232
the text 	 0.017993079584775088
Extracted sentences 	 1.0
with neural 	 0.00546448087431694
simply too 	 0.08333333333333333
then generated 	 0.02857142857142857
for verification 	 0.0036101083032490976
<s> Performing 	 0.0007686395080707148
have become 	 0.009615384615384616
if they 	 0.03571428571428571
reCAPTCHA system 	 1.0
Science , 	 0.5
's GenEx 	 0.0196078431372549
`` shallow 	 0.005291005291005291
and lexical 	 0.001445086705202312
statistically evaluated 	 1.0
helicopter pilot 	 0.25
superseded by 	 1.0
a rainbow 	 0.001226993865030675
may have 	 0.038461538461538464
, question 	 0.0011229646266142617
basics and 	 1.0
then chosen 	 0.02857142857142857
which could 	 0.007246376811594203
speaker deliberately 	 0.05555555555555555
languages are 	 0.02
and depth 	 0.001445086705202312
for training 	 0.010830324909747292
science and 	 0.1
the Wall 	 0.001384083044982699
discussing what 	 0.5
so the 	 0.23333333333333334
or opinion 	 0.0045045045045045045
must make 	 0.07142857142857142
and other 	 0.01300578034682081
relationships between 	 0.16666666666666666
glossary or 	 0.5
recognition and 	 0.05785123966942149
<s> LexisNexis 	 0.0007686395080707148
robustness in 	 0.25
above -RRB- 	 0.07692307692307693
IEEE ASRU 	 0.3333333333333333
group at 	 0.25
difficult , 	 0.03571428571428571
Speech Recognition 	 0.0967741935483871
word , 	 0.03333333333333333
Tauschek was 	 0.5
contribute to 	 1.0
un-supervised '' 	 1.0
: whereas 	 0.00980392156862745
and Callaghan 	 0.001445086705202312
More advanced 	 0.1111111111111111
, -LRB- 	 0.0016844469399213925
to separate 	 0.0013280212483399733
capabilities , 	 0.2
This article 	 0.015873015873015872
Intelligent '' 	 0.3333333333333333
at least 	 0.07352941176470588
Record or 	 1.0
recent developments 	 0.125
improve document 	 0.07692307692307693
environments . 	 1.0
this data 	 0.01098901098901099
feature\/aspect-based sentiment 	 1.0
systems favor 	 0.008928571428571428
<s> What 	 0.0030745580322828594
the start 	 0.002768166089965398
David R. 	 0.25
and previously-written 	 0.001445086705202312
breaks are 	 0.5
'' would 	 0.010309278350515464
clearly not 	 0.3333333333333333
computing : 	 0.5
software , 	 0.037037037037037035
Despite the 	 1.0
Algorithms Both 	 0.5
shifting to 	 1.0
is importance 	 0.0020325203252032522
English verbs 	 0.02702702702702703
, unlike 	 0.0005614823133071309
algorithm ? 	 0.03571428571428571
must compute 	 0.07142857142857142
The 1970s 	 0.005208333333333333
deal of 	 0.25
analyzing a 	 0.2
then end 	 0.02857142857142857
Words : 	 0.25
or phrase 	 0.0045045045045045045
For most 	 0.01639344262295082
Processing -LRB- 	 0.75
with known 	 0.01092896174863388
, cited 	 0.0005614823133071309
very recent 	 0.024390243902439025
popular example 	 0.1111111111111111
: Translations 	 0.00980392156862745
Note that 	 0.7777777777777778
distortion , 	 1.0
for word 	 0.0036101083032490976
-RRB- approach 	 0.0027100271002710027
NLP systems 	 0.06382978723404255
document before 	 0.027777777777777776
The relationships 	 0.005208333333333333
current efforts 	 0.14285714285714285
lexical segment 	 0.07692307692307693
the E-set 	 0.0006920415224913495
most fundamental 	 0.017241379310344827
started around 	 0.25
entity , 	 0.4
processing tasks 	 0.018518518518518517
low-resolution , 	 1.0
Translator -RRB- 	 1.0
you want 	 0.07692307692307693
: compare 	 0.00980392156862745
among the 	 0.125
mental representations 	 0.3333333333333333
no distinction 	 0.07692307692307693
analysis or 	 0.046153846153846156
systems however 	 0.008928571428571428
would both 	 0.018867924528301886
vs. preposition 	 0.08333333333333333
It 's 	 0.05263157894736842
to Recognize 	 0.0013280212483399733
question-answering engines 	 0.5
will give 	 0.02857142857142857
a speaker 	 0.00245398773006135
his company 	 0.08333333333333333
both individual 	 0.03225806451612903
for singular 	 0.007220216606498195
performance and 	 0.1111111111111111
, Pang 	 0.0005614823133071309
recognizing named 	 0.2
Tipster project 	 1.0
whether `` 	 0.07692307692307693
us with 	 0.5
intrinsic and 	 0.25
world -LRB- 	 0.06666666666666667
multitude of 	 1.0
use of 	 0.2916666666666667
naval resource 	 0.6666666666666666
: Hidden 	 0.00980392156862745
, recognizing 	 0.0005614823133071309
, followed 	 0.0005614823133071309
linguistic and 	 0.0625
common ground 	 0.04
many are 	 0.019230769230769232
are analytical 	 0.004149377593360996
the strengths 	 0.0006920415224913495
Code , 	 1.0
results reported 	 0.047619047619047616
sentenced separated 	 1.0
similarity classes 	 0.1
approach for 	 0.02857142857142857
of action 	 0.00089126559714795
Systems based 	 0.25
often do 	 0.022727272727272728
named Interspeech 	 0.14285714285714285
<s> LREC 	 0.0015372790161414297
choices What 	 0.4
Yehoshua Bar-Hillel 	 1.0
n being 	 0.5
languages . 	 0.16
this article 	 0.04395604395604396
hybrid approach 	 0.5
continued , 	 0.1111111111111111
well , 	 0.03571428571428571
1933 -LRB- 	 1.0
computationally ; 	 0.5
extraction -LRB- 	 0.06451612903225806
section titles 	 0.16666666666666666
languages like 	 0.02
state -LRB- 	 0.07142857142857142
-LRB- subject 	 0.0027100271002710027
are put 	 0.004149377593360996
unit vertices 	 0.3333333333333333
module looks 	 0.3333333333333333
no significant 	 0.07692307692307693
BORIS system 	 1.0
limiting the 	 1.0
segmentation task 	 0.030303030303030304
`` words 	 0.005291005291005291
ensure verifiability 	 1.0
on how 	 0.009433962264150943
viewed as 	 1.0
A promising 	 0.02
in four 	 0.003745318352059925
difficult and 	 0.03571428571428571
increase recognition 	 0.25
MUC and 	 1.0
taken to 	 0.3333333333333333
Their algorithm 	 0.5
Windows Mobile 	 1.0
formats like 	 1.0
that occurs 	 0.0035460992907801418
terms do 	 0.07692307692307693
observe patterns 	 1.0
optimizing a 	 1.0
'' set 	 0.005154639175257732
to grow 	 0.0013280212483399733
summaries . 	 0.13953488372093023
fall into 	 0.5
called recursively 	 0.05555555555555555
combining the 	 0.25
e.g. Syntactic 	 0.017857142857142856
, part-of-speech 	 0.0011229646266142617
is accomplished 	 0.0020325203252032522
and by 	 0.001445086705202312
broad and 	 0.25
line in 	 0.3333333333333333
because recognition 	 0.03333333333333333
the space 	 0.0020761245674740486
two simple 	 0.034482758620689655
: setting 	 0.00980392156862745
active area 	 0.5
content present 	 0.08333333333333333
Environmental noise 	 1.0
work progressed 	 0.041666666666666664
summary of 	 0.07142857142857142
tagging system 	 0.04
-LRB- DA 	 0.005420054200542005
travel . 	 1.0
much harder 	 0.045454545454545456
<s> Search 	 0.0015372790161414297
compare them 	 0.14285714285714285
expensive , 	 0.42857142857142855
review . 	 0.3333333333333333
levels or 	 0.045454545454545456
only way 	 0.02631578947368421
sentence that 	 0.041666666666666664
factor . 	 0.5
morphology -LRB- 	 0.14285714285714285
four decades 	 0.14285714285714285
→ barmaid 	 0.3333333333333333
example Wireless 	 0.012345679012345678
or future 	 0.0045045045045045045
and question 	 0.001445086705202312
as information 	 0.003484320557491289
Scotland , 	 0.2
Nelson Francis 	 1.0
and space 	 0.001445086705202312
is distinct 	 0.0020325203252032522
`` be 	 0.010582010582010581
of patterns 	 0.00089126559714795
common case 	 0.04
extractive -LRB- 	 0.14285714285714285
solid state 	 1.0
identification pattern 	 0.2
Initial results 	 1.0
successive words 	 0.5
it enumerated 	 0.008547008547008548
difference between 	 0.25
also experimented 	 0.014492753623188406
manipulate . 	 0.3333333333333333
by this 	 0.005714285714285714
implementation of 	 1.0
Scansoft , 	 1.0
Method -RRB- 	 1.0
the UC 	 0.0006920415224913495
structures that 	 0.4
the British 	 0.0006920415224913495
This covers 	 0.031746031746031744
One example 	 0.07692307692307693
output scores 	 0.038461538461538464
with each 	 0.00546448087431694
; the 	 0.0851063829787234
This device 	 0.015873015873015872
LILOG projects 	 0.5
their ratings 	 0.029411764705882353
very effective 	 0.024390243902439025
Like the 	 0.5
of coherent 	 0.00089126559714795
such problems 	 0.008130081300813009
is considerable 	 0.0020325203252032522
and context 	 0.004335260115606936
if you 	 0.07142857142857142
an autopilot 	 0.007575757575757576
above , 	 0.3076923076923077
-LRB- UC 	 0.0027100271002710027
written scripts 	 0.038461538461538464
focus and 	 0.14285714285714285
attitude of 	 0.5
to eliminate 	 0.0026560424966799467
as working 	 0.003484320557491289
its designers 	 0.02857142857142857
the target 	 0.006228373702422145
by describing 	 0.005714285714285714
for database 	 0.0036101083032490976
foster the 	 1.0
case of 	 0.35294117647058826
two categories 	 0.034482758620689655
for grammars 	 0.0036101083032490976
require minimal 	 0.045454545454545456
and creation 	 0.001445086705202312
machine is 	 0.012658227848101266
uninterrupted and 	 1.0
methods need 	 0.045454545454545456
accidentally omitted 	 1.0
the accompanying 	 0.0006920415224913495
people untrained 	 0.0625
some perception 	 0.012048192771084338
ATNs and 	 0.3333333333333333
<s> E. 	 0.0007686395080707148
a pre-structured 	 0.001226993865030675
their effectiveness 	 0.029411764705882353
Our evaluation 	 0.3333333333333333
is based 	 0.008130081300813009
parse natural 	 0.1111111111111111
compare generated 	 0.14285714285714285
only specific 	 0.02631578947368421
is clear 	 0.0020325203252032522
Machine Aided 	 0.1111111111111111
values , 	 0.125
important Web 	 0.0625
-LRB- allowing 	 0.0027100271002710027
to match 	 0.0026560424966799467
code on 	 0.14285714285714285
Recognition -LRB- 	 0.125
other native 	 0.014285714285714285
each unigram 	 0.044444444444444446
like a 	 0.07142857142857142
sources are 	 0.16666666666666666
and typically 	 0.002890173410404624
as adjectives 	 0.003484320557491289
a verb 	 0.007361963190184049
Henry Widdowson 	 0.5
: Digitize 	 0.00980392156862745
either positive 	 0.1
sounds , 	 0.13333333333333333
and usefulness 	 0.001445086705202312
analysis tasks 	 0.015384615384615385
find ways 	 0.07692307692307693
manually or 	 0.25
expected to 	 0.2857142857142857
even more 	 0.037037037037037035
use multiple 	 0.013888888888888888
rejecting those 	 0.3333333333333333
fields such 	 0.16666666666666666
The importance 	 0.005208333333333333
for accuracy 	 0.0036101083032490976
Customized OCR 	 1.0
most upper 	 0.017241379310344827
can condense 	 0.0055248618784530384
the times 	 0.0006920415224913495
business rules 	 0.25
statistical analysis 	 0.030303030303030304
could be 	 0.25
would like 	 0.03773584905660377
or turns-at-talk 	 0.0045045045045045045
is crucial 	 0.0020325203252032522
computers have 	 0.1111111111111111
David H. 	 0.25
collections of 	 0.25
checking that 	 1.0
analyses . 	 0.4
Jacob Rabinow 	 1.0
online opinion 	 0.125
phrases are 	 0.0625
Carnegie Mellon 	 1.0
-LRB- on 	 0.005420054200542005
texts and 	 0.058823529411764705
stochastic , 	 0.25
deeply , 	 1.0
utterance `` 	 0.3333333333333333
error types 	 0.08333333333333333
get some 	 0.14285714285714285
for American 	 0.0036101083032490976
comes from 	 0.4
magazine 's 	 1.0
Parliament . 	 0.5
alone , 	 0.25
LexRank applies 	 0.08333333333333333
Clancy 's 	 1.0
editing the 	 0.5
training systems 	 0.03571428571428571
considerable interest 	 0.2
, allowing 	 0.0005614823133071309
the future 	 0.001384083044982699
Jan Blommaert 	 1.0
or quantities 	 0.0045045045045045045
and that 	 0.002890173410404624
data at 	 0.012987012987012988
Lander Automatic 	 0.5
, will 	 0.0011229646266142617
-- if 	 0.08
correct software 	 0.06666666666666667
a row 	 0.001226993865030675
row , 	 1.0
eliminate the 	 0.5
further discussed 	 0.125
who maintains 	 0.1
essentially calculates 	 0.125
from multimedia 	 0.009615384615384616
every combination 	 0.3333333333333333
and negative 	 0.002890173410404624
stored more 	 1.0
Even though 	 1.0
scores are 	 0.2
Since then 	 0.2
, Stef 	 0.0005614823133071309
2,663,758 . 	 1.0
be studied 	 0.004219409282700422
sometimes had 	 0.07692307692307693
typewritten pages 	 0.2
person or 	 0.10526315789473684
recognize all 	 0.1111111111111111
output that 	 0.07692307692307693
lines , 	 0.3333333333333333
complex spoken 	 0.041666666666666664
Pronunciation evaluation 	 1.0
key phrases 	 0.16666666666666666
<s> Short 	 0.0007686395080707148
or custom 	 0.0045045045045045045
word sequences 	 0.016666666666666666
constructed by 	 0.5
yesterday and 	 0.3333333333333333
give predicted 	 0.25
Then we 	 0.2
reports -RRB- 	 0.4
three to 	 0.3333333333333333
MT -LRB- 	 0.2
and sources 	 0.001445086705202312
is working 	 0.0040650406504065045
-LRB- 1995 	 0.0027100271002710027
table that 	 0.14285714285714285
needed to 	 0.09523809523809523
a 4 	 0.001226993865030675
walk on 	 0.4
the geological 	 0.0006920415224913495
of Generation 	 0.00089126559714795
for cartoon 	 0.0036101083032490976
of its 	 0.0071301247771836
by Xuedong 	 0.005714285714285714
contain strings 	 0.08333333333333333
uses stochastic 	 0.07142857142857142
ASR in 	 0.5
of Peru 	 0.00089126559714795
received a 	 0.5
most other 	 0.017241379310344827
like what 	 0.03571428571428571
against feature 	 0.2
or language 	 0.0045045045045045045
data needed 	 0.012987012987012988
be similar 	 0.004219409282700422
-- on 	 0.04
recognized or 	 0.16666666666666666
the new 	 0.0006920415224913495
In short 	 0.009523809523809525
sections of 	 1.0
driving social 	 1.0
cost of 	 0.5
pilots in 	 0.5
commercial systems 	 0.09090909090909091
which are 	 0.08695652173913043
a qualitative 	 0.00245398773006135
arise because 	 1.0
be answered 	 0.004219409282700422
by Piron 	 0.005714285714285714
, employs 	 0.0005614823133071309
the presence 	 0.0006920415224913495
include versions 	 0.037037037037037035
states that 	 0.25
approach was 	 0.02857142857142857
environment in 	 0.16666666666666666
semantic disambiguation 	 0.047619047619047616
document formats 	 0.027777777777777776
on SourceForge 	 0.0047169811320754715
and shallowest 	 0.001445086705202312
i.e. text 	 0.05263157894736842
NLG ; 	 0.047619047619047616
extraction or 	 0.03225806451612903
reliable sources 	 0.25
, encoding 	 0.0005614823133071309
a recent 	 0.001226993865030675
naive semantics 	 0.5
very complex 	 0.024390243902439025
were very 	 0.04878048780487805
discussion of 	 0.5
-LRB- written 	 0.0027100271002710027
application domain 	 0.07142857142857142
famous early 	 0.3333333333333333
` best 	 0.0625
the annotation 	 0.0006920415224913495
the breadth 	 0.0006920415224913495
telephone . 	 0.5
generate jokes 	 0.05555555555555555
soon become 	 0.3333333333333333
research to 	 0.023809523809523808
door '' 	 0.5
the structure 	 0.0020761245674740486
authors found 	 0.2
, QUALM 	 0.0005614823133071309
weaknesses . 	 1.0
a hybrid 	 0.00245398773006135
dialing -LRB- 	 1.0
`` higher 	 0.005291005291005291
answer is 	 0.06666666666666667
word that 	 0.03333333333333333
reports into 	 0.4
keyphrases from 	 0.02857142857142857
each such 	 0.022222222222222223
combines the 	 1.0
, various 	 0.0005614823133071309
vehicle Navigation 	 1.0
ATN -RRB- 	 1.0
Moreover , 	 1.0
In many 	 0.01904761904761905
sailor dogs 	 0.2
Romanseval campaigns 	 1.0
idea , 	 0.14285714285714285
to run 	 0.0013280212483399733
community , 	 1.0
unique and 	 1.0
answers the 	 0.08333333333333333
to develop 	 0.006640106241699867
for test 	 0.0036101083032490976
a recall-based 	 0.001226993865030675
2 -- 	 0.2
and one 	 0.001445086705202312
recognition accuracy 	 0.05785123966942149
program got 	 0.045454545454545456
the entire 	 0.0006920415224913495
way that 	 0.125
VISTA -RRB- 	 1.0
Character Recognition 	 1.0
it led 	 0.008547008547008548
Precision measures 	 1.0
the values 	 0.0006920415224913495
PhD thesis 	 1.0
and correctly-developed 	 0.001445086705202312
generating natural 	 0.2
newspaper pages 	 0.3333333333333333
a pre-existing 	 0.001226993865030675
can of 	 0.0055248618784530384
<s> Throughout 	 0.0007686395080707148
summarization Like 	 0.02
real ATC 	 0.1111111111111111
entity about 	 0.2
Rogerian psychotherapist 	 1.0
short textual 	 0.125
information of 	 0.021739130434782608
in virtually 	 0.0018726591760299626
, Deirdre 	 0.0005614823133071309
human languages 	 0.021739130434782608
semantics of 	 0.07142857142857142
a bunch 	 0.001226993865030675
, matching 	 0.0005614823133071309
undertook recognition 	 1.0
see that 	 0.05
being referred 	 0.05555555555555555
many programmers 	 0.019230769230769232
'' `` 	 0.005154639175257732
taxonomies . 	 1.0
hand-printed text 	 0.5
program were 	 0.045454545454545456
ambiguous there 	 0.08333333333333333
digitized , 	 1.0
characters for 	 0.0625
Algorithm -RRB- 	 1.0
um '' 	 1.0
: objective 	 0.00980392156862745
generalized ATNs 	 1.0
who used 	 0.1
interpreters require 	 1.0
, either 	 0.0011229646266142617
has fueled 	 0.011904761904761904
it off 	 0.008547008547008548
The CyberEmotions 	 0.005208333333333333
and analysis 	 0.001445086705202312
big a 	 0.5
and corrected 	 0.001445086705202312
first stage 	 0.030303030303030304
tree -LRB- 	 1.0
a naive 	 0.001226993865030675
development in 	 0.08333333333333333
of reproducing 	 0.00089126559714795
HMMs involve 	 0.125
, hence 	 0.0005614823133071309
or neutral 	 0.0045045045045045045
One approach 	 0.07692307692307693
<s> might 	 0.0007686395080707148
, French 	 0.0005614823133071309
, substantial 	 0.0005614823133071309
essentially identical 	 0.125
may depend 	 0.019230769230769232
Rates Increase 	 1.0
the informational 	 0.0006920415224913495
purposes -LRB- 	 0.25
theory Conversation 	 0.07692307692307693
Corpus -RRB- 	 0.1875
would have 	 0.05660377358490566
The objects 	 0.005208333333333333
Mars Microphone 	 0.5
different sentences 	 0.061224489795918366
pollen count 	 0.07692307692307693
Evaluation -LRB- 	 0.1111111111111111
was painstakingly 	 0.012987012987012988
besides words 	 1.0
: it 	 0.00980392156862745
segmentation , 	 0.09090909090909091
these devices 	 0.023809523809523808
into several 	 0.01282051282051282
, you 	 0.0005614823133071309
are interpreted 	 0.004149377593360996
subtasks that 	 0.5
topics and 	 0.14285714285714285
using database 	 0.01694915254237288
be formally 	 0.004219409282700422
chatterbots were 	 0.5
of internal 	 0.00089126559714795
database of 	 0.2
at an 	 0.014705882352941176
the questioner 	 0.002768166089965398
pre-defined or 	 0.5
sets in 	 0.09090909090909091
But from 	 0.16666666666666666
EHR -RRB- 	 0.3333333333333333
the expression 	 0.001384083044982699
100 % 	 0.6666666666666666
on part-of-speech 	 0.0047169811320754715
on . 	 0.018867924528301886
<s> Working 	 0.0007686395080707148
only one 	 0.02631578947368421
has plateaued 	 0.011904761904761904
modeling of 	 0.14285714285714285
what original 	 0.03125
and time-consuming 	 0.001445086705202312
and reporting 	 0.001445086705202312
comparative depths 	 1.0
Statistical machine 	 0.2222222222222222
Coulthard , 	 1.0
be expressed 	 0.012658227848101266
sentiment in 	 0.08
appends the 	 1.0
<s> Discursive 	 0.0007686395080707148
of converting 	 0.0017825311942959
trade offs 	 0.5
the reader 	 0.002768166089965398
with respect 	 0.03825136612021858
spending limit 	 1.0
the Cuzco 	 0.0006920415224913495
though other 	 0.1
one place 	 0.015384615384615385
are four 	 0.004149377593360996
are complicated 	 0.004149377593360996
in part 	 0.0018726591760299626
ask for 	 0.25
Huang etc. 	 1.0
digital . 	 0.14285714285714285
-LRB- ParaEval 	 0.0027100271002710027
contexts in 	 0.14285714285714285
very deep 	 0.024390243902439025
sentence-ending markers 	 1.0
as first-order 	 0.003484320557491289
, text 	 0.0016844469399213925
or answers 	 0.0045045045045045045
having to 	 0.2
language expression 	 0.006756756756756757
automatically , 	 0.047619047619047616
bases , 	 1.0
classifier , 	 0.14285714285714285
WebOCR & 	 0.75
for help 	 0.0036101083032490976
language without 	 0.006756756756756757
his students 	 0.16666666666666666
those it 	 0.045454545454545456
' lengths 	 0.05263157894736842
references -RRB- 	 0.5
the interlingua 	 0.0006920415224913495
next word 	 0.2857142857142857
be satisfactory 	 0.004219409282700422
, parsing 	 0.0016844469399213925
points of 	 0.5
Isolated , 	 1.0
of sublanguage 	 0.0017825311942959
writing , 	 0.2222222222222222
necessary anymore 	 0.1
on text 	 0.0047169811320754715
the high 	 0.001384083044982699
1,500 documents 	 1.0
orally speaking 	 1.0
a scaling 	 0.001226993865030675
Telematics -LRB- 	 1.0
for Amharic 	 0.0036101083032490976
keyphrases as 	 0.02857142857142857
the conversion 	 0.0006920415224913495
equivalence is 	 0.5
the built 	 0.0006920415224913495
them but 	 0.05263157894736842
disambiguation concerns 	 0.1
both learn 	 0.03225806451612903
Language understanding 	 0.08333333333333333
OCR software 	 0.08163265306122448
then combining 	 0.02857142857142857
to 7 	 0.00398406374501992
gets around 	 0.5
, extracting 	 0.0005614823133071309
words commonly 	 0.009174311926605505
fine degrees 	 0.5
businesses looking 	 0.5
the unigram 	 0.0006920415224913495
usually not 	 0.03125
translation was 	 0.02702702702702703
product of 	 0.14285714285714285
, semantically 	 0.0005614823133071309
experiment was 	 0.4
2010 ? 	 0.3333333333333333
a meaningful 	 0.00245398773006135
head hurts 	 1.0
the Bayes 	 0.0006920415224913495
applications . 	 0.16
simplest -LRB- 	 1.0
is another 	 0.0040650406504065045
Basically , 	 1.0
with models 	 0.00546448087431694
will replace 	 0.02857142857142857
, Case 	 0.0005614823133071309
, Driver-license 	 0.0005614823133071309
applied successfully 	 0.06666666666666667
sometimes open-ended 	 0.07692307692307693
Evaluation exercises 	 0.1111111111111111
matter of 	 0.3333333333333333
above text 	 0.07692307692307693
, Chantal 	 0.0005614823133071309
a toy 	 0.00245398773006135
by which 	 0.005714285714285714
assign positive 	 0.2
deterministic rules 	 0.25
that have 	 0.02127659574468085
conjunction with 	 0.6666666666666666
rich languages 	 0.2
providing a 	 0.5
express the 	 0.4
-RRB- applications 	 0.0027100271002710027
, signed 	 0.0005614823133071309
`` What 	 0.015873015873015872
interact with 	 1.0
`` Dog 	 0.005291005291005291
translation Statistical 	 0.013513513513513514
operated on 	 0.5
enumerated all 	 1.0
Contains Confusable 	 1.0
17 ambiguous 	 1.0
while an 	 0.05
accelerations and 	 1.0
fairly simple 	 0.25
constructs -RRB- 	 0.3333333333333333
<s> Please 	 0.0023059185242121443
different speaker 	 0.02040816326530612
voice and 	 0.07692307692307693
summaries do 	 0.023255813953488372
rev , 	 1.0
-RRB- Modern 	 0.0027100271002710027
test how 	 0.1
, John 	 0.002807411566535654
, Gender 	 0.0005614823133071309
makes intuitive 	 0.125
keyphrase containing 	 0.05263157894736842
1971 and 	 0.3333333333333333
analysis -LRB- 	 0.06153846153846154
a component 	 0.00245398773006135
for people 	 0.007220216606498195
technology has 	 0.045454545454545456
evidence for 	 0.5
was entertaining 	 0.012987012987012988
, 1977 	 0.0005614823133071309
of itself 	 0.00089126559714795
Accuracy rates 	 0.2857142857142857
lexical similarity 	 0.07692307692307693
for voice 	 0.0036101083032490976
because it 	 0.1
list , 	 0.09090909090909091
human-written texts 	 0.5
, Alessandro 	 0.0005614823133071309
cases -LRB- 	 0.05555555555555555
Electronic Medical 	 0.5
to reliable 	 0.0013280212483399733
was all 	 0.012987012987012988
to +5 	 0.0013280212483399733
way sentiment 	 0.041666666666666664
vertices are 	 0.1111111111111111
find an 	 0.15384615384615385
read 23 	 0.14285714285714285
retrieval or 	 0.14285714285714285
Grass pollen 	 1.0
order , 	 0.14285714285714285
may use 	 0.019230769230769232
same general 	 0.04
with payments 	 0.00546448087431694
sentence breaks 	 0.020833333333333332
has received 	 0.011904761904761904
LexRank was 	 0.08333333333333333
to extract 	 0.00398406374501992
a coherent 	 0.00245398773006135
is typically 	 0.006097560975609756
TextRank was 	 0.14285714285714285
Accuracy of 	 0.42857142857142855
suitability for 	 0.5
which describe 	 0.007246376811594203
to build 	 0.0013280212483399733
from single 	 0.009615384615384616
application may 	 0.07142857142857142
a specialist 	 0.001226993865030675
1987 , 	 0.6666666666666666
this system 	 0.01098901098901099
subtopic of 	 1.0
Computed every 	 1.0
JSF -RRB- 	 1.0
MIT . 	 0.5
of ambitious 	 0.00089126559714795
individual phones 	 0.08333333333333333
The following 	 0.020833333333333332
robust when 	 0.5
test and 	 0.2
speech is 	 0.006578947368421052
phase is 	 1.0
only rely 	 0.02631578947368421
to carry 	 0.0013280212483399733
very simple 	 0.04878048780487805
approaches : 	 0.14285714285714285
, humans 	 0.0005614823133071309
first-order logic 	 1.0
structure is 	 0.08333333333333333
listening to 	 1.0
National Corpus 	 0.3333333333333333
recognition models 	 0.008264462809917356
is prone 	 0.0020325203252032522
methods were 	 0.045454545454545456
now largely 	 0.07692307692307693
core database 	 0.5
order for 	 0.07142857142857142
, Perceptron 	 0.0005614823133071309
appraisal theory 	 1.0
Compare speech 	 1.0
present special 	 0.16666666666666666
special image 	 0.2
Jim Martin 	 1.0
automated , 	 0.14285714285714285
computer forecasts 	 0.022727272727272728
determine what 	 0.043478260869565216
ISO\/TC37 and 	 1.0
the definition 	 0.0020761245674740486
Technolangue\/Easy project 	 0.5
by spaces 	 0.005714285714285714
John 's 	 0.25
<s> Context-free 	 0.0007686395080707148
, news 	 0.0005614823133071309
Constraints e.g. 	 0.3333333333333333
a fair 	 0.001226993865030675
FAS -RRB- 	 1.0
In corpus 	 0.009523809523809525
language modeling 	 0.006756756756756757
by numbers 	 0.005714285714285714
corpora and 	 0.09090909090909091
, Carla 	 0.0005614823133071309
and large-scale 	 0.001445086705202312
LexRank uses 	 0.08333333333333333
trigrams , 	 0.5
-RRB- by 	 0.0027100271002710027
a machine 	 0.008588957055214725
functioning of 	 0.3333333333333333
Speech processing 	 0.03225806451612903
knowledge base 	 0.14814814814814814
colloquially termed 	 1.0
the expectations 	 0.0006920415224913495
at run-time 	 0.014705882352941176
`` processing 	 0.010582010582010581
extract subjective 	 0.25
Although humans 	 0.125
study language 	 0.25
different method 	 0.02040816326530612
; and 	 0.0851063829787234
mentioned by 	 0.16666666666666666
write ` 	 1.0
symbol of 	 0.25
simple sentence 	 0.038461538461538464
with collecting 	 0.00546448087431694
dogs -RRB- 	 0.14285714285714285
portable to 	 0.3333333333333333
an article 	 0.015151515151515152
Racter , 	 1.0
excellent application 	 1.0
common use 	 0.08
extraction process 	 0.03225806451612903
, web 	 0.0005614823133071309
large probabilities 	 0.043478260869565216
Roger Fowler 	 0.25
its component 	 0.02857142857142857
computer-type OCR 	 1.0
<s> Modern 	 0.0007686395080707148
a generic 	 0.001226993865030675
a pre-processing 	 0.001226993865030675
single PC 	 0.07142857142857142
thereby editing 	 1.0
real progress 	 0.1111111111111111
natural-language processing 	 1.0
algorithms differ 	 0.02857142857142857
the social 	 0.0020761245674740486
alphabet are 	 0.3333333333333333
R. , 	 0.3333333333333333
a combination 	 0.00245398773006135
this technology 	 0.01098901098901099
Phrases , 	 1.0
NLG may 	 0.047619047619047616
spite of 	 1.0
probabilities are 	 0.09090909090909091
with restricted 	 0.00546448087431694
These standards 	 0.058823529411764705
A post 	 0.02
computer can 	 0.022727272727272728
<s> HMMs 	 0.0023059185242121443
be the 	 0.012658227848101266
online assistant 	 0.125
abstracts or 	 0.5
word-frequency and 	 1.0
probability -RRB- 	 0.2857142857142857
shorter and 	 0.5
project were 	 0.07692307692307693
function as 	 0.125
on speaker 	 0.0047169811320754715
characters can 	 0.1875
of charge 	 0.00089126559714795
closest the 	 0.5
air traffic 	 0.6
the eigenvector 	 0.0006920415224913495
with sentences 	 0.00546448087431694
are written 	 0.004149377593360996
using both 	 0.01694915254237288
The SATZ 	 0.005208333333333333
now more 	 0.07692307692307693
essential difference 	 1.0
adapt to 	 1.0
AVRADA -RRB- 	 0.5
Some classifiers 	 0.047619047619047616
Merging of 	 1.0
humans to 	 0.08333333333333333
<s> ROUGE-1 	 0.0007686395080707148
working examples 	 0.14285714285714285
the occurrence 	 0.0006920415224913495
and pragmatics 	 0.001445086705202312
<s> Machine 	 0.0023059185242121443
as multi-document 	 0.003484320557491289
in domains 	 0.0018726591760299626
's Mars 	 0.0196078431372549
are highly 	 0.004149377593360996
-LRB- MMR 	 0.0027100271002710027
, spoken 	 0.0005614823133071309
forecasts used 	 0.2
choices Designing 	 0.2
released `` 	 0.5
suffer . 	 1.0
lexicon , 	 0.1111111111111111
its understanding 	 0.02857142857142857
could search 	 0.0625
could recognize 	 0.0625
Examples include 	 0.3333333333333333
in 1984 	 0.0018726591760299626
target language 	 0.7272727272727273
at revealing 	 0.014705882352941176
classes Different 	 0.2
or otherwise 	 0.0045045045045045045
derived meaning 	 0.16666666666666666
Tannen , 	 1.0
was . 	 0.012987012987012988
is insufficient 	 0.0020325203252032522
lexicon and 	 0.1111111111111111
technology useful 	 0.045454545454545456
layer of 	 1.0
phonemes is 	 0.16666666666666666
voice-activation , 	 1.0
devoted to 	 0.6
methods more 	 0.022727272727272728
book a 	 0.125
learned model 	 0.2
only in 	 0.05263157894736842
might refer 	 0.038461538461538464
<s> By 	 0.0007686395080707148
complicated statistical 	 0.3333333333333333
the CoNLL 	 0.0006920415224913495
-RRB- Critical 	 0.0027100271002710027
are then 	 0.004149377593360996
Human-machine interaction 	 1.0
To address 	 0.1111111111111111
correct answer 	 0.06666666666666667
that represents 	 0.0035460992907801418
he proposed 	 0.14285714285714285
meaning `` 	 0.043478260869565216
has also 	 0.03571428571428571
transcription of 	 0.5
candidacies and 	 1.0
section needs 	 0.16666666666666666
pronouns with 	 0.5
to be 	 0.057104913678618856
voice in 	 0.07692307692307693
substantial resources 	 0.2
Z '' 	 1.0
ideas -RRB- 	 0.25
-RRB- languages 	 0.0027100271002710027
and waves 	 0.001445086705202312
first came 	 0.030303030303030304
the impact 	 0.001384083044982699
discover these 	 1.0
is relevant 	 0.0020325203252032522
to construct 	 0.0013280212483399733
Recall this 	 0.3333333333333333
, including 	 0.004491858506457047
lessened . 	 1.0
predicting star 	 0.5
, abstractive 	 0.0005614823133071309
input features 	 0.024390243902439025
early AI 	 0.2
How many 	 0.14285714285714285
contain periods 	 0.08333333333333333
evaluated by 	 0.14285714285714285
computer extracting 	 0.022727272727272728
sentences -RRB- 	 0.02631578947368421
produce textual 	 0.045454545454545456
a stationary 	 0.00245398773006135
questions asking 	 0.038461538461538464
the authors 	 0.0006920415224913495
humans as 	 0.08333333333333333
were based 	 0.024390243902439025
ELIZA gained 	 0.1111111111111111
grammar . 	 0.10810810810810811
1999 -RRB- 	 0.5
and see 	 0.001445086705202312
<s> Rules 	 0.0007686395080707148
on large 	 0.0047169811320754715
and semantic 	 0.004335260115606936
language input 	 0.02027027027027027
where one 	 0.02857142857142857
-LRB- VTLN 	 0.0027100271002710027
theory in 	 0.07692307692307693
interfaces such 	 0.5
operators need 	 1.0
computer , 	 0.022727272727272728
output from 	 0.038461538461538464
dialogue with 	 0.5
task can 	 0.023809523809523808
above 95 	 0.07692307692307693
-RRB- task-based 	 0.005420054200542005
modeling and 	 0.14285714285714285
e.g. stating 	 0.017857142857142856
focused solely 	 0.09090909090909091
readily reveal 	 0.3333333333333333
use by 	 0.027777777777777776
often required 	 0.022727272727272728
-LRB- Meehan 	 0.0027100271002710027
utilize an 	 0.5
same person 	 0.04
speech which 	 0.006578947368421052
of Business-card 	 0.00089126559714795
of research 	 0.0071301247771836
`` entities 	 0.005291005291005291
were undertaken 	 0.024390243902439025
the particular 	 0.001384083044982699
using either 	 0.01694915254237288
a fully 	 0.001226993865030675
rapidly changing 	 0.5
builds up 	 0.5
and LR 	 0.001445086705202312
a short-time 	 0.001226993865030675
text analytics 	 0.006289308176100629
on technology 	 0.0047169811320754715
annotation or 	 0.25
that has 	 0.02127659574468085
TextRank does 	 0.07142857142857142
known what 	 0.038461538461538464
phonetic segments 	 0.5
allowing greater 	 0.3333333333333333
parser and 	 0.0625
the fly 	 0.0006920415224913495
mechanism for 	 1.0
system operators 	 0.010752688172043012
business , 	 0.25
word use 	 0.016666666666666666
a news 	 0.00245398773006135
Statistical models 	 0.1111111111111111
installed defective 	 0.3333333333333333
table of 	 0.42857142857142855
harder to 	 0.2857142857142857
Mandarin and 	 1.0
Shared tasks 	 1.0
syllables but 	 0.5
Evaluation -RRB- 	 0.2222222222222222
legal word 	 0.3333333333333333
positive or 	 0.2857142857142857
regardless of 	 1.0
evaluation tests 	 0.037037037037037035
answering have 	 0.08333333333333333
to contain 	 0.0013280212483399733
the European 	 0.001384083044982699
simulates the 	 1.0
computerized text 	 0.5
Mention must 	 1.0
reduced . 	 0.5
greatly affect 	 0.14285714285714285
are popular 	 0.004149377593360996
1993 -RRB- 	 0.3333333333333333
, both 	 0.0016844469399213925
and right-most 	 0.001445086705202312
the common 	 0.0006920415224913495
, keyphrases 	 0.0005614823133071309
methods did 	 0.022727272727272728
a result 	 0.0036809815950920245
which merged 	 0.007246376811594203
these databases 	 0.023809523809523808
that for 	 0.0035460992907801418
Speech recognition 	 0.2903225806451613
complete sentences 	 1.0
Japanese camp 	 0.125
compare their 	 0.14285714285714285
possible unigrams 	 0.041666666666666664
Early versions 	 0.5
on either 	 0.0047169811320754715
and features 	 0.001445086705202312
have problems 	 0.009615384615384616
straightforward PCFGs 	 1.0
continuously rendered 	 1.0
a written-out 	 0.001226993865030675
process . 	 0.1388888888888889
than 1 	 0.022222222222222223
Meaningful Use 	 1.0
that generate 	 0.0035460992907801418
state-of-the-art abstractive 	 0.5
helicopters is 	 0.5
simply based 	 0.08333333333333333
and control 	 0.004335260115606936
-LRB- HAMS 	 0.0027100271002710027
accepts a 	 0.5
the Turney 	 0.0006920415224913495
the subjectivity 	 0.0006920415224913495
work from 	 0.041666666666666664
they take 	 0.025
house -LRB- 	 0.5
labeled as 	 0.3333333333333333
has continued 	 0.011904761904761904
is -LRB- 	 0.0040650406504065045
<s> Another 	 0.009992313604919293
Search to 	 0.5
a real 	 0.00245398773006135
other work 	 0.014285714285714285
Wetherell , 	 1.0
turns-at-talk . 	 1.0
are free 	 0.004149377593360996
Products , 	 0.5
of personalised 	 0.00089126559714795
on pattern 	 0.0047169811320754715
by ears 	 0.005714285714285714
, entering 	 0.0011229646266142617
a pollen 	 0.001226993865030675
, Reader 	 0.0005614823133071309
reasonable approximation 	 0.5
sources . 	 0.3333333333333333
boundary ' 	 0.16666666666666666
particular note 	 0.07692307692307693
simple procedure 	 0.038461538461538464
1977 -RRB- 	 1.0
Error Rate 	 0.5
expected for 	 0.14285714285714285
in-principle obstacles 	 1.0
discussed in 	 0.14285714285714285
would contribute 	 0.018867924528301886
'' of 	 0.005154639175257732
ICASSP , 	 1.0
machine reading 	 0.012658227848101266
learning such 	 0.023255813953488372
computer-aided language 	 0.3333333333333333
, studying 	 0.0005614823133071309
language model 	 0.006756756756756757
a limit 	 0.001226993865030675
specific error 	 0.047619047619047616
N in 	 0.3333333333333333
review of 	 0.3333333333333333
exclusively to 	 1.0
ASR . 	 0.16666666666666666
blocks to 	 0.25
linear algebra 	 0.14285714285714285
sound waves 	 0.05
These rules 	 0.058823529411764705
this sentence 	 0.01098901098901099
10 -RRB- 	 0.125
statistically-based speech 	 1.0
processing plain 	 0.018518518518518517
of scholars 	 0.00089126559714795
computer user 	 0.022727272727272728
ICR software 	 0.3333333333333333
The features 	 0.005208333333333333
very large 	 0.024390243902439025
program a 	 0.09090909090909091
while capturing 	 0.05
a superset 	 0.001226993865030675
jet fighter 	 1.0
waves describe 	 0.14285714285714285
is clearly 	 0.0040650406504065045
, separate 	 0.0011229646266142617
summaries but 	 0.023255813953488372
Mars Polar 	 0.5
systems : 	 0.008928571428571428
and achieved 	 0.001445086705202312
the summarization 	 0.0020761245674740486
people or 	 0.0625
`` President 	 0.005291005291005291
is able 	 0.0020325203252032522
to help 	 0.0026560424966799467
data . 	 0.22077922077922077
laws calling 	 1.0
well it 	 0.07142857142857142
of artifacts 	 0.00089126559714795
the rules 	 0.0034602076124567475
the ones 	 0.001384083044982699
extension of 	 1.0
single binary 	 0.07142857142857142
significant task 	 0.1111111111111111
Santoni B. 	 1.0
so we 	 0.03333333333333333
scoring Truecasing 	 0.5
Amharic and 	 1.0
, processing 	 0.0005614823133071309
` local 	 0.0625
source for 	 0.041666666666666664
DTW . 	 0.3333333333333333
or worse 	 0.0045045045045045045
' . 	 0.10526315789473684
frequencies , 	 0.5
known . 	 0.038461538461538464
the US 	 0.001384083044982699
and required 	 0.001445086705202312
is both 	 0.0020325203252032522
keywords , 	 0.5
understanding approximates 	 0.030303030303030304
not new 	 0.008928571428571428
textbook is 	 0.5
logic oriented 	 0.25
intermediary representation 	 0.6666666666666666
Homayoon Beigi 	 1.0
, most 	 0.004491858506457047
keyphrases of 	 0.02857142857142857
book on 	 0.125
the potentially 	 0.0006920415224913495
texts seems 	 0.058823529411764705
intonation , 	 1.0
a dictionary 	 0.0036809815950920245
cursive handwriting 	 0.2
stating that 	 1.0
cosine similarity 	 0.3333333333333333
two approaches 	 0.034482758620689655
physicians who 	 1.0
analyst is 	 1.0
already placed 	 0.2
tagged '' 	 0.6666666666666666
workload , 	 1.0
accepted , 	 1.0
but in 	 0.029411764705882353
and relevant 	 0.001445086705202312
and sub-categories 	 0.001445086705202312
and Arabic 	 0.002890173410404624
- Top-down 	 0.0625
provides additional 	 0.5
In particular 	 0.02857142857142857
where natural 	 0.02857142857142857
whole . 	 0.1111111111111111
resulting in 	 0.25
like relevance 	 0.03571428571428571
it helps 	 0.008547008547008548
ten-year-long research 	 1.0
recommendations and 	 1.0
summarization faces 	 0.02
, consisting 	 0.0005614823133071309
that combines 	 0.0035460992907801418
sense and 	 0.125
given rise 	 0.041666666666666664
like that 	 0.03571428571428571
grammars of 	 0.07142857142857142
Arabic -RRB- 	 0.25
edges after 	 0.14285714285714285
e-communities through 	 0.5
an error 	 0.007575757575757576
sentiments expressed 	 1.0
Since 2000 	 0.2
for those 	 0.007220216606498195
to cope 	 0.0013280212483399733
, relationship 	 0.0005614823133071309
NLP task 	 0.02127659574468085
<s> Read 	 0.0007686395080707148
achieves its 	 0.5
of 500 	 0.0017825311942959
discourse and 	 0.1111111111111111
up differently 	 0.045454545454545456
similar to 	 0.5555555555555556
E. Longacre 	 0.5
output nodes 	 0.07692307692307693
In 2004 	 0.009523809523809525
very dependent 	 0.024390243902439025
Given enough 	 0.07142857142857142
assistants such 	 1.0
<s> Unsourced 	 0.0007686395080707148
Hybrid machine 	 0.5
selecting and 	 0.2
that difference 	 0.0035460992907801418
Tagset '' 	 1.0
dissertation at 	 0.3333333333333333
hard task 	 0.16666666666666666
comprises all 	 1.0
Michael Dyer 	 0.25
simple keyword 	 0.038461538461538464
by whom 	 0.005714285714285714
edited and 	 1.0
end a 	 0.125
Robert de 	 0.25
lengths -RRB- 	 1.0
this , 	 0.04395604395604396
not present 	 0.026785714285714284
script used 	 0.25
network is 	 0.16666666666666666
to decide 	 0.0026560424966799467
<s> e.g. 	 0.0015372790161414297
software is 	 0.07407407407407407
-LRB- DeRose 	 0.0027100271002710027
`` to 	 0.010582010582010581
equipment based 	 0.3333333333333333
spaces , 	 0.2
nice properties 	 0.25
in word 	 0.0018726591760299626
that users 	 0.0035460992907801418
also concluded 	 0.014492753623188406
pre - 	 1.0
the hypothesis 	 0.0006920415224913495
equivalent set 	 0.2
news articles 	 0.23076923076923078
, size 	 0.0005614823133071309
tasks include 	 0.03125
<s> He 	 0.005380476556495004
abstraction involves 	 0.25
, audio 	 0.0011229646266142617
POS -RRB- 	 0.07692307692307693
In France 	 0.01904761904761905
2500 articles 	 1.0
Corpus -LRB- 	 0.0625
together . 	 0.125
, Brill 	 0.0005614823133071309
'' the 	 0.005154639175257732
written for 	 0.038461538461538464
progressed over 	 1.0
and placed 	 0.001445086705202312
large-scale content-analysis 	 1.0
single sentence 	 0.07142857142857142
systems dynamically 	 0.008928571428571428
`` sad 	 0.005291005291005291
and statistics 	 0.002890173410404624
languages semantics 	 0.02
a construct 	 0.001226993865030675
Canada are 	 0.16666666666666666
the potential 	 0.0020761245674740486
semantic constraints 	 0.047619047619047616
algorithms one 	 0.02857142857142857
English-like syntax 	 0.3333333333333333
constructed , 	 0.5
can be 	 0.5027624309392266
pre-structured database 	 1.0
attempts have 	 0.16666666666666666
typically include 	 0.05555555555555555
are expected 	 0.004149377593360996
other than 	 0.014285714285714285
to it 	 0.0013280212483399733
the ARNS 	 0.0006920415224913495
of sound 	 0.0017825311942959
working in 	 0.2857142857142857
are arranged 	 0.004149377593360996
standard result 	 0.07142857142857142
characters using 	 0.0625
productions . 	 1.0
is functioning 	 0.0020325203252032522
authors claimed 	 0.4
comparison , 	 0.3333333333333333
, George 	 0.0005614823133071309
readers . 	 0.5
method based 	 0.125
pursued after 	 1.0
document\/text summarization 	 0.5
speaker as 	 0.05555555555555555
2009 -LRB- 	 0.3333333333333333
summaries qualitatively 	 0.023255813953488372
be unrealistically 	 0.004219409282700422
estimate the 	 0.5
discussed involve 	 0.14285714285714285
on broad 	 0.0047169811320754715
be compared 	 0.004219409282700422
, making 	 0.0005614823133071309
`` right 	 0.005291005291005291
natural as 	 0.013333333333333334
threshold to 	 0.25
<s> Neural 	 0.0015372790161414297
are obtained 	 0.008298755186721992
despite warnings 	 0.3333333333333333
One of 	 0.15384615384615385
as PC 	 0.003484320557491289
limited in 	 0.1
English grammars 	 0.02702702702702703
information is 	 0.043478260869565216
semantics , 	 0.2857142857142857
DTW -RRB- 	 0.3333333333333333
answering . 	 0.16666666666666666
analyses to 	 0.2
to them 	 0.0026560424966799467
see computational 	 0.05
information that 	 0.021739130434782608
<s> Results 	 0.0007686395080707148
1991 A 	 0.3333333333333333
James Deese 	 0.25
and 2500 	 0.001445086705202312
Around the 	 1.0
companies -LRB- 	 0.5
brain . 	 0.3333333333333333
-RRB- represents 	 0.0027100271002710027
different related 	 0.02040816326530612
Business-card OCR 	 1.0
importance would 	 0.16666666666666666
deep understanding 	 0.2857142857142857
expectancy of 	 1.0
tasks -LRB- 	 0.03125
1952 and 	 0.5
extraction is 	 0.06451612903225806
turned into 	 1.0
is subjectivity\/objectivity 	 0.0020325203252032522
contains a 	 0.1
overlap . 	 0.25
form logical 	 0.05
using a 	 0.1694915254237288
-LRB- with 	 0.008130081300813009
other academic 	 0.014285714285714285
performance continued 	 0.05555555555555555
various NLP 	 0.05555555555555555
negative or 	 0.125
or structured 	 0.0045045045045045045
an hour 	 0.007575757575757576
, sentiment 	 0.0005614823133071309
, depends 	 0.0005614823133071309
this graph 	 0.01098901098901099
-LRB- RAE 	 0.0027100271002710027
artificial neural 	 0.09090909090909091
than supervised 	 0.022222222222222223
with it 	 0.01092896174863388
and worked 	 0.001445086705202312
the Technolangue\/Easy 	 0.0006920415224913495
ratings are 	 0.2222222222222222
<s> Accuracy 	 0.003843197540353574
ratings produced 	 0.1111111111111111
comprehensive model 	 0.2
English has 	 0.05405405405405406
a corpus 	 0.0036809815950920245
priorities . 	 1.0
digitize the 	 1.0
7 in 	 0.2857142857142857
target -LRB- 	 0.09090909090909091
you can 	 0.15384615384615385
<s> → 	 0.0007686395080707148
lack of 	 1.0
models of 	 0.038461538461538464
The only 	 0.010416666666666666
the importance 	 0.001384083044982699
of document 	 0.00089126559714795
: Translation 	 0.00980392156862745
respectively . 	 1.0
technology in 	 0.045454545454545456
boundaries . 	 0.36363636363636365
like HTML 	 0.03571428571428571
counts are 	 1.0
In 2002 	 0.009523809523809525
Multimodal interaction 	 1.0
removing stopwords 	 0.5
strengths of 	 0.5
<s> Their 	 0.0015372790161414297
desired identification 	 0.2
confusability Speaker 	 1.0
preclude using 	 1.0
such keyphrases 	 0.008130081300813009
be also 	 0.004219409282700422
digital speech-to-text 	 0.14285714285714285
used speech 	 0.008849557522123894
linear-time versions 	 1.0
optimized for 	 1.0
examples are 	 0.041666666666666664
in Star 	 0.0018726591760299626
speaking speeds 	 0.125
the advent 	 0.0006920415224913495
thirty years 	 1.0
on Shepard 	 0.0047169811320754715
, different 	 0.0016844469399213925
of using 	 0.00089126559714795
or in 	 0.0045045045045045045
<s> Furthermore 	 0.004611837048424289
-RRB- NASA 	 0.0027100271002710027
grammar in 	 0.02702702702702703
made up 	 0.0625
the informativeness 	 0.0006920415224913495
these problems 	 0.023809523809523808
structures , 	 0.2
which entertaining 	 0.007246376811594203
arm to 	 1.0
however empirical 	 0.07692307692307693
contractions , 	 0.5
phonemes . 	 0.16666666666666666
a non-whitespace 	 0.001226993865030675
sequences are 	 0.1111111111111111
of key 	 0.00089126559714795
by grid 	 0.005714285714285714
, medicine 	 0.0005614823133071309
Knowing this 	 1.0
Establishment -LRB- 	 1.0
though much 	 0.1
simply requires 	 0.08333333333333333
life experience 	 0.25
However the 	 0.02702702702702703
Summarization of 	 0.25
<s> Technologies 	 0.0007686395080707148
written text 	 0.11538461538461539
the phrase 	 0.002768166089965398
integrated into 	 0.3333333333333333
This unreferenced 	 0.015873015873015872
-- is 	 0.04
a dialogue 	 0.00245398773006135
product reviews 	 0.14285714285714285
Another term 	 0.07692307692307693
tasks are 	 0.125
whether to 	 0.07692307692307693
unfortunately , 	 1.0
<s> Knowledge 	 0.0007686395080707148
for speech 	 0.01444043321299639
summarise electronic 	 0.3333333333333333
automate about 	 0.3333333333333333
System -RRB- 	 1.0
TaleSpin -LRB- 	 1.0
have humans 	 0.009615384615384616
and is 	 0.008670520231213872
<s> Individuals 	 0.0007686395080707148
your head 	 0.5
question domain 	 0.023809523809523808
Canada to 	 0.16666666666666666
nascent online 	 1.0
prose text 	 1.0
much funding 	 0.045454545454545456
Deferred speech 	 1.0
democracy . 	 1.0
This was 	 0.015873015873015872
take advantage 	 0.4
likely not 	 0.0625
method that 	 0.0625
Sept. 1955 	 1.0
we register 	 0.022222222222222223
rise to 	 0.5
<s> Of 	 0.0007686395080707148
simple morphology 	 0.038461538461538464
later users 	 0.1
<s> Deep 	 0.0007686395080707148
cases and 	 0.05555555555555555
so on 	 0.16666666666666666
required many 	 0.14285714285714285
when moved 	 0.02857142857142857
understand why 	 0.14285714285714285
formalization of 	 0.5
<s> Word 	 0.003843197540353574
program , 	 0.045454545454545456
science -LRB- 	 0.1
approached in 	 0.5
a rich 	 0.0036809815950920245
alphabetic heritage 	 1.0
tagset by 	 1.0
model for 	 0.06666666666666667
assignment . 	 0.5
where formal 	 0.02857142857142857
entries for 	 0.5
the specification 	 0.001384083044982699
semantic interpretation 	 0.047619047619047616
the individual 	 0.001384083044982699
recognition , 	 0.11570247933884298
humans often 	 0.08333333333333333
Koine Greek 	 1.0
1964 to 	 1.0
in reconfiguring 	 0.0018726591760299626
<s> Substantial 	 0.0007686395080707148
the completion 	 0.0006920415224913495
either manually 	 0.1
not resulted 	 0.008928571428571428
media such 	 0.16666666666666666
1950 , 	 1.0
a rules 	 0.001226993865030675
an extrinsic 	 0.007575757575757576
, `` 	 0.01403705783267827
read text 	 0.14285714285714285
Dog bites 	 1.0
role the 	 0.25
at language 	 0.014705882352941176
documents have 	 0.02631578947368421
Query expansion 	 1.0
probably `` 	 0.25
to Xerox 	 0.0013280212483399733
Each level 	 0.16666666666666666
98 % 	 1.0
as business 	 0.003484320557491289
in embedded 	 0.0018726591760299626
Treebank data 	 0.16666666666666666
ontology are 	 0.5
Why do 	 0.14285714285714285
Grows : 	 1.0
articles rarely 	 0.125
and techniques 	 0.001445086705202312
Aviation Authorities 	 1.0
successful HMM-based 	 0.1111111111111111
to unigram 	 0.0013280212483399733
on pilot 	 0.0047169811320754715
multiscript texts 	 1.0
Pallet D.S. 	 0.5
learning algorithm 	 0.11627906976744186
, who 	 0.0005614823133071309
factors affect 	 0.3333333333333333
been much 	 0.014705882352941176
of 1 	 0.00089126559714795
with morphological 	 0.00546448087431694
grammar Text 	 0.02702702702702703
or verb 	 0.0045045045045045045
summaries using 	 0.023255813953488372
cutoff to 	 1.0
the surrounding 	 0.001384083044982699
in use 	 0.003745318352059925
Thai do 	 0.5
cursive text 	 0.2
protect New 	 1.0
, graphic 	 0.0005614823133071309
of morphologically 	 0.00089126559714795
to measure 	 0.005312084993359893
only five 	 0.02631578947368421
hand-compiled list 	 1.0
`` Japanese 	 0.005291005291005291
details of 	 0.5
methods assess 	 0.022727272727272728
7 across 	 0.42857142857142855
translation tries 	 0.013513513513513514
7 distinct 	 0.14285714285714285
Asia Online 	 1.0
match between 	 0.16666666666666666
OCR '' 	 0.04081632653061224
are unambiguous 	 0.004149377593360996
has thousands 	 0.011904761904761904
invented examples 	 0.5
to eigenvalue 	 0.0013280212483399733
of global 	 0.00089126559714795
Heritage , 	 1.0
The Army 	 0.005208333333333333
important distinction 	 0.125
data of 	 0.012987012987012988
Harris 1991 	 0.1111111111111111
Issues While 	 0.5
recognition technology 	 0.008264462809917356
has two 	 0.011904761904761904
Pang and 	 0.3333333333333333
Statistical techniques 	 0.1111111111111111
, voice-activation 	 0.0005614823133071309
condition that 	 1.0
good translation 	 0.07692307692307693
as decision 	 0.010452961672473868
<s> Battle 	 0.0007686395080707148
to produce 	 0.013280212483399735
exigencies of 	 1.0
fighter applications 	 0.16666666666666666
Most modern 	 0.5
rate crossed 	 0.09090909090909091
notations of 	 0.5
importance of 	 0.5
likely be 	 0.0625
use Machine 	 0.013888888888888888
models derived 	 0.038461538461538464
wrote a 	 0.16666666666666666
the design 	 0.001384083044982699
source text 	 0.20833333333333334
written languages 	 0.19230769230769232
EVALITA web 	 0.5
containing these 	 0.125
to tell 	 0.0013280212483399733
<s> Edges 	 0.0015372790161414297
NLP problem 	 0.0425531914893617
returned from 	 0.5
simply focused 	 0.08333333333333333
abbreviation , 	 0.5
is no 	 0.0020325203252032522
, Flickinger 	 0.0005614823133071309
paying attention 	 1.0
'' , 	 0.15463917525773196
area are 	 0.09090909090909091
its entirety 	 0.02857142857142857
or using 	 0.009009009009009009
of pollen 	 0.0017825311942959
500 texts 	 0.5
<s> See 	 0.0007686395080707148
human-made summaries 	 0.5
tests , 	 0.25
expression which 	 0.1
shallow approach 	 0.3333333333333333
, which 	 0.031443009545199324
was first 	 0.012987012987012988
varies greatly 	 1.0
the magazine 	 0.0006920415224913495
purpose when 	 0.2
speech tagging 	 0.013157894736842105
although these 	 0.16666666666666666
instead recognizes 	 0.14285714285714285
customisation by 	 1.0
or uttered 	 0.0045045045045045045
through a 	 0.25
Association for 	 1.0
not its 	 0.008928571428571428
training '' 	 0.03571428571428571
the Turing 	 0.0006920415224913495
-RRB- - 	 0.0027100271002710027
same column 	 0.04
above are 	 0.07692307692307693
methods build 	 0.022727272727272728
of multimedia 	 0.00089126559714795
field comes 	 0.037037037037037035
different distances 	 0.02040816326530612
non-existent words 	 1.0
terms in 	 0.07692307692307693
information needed 	 0.021739130434782608
the purpose 	 0.001384083044982699
He then 	 0.125
V.J. , 	 1.0
, producing 	 0.0005614823133071309
and combine 	 0.001445086705202312
. <s/> 	 0.9875195007800313
essentially perfectly 	 0.125
the two 	 0.0034602076124567475
and length 	 0.001445086705202312
Some speech 	 0.047619047619047616
well the 	 0.03571428571428571
Avionics Research 	 1.0
the meantime 	 0.0006920415224913495
pumps '' 	 0.5
and Chinese 	 0.001445086705202312
important part 	 0.0625
as models 	 0.003484320557491289
the basis 	 0.002768166089965398
to identify 	 0.006640106241699867
all get 	 0.023255813953488372
Recognize if 	 1.0
new scientific 	 0.041666666666666664
general ontologies 	 0.045454545454545456
hand-crafted knowledge 	 0.5
decide to 	 0.25
psychotherapy . 	 1.0
, not 	 0.0039303761931499155
A number 	 0.06
is sometimes 	 0.006097560975609756
Results have 	 1.0
learning algorithms 	 0.11627906976744186
or may 	 0.009009009009009009
-LRB- , 	 0.0027100271002710027
vocabulary and 	 0.25
methods and 	 0.022727272727272728
prior ranking 	 0.3333333333333333
'' other 	 0.005154639175257732
negative up 	 0.125
often under 	 0.022727272727272728
lexicon reached 	 0.1111111111111111
conditions Environmental 	 0.2
the hidden 	 0.0006920415224913495
`` Natural 	 0.005291005291005291
shape of 	 1.0
investigates the 	 1.0
directly . 	 0.2
, statement 	 0.0005614823133071309
its polarity 	 0.02857142857142857
Processes may 	 1.0
human . 	 0.06521739130434782
and Web 	 0.001445086705202312
In other 	 0.01904761904761905
then taking 	 0.02857142857142857
virtually impossible 	 0.5
anaphora . 	 1.0
sometimes referred 	 0.23076923076923078
Cross-Sentence Information 	 1.0
usually creating 	 0.03125
mixture of 	 1.0
as 7 	 0.003484320557491289
similar contexts 	 0.037037037037037035
tagging work 	 0.04
to impersonate 	 0.0013280212483399733
release parameters 	 0.3333333333333333
final keyphrases 	 0.2222222222222222
speech and 	 0.013157894736842105
of reasoned 	 0.00089126559714795
have shown 	 0.009615384615384616
analysis on 	 0.015384615384615385
Descartes proposed 	 1.0
being conducted 	 0.05555555555555555
that connects 	 0.0035460992907801418
recognize the 	 0.4444444444444444
1978 Kurzweil 	 0.3333333333333333
different problem 	 0.02040816326530612
Units -LRB- 	 1.0
making more 	 0.14285714285714285
by statistics 	 0.005714285714285714
of computational 	 0.00267379679144385
Cuzco area 	 1.0
and isolated 	 0.001445086705202312
adaptation . 	 0.6666666666666666
to test 	 0.0026560424966799467
must produce 	 0.07142857142857142
verifiability . 	 1.0
The most 	 0.026041666666666668
getting enough 	 0.25
as knowledge 	 0.003484320557491289
all lower 	 0.023255813953488372
U.S. Patent 	 0.42857142857142855
other NLP 	 0.014285714285714285
were conducted 	 0.024390243902439025
measures how 	 0.3333333333333333
generally rely 	 0.09090909090909091
sets of 	 0.36363636363636365
of 98 	 0.00089126559714795
We also 	 0.14285714285714285
than text 	 0.022222222222222223
Poncini , 	 1.0
First summarizes 	 1.0
levels are 	 0.045454545454545456
a commercial 	 0.00245398773006135
combination hidden 	 0.2
or ` 	 0.0045045045045045045
years long 	 0.047619047619047616
Performance The 	 1.0
when processed 	 0.02857142857142857
included question-answering 	 0.125
probability , 	 0.14285714285714285
of co-articulation 	 0.00089126559714795
A useful 	 0.02
active research 	 0.5
allow a 	 0.2
or form 	 0.0045045045045045045
within a 	 0.2777777777777778
technologies for 	 0.25
often has 	 0.045454545454545456
phrase ` 	 0.1
or sometimes 	 0.0045045045045045045
polarity '' 	 0.25
a team 	 0.001226993865030675
naval battle 	 0.3333333333333333
sensible manner 	 1.0
related questions 	 0.06666666666666667
TWA where 	 1.0
Ideally , 	 1.0
source code 	 0.041666666666666664
the human-readable 	 0.0006920415224913495
`` centrality 	 0.005291005291005291
President Bush 	 0.5
believed that 	 1.0
subject -RRB- 	 0.125
Baum-Welch algorithm 	 1.0
-RRB- the 	 0.0027100271002710027
could therefore 	 0.0625
into consideration 	 0.01282051282051282
An automated 	 0.0625
search engines 	 0.18181818181818182
well that 	 0.03571428571428571
itself to 	 0.2
topics automatically 	 0.14285714285714285
Northern Isles 	 0.6666666666666666
Analysis and 	 0.2
Mobile telephony 	 0.3333333333333333
recursion in 	 1.0
each feature\/aspect 	 0.022222222222222223
the current 	 0.001384083044982699
`` STT 	 0.005291005291005291
number on 	 0.023255813953488372
sense disambiguation 	 0.25
Parsing is 	 0.4
although usually 	 0.16666666666666666
characters \* 	 0.0625
or fuse 	 0.0045045045045045045
many instances 	 0.019230769230769232
questions or 	 0.038461538461538464
but robustness 	 0.014705882352941176
a vertex 	 0.00245398773006135
search by 	 0.09090909090909091
computer read 	 0.022727272727272728
Command Success 	 0.5
conversational content 	 1.0
-LRB- normalized 	 0.0027100271002710027
formed the 	 0.2
71 % 	 1.0
emergence of 	 1.0
simple tasks 	 0.038461538461538464
a basic 	 0.00245398773006135
its nascent 	 0.02857142857142857
as debates 	 0.003484320557491289
, styles 	 0.0005614823133071309
score if 	 0.16666666666666666
whether it 	 0.07692307692307693
processed with 	 0.3333333333333333
chunks of 	 1.0
or match 	 0.0045045045045045045
stemming -RRB- 	 0.5
sentences Grass 	 0.013157894736842105
matter . 	 0.3333333333333333
is hard 	 0.0020325203252032522
for French 	 0.010830324909747292
and Grass 	 0.001445086705202312
, sometimes 	 0.0005614823133071309
Using the 	 0.5
life . 	 0.25
start experimenting 	 0.14285714285714285
the nautical 	 0.0006920415224913495
according to 	 1.0
of life 	 0.00089126559714795
, are 	 0.0011229646266142617
When used 	 0.14285714285714285
base , 	 0.5
that shift 	 0.0035460992907801418
properly the 	 0.5
own it 	 0.16666666666666666
nasality , 	 1.0
sentence transformations 	 0.020833333333333332
-LRB- CSIS 	 0.0027100271002710027
for keyphrase 	 0.0036101083032490976
paradigm includes 	 0.3333333333333333
describe developments 	 0.16666666666666666
simple voice 	 0.038461538461538464
a huge 	 0.001226993865030675
gather information 	 1.0
tasks , 	 0.125
keeping a 	 0.5
unsupervised `` 	 0.125
, opened 	 0.0005614823133071309
of negative 	 0.00089126559714795
summarization , 	 0.08
the reported 	 0.0006920415224913495
used more 	 0.008849557522123894
learned from 	 0.2
% on 	 0.02564102564102564
making decisions 	 0.14285714285714285
multiple source 	 0.07692307692307693
on both 	 0.0047169811320754715
of pilot 	 0.00089126559714795
manner rather 	 0.25
20 % 	 1.0
introduction of 	 1.0
language and 	 0.013513513513513514
recommend '' 	 1.0
are structured 	 0.004149377593360996
of London 	 0.00089126559714795
the document 	 0.004152249134948097
, even 	 0.0039303761931499155
sentence Grass 	 0.020833333333333332
tested the 	 0.5
threshold or 	 0.5
talking about 	 1.0
only complete 	 0.02631578947368421
disease , 	 1.0
Mr. is 	 0.5
a display 	 0.001226993865030675
banking system 	 1.0
corpus of 	 0.22580645161290322
coefficients . 	 0.25
of hand-printed 	 0.0017825311942959
medium or 	 0.3333333333333333
Petrov , 	 1.0
extrinsic performance 	 0.16666666666666666
dynamically creating 	 0.5
invention of 	 1.0
translation can 	 0.02702702702702703
together with 	 0.125
problems , 	 0.35294117647058826
the preceding 	 0.0006920415224913495
after stemming 	 0.08333333333333333
derived by 	 0.3333333333333333
world with 	 0.06666666666666667
and are 	 0.0072254335260115606
fine-grained analysis 	 1.0
of international 	 0.00089126559714795
with high 	 0.01092896174863388
's coherence 	 0.0196078431372549
Mr. Smith 	 0.5
, similarity 	 0.0005614823133071309
project , 	 0.38461538461538464
also attempt 	 0.014492753623188406
Black-box evaluation 	 0.5
grammar methods 	 0.02702702702702703
, real-time 	 0.0005614823133071309
the machine-learning 	 0.0006920415224913495
OCR . 	 0.02040816326530612
simply guessed 	 0.08333333333333333
emoticons , 	 1.0
of descriptive 	 0.00089126559714795
Pang who 	 0.3333333333333333
explicit features 	 0.2
the historical 	 0.0006920415224913495
the overriding 	 0.0006920415224913495
specialist textbook 	 1.0
, decimal 	 0.0005614823133071309
feature . 	 0.15384615384615385
English like 	 0.02702702702702703
to perform 	 0.005312084993359893
lower level 	 0.4
Street Journal 	 0.6666666666666666
An important 	 0.125
up an 	 0.045454545454545456
linguist working 	 0.5
that says 	 0.0035460992907801418
are likely 	 0.016597510373443983
, Naive 	 0.0005614823133071309
his visit 	 0.08333333333333333
much less 	 0.045454545454545456
their stationary 	 0.029411764705882353
real world 	 0.3333333333333333
<s> Unsupervised 	 0.003843197540353574
arranged hierarchically 	 1.0
-LRB- IMR 	 0.0027100271002710027
with a 	 0.1092896174863388
explanation , 	 1.0
Hollenbach 1970 	 1.0
error analysis 	 0.08333333333333333
a lunar 	 0.001226993865030675
used by 	 0.07964601769911504
to medical 	 0.0013280212483399733
entered John 	 0.5
sentences for 	 0.013157894736842105
script . 	 0.5
sentiment . 	 0.04
among other 	 0.375
NLP methods 	 0.02127659574468085
combining those 	 0.25
rules : 	 0.046511627906976744
probability what 	 0.14285714285714285
Hard to 	 0.5
conflicting objectives 	 1.0
process to 	 0.1111111111111111
i.e. requiring 	 0.05263157894736842
away -LRB- 	 0.5
domain ontologies 	 0.1
parser can 	 0.0625
As described 	 0.05555555555555555
defining programming 	 1.0
consists of 	 1.0
predicate logic 	 1.0
but it 	 0.058823529411764705
` hitcha 	 0.0625
analyze ` 	 0.25
library , 	 0.5
-RRB- BioCreative 	 0.0027100271002710027
from printed 	 0.009615384615384616
documents per 	 0.02631578947368421
Xerox eventually 	 0.5
and naturalness 	 0.001445086705202312
English or 	 0.02702702702702703
understanding in 	 0.030303030303030304
questioned the 	 1.0
with human 	 0.01092896174863388
an entity 	 0.007575757575757576
discrete terms 	 0.3333333333333333
of identifiers 	 0.00089126559714795
and widely 	 0.001445086705202312
more strongly 	 0.010526315789473684
includes a 	 0.14285714285714285
and Unsupervised 	 0.001445086705202312
Envelopes may 	 1.0
following words 	 0.06666666666666667
areas of 	 0.3333333333333333
of Speaker 	 0.00089126559714795
<s> Warren 	 0.0007686395080707148
Some unsupervised 	 0.047619047619047616
clues not 	 0.3333333333333333
measure -LRB- 	 0.09090909090909091
Morpheme Analysis 	 1.0
`` main 	 0.005291005291005291
pumps last 	 0.5
toolkit -RRB- 	 0.5
paper documents 	 0.09090909090909091
second layer 	 0.2
syntax and 	 0.09090909090909091
input six 	 0.024390243902439025
'' . 	 0.06701030927835051
worked on 	 0.4
even articles 	 0.037037037037037035
attribute grammars 	 0.5
hearings -RRB- 	 1.0
ostensibly simple 	 1.0
select individual 	 0.16666666666666666
mechanisms , 	 0.5
the mid-1960s 	 0.0006920415224913495
static shape 	 1.0
to involved 	 0.0013280212483399733
, T 	 0.0005614823133071309
The paradigm 	 0.005208333333333333
then characterized 	 0.02857142857142857
a human 	 0.013496932515337423
smoothing to 	 1.0
Inc. and 	 0.5
Automatically translate 	 1.0
different levels 	 0.02040816326530612
the research 	 0.0020761245674740486
Auto plant 	 1.0
capitalized , 	 0.6666666666666666
distinction , 	 0.2
parsers are 	 0.15384615384615385
by silence 	 0.005714285714285714
manually designed 	 0.25
with American 	 0.00546448087431694
only a 	 0.05263157894736842
which many 	 0.007246376811594203
psychology Response 	 0.25
`` computer 	 0.005291005291005291
Trek . 	 1.0
described as 	 0.3333333333333333
to closely 	 0.0013280212483399733
speech to 	 0.019736842105263157
other scripts 	 0.014285714285714285
template containing 	 0.25
Sublanguage analysis 	 1.0
the advantage 	 0.0006920415224913495
of information 	 0.004456327985739751
However some 	 0.02702702702702703
deep '' 	 0.14285714285714285
, Cynthia 	 0.0005614823133071309
using algorithms 	 0.01694915254237288
which found 	 0.014492753623188406
`` bag 	 0.005291005291005291
first raised 	 0.030303030303030304
as closed 	 0.003484320557491289
notion of 	 0.75
input feature 	 0.024390243902439025
availability of 	 1.0
different similarity 	 0.02040816326530612
map one 	 0.5
analyze a 	 0.25
seminal paper 	 1.0
shared concepts 	 0.5
some programming 	 0.012048192771084338
the Amount 	 0.0006920415224913495
keyphrases can 	 0.05714285714285714
anything , 	 1.0
30 % 	 0.3333333333333333
planning and 	 0.5
criteria and 	 0.25
response , 	 0.5
rank individual 	 0.16666666666666666
grammatical constituents 	 0.09090909090909091
Latin very 	 0.25
summarization -LRB- 	 0.04
faster computers 	 0.3333333333333333
Markov chain 	 0.05555555555555555
, Jonathan 	 0.0005614823133071309
indicate that 	 0.3333333333333333
what the 	 0.125
dictionary . 	 0.14285714285714285
Edward Robinson 	 1.0
alignment software 	 0.5
GALE project 	 1.0
for 1-July-2005 	 0.0036101083032490976
other to 	 0.014285714285714285
discussions about 	 0.3333333333333333
implementations of 	 1.0
<s> First 	 0.0007686395080707148
found most 	 0.07142857142857142
publication devoted 	 0.3333333333333333
grow without 	 1.0
Loriot & 	 1.0
input character 	 0.024390243902439025
e.g. pictures 	 0.017857142857142856
analysis was 	 0.03076923076923077
information -LRB- 	 0.021739130434782608
possible answers 	 0.041666666666666664
not easily 	 0.026785714285714284
, abstraction 	 0.0005614823133071309
to book 	 0.0013280212483399733
they must 	 0.025
still disagree 	 0.06666666666666667
of training 	 0.0035650623885918
approaches Supervised 	 0.03571428571428571
based , 	 0.037037037037037035
it must 	 0.008547008547008548
National Federation 	 0.3333333333333333
known keyphrase 	 0.038461538461538464
grammar because 	 0.02702702702702703
plural noun 	 0.4
noise pertain 	 0.125
Some tag 	 0.047619047619047616
whereas when 	 0.3333333333333333
When processing 	 0.14285714285714285
it will 	 0.017094017094017096
, multi-document 	 0.0005614823133071309
time-consuming and 	 0.3333333333333333
notably to 	 0.3333333333333333
1952 . 	 0.5
3 +4 	 0.2
full sentenced 	 0.2
discriminate because 	 0.3333333333333333
most research 	 0.017241379310344827
of Question 	 0.00089126559714795
enumerate every 	 1.0
summarization hopes 	 0.02
invalid constructs 	 1.0
meanings depending 	 0.25
rules -- 	 0.023255813953488372
logical assertions 	 0.16666666666666666
a typical 	 0.00245398773006135
fraction of 	 1.0
sentence there 	 0.020833333333333332
, length 	 0.0005614823133071309
lexical segmentation 	 0.07692307692307693
sample corpus 	 0.3333333333333333
quite similar 	 0.125
sufficiently well 	 1.0
, opens 	 0.0005614823133071309
to disseminate 	 0.0013280212483399733
asked within 	 0.3333333333333333
connected Web 	 0.2
comprehension . 	 0.42857142857142855
keyphrases . 	 0.3142857142857143
algorithms requires 	 0.02857142857142857
most spoken 	 0.034482758620689655
dissertation . 	 0.3333333333333333
U.S. Department 	 0.14285714285714285
-LRB- Schank 	 0.0027100271002710027
Computing + 	 0.5
and the 	 0.059248554913294796
tagged as 	 0.3333333333333333
can present 	 0.0055248618784530384
non-trivial , 	 0.5
, volume 	 0.0005614823133071309
programming algorithms 	 0.2
HMM parameter 	 0.3333333333333333
lowering of 	 1.0
<s> Digitized 	 0.0007686395080707148
the label 	 0.0006920415224913495
a Rogerian 	 0.001226993865030675
subtasks . 	 0.5
noun than 	 0.07142857142857142
purpose graph-based 	 0.4
Jay Lemke 	 1.0
documents than 	 0.02631578947368421
HMMs , 	 0.125
time -LRB- 	 0.06060606060606061
's results 	 0.0196078431372549
broken English 	 0.2
to treat 	 0.0013280212483399733
approaches can 	 0.03571428571428571
<s> On 	 0.003843197540353574
as keyphrases 	 0.003484320557491289
to determining 	 0.0013280212483399733
or two 	 0.009009009009009009
with deep 	 0.00546448087431694
a period 	 0.00245398773006135
Regardless of 	 1.0
hitcha ' 	 1.0
the expectancy 	 0.0006920415224913495
<s> Solutions 	 0.0007686395080707148
While some 	 0.2
Klavans J. 	 1.0
Institute -LRB- 	 1.0
, typewritten 	 0.0011229646266142617
semantics without 	 0.07142857142857142
for computers 	 0.0036101083032490976
unseen data 	 1.0
Further applications 	 0.3333333333333333
of HMM-based 	 0.00089126559714795
word at 	 0.016666666666666666
relationships in 	 0.16666666666666666
opens , 	 1.0
such capabilities 	 0.008130081300813009
representations . 	 0.25
or dictionary 	 0.0045045045045045045
, Japanese 	 0.0011229646266142617
, when 	 0.003368893879842785
of automated 	 0.00089126559714795
and HLT 	 0.001445086705202312
its domain 	 0.05714285714285714
treat words 	 0.5
corpus linguistics 	 0.0967741935483871
the vertices 	 0.0006920415224913495
wrote The 	 0.16666666666666666
, 1978 	 0.0011229646266142617
2005 -RRB- 	 1.0
, unless 	 0.0005614823133071309
marks and 	 0.25
the basics 	 0.0006920415224913495
that performance 	 0.010638297872340425
on Mirage 	 0.0047169811320754715
then can 	 0.02857142857142857
Languages which 	 0.3333333333333333
metrics often 	 0.1111111111111111
settle on 	 1.0
reported accuracy 	 0.2
large dictionaries 	 0.043478260869565216
sentences `` 	 0.02631578947368421
democratizing data 	 0.5
mining of 	 0.2
will be 	 0.2571428571428571
problem than 	 0.022727272727272728
reading and 	 0.25
features ? 	 0.038461538461538464
field that 	 0.07407407407407407
OCR to 	 0.061224489795918366
's software 	 0.0196078431372549
are already 	 0.004149377593360996
material . 	 0.5
results were 	 0.047619047619047616
, James 	 0.0022459292532285235
and recognition 	 0.001445086705202312
door of 	 0.25
closed-domain might 	 1.0
The Eurofighter 	 0.005208333333333333
measured can 	 0.16666666666666666
past the 	 0.3333333333333333
been based 	 0.014705882352941176
sets -LRB- 	 0.09090909090909091
a heuristic 	 0.00245398773006135
, domain 	 0.0005614823133071309
automatic speech 	 0.13043478260869565
of elaborate 	 0.00089126559714795
which simulates 	 0.007246376811594203
improve this 	 0.15384615384615385
where at 	 0.02857142857142857
`` deep 	 0.005291005291005291
controlling flight 	 1.0
, grammars 	 0.0005614823133071309
explore critical 	 0.25
vector . 	 0.3333333333333333
the early 	 0.0020761245674740486
year . 	 0.5
have developed 	 0.009615384615384616
errors in 	 0.2
tools deploy 	 0.16666666666666666
neural nets 	 0.06666666666666667
structured documents 	 0.16666666666666666
automatic procedures 	 0.043478260869565216
distortions -LRB- 	 1.0
important example 	 0.0625
sophisticated algorithms 	 0.2857142857142857
category that 	 0.5
a necessary 	 0.001226993865030675
The profile 	 0.005208333333333333
Jelinek F. 	 0.5
thought or 	 0.3333333333333333
topics . 	 0.14285714285714285
language analysis 	 0.006756756756756757
general cursive 	 0.045454545454545456
filling may 	 1.0
to read 	 0.0013280212483399733
one detail 	 0.015384615384615385
Sample a 	 1.0
the frequency 	 0.0006920415224913495
marks are 	 0.25
-LRB- selecting 	 0.0027100271002710027
resource -LRB- 	 0.2
not represent 	 0.008928571428571428
level we 	 0.05
parses the 	 0.5
research had 	 0.047619047619047616
parser with 	 0.0625
and large 	 0.001445086705202312
languages -RRB- 	 0.04
Recent research 	 0.6666666666666666
recursively defines 	 0.5
-LRB- Some 	 0.0027100271002710027
rules . 	 0.13953488372093023
vary with 	 0.16666666666666666
the opposite 	 0.001384083044982699
Stanford University 	 0.5
smaller tag-sets 	 0.14285714285714285
recognition we 	 0.008264462809917356
research and 	 0.11904761904761904
several alternative 	 0.045454545454545456
enables several 	 1.0
affect the 	 0.3333333333333333
the reference 	 0.0006920415224913495
an answer 	 0.015151515151515152
, sociolinguistics 	 0.0005614823133071309
produced like 	 0.1111111111111111
includes -LRB- 	 0.14285714285714285
Ethnomethodology . 	 1.0
Applications The 	 0.5
quotations , 	 1.0
whose easy-to-use 	 0.3333333333333333
are words 	 0.004149377593360996
key words 	 0.16666666666666666
clusters . 	 1.0
to computers 	 0.0013280212483399733
sound . 	 0.05
almost no 	 1.0
to blind 	 0.0013280212483399733
printer using 	 1.0
1,000,000 words 	 1.0
'' a 	 0.015463917525773196
tool . 	 0.5
in 1933 	 0.0018726591760299626
which usually 	 0.007246376811594203
taggers for 	 0.14285714285714285
ICR make 	 0.3333333333333333
which involves 	 0.007246376811594203
reason , 	 0.5
installing speech 	 1.0
`` defective 	 0.005291005291005291
in 2004 	 0.0018726591760299626
<s> One 	 0.009223674096848577
notion that 	 0.25
-LRB- 2000 	 0.0027100271002710027
walk is 	 0.2
it works 	 0.008547008547008548
Noise in 	 1.0
sentences ' 	 0.013157894736842105
, commanding 	 0.0005614823133071309
tagging systems 	 0.04
rainbow form 	 1.0
stands for 	 1.0
interpret and 	 1.0
to include 	 0.009296148738379814
<s> Current 	 0.0023059185242121443
and address 	 0.001445086705202312
by any 	 0.005714285714285714
reference model 	 0.125
particular types 	 0.07692307692307693
patient '' 	 1.0
interest , 	 0.09090909090909091
U.S. Army 	 0.14285714285714285
looks for 	 0.25
their device 	 0.029411764705882353
best that 	 0.1111111111111111
written language 	 0.11538461538461539
can learn 	 0.0055248618784530384
a summary 	 0.0098159509202454
segmentation and 	 0.06060606060606061
to ignore 	 0.0013280212483399733
and of 	 0.001445086705202312
combined with 	 0.5
stems in 	 0.5
formal rules 	 0.1111111111111111
combine various 	 0.6666666666666666
now named 	 0.15384615384615385
authenticate or 	 1.0
program in 	 0.09090909090909091
which ? 	 0.007246376811594203
a string 	 0.00245398773006135
see for 	 0.05
non-Western scripts 	 1.0
summary tackles 	 0.023809523809523808
umbrella term 	 1.0
a series 	 0.007361963190184049
that human 	 0.0070921985815602835
on bilingual 	 0.0047169811320754715
the humanities 	 0.0006920415224913495
very rudimentary 	 0.024390243902439025
, nasality 	 0.0005614823133071309
elements , 	 0.25
cited the 	 1.0
subject . 	 0.25
Such perceptions 	 0.125
less standardised 	 0.08333333333333333
a data 	 0.001226993865030675
expressions . 	 0.6666666666666666
This new 	 0.015873015873015872
phonemes -LRB- 	 0.16666666666666666
with , 	 0.00546448087431694
Word splitting 	 0.2857142857142857
computer based 	 0.022727272727272728
Jef Verschueren 	 1.0
`` have 	 0.005291005291005291
, Vito 	 0.0005614823133071309
be obtained 	 0.004219409282700422
develop as 	 0.2
feature\/aspect is 	 1.0
language that 	 0.006756756756756757
extent with 	 0.25
why automatic 	 0.14285714285714285
and RCA 	 0.001445086705202312
a revolution 	 0.001226993865030675
French . 	 0.25
RCA 301 	 0.2
translation capabilities 	 0.013513513513513514
has improved 	 0.011904761904761904
, science 	 0.0005614823133071309
the ACL 	 0.0006920415224913495
considered as 	 0.1111111111111111
On what 	 0.16666666666666666
size and 	 0.3333333333333333
vertices . 	 0.2222222222222222
' , 	 0.3157894736842105
saw the 	 1.0
as Maximal 	 0.003484320557491289
definite on 	 1.0
two methods 	 0.034482758620689655
, Janet 	 0.0005614823133071309
learning methods 	 0.023255813953488372
proper lexical 	 0.14285714285714285
the voice 	 0.0006920415224913495
, perhaps 	 0.0016844469399213925
whose theoretical 	 0.3333333333333333
processing is 	 0.037037037037037035
evaluation campaign 	 0.018518518518518517
the models 	 0.0006920415224913495
A second 	 0.02
right context 	 0.1
, incomplete 	 0.0005614823133071309
simple conditions 	 0.038461538461538464
of neural 	 0.00089126559714795
oral talk-in-interaction 	 1.0
evaluate the 	 0.5
the system 	 0.01384083044982699
did Christmas 	 0.2
to describe 	 0.0026560424966799467
De Guzman 	 1.0
The Association 	 0.005208333333333333
high as 	 0.05555555555555555
graph specially 	 0.07692307692307693
conversation , 	 0.25
like to 	 0.07142857142857142
grammar for 	 0.02702702702702703
translation Interlingual 	 0.02702702702702703
specific tasks 	 0.047619047619047616
analog signal 	 0.5
1955 , 	 1.0
section requires 	 0.3333333333333333
was based 	 0.012987012987012988
extracting answers 	 0.2
mimic the 	 1.0
between two 	 0.05128205128205128
like writing 	 0.03571428571428571
incomplete sentences 	 1.0
Facebook -RRB- 	 1.0
soft , 	 0.5
the whole 	 0.0006920415224913495
process correctly 	 0.027777777777777776
nets . 	 1.0
<s> LL 	 0.0015372790161414297
systems trade 	 0.008928571428571428
a movie 	 0.00245398773006135
were realized 	 0.024390243902439025
to examples 	 0.0013280212483399733
car or 	 1.0
's and 	 0.0196078431372549
Pragmatics , 	 1.0
increasing the 	 0.3333333333333333
`` Meaningful 	 0.005291005291005291
The ability 	 0.005208333333333333
2PR \/ 	 1.0
and Markov 	 0.001445086705202312
domain is 	 0.05
'' is 	 0.04639175257731959
more complex 	 0.08421052631578947
will tend 	 0.02857142857142857
in NIST 	 0.0018726591760299626
e.g. Querying 	 0.017857142857142856
we have 	 0.06666666666666667
Larry Page 	 0.5
specialized algorithms 	 0.5
, syllables 	 0.0005614823133071309
navigation systems 	 0.5
<s> A 	 0.033820138355111454
pulled directly 	 1.0
of User 	 0.00089126559714795
Duranti , 	 1.0
What distinguishes 	 0.09090909090909091
that tell 	 0.0035460992907801418
Furthermore , 	 1.0
and life 	 0.001445086705202312
aspect , 	 0.5
due both 	 0.4
<s> Training 	 0.0007686395080707148
decide that 	 0.25
time on 	 0.030303030303030304
by teletype 	 0.005714285714285714
Extrinsic evaluation 	 0.5
degrees depending 	 0.5
book applies 	 0.125
continuous text 	 0.16666666666666666
segment in 	 0.1111111111111111
into an 	 0.02564102564102564
movement to 	 1.0
approximation of 	 0.16666666666666666
information then 	 0.021739130434782608
`` 12 	 0.010582010582010581
, degraded-images 	 0.0005614823133071309
health and 	 1.0
than trying 	 0.022222222222222223
opinions -RRB- 	 0.5
worked out 	 0.2
passage web 	 1.0
Substantial efforts 	 0.5
-LRB- or 	 0.02710027100271003
to database 	 0.0013280212483399733
of domains 	 0.00089126559714795
AFTI -RRB- 	 1.0
approximated as 	 1.0
expected -LRB- 	 0.14285714285714285
the POS 	 0.0020761245674740486
entropy model 	 0.2
scripts -LRB- 	 0.6666666666666666
might select 	 0.038461538461538464
, Gdaniec 	 0.0005614823133071309
is capitalized 	 0.0020325203252032522
been explored 	 0.014705882352941176
causes a 	 1.0
FAA document 	 0.5
spoken , 	 0.14285714285714285
it ends 	 0.017094017094017096
the examples 	 0.0020761245674740486
of that 	 0.0017825311942959
organized notations 	 1.0
and later 	 0.001445086705202312
a hidden 	 0.001226993865030675
kept either 	 1.0
only cares 	 0.02631578947368421
Intrinsic evaluation 	 0.3333333333333333
field is 	 0.037037037037037035
himself with 	 0.5
hurts ? 	 0.5
that most 	 0.0035460992907801418
although capabilities 	 0.16666666666666666
be automated 	 0.004219409282700422
found . 	 0.07142857142857142
Major issues 	 0.5
defined in 	 0.16666666666666666
elaborate theories 	 1.0
digital assistants 	 0.14285714285714285
artifacts , 	 1.0
reliability , 	 0.5
standard -LRB- 	 0.14285714285714285
useful in 	 0.14285714285714285
gives less 	 0.5
to more 	 0.0026560424966799467
Advanced Research 	 0.2
of part 	 0.00089126559714795
has grown 	 0.011904761904761904
multiplying together 	 1.0
often possible 	 0.022727272727272728
understanding also 	 0.030303030303030304
approximate at 	 0.5
Animate = 	 1.0
history . 	 0.25
queries to 	 0.3333333333333333
-RRB- ^ 	 0.0027100271002710027
<s> Recall 	 0.0023059185242121443
between them 	 0.05128205128205128
heavy-noise , 	 1.0
is to 	 0.03861788617886179
to publish 	 0.0013280212483399733
the GALE 	 0.001384083044982699
another language 	 0.23076923076923078
provides for 	 0.5
then using 	 0.02857142857142857
be sufficient 	 0.004219409282700422
inspired the 	 1.0
task-effectiveness well 	 0.5
relevant entities 	 0.14285714285714285
person does 	 0.05263157894736842
remember the 	 1.0
modeling , 	 0.14285714285714285
of marking 	 0.00089126559714795
unlikely analyses 	 1.0
negative emotions 	 0.125
The program 	 0.005208333333333333
F. , 	 1.0
significant semiotic 	 0.1111111111111111
occur in 	 0.4
one feasibility 	 0.015384615384615385
text linguistics 	 0.006289308176100629
compared . 	 0.2857142857142857
, 7 	 0.0005614823133071309
telephone speech 	 0.5
this understanding 	 0.01098901098901099
<s> Open 	 0.0007686395080707148
life scientists 	 0.25
has characterized 	 0.011904761904761904
orthogonal to 	 1.0
Words , 	 0.25
an approach 	 0.030303030303030304
fulfill expectations 	 0.5
, whether 	 0.0005614823133071309
Users were 	 1.0
European Parliament 	 0.3333333333333333
human assessments 	 0.021739130434782608
'' about 	 0.005154639175257732
the typewritten 	 0.0006920415224913495
single character 	 0.07142857142857142
of intermediary 	 0.00089126559714795
information Popular 	 0.021739130434782608
to rewrite 	 0.0013280212483399733
existing hand-written 	 0.2
computer process 	 0.022727272727272728
to databases 	 0.0013280212483399733
different dialogues 	 0.02040816326530612
to make 	 0.005312084993359893
some interrogative 	 0.012048192771084338
be asking 	 0.004219409282700422
pioneered the 	 0.3333333333333333
to lower 	 0.0013280212483399733
does a 	 0.2
time are 	 0.030303030303030304
usually have 	 0.03125
<s> Hence 	 0.0015372790161414297
also because 	 0.014492753623188406
similar sentences 	 0.1111111111111111
summarization task 	 0.02
parliament and 	 1.0
feature of 	 0.23076923076923078
human translators 	 0.021739130434782608
Two particular 	 0.14285714285714285
known labeled 	 0.038461538461538464
by A. 	 0.005714285714285714
the pilot 	 0.0006920415224913495
year later 	 0.16666666666666666
preferable , 	 1.0
does not 	 0.5
with 17 	 0.00546448087431694
adjective ; 	 0.14285714285714285
mobile email 	 0.5
a purely 	 0.001226993865030675
point , 	 0.6666666666666666
case is 	 0.058823529411764705
which , 	 0.007246376811594203
broader and 	 1.0
Further restricted-domain 	 0.3333333333333333
machine for 	 0.012658227848101266
<s> People 	 0.0007686395080707148
involves preliminary 	 0.1
an infinitive 	 0.007575757575757576
an Australian 	 0.007575757575757576
intelligence and 	 0.125
a newspaper 	 0.001226993865030675
different ways 	 0.02040816326530612
support the 	 0.25
entertaining last 	 0.5
based representation 	 0.018518518518518517
correct -RRB- 	 0.06666666666666667
the possibilities 	 0.0006920415224913495
business letters 	 0.25
deal with 	 0.5
lies the 	 0.5
process Main 	 0.027777777777777776
efficient manner 	 0.3333333333333333
figure on 	 0.5
again statistically 	 1.0
David 's 	 0.25
, coreference 	 0.0005614823133071309
<s> and 	 0.0007686395080707148
recognized essentially 	 0.16666666666666666
1990s there 	 0.3333333333333333
include contractions 	 0.037037037037037035
that detected 	 0.0035460992907801418
, split 	 0.0005614823133071309
typically one 	 0.05555555555555555
an investigation 	 0.007575757575757576
semantic from 	 0.047619047619047616
-RRB- one 	 0.0027100271002710027
Robinson , 	 1.0
other cases 	 0.02857142857142857
, Steven 	 0.0005614823133071309
rules should 	 0.023255813953488372
standard method 	 0.07142857142857142
of related 	 0.00267379679144385
marks may 	 0.25
polarity helped 	 0.125
a variety 	 0.008588957055214725
system will 	 0.010752688172043012
Yes\/No vs. 	 1.0
the sequences 	 0.0006920415224913495
variety of 	 1.0
Digest coupons 	 0.3333333333333333
automated technologies 	 0.14285714285714285
, tables 	 0.0005614823133071309
Unix operating 	 0.5
text fragments 	 0.006289308176100629
uses a 	 0.2857142857142857
serving a 	 1.0
is the 	 0.09146341463414634
assistance of 	 1.0
use lexical 	 0.027777777777777776
recognition systems 	 0.08264462809917356
generic summaries 	 0.3333333333333333
speech designed 	 0.006578947368421052
being the 	 0.05555555555555555
reliance on 	 1.0
a similar 	 0.001226993865030675
on some 	 0.04245283018867924
after Fourier 	 0.08333333333333333
likelihood for 	 0.3333333333333333
whom ? 	 0.5
, Wayne 	 0.0005614823133071309
formal modeling 	 0.1111111111111111
, quite 	 0.0005614823133071309
validity and 	 1.0
for natural 	 0.01444043321299639
possessives into 	 1.0
but article 	 0.014705882352941176
<s> Answer 	 0.0015372790161414297
, interlingual 	 0.0011229646266142617
more effectively 	 0.010526315789473684
helped the 	 0.3333333333333333
are directly 	 0.004149377593360996
as words 	 0.003484320557491289
features in 	 0.038461538461538464
designers , 	 1.0
in machine 	 0.009363295880149813
these environments 	 0.023809523809523808
research focus 	 0.023809523809523808
central '' 	 0.6666666666666666
represent varies 	 0.1111111111111111
given NLP 	 0.041666666666666664
smaller more 	 0.14285714285714285
format called 	 0.5
logical inference 	 0.16666666666666666
paper -RRB- 	 0.09090909090909091
final sounds 	 0.1111111111111111
about any 	 0.025
equivalent to 	 0.2
open world 	 0.25
Turing published 	 0.5
speech recognition 	 0.42105263157894735
, Theo 	 0.0005614823133071309
prior attempts 	 0.3333333333333333
to Iraq 	 0.0013280212483399733
of Energy 	 0.00089126559714795
good candidates 	 0.23076923076923078
two benefits 	 0.034482758620689655
by looking 	 0.005714285714285714
also continue 	 0.014492753623188406
overlap metrics 	 0.25
Leo Spitzer 	 1.0
processing . 	 0.12962962962962962
unclear whether 	 1.0
both rules 	 0.03225806451612903
fields , 	 0.3333333333333333
detail . 	 0.5
can help 	 0.0055248618784530384
recognizing hand-printed 	 0.2
as maximum 	 0.003484320557491289
frequencies -LRB- 	 0.5
paper data 	 0.09090909090909091
<s> Some 	 0.012298232129131437
meanings , 	 0.25
even so 	 0.037037037037037035
the availability 	 0.0006920415224913495
See Peter 	 0.16666666666666666
Claude Piron 	 1.0
Joseph Weizenbaum 	 1.0
generate a 	 0.3333333333333333
uttered ; 	 0.3333333333333333
my hand-compiled 	 1.0
on its 	 0.009433962264150943
databases into 	 0.125
a lot 	 0.0036809815950920245
understanding to 	 0.06060606060606061
synthesizer . 	 1.0
The 26 	 0.005208333333333333
, topics 	 0.0005614823133071309
printed texts 	 0.08333333333333333
Talmy Givón 	 1.0
are selected 	 0.004149377593360996
or content 	 0.0045045045045045045
the 2011 	 0.0006920415224913495
analysis aims 	 0.015384615384615385
College -LRB- 	 0.5
and EUROPARL 	 0.001445086705202312
<s> There 	 0.006917755572636433
available resources 	 0.058823529411764705
most famous 	 0.034482758620689655
a real-time 	 0.001226993865030675
Processor is 	 1.0
Black-box vs. 	 0.5
emphasize different 	 1.0
models , 	 0.07692307692307693
and business 	 0.001445086705202312
and power 	 0.001445086705202312
after John 	 0.08333333333333333
labeled data 	 0.3333333333333333
yielding thousands 	 1.0
a learning 	 0.00245398773006135
searching -LRB- 	 0.3333333333333333
criterion of 	 0.5
detect the 	 1.0
class they 	 0.25
command interpreters 	 0.5
developed . 	 0.07692307692307693
controllers -LRB- 	 0.3333333333333333
features making 	 0.038461538461538464
for parsing 	 0.0036101083032490976
American prisoners 	 0.2
produced tones 	 0.1111111111111111
out in 	 0.14285714285714285
used when 	 0.017699115044247787
such corpora 	 0.016260162601626018
sentence structure 	 0.020833333333333332
matrix , 	 1.0
candidate passages 	 0.3333333333333333
document 7110.65 	 0.027777777777777776
eat '' 	 1.0
of corpus 	 0.00089126559714795
can exploit 	 0.0055248618784530384
audio , 	 1.0
Translations are 	 1.0
automatically learning 	 0.047619047619047616
Vice President 	 1.0
summarization have 	 0.02
acoustic noise 	 0.3333333333333333
, P 	 0.0005614823133071309
linguistic discourse 	 0.0625
document collections 	 0.027777777777777776
analysis Applied 	 0.015384615384615385
, maximum 	 0.0005614823133071309
In Italy 	 0.009523809523809525
in operational 	 0.0018726591760299626
segmentation problems 	 0.06060606060606061
directed . 	 1.0
delimiter . 	 1.0
the Austrian 	 0.0006920415224913495
performed , 	 0.2
but much 	 0.014705882352941176
from some 	 0.009615384615384616
campaign was 	 0.2
overlap should 	 0.25
called GRASSHOPPER 	 0.05555555555555555
and Church 	 0.001445086705202312
order in 	 0.07142857142857142
result will 	 0.09090909090909091
personalised business 	 1.0
metrics . 	 0.1111111111111111
consideration of 	 0.3333333333333333
French and 	 0.25
normally requires 	 0.5
free as 	 0.25
between closely 	 0.02564102564102564
first major 	 0.06060606060606061
a preposition 	 0.001226993865030675
machine language 	 0.0379746835443038
developed CLAWS 	 0.038461538461538464
2001 -RRB- 	 0.5
with some 	 0.02185792349726776
cognitive operation 	 0.5
sub-signals . 	 1.0
be -RRB- 	 0.004219409282700422
Sonic Extractor 	 1.0
was delayed 	 0.012987012987012988
between the 	 0.1794871794871795
systems such 	 0.017857142857142856
or Continuous 	 0.0045045045045045045
January 13 	 0.25
on corpora 	 0.0047169811320754715
Sync -RRB- 	 1.0
research is 	 0.047619047619047616
automated sentiment 	 0.14285714285714285
essence of 	 1.0
results when 	 0.09523809523809523
simple rules 	 0.038461538461538464
, outputting 	 0.0005614823133071309
seen the 	 0.1
printed messages 	 0.08333333333333333
involved disabilities 	 0.16666666666666666
with hand-written 	 0.00546448087431694
abstraction . 	 0.25
retrieval and 	 0.2857142857142857
and eigenvector 	 0.001445086705202312
least one 	 0.2
after , 	 0.08333333333333333
-LRB- roughly 	 0.005420054200542005
larger chunk 	 0.0625
linguistic way 	 0.0625
chart parsing 	 1.0
function words 	 0.125
grammatical analysis 	 0.09090909090909091
template . 	 0.25
instructions . 	 1.0
was greatly 	 0.012987012987012988
probably important 	 0.25
their underlying 	 0.029411764705882353
tag the 	 0.0625
East Asian 	 1.0
language or 	 0.013513513513513514
simulation vendors 	 0.3333333333333333
users and 	 0.1111111111111111
often rely 	 0.022727272727272728
etc. ; 	 0.045454545454545456
into two 	 0.01282051282051282
previous questions 	 0.3333333333333333
resulted in 	 1.0
'' 100 	 0.005154639175257732
no effects 	 0.07692307692307693
nouns -LRB- 	 0.1111111111111111
number . 	 0.046511627906976744
This has 	 0.015873015873015872
common cases 	 0.04
since 1965 	 0.1
EUROPARL , 	 1.0
phenomenon may 	 0.2
processing Statistical 	 0.018518518518518517
where `` 	 0.02857142857142857
originally developed 	 0.5
same topic 	 0.04
A series 	 0.02
is less 	 0.0020325203252032522
right-to-left , 	 1.0
a hard 	 0.00245398773006135
11 point 	 1.0
A semantic 	 0.02
the phrases 	 0.0006920415224913495
other research 	 0.014285714285714285
paradigms . 	 1.0
their more 	 0.029411764705882353
, opening 	 0.0005614823133071309
a plural 	 0.001226993865030675
or stochastic 	 0.0045045045045045045
a user-provided 	 0.001226993865030675
, the 	 0.058394160583941604
optimistic about 	 1.0
and generate 	 0.001445086705202312
evaluators . 	 1.0
text-to-speech synthesizer 	 0.25
is growing 	 0.0020325203252032522
different output 	 0.02040816326530612
markers , 	 0.3333333333333333
time in 	 0.030303030303030304
quality . 	 0.1
into text 	 0.038461538461538464
from other 	 0.009615384615384616
generation of 	 0.1111111111111111
use heteroscedastic 	 0.013888888888888888
human-written ones 	 0.5
express this 	 0.2
W. G. 	 0.5
pages getting 	 0.14285714285714285
same way 	 0.04
Michigan , 	 1.0
often addressed 	 0.022727272727272728
vital . 	 1.0
Parsing : 	 0.2
to low 	 0.0013280212483399733
it takes 	 0.008547008547008548
and enterprise 	 0.001445086705202312
be learned 	 0.004219409282700422
important parts 	 0.0625
, picture 	 0.0005614823133071309
Informally , 	 1.0
is unusual 	 0.0020325203252032522
later , 	 0.5
meets two 	 0.5
English text 	 0.02702702702702703
word senses 	 0.016666666666666666
noun -LRB- 	 0.07142857142857142
quoting people 	 1.0
the average 	 0.0006920415224913495
see Handwriting 	 0.05
proportional to 	 1.0
slower , 	 1.0
any case 	 0.0967741935483871
definitional questions 	 1.0
the success 	 0.001384083044982699
context in 	 0.030303030303030304
key theorists 	 0.16666666666666666
point scale 	 0.3333333333333333
a co-occurrence 	 0.001226993865030675
become clear 	 0.25
can function 	 0.0055248618784530384
Intelligent Character 	 0.3333333333333333
-LRB- free 	 0.0027100271002710027
be solved 	 0.004219409282700422
<s> Even 	 0.0007686395080707148
pruned to 	 1.0
in US 	 0.0018726591760299626
approached keyphrase 	 0.5
a grammar 	 0.00245398773006135
text -RRB- 	 0.006289308176100629
usually called 	 0.03125
, by 	 0.002807411566535654
by domain 	 0.005714285714285714
new opportunities 	 0.041666666666666664
tables to 	 0.3333333333333333
single verbal 	 0.07142857142857142
how well 	 0.20689655172413793
of segmentation 	 0.00089126559714795
finding a 	 0.4
Force for 	 0.5
this case 	 0.01098901098901099
papers on 	 0.6666666666666666
conventional computer 	 1.0
intelligence technologies 	 0.125
choice with 	 0.125
specific letters 	 0.047619047619047616
ELIZA was 	 0.1111111111111111
If web 	 0.1
UK RAF 	 0.25
imagery , 	 1.0
clear why 	 0.25
OCR service 	 0.04081632653061224
Please help 	 0.6666666666666666
Automatic learning 	 0.1111111111111111
research . 	 0.09523809523809523
be deployed 	 0.004219409282700422
-LRB- citation 	 0.03523035230352303
After the 	 0.3333333333333333
<s> Hulth 	 0.0023059185242121443
it even 	 0.008547008547008548
would thus 	 0.018867924528301886
, communicative 	 0.0005614823133071309
, Thai 	 0.0005614823133071309
a score 	 0.001226993865030675
and replicated 	 0.001445086705202312
this prior 	 0.01098901098901099
<s> Leading 	 0.0007686395080707148
whether the 	 0.15384615384615385
implicit assumptions 	 1.0
protection from 	 1.0
little interference 	 0.3333333333333333
modern parsers 	 0.2
coherent summary 	 0.2
part may 	 0.037037037037037035
robustness against 	 0.5
a proper 	 0.001226993865030675
statistical machine 	 0.09090909090909091
-LRB- that 	 0.01084010840108401
Automatic summarization 	 0.2222222222222222
emotional state 	 0.25
these are 	 0.047619047619047616
suitable translation 	 0.25
used include 	 0.008849557522123894
their translation 	 0.029411764705882353
a bill 	 0.001226993865030675
Greek and 	 0.3333333333333333
relating to 	 1.0
-RRB- Interactional 	 0.0027100271002710027
decade in 	 0.3333333333333333
queries on 	 0.3333333333333333
Our brain 	 0.6666666666666666
of achieving 	 0.0017825311942959
having a 	 0.2
linguistic controversy 	 0.0625
stochastic matrix 	 0.125
classifiers make 	 0.5
it has 	 0.03418803418803419
OCR Document 	 0.02040816326530612
cultural factors 	 1.0
the opinions 	 0.0006920415224913495
<s> Given 	 0.0023059185242121443
-LRB- FAS 	 0.0027100271002710027
or MLLT 	 0.0045045045045045045
instance when 	 0.07142857142857142
: Instead 	 0.00980392156862745
or fade 	 0.0045045045045045045
-LRB- orange 	 0.0027100271002710027
tasks due 	 0.03125
context , 	 0.12121212121212122
ever appear 	 1.0
other languages 	 0.07142857142857142
-RRB- while 	 0.0027100271002710027
NIST role 	 0.5
unigrams and 	 0.08333333333333333
of whom 	 0.00089126559714795
of effort 	 0.00089126559714795
categories themselves 	 0.1111111111111111
though this 	 0.1
effective and 	 0.16666666666666666
Afghanistan or 	 1.0
tagging : 	 0.04
plain text 	 1.0
simulation of 	 0.3333333333333333
customers . 	 0.5
requires specialized 	 0.0625
commercial version 	 0.09090909090909091
are `` 	 0.004149377593360996
encouraging , 	 1.0
for generating 	 0.0036101083032490976
segment text 	 0.2222222222222222
is in 	 0.006097560975609756
improve robustness 	 0.07692307692307693
be as 	 0.012658227848101266
the N-best 	 0.0006920415224913495
automated semantic 	 0.14285714285714285
of sentences 	 0.006238859180035651
test example 	 0.1
consult information 	 1.0
summarization in 	 0.04
assume there 	 0.5
walking patterns 	 0.3333333333333333
some states 	 0.012048192771084338
group developed 	 0.25
emerged as 	 1.0
, hypothetical 	 0.0005614823133071309
reported -LRB- 	 0.2
Open source 	 1.0
-LRB- SemEval 	 0.0027100271002710027
into words 	 0.038461538461538464
<s> Vulcan 	 0.0007686395080707148
how useful 	 0.034482758620689655
Why it 	 0.14285714285714285
unveiled during 	 1.0
the fixed 	 0.0006920415224913495
of users 	 0.0017825311942959
expressed on 	 0.3333333333333333
unsupervised '' 	 0.125
of individual 	 0.0017825311942959
processing would 	 0.018518518518518517
the figure 	 0.0006920415224913495
questions can 	 0.038461538461538464
readily produces 	 0.3333333333333333
the number 	 0.004844290657439446
specific context 	 0.047619047619047616
avoids overfitting 	 1.0
see appraisal 	 0.05
capabilities by 	 0.2
WebOCR also 	 0.25
Apparatus for 	 1.0
of existing 	 0.0017825311942959
segmentation tools 	 0.06060606060606061
history of 	 0.5
as natural 	 0.003484320557491289
work , 	 0.125
pasted , 	 1.0
high recognition 	 0.05555555555555555
1966 -RRB- 	 0.3333333333333333
's necessary 	 0.0196078431372549
scanning applications 	 0.5
for approximating 	 0.0036101083032490976
project was 	 0.07692307692307693
had to 	 0.07142857142857142
evaluation : 	 0.037037037037037035
theory to 	 0.07692307692307693
as discussed 	 0.003484320557491289
subjectivity of 	 0.5
sets for 	 0.09090909090909091
result -LRB- 	 0.09090909090909091
extent to 	 0.25
it in 	 0.008547008547008548
applied different 	 0.06666666666666667
with specialised 	 0.00546448087431694
Screenshot OCR 	 1.0
resolution : 	 0.25
precisely an 	 1.0
was able 	 0.05194805194805195
subject to 	 0.125
BASEBALL answered 	 0.5
be presented 	 0.004219409282700422
rank . 	 0.16666666666666666
languages contain 	 0.02
study , 	 0.25
existing multilingual 	 0.2
that allows 	 0.0035460992907801418
the United 	 0.004844290657439446
orthography to 	 0.5
that will 	 0.0070921985815602835
reviews respectively 	 0.16666666666666666
Warren Weaver 	 1.0
earlier term 	 0.25
sound blocks 	 0.05
`` un-supervised 	 0.005291005291005291
by analogy 	 0.005714285714285714
feasible the 	 0.5
translations , 	 0.5
a voice 	 0.001226993865030675
reader . 	 0.2
program may 	 0.045454545454545456
or lexical 	 0.009009009009009009
<s> Language 	 0.0007686395080707148
, preparation 	 0.0005614823133071309
, location 	 0.0005614823133071309
potentially exponential 	 0.3333333333333333
paper , 	 0.09090909090909091
recorded all 	 0.5
certain restrictions 	 0.14285714285714285
C. , 	 1.0
Higher rates 	 1.0
10msec , 	 0.5
, correlation 	 0.0005614823133071309
developed in 	 0.23076923076923078
a significant 	 0.001226993865030675
Matches between 	 1.0
speaker or 	 0.05555555555555555
the evaluation 	 0.0034602076124567475
incorporate logical 	 1.0
Tags usually 	 1.0
where POS 	 0.02857142857142857
wave . 	 0.2222222222222222
graph would 	 0.07692307692307693
produce , 	 0.045454545454545456
abstraction Broadly 	 0.25
of OCR 	 0.0035650623885918
general term 	 0.045454545454545456
SR system 	 0.3333333333333333
after testing 	 0.08333333333333333
northeast of 	 1.0
do we 	 0.038461538461538464
-- only 	 0.04
software produces 	 0.037037037037037035
up in 	 0.09090909090909091
languages such 	 0.1
'' and 	 0.06701030927835051
sociology , 	 1.0
examples can 	 0.041666666666666664
processing the 	 0.018518518518518517
`` Gismo 	 0.010582010582010581
demonstration was 	 0.2
, ELIZA 	 0.0011229646266142617
pronoun , 	 1.0
was sold 	 0.012987012987012988
forums -LRB- 	 1.0
the supervised 	 0.0006920415224913495
tools mostly 	 0.16666666666666666
a second 	 0.0036809815950920245
as opposed 	 0.003484320557491289
to prepare 	 0.0013280212483399733
hear this 	 0.5
and Lifeline 	 0.001445086705202312
some sort 	 0.012048192771084338
with maximal 	 0.00546448087431694
post-processing step 	 0.6666666666666666
by Makoto 	 0.005714285714285714
Another resource 	 0.07692307692307693
various term 	 0.05555555555555555
, a 	 0.02695115103874228
addition might 	 0.16666666666666666
, UMLS 	 0.0005614823133071309
content question 	 0.08333333333333333
be keyphrases 	 0.004219409282700422
as controlled 	 0.003484320557491289
United States 	 0.7777777777777778
simple and 	 0.07692307692307693
of diagonal 	 0.00089126559714795
forecasts from 	 0.2
dependency grammar 	 0.2
pauses between 	 0.5
one typically 	 0.015384615384615385
their systems 	 0.058823529411764705
evaluate summaries 	 0.25
, pollen 	 0.0005614823133071309
: GRASSHOPPER 	 0.00980392156862745
cosine values 	 0.3333333333333333
phrased in 	 1.0
required . 	 0.14285714285714285
-RRB- Speech 	 0.0027100271002710027
Symantec changed 	 0.5
Speaker independent 	 0.16666666666666666
in driving 	 0.0018726591760299626
given -LRB- 	 0.041666666666666664
beings , 	 1.0
Programming languages 	 0.6666666666666666
hypothetical , 	 1.0
example shows 	 0.012345679012345678
development has 	 0.08333333333333333
recognition-related project 	 1.0
the corresponding 	 0.001384083044982699
radiology report 	 1.0
grammars alone 	 0.07142857142857142
their reputations 	 0.029411764705882353
of understanding 	 0.00089126559714795
perform data 	 0.09090909090909091
approaches are 	 0.03571428571428571
In such 	 0.009523809523809525
done both 	 0.09090909090909091
time factor 	 0.030303030303030304
the relationship 	 0.0006920415224913495
first customers 	 0.030303030303030304
Systems that 	 0.3333333333333333
GRM library 	 1.0
fairly non-trivial 	 0.25
to model 	 0.0013280212483399733
applications of 	 0.04
strategies to 	 0.5
contrast -RRB- 	 0.125
as voice 	 0.003484320557491289
techniques for 	 0.08695652173913043
Evaluation The 	 0.1111111111111111
shop or 	 1.0
LOB Corpus 	 1.0
general-purpose speech 	 1.0
nouns were 	 0.1111111111111111
an EHR 	 0.007575757575757576
, Adriana 	 0.0005614823133071309
with speech 	 0.01092896174863388
system which 	 0.010752688172043012
step . 	 0.13333333333333333
of triple 	 0.00089126559714795
The ideal 	 0.005208333333333333
decisions -- 	 0.1
for tagging 	 0.0036101083032490976
space is 	 0.2
document production 	 0.027777777777777776
which class 	 0.007246376811594203
maximum likelihood 	 0.3333333333333333
on programer 	 0.0047169811320754715
'' arguably 	 0.005154639175257732
and VOLSUNGA 	 0.001445086705202312
fragments that 	 1.0
<s> Typical 	 0.0015372790161414297
language comprehension 	 0.006756756756756757
segmentation depends 	 0.030303030303030304
a product 	 0.001226993865030675
and instead 	 0.001445086705202312
problems and 	 0.11764705882352941
a cluster 	 0.00245398773006135
waves that 	 0.14285714285714285
numbers after 	 0.14285714285714285
vocabulary sizes 	 0.125
An ISO 	 0.0625
what day 	 0.03125
1629 , 	 1.0
can find 	 0.0055248618784530384
management system 	 0.14285714285714285
any topic 	 0.06451612903225806
a subset 	 0.0036809815950920245
say , 	 0.2857142857142857
, similarities 	 0.0005614823133071309
years to 	 0.047619047619047616
region in 	 1.0
appear within 	 0.0625
heard or 	 1.0
highlighted , 	 1.0
-LRB- usually 	 0.005420054200542005
above to 	 0.07692307692307693
is Shift-Reduce 	 0.0020325203252032522
input characters 	 0.024390243902439025
as semiotics 	 0.003484320557491289
words have 	 0.009174311926605505
system to 	 0.053763440860215055
network has 	 0.16666666666666666
and documents 	 0.001445086705202312
that sentences 	 0.0035460992907801418
speech of 	 0.006578947368421052
approaches have 	 0.07142857142857142
-RRB- tagger 	 0.0027100271002710027
than only 	 0.044444444444444446
brain recognizes 	 0.3333333333333333
for programming 	 0.0036101083032490976
a startlingly 	 0.001226993865030675
of researchers 	 0.00089126559714795
, video 	 0.0011229646266142617
been debated 	 0.014705882352941176
's tagger 	 0.0196078431372549
Many ATC 	 0.08333333333333333
rare . 	 0.5
domain or 	 0.05
Real progress 	 0.5
approach is 	 0.14285714285714285
to minimize 	 0.0013280212483399733
than others 	 0.044444444444444446
sentence of 	 0.020833333333333332
they improved 	 0.025
with regard 	 0.02185792349726776
different people 	 0.02040816326530612
<s> Effective 	 0.0007686395080707148
language interaction 	 0.006756756756756757
not possible 	 0.008928571428571428
construct over 	 0.3333333333333333
<s> Once 	 0.003843197540353574
framework . 	 0.75
indeed that 	 0.3333333333333333
An extractive 	 0.0625
, V.J. 	 0.0005614823133071309
direct translation 	 0.16666666666666666
payments . 	 1.0
small text 	 0.1111111111111111
they helped 	 0.025
For example 	 0.6229508196721312
tagger . 	 0.1111111111111111
been heard 	 0.014705882352941176
simply verbs 	 0.08333333333333333
corpora such 	 0.09090909090909091
scientific fields 	 0.5
Russian sentences 	 1.0
F35 Lightning 	 1.0
in more 	 0.0018726591760299626
phenomenon is 	 0.2
logical form 	 0.16666666666666666
language generation 	 0.033783783783783786
not invented 	 0.008928571428571428
, favor 	 0.0005614823133071309
- A 	 0.0625
robust to 	 0.25
a diverse 	 0.001226993865030675
hand-produced rules 	 1.0
close the 	 1.0
the Levenshtein 	 0.0006920415224913495
Realtime Speech 	 1.0
as either 	 0.003484320557491289
to integrate 	 0.0013280212483399733
left recursive 	 0.16666666666666666
not map 	 0.008928571428571428
its output 	 0.08571428571428572
can represent 	 0.011049723756906077
fully up 	 0.16666666666666666
often mentioned 	 0.022727272727272728
-LRB- POS 	 0.005420054200542005
Discursive psychology 	 1.0
or evaluation 	 0.0045045045045045045
cases are 	 0.05555555555555555
The shapes 	 0.005208333333333333
called machine 	 0.05555555555555555
systems indicate 	 0.008928571428571428
-LRB- p. 	 0.0027100271002710027
those pauses 	 0.045454545454545456
to summarise 	 0.00398406374501992
look '' 	 0.2
Willig , 	 1.0
Sentence boundaries 	 0.2
vastly less 	 1.0
two include 	 0.034482758620689655
Methods Computers 	 0.25
D.S. 1998 	 1.0
capital of 	 0.6666666666666666
12 such 	 0.2
; this 	 0.06382978723404255
the equivalent 	 0.0006920415224913495
in 1997 	 0.003745318352059925
morphological distinctions 	 0.3333333333333333
formed by 	 0.2
of 1,500 	 0.00089126559714795
mean of 	 0.5
presence of 	 1.0
: Speech 	 0.00980392156862745
a group 	 0.001226993865030675
IBM Research 	 0.3333333333333333
possible , 	 0.125
are variously 	 0.004149377593360996
future . 	 0.3333333333333333
very much 	 0.024390243902439025
discards any 	 1.0
is presented 	 0.0040650406504065045
it could 	 0.008547008547008548
abbreviations . 	 0.2
<s> Therein 	 0.0007686395080707148
item may 	 1.0
method simply 	 0.0625
Corpus , 	 0.0625
<s> Task 	 0.0007686395080707148
Learning Some 	 1.0
-RRB- formal 	 0.0027100271002710027
visual and\/or 	 0.5
within Tipster 	 0.05555555555555555
Web -RRB- 	 0.1111111111111111
wave in 	 0.1111111111111111
in text 	 0.0018726591760299626
can only 	 0.011049723756906077
detected important 	 0.5
set a 	 0.05128205128205128
studies of 	 0.25
, Jim 	 0.0005614823133071309
or any 	 0.013513513513513514
n't see 	 0.25
Annual Test 	 1.0
summaries -RRB- 	 0.023255813953488372
1956 and 	 1.0
sound , 	 0.1
be applied 	 0.012658227848101266
where an 	 0.02857142857142857
answer extraction 	 0.06666666666666667
producing the 	 0.3333333333333333
parse tree 	 0.1111111111111111
for reasons 	 0.0036101083032490976
UC and 	 0.5
usually separated 	 0.03125
important words 	 0.0625
practical dimensions 	 0.5
various optimization 	 0.05555555555555555
their routing 	 0.029411764705882353
acquire basic 	 1.0
vendors in 	 0.25
sounds on 	 0.06666666666666667
% in 	 0.02564102564102564
word and 	 0.016666666666666666
regression -LRB- 	 1.0
is compounded 	 0.0020325203252032522
many cases 	 0.038461538461538464
language characters 	 0.006756756756756757
proved negligibly 	 0.3333333333333333
, with 	 0.004491858506457047
rapidly growing 	 0.5
on less 	 0.0047169811320754715
to program 	 0.0013280212483399733
what you 	 0.03125
considers the 	 0.5
: control 	 0.00980392156862745
, conjunction 	 0.0005614823133071309
just seen 	 0.1111111111111111
under ultraviolet 	 0.2
great importance 	 0.3333333333333333
systems developed 	 0.017857142857142856
the earlier 	 0.0006920415224913495
test documents 	 0.2
Kurzweil started 	 0.14285714285714285
would , 	 0.018867924528301886
reformulate the 	 1.0
preparation of 	 1.0
n-dimensional real-valued 	 1.0
split , 	 0.25
is adaptive 	 0.0020325203252032522
NLP comprises 	 0.02127659574468085
by Ask.com 	 0.005714285714285714
; or 	 0.02127659574468085
Information extraction 	 0.2
from knowledge 	 0.009615384615384616
, and 	 0.10612015721504772
gold-standard against 	 1.0
often represented 	 0.022727272727272728
meet Wikipedia 	 0.25
and Semantic 	 0.001445086705202312
summarise conditions 	 0.3333333333333333
amongst a 	 1.0
Large-scale evaluation 	 1.0
English-like sentences 	 0.3333333333333333
Journal corpus 	 0.3333333333333333
optical character 	 1.0
and social 	 0.004335260115606936
many in 	 0.019230769230769232
Back-End or 	 1.0
segmentation is 	 0.2727272727272727
abbreviated to 	 1.0
term used 	 0.1111111111111111
as dynamic 	 0.003484320557491289
yet to 	 0.5
holder of 	 1.0
base or 	 0.25
of processing 	 0.00089126559714795
sample of 	 0.3333333333333333
a larger 	 0.0049079754601227
-LRB- plural 	 0.0027100271002710027
, Judith 	 0.0005614823133071309
: To 	 0.00980392156862745
Judith M. 	 1.0
Rate -LRB- 	 1.0
generated from 	 0.13333333333333333
and Natural 	 0.001445086705202312
OCR Software 	 0.02040816326530612
the tag 	 0.001384083044982699
input . 	 0.04878048780487805
evident that 	 0.5
patterns in 	 0.2
red . 	 1.0
analyze all 	 0.25
<s> While 	 0.0023059185242121443
' stability 	 0.05263157894736842
and making 	 0.002890173410404624
Sandra Thompson 	 1.0
applications discussed 	 0.04
from advertisements 	 0.009615384615384616
accuracy can 	 0.03225806451612903
it becomes 	 0.03418803418803419
facing more 	 1.0
produce consonants 	 0.045454545454545456
see context-free 	 0.05
to that 	 0.0026560424966799467
what we 	 0.09375
on increasingly 	 0.0047169811320754715
first occurrence 	 0.030303030303030304
abstract synopsis 	 1.0
handling differences 	 0.5
noise , 	 0.125
to understand 	 0.00398406374501992
descriptions ; 	 1.0
best path 	 0.05555555555555555
is critical 	 0.0020325203252032522
many examples 	 0.019230769230769232
simply by 	 0.08333333333333333
and ` 	 0.001445086705202312
'' with 	 0.020618556701030927
approach 90 	 0.02857142857142857
having considerable 	 0.2
Words and 	 0.25
forms , 	 0.16666666666666666
personnel . 	 1.0
of sequential 	 0.00089126559714795
and his 	 0.001445086705202312
publication of 	 0.6666666666666666
hand-annotated with 	 1.0
-LRB- ATN 	 0.0027100271002710027
information and 	 0.021739130434782608
Invoice OCR 	 1.0
analysis '' 	 0.015384615384615385
of process 	 0.00089126559714795
as from 	 0.003484320557491289
the Penpoint 	 0.0006920415224913495
results for 	 0.047619047619047616
to 98 	 0.0013280212483399733
makes it 	 0.25
sentence extraction 	 0.020833333333333332
recognition . 	 0.05785123966942149
<s> NLG 	 0.0015372790161414297
Segmentation , 	 1.0
certain sequences 	 0.14285714285714285
that determines 	 0.0070921985815602835
expressed like 	 0.16666666666666666
problem may 	 0.022727272727272728
possible to 	 0.125
names of 	 0.14285714285714285
: Dictionary-based 	 0.00980392156862745
issue for 	 0.125
the longest 	 0.0006920415224913495
which were 	 0.014492753623188406
by comparing 	 0.005714285714285714
The set 	 0.005208333333333333
basic OCR 	 0.07692307692307693
system typically 	 0.010752688172043012
into rule-based 	 0.01282051282051282
considerably from 	 1.0
data in 	 0.025974025974025976
more categories 	 0.010526315789473684
to segment 	 0.00398406374501992
tell it 	 0.3333333333333333
arbitrary length 	 0.3333333333333333
programs have 	 0.09090909090909091
HTK toolkit 	 0.5
developed into 	 0.038461538461538464
, preposition 	 0.0005614823133071309
's the 	 0.0196078431372549
containing examples 	 0.125
between computers 	 0.02564102564102564
question posed 	 0.047619047619047616
<s> Example-based 	 0.0007686395080707148
late 70s 	 0.1111111111111111
are performed 	 0.004149377593360996
later became 	 0.1
regard to 	 0.8
representation language 	 0.10526315789473684
but a 	 0.014705882352941176
quantity -LRB- 	 0.3333333333333333
, however 	 0.006176305446378439
the subsequent 	 0.0006920415224913495
is dependency 	 0.0020325203252032522
task should 	 0.023809523809523808
of count 	 0.00089126559714795
Commissioned by 	 1.0
exceeded the 	 1.0
Meehan , 	 1.0
When we 	 0.14285714285714285
range . 	 0.14285714285714285
most sense 	 0.017241379310344827
model would 	 0.1
all alternative 	 0.023255813953488372
<s> Scope 	 0.0007686395080707148
training techniques 	 0.03571428571428571
the test 	 0.001384083044982699
from that 	 0.009615384615384616
co-occur with 	 0.5
that it 	 0.010638297872340425
processing techniques 	 0.037037037037037035
ambiguous words 	 0.16666666666666666
may not 	 0.09615384615384616
all view 	 0.023255813953488372
objectives of 	 0.5
, Zellig 	 0.0011229646266142617
position in 	 0.5
issues are 	 0.2
<s> Use 	 0.0007686395080707148
real-time character 	 0.5
profession -LRB- 	 1.0
, Speereo 	 0.0005614823133071309
and broadband 	 0.001445086705202312
best word 	 0.05555555555555555
of most 	 0.00089126559714795
largest speech 	 1.0
, UPV 	 0.0005614823133071309
evaluation Depending 	 0.018518518518518517
to performance 	 0.0013280212483399733
each input 	 0.022222222222222223
their system 	 0.029411764705882353
SCU in 	 1.0
information databases 	 0.021739130434782608
trigrams without 	 0.5
training are 	 0.07142857142857142
, after 	 0.0005614823133071309
titles and 	 0.5
threshold is 	 0.25
necessary , 	 0.1
are currently 	 0.008298755186721992
Mouffe , 	 1.0
includes transfer-based 	 0.14285714285714285
statement , 	 1.0
3 or 	 0.2
working to 	 0.14285714285714285
to generate 	 0.00796812749003984
based recognition 	 0.018518518518518517
Turney with 	 0.1111111111111111
Dynamic time 	 0.8
In 1993 	 0.009523809523809525
's intent 	 0.0196078431372549
in 1953 	 0.0018726591760299626
languages of 	 0.02
distant from 	 1.0
recently updated 	 0.3333333333333333
that area 	 0.0035460992907801418
EBMT -RRB- 	 1.0
texts by 	 0.058823529411764705
request can 	 1.0
enough for 	 0.2
should not 	 0.05263157894736842
speaker adaptation 	 0.1111111111111111
commonplace and 	 1.0
a quantitative 	 0.00245398773006135
an edge 	 0.015151515151515152
<s> Translation 	 0.0007686395080707148
could usefully 	 0.0625
n Computer 	 0.5
should soon 	 0.05263157894736842
with reference 	 0.00546448087431694
most common 	 0.10344827586206896
<s> Books 	 0.0007686395080707148
ISO standards 	 0.5
between discourse 	 0.1282051282051282
: Putting 	 0.00980392156862745
also includes 	 0.014492753623188406
array . 	 1.0
five pages 	 0.2
large amount 	 0.043478260869565216
were identified 	 0.024390243902439025
imagine the 	 1.0
subjective Yes\/No 	 0.16666666666666666
the adjacent 	 0.0006920415224913495
non-textual components 	 1.0
define these 	 0.5
resource consumption 	 0.2
or ICR 	 0.0045045045045045045
Success Rate 	 1.0
following manner 	 0.06666666666666667
other possible 	 0.014285714285714285
contain multiple 	 0.08333333333333333
has gone 	 0.011904761904761904
used , 	 0.07079646017699115
exactly as 	 0.3333333333333333
on whether 	 0.0047169811320754715
theory and 	 0.07692307692307693
and simply 	 0.001445086705202312
comparing its 	 0.5
capable of 	 1.0
left-recursion and 	 1.0
Current state 	 0.2
Unsupervised keyphrase 	 0.3333333333333333
rubric includes 	 1.0
a model 	 0.001226993865030675
government . 	 0.3333333333333333
Italy , 	 1.0
normalization it 	 0.16666666666666666
recursive-descent parser 	 1.0
a rate 	 0.001226993865030675
Profile templates 	 1.0
words , 	 0.13761467889908258
OCR or 	 0.02040816326530612
semantic analysis 	 0.09523809523809523
recognized draft 	 0.16666666666666666
NER -RRB- 	 1.0
1995 -RRB- 	 1.0
Weizenbaum at 	 0.3333333333333333
forms can 	 0.16666666666666666
an EMR 	 0.007575757575757576
declared before 	 0.5
an array 	 0.007575757575757576
to : 	 0.0013280212483399733
% -RRB- 	 0.02564102564102564
is digital 	 0.0020325203252032522
paragraphs . 	 0.25
despite being 	 0.3333333333333333
of contextual 	 0.00089126559714795
mouse driven 	 1.0
, Robyn 	 0.0005614823133071309
while `` 	 0.05
Glass-box evaluation 	 1.0
portable , 	 0.3333333333333333
learn features 	 0.07692307692307693
Stilstudien -LRB- 	 1.0
naturalness . 	 1.0
The Brown 	 0.015625
make better 	 0.05
assume no 	 0.5
solve algebra 	 0.25
Harris 's 	 0.2222222222222222
' is 	 0.05263157894736842
multimedia documents 	 0.5
Design choices 	 1.0
recognize text 	 0.1111111111111111
would choose 	 0.018867924528301886
grounded in 	 1.0
ROUGE-1 is 	 0.2
by heteroscedastic 	 0.005714285714285714
-LRB- QA 	 0.0027100271002710027
relationship extraction 	 0.3333333333333333
, among 	 0.0005614823133071309
machine-aided human 	 1.0
which sounds 	 0.007246376811594203
since 1971 	 0.1
as questions 	 0.003484320557491289
and\/or producing 	 0.3333333333333333
length of 	 0.25
, rejecting 	 0.0005614823133071309
The NIST 	 0.005208333333333333
MPE -RRB- 	 1.0
only by 	 0.05263157894736842
display for 	 0.5
The progress 	 0.005208333333333333
Larry R. 	 0.5
stage of 	 0.4
current research 	 0.14285714285714285
on upper 	 0.0047169811320754715
for recognizing 	 0.0036101083032490976
Weizenbaum between 	 0.3333333333333333
state -RRB- 	 0.07142857142857142
many of 	 0.038461538461538464
recognition for 	 0.008264462809917356
world currently 	 0.06666666666666667
science of 	 0.1
the augmented 	 0.0006920415224913495
part-of-speech assignment 	 0.06666666666666667
Independence : 	 1.0
isloated and 	 1.0
other text 	 0.02857142857142857
the human 	 0.0020761245674740486
these heuristics 	 0.023809523809523808
context The 	 0.06060606060606061
processes implemented 	 0.2
at Stanford 	 0.014705882352941176
for special 	 0.0036101083032490976
software user 	 0.037037037037037035
more accurately 	 0.010526315789473684
then given 	 0.02857142857142857
memorandum `` 	 1.0
`` open 	 0.005291005291005291
example The 	 0.012345679012345678
the goal 	 0.002768166089965398
unusual in 	 1.0
the concepts 	 0.0006920415224913495
-LRB- by 	 0.005420054200542005
systems perform 	 0.008928571428571428
speaker independent 	 0.05555555555555555
sentences into 	 0.039473684210526314
The term 	 0.020833333333333332
up pronouns 	 0.045454545454545456
segments besides 	 0.2
NLP comes 	 0.02127659574468085
largely because 	 0.2
major issues 	 0.08333333333333333
can start 	 0.0055248618784530384
sentences so 	 0.013157894736842105
which we 	 0.007246376811594203
into subfields 	 0.01282051282051282
have higher 	 0.009615384615384616
segmentation Sentence 	 0.030303030303030304
its users 	 0.02857142857142857
and Reinvestment 	 0.001445086705202312
better understanding 	 0.1111111111111111
grammar formalisms 	 0.02702702702702703
grammatical parts 	 0.09090909090909091
than T 	 0.022222222222222223
beyond polarity 	 0.16666666666666666
worlds '' 	 1.0
include papers 	 0.037037037037037035
sent in 	 1.0
converts a 	 1.0
reviews to 	 0.16666666666666666
speech data 	 0.006578947368421052
system should 	 0.010752688172043012
greater than 	 0.3333333333333333
business data 	 0.25
input-stream by 	 1.0
, punctuation 	 0.0005614823133071309
major limitation 	 0.08333333333333333
`` out 	 0.005291005291005291
, resulting 	 0.0005614823133071309
noun reading 	 0.07142857142857142
animation , 	 1.0
speech sounds 	 0.006578947368421052
be defined 	 0.004219409282700422
when necessary 	 0.02857142857142857
and analytical 	 0.001445086705202312
psychotherapist , 	 1.0
separated out 	 0.3333333333333333
difference was 	 0.25
their spoken 	 0.029411764705882353
developed transformational 	 0.038461538461538464
this application 	 0.01098901098901099
IR relies 	 0.3333333333333333
This problem 	 0.06349206349206349
Anaphora resolution 	 1.0
potential and 	 0.14285714285714285
front door 	 1.0
dismiss the 	 1.0
themselves . 	 0.25
the tagset 	 0.0006920415224913495
grammars -LRB- 	 0.07142857142857142
developed dynamic 	 0.038461538461538464
experiment -LRB- 	 0.2
discontinuous , 	 0.3333333333333333
other areas 	 0.02857142857142857
-LRB- ISRI 	 0.0027100271002710027
`` uh 	 0.005291005291005291
: Example-based 	 0.00980392156862745
is subject 	 0.0020325203252032522
Error Rates 	 0.5
assess how 	 0.6666666666666666
trainer . 	 1.0
book Language 	 0.125
Hendrix formed 	 1.0
questions -LRB- 	 0.038461538461538464
About 47 	 0.5
involves doing 	 0.1
delimited -LRB- 	 0.25
of examples 	 0.00089126559714795
is different 	 0.0020325203252032522
and regions 	 0.001445086705202312
`` universal 	 0.010582010582010581
models . 	 0.11538461538461539
the boundaries 	 0.0020761245674740486
a part-of-speech 	 0.001226993865030675
of each 	 0.006238859180035651
might provide 	 0.038461538461538464
that state 	 0.0035460992907801418
major OCR 	 0.16666666666666666
problems with 	 0.058823529411764705
electronic conversion 	 0.5
cues help 	 1.0
answering -LRB- 	 0.08333333333333333
computer that 	 0.022727272727272728
schemes to 	 0.5
is known 	 0.006097560975609756
short-time stationary 	 0.5
short paragraph 	 0.125
have error 	 0.009615384615384616
the country 	 0.0020761245674740486
may denote 	 0.019230769230769232
they are 	 0.175
the recognized 	 0.001384083044982699
like this 	 0.03571428571428571
the clusters 	 0.0006920415224913495
and `` 	 0.028901734104046242
Ncmsan , 	 1.0
takes as 	 0.3333333333333333
known type 	 0.038461538461538464
SpeechTEK Europe 	 0.5
automata that 	 1.0
estimation and 	 1.0
for Reading 	 0.0036101083032490976
variously defined 	 1.0
would allow 	 0.018867924528301886
probably the 	 0.25
disseminate it 	 1.0
international relations 	 0.5
<s> Rule-based 	 0.0007686395080707148
Searches , 	 1.0
confusion with 	 1.0
would be 	 0.16981132075471697
information theory 	 0.021739130434782608
testing is 	 0.2
and trigram 	 0.001445086705202312
the larger 	 0.0006920415224913495
uttered one 	 0.3333333333333333
larger volume 	 0.0625
a domain-specific 	 0.001226993865030675
to process 	 0.00398406374501992
perceptions are 	 1.0
succeeding on 	 1.0
barmaid '' 	 0.3333333333333333
of complexity 	 0.00089126559714795
is very 	 0.012195121951219513
those influenced 	 0.045454545454545456
Standard Oil 	 0.5
far too 	 0.125
the textual 	 0.0006920415224913495
processing to 	 0.018518518518518517
competitions devoted 	 1.0
graph is 	 0.23076923076923078
For some 	 0.01639344262295082
Sacks , 	 1.0
German and 	 0.25
part-of-speech tags 	 0.06666666666666667
algorithm As 	 0.03571428571428571
but switched 	 0.014705882352941176
charge services 	 1.0
corpus -RRB- 	 0.12903225806451613
disambiguation often 	 0.1
template , 	 0.25
are vulnerable 	 0.004149377593360996
in depth 	 0.0018726591760299626
out -LRB- 	 0.07142857142857142
as Lisp 	 0.003484320557491289
retrieval -LRB- 	 0.14285714285714285
over hand-produced 	 0.08333333333333333
scholars -LRB- 	 0.5
into meaningful 	 0.02564102564102564
expressed with 	 0.16666666666666666
parsing , 	 0.07142857142857142
application for 	 0.07142857142857142
includes mainly 	 0.14285714285714285
means the 	 0.16666666666666666
-RRB- is 	 0.02981029810298103
SHRDLU , 	 0.16666666666666666
score based 	 0.16666666666666666
Oil Company 	 1.0
users after 	 0.1111111111111111
occur on 	 0.2
More sophisticated 	 0.3333333333333333
He received 	 0.125
all the 	 0.13953488372093023
of mouse 	 0.00089126559714795
scores significantly 	 0.2
optimize some 	 1.0
signal is 	 0.16666666666666666
that output 	 0.0035460992907801418
is why 	 0.0020325203252032522
and minimum 	 0.001445086705202312
currently using 	 0.14285714285714285
from naive 	 0.009615384615384616
before -RRB- 	 0.3333333333333333
popular in 	 0.1111111111111111
expand our 	 1.0
: Sample 	 0.00980392156862745
Front-End speech 	 1.0
features involve 	 0.038461538461538464
has wide 	 0.011904761904761904
not included 	 0.008928571428571428
basic sub-signals 	 0.07692307692307693
1983 , 	 1.0
late Claude 	 0.1111111111111111
1999 L'action 	 0.5
etc. -RRB- 	 0.4090909090909091
other approaches 	 0.014285714285714285
in deaf 	 0.0018726591760299626
appear consecutively 	 0.0625
typology , 	 1.0
also granted 	 0.014492753623188406
rule-based , 	 0.14285714285714285
main ideas 	 0.125
on finding 	 0.0047169811320754715
<s> vs. 	 0.0015372790161414297
A linguist 	 0.02
Morphological segmentation 	 1.0
also Machine 	 0.014492753623188406
systems simply 	 0.008928571428571428
input which 	 0.024390243902439025
mutual information 	 1.0
modules , 	 0.5
R. McDonald 	 0.16666666666666666
<s> More 	 0.006149116064565719
, special 	 0.0005614823133071309
lexical segments 	 0.07692307692307693
unigrams appear 	 0.08333333333333333
data entry 	 0.03896103896103896
with language 	 0.00546448087431694
even languages 	 0.037037037037037035
a count 	 0.001226993865030675
few sentences 	 1.0
medium -RRB- 	 0.3333333333333333
for quantitatively 	 0.0036101083032490976
stems from 	 0.5
which various 	 0.007246376811594203
more accurate 	 0.031578947368421054
segments and 	 0.2
, columns 	 0.0005614823133071309
and phonemes 	 0.001445086705202312
see above 	 0.05
<s> On-line 	 0.0023059185242121443
Hearing , 	 1.0
as if 	 0.003484320557491289
for terms 	 0.0036101083032490976
but rather 	 0.029411764705882353
IR -RRB- 	 0.3333333333333333
which simply 	 0.007246376811594203
on it 	 0.0047169811320754715
Keyphrase extractors 	 0.25
comprehensive hand-crafted 	 0.2
the orthography 	 0.0006920415224913495
rule induction 	 0.3333333333333333
also very 	 0.028985507246376812
Artificial neural 	 0.5
software finding 	 0.037037037037037035
particular event 	 0.07692307692307693
-RRB- that 	 0.005420054200542005
single document 	 0.07142857142857142
of Scotland 	 0.0017825311942959
, or 	 0.018528916339135316
usually measured 	 0.03125
and characterizes 	 0.001445086705202312
Louise J. 	 1.0
of canned 	 0.00089126559714795
keyphrase extraction 	 0.631578947368421
opportunity to 	 0.5
& Critical 	 0.125
of tags 	 0.00089126559714795
semantics formalization 	 0.07142857142857142
words that 	 0.009174311926605505
seen in 	 0.1
are now 	 0.016597510373443983
making the 	 0.2857142857142857
numbers which 	 0.14285714285714285
feature in 	 0.07692307692307693
languages was 	 0.02
-LRB- Asia 	 0.0027100271002710027
answering : 	 0.08333333333333333
have started 	 0.009615384615384616
: Natural 	 0.00980392156862745
expensive and 	 0.14285714285714285
a preliminary 	 0.001226993865030675
designed to 	 0.7142857142857143
solved . 	 0.2
the development 	 0.0034602076124567475
often work 	 0.022727272727272728
One key 	 0.07692307692307693
question can 	 0.023809523809523808
large variety 	 0.043478260869565216
realistic grammars 	 1.0
camp is 	 0.25
weighted by 	 0.3333333333333333
, understanding 	 0.0005614823133071309
importance . 	 0.16666666666666666
States , 	 0.14285714285714285
coefficients , 	 0.25
so forth 	 0.03333333333333333
syntactic structure 	 0.07692307692307693
product . 	 0.14285714285714285
output with 	 0.038461538461538464
focused on 	 0.9090909090909091
match the 	 0.3333333333333333
syntax to 	 0.18181818181818182
steady increase 	 0.5
; later 	 0.02127659574468085
keyphrase to 	 0.05263157894736842
determination : 	 1.0
to speech 	 0.00398406374501992
speech feature 	 0.006578947368421052
might skip 	 0.038461538461538464
of triples 	 0.00089126559714795
often ambiguous 	 0.022727272727272728
vs. Spontaneous 	 0.08333333333333333
new domains 	 0.041666666666666664
statistical approach 	 0.030303030303030304
scale -RRB- 	 0.16666666666666666
how they 	 0.10344827586206896
and differing 	 0.001445086705202312
model and 	 0.03333333333333333
can form 	 0.0055248618784530384
sub-problems , 	 1.0
` conceptual 	 0.0625
speech-to-text -RRB- 	 0.5
sciences concurrently 	 0.5
demonstrated at 	 1.0
accommodate direct 	 0.2
way is 	 0.041666666666666664
achieved with 	 0.2
be separated 	 0.004219409282700422
, support 	 0.0005614823133071309
allows for 	 0.125
as its 	 0.010452961672473868
a text-understanding 	 0.001226993865030675
safety critical 	 1.0
for naval 	 0.0036101083032490976
-LRB- including 	 0.0027100271002710027
unit block 	 0.3333333333333333
driven , 	 1.0
and then 	 0.010115606936416185
evaluations have 	 0.16666666666666666
SHRDLU and 	 0.16666666666666666
Encouraging results 	 1.0
Beginning with 	 0.5
human beings 	 0.021739130434782608
stories on 	 1.0
vs. spontaneous 	 0.08333333333333333
serve other 	 0.2
because NLP 	 0.03333333333333333
both its 	 0.03225806451612903
choice in 	 0.125
task-based -LRB- 	 0.25
, thanks 	 0.0005614823133071309
to re-encode 	 0.0013280212483399733
The methods 	 0.010416666666666666
databases as 	 0.125
word delimiter 	 0.016666666666666666
coefficients to 	 0.25
's specific 	 0.0196078431372549
as gold 	 0.003484320557491289
paper-intensive industry 	 1.0
has not 	 0.023809523809523808
that contain 	 0.010638297872340425
of the 	 0.17379679144385027
logic structures 	 0.25
font at 	 0.3333333333333333
being followed 	 0.05555555555555555
language constraints 	 0.006756756756756757
EARS project 	 1.0
new application 	 0.041666666666666664
not capitalize 	 0.008928571428571428
explicitly present 	 0.25
telephony , 	 0.6666666666666666
or 100000 	 0.0045045045045045045
sailor → 	 0.2
Information Subsumption 	 0.2
and geospatial 	 0.001445086705202312
Why did 	 0.14285714285714285
this step 	 0.01098901098901099
worked by 	 0.2
when writing 	 0.05714285714285714
Armed Forces 	 1.0
news documents 	 0.07692307692307693
but was 	 0.014705882352941176
segment to 	 0.1111111111111111
`` Part-of-speech 	 0.005291005291005291
using the 	 0.1016949152542373
noise and 	 0.125
statistics . 	 0.375
that spontaneous 	 0.0035460992907801418
single datum 	 0.07142857142857142
Significant advances 	 1.0
negative labels 	 0.125
time , 	 0.3333333333333333
were easy 	 0.024390243902439025
automatic translation 	 0.08695652173913043
A corpus 	 0.02
some large 	 0.012048192771084338
<s> Important 	 0.0007686395080707148
by selecting 	 0.011428571428571429
the HMM 	 0.0006920415224913495
logical deduction 	 0.16666666666666666
objectives : 	 0.5
in universities 	 0.0018726591760299626
The process 	 0.015625
Stephen H. 	 1.0
at the 	 0.22058823529411764
all about 	 0.023255813953488372
observation . 	 1.0
enable the 	 1.0
is sufficient 	 0.0040650406504065045
, in 	 0.01909039865244245
ranked highly 	 0.4
an American 	 0.007575757575757576
challenges . 	 0.5
in 1952 	 0.0018726591760299626
interrogative -LRB- 	 1.0
a known 	 0.00245398773006135
say ` 	 0.2857142857142857
estate advertisements 	 1.0
represented using 	 0.16666666666666666
typewritten text 	 0.2
the actual 	 0.0020761245674740486
e.g. feature 	 0.017857142857142856
Big wave 	 1.0
to internal 	 0.0013280212483399733
and neural 	 0.002890173410404624
programmers began 	 1.0
rich information 	 0.2
he talking 	 0.14285714285714285
mainly from 	 0.16666666666666666
from small 	 0.019230769230769232
Virtually any 	 1.0
commands are 	 0.2
High-order n-gram 	 1.0
Unsourced material 	 1.0
evaluation with 	 0.018518518518518517
found no 	 0.07142857142857142
assumption -LRB- 	 0.5
an experiment 	 0.007575757575757576
a tagging 	 0.001226993865030675
the Tablet 	 0.0006920415224913495
transformation , 	 1.0
more times 	 0.010526315789473684
that used 	 0.0035460992907801418
including PARRY 	 0.07142857142857142
the predicted 	 0.0006920415224913495
biomedical domain 	 1.0
deployed was 	 0.5
between automatically 	 0.02564102564102564
speech-recognition machine 	 0.3333333333333333
problems of 	 0.058823529411764705
entry . 	 0.25
was drawn 	 0.012987012987012988
major corpus 	 0.08333333333333333
excerpt containing 	 1.0
the course 	 0.0006920415224913495
Defense Advanced 	 1.0
uses -LRB- 	 0.07142857142857142
Arabic in 	 0.25
we will 	 0.08888888888888889
the representation 	 0.0006920415224913495
system . 	 0.11827956989247312
which he 	 0.007246376811594203
engaging in 	 1.0
eyes-busy environment 	 1.0
provided within 	 0.2
developments in 	 0.6666666666666666
governmental proceedings 	 1.0
profiling for 	 1.0
Sociologist Harold 	 1.0
processing Objectives 	 0.018518518518518517
research also 	 0.023809523809523808
ranked with 	 0.2
etc. . 	 0.4090909090909091
or places 	 0.0045045045045045045
top of 	 0.2
characterize keyphrases 	 0.5
, this 	 0.003368893879842785
developed at 	 0.07692307692307693
the more 	 0.0034602076124567475
accordance with 	 1.0
to aid 	 0.0013280212483399733
extrinsic evaluation 	 0.5
parsing accuracy 	 0.03571428571428571
LexRank has 	 0.08333333333333333
some set 	 0.012048192771084338
the interactions 	 0.0006920415224913495
first such 	 0.030303030303030304
are displayed 	 0.004149377593360996
or existing 	 0.0045045045045045045
, Systran 	 0.0005614823133071309
automate the 	 0.3333333333333333
reason is 	 0.25
carried on 	 0.5
or automatically 	 0.009009009009009009
: lessons 	 0.00980392156862745
With the 	 0.2857142857142857
, rhetoric 	 0.0005614823133071309
following -LRB- 	 0.06666666666666667
NLG input 	 0.047619047619047616
scope of 	 1.0
's methods 	 0.0196078431372549
advanced pattern 	 0.2
, 1975 	 0.0005614823133071309
French were 	 0.25
as interactivity 	 0.003484320557491289
tasks ; 	 0.03125
the model 	 0.0020761245674740486
a keyboard 	 0.001226993865030675
The Turney 	 0.005208333333333333
The shorter 	 0.005208333333333333
relief is 	 1.0
This term 	 0.015873015873015872
cut and 	 1.0
an arbitrarily 	 0.007575757575757576
DeRose used 	 0.2
The English 	 0.005208333333333333
on Speech 	 0.0047169811320754715
, of 	 0.0022459292532285235
using online 	 0.01694915254237288
are starting 	 0.004149377593360996
approaches presume 	 0.03571428571428571
support personnel 	 0.25
, low-resolution 	 0.0005614823133071309
sound pattern 	 0.05
changed from 	 0.5
procedures , 	 0.25
source . 	 0.041666666666666664
and segment 	 0.001445086705202312
algorithms have 	 0.08571428571428572
up for 	 0.09090909090909091
<s> Vocabulary 	 0.0007686395080707148
have to 	 0.019230769230769232
solved in 	 0.2
arithmetic expression 	 1.0
outputting language 	 0.5
humans when 	 0.08333333333333333
research efforts 	 0.023809523809523808
-LRB- Dec. 	 0.0027100271002710027
program with 	 0.045454545454545456
or less 	 0.018018018018018018
identification is 	 0.2
automatically tuned 	 0.047619047619047616
next item 	 0.14285714285714285
together ? 	 0.125
M-346 Master 	 1.0
was evident 	 0.012987012987012988
presents the 	 1.0
sounds of 	 0.13333333333333333
n't end 	 0.25
's informativeness 	 0.0196078431372549
this constraint 	 0.01098901098901099
G. Lehnart 	 0.5
other we 	 0.014285714285714285
final phase 	 0.1111111111111111
Ruth Wodak 	 1.0
systems '' 	 0.008928571428571428
that funding 	 0.0035460992907801418
even allows 	 0.037037037037037035
an allowable 	 0.007575757575757576
new POS 	 0.041666666666666664
Category : 	 0.5
require exponential 	 0.045454545454545456
Nielsen automatically 	 1.0
wave from 	 0.1111111111111111
multilingual questions 	 0.3333333333333333
`` inadequate 	 0.005291005291005291
of public 	 0.00089126559714795
time window 	 0.030303030303030304
splitting is 	 0.5
, plus 	 0.0005614823133071309
trying to 	 1.0
used to 	 0.19469026548672566
documents generally 	 0.02631578947368421
addressed in 	 0.5
50 % 	 0.6666666666666666
which accepts 	 0.007246376811594203
Apollo moon 	 1.0
Separate a 	 0.5
for English 	 0.0036101083032490976
which should 	 0.007246376811594203
those which 	 0.045454545454545456
Rajman M. 	 1.0
since words 	 0.1
1929 Gustav 	 1.0
for translation 	 0.0036101083032490976
large ; 	 0.043478260869565216
black holes 	 1.0
by Su 	 0.005714285714285714
system using 	 0.010752688172043012
text 's 	 0.006289308176100629
summary might 	 0.023809523809523808
up '' 	 0.045454545454545456
set of 	 0.717948717948718
summary covers 	 0.023809523809523808
phrasing the 	 1.0
Audio , 	 0.5
or keep 	 0.0045045045045045045
<s> Canada 	 0.0007686395080707148
put this 	 0.25
electronically searched 	 1.0
modal . 	 1.0
sentence or 	 0.041666666666666664
founder of 	 1.0
program that 	 0.09090909090909091
search -LRB- 	 0.09090909090909091
into battle 	 0.01282051282051282
Inuit virtually 	 1.0
indeed , 	 0.3333333333333333
in 1965 	 0.0018726591760299626
without confusions 	 0.07692307692307693
much slower 	 0.09090909090909091
icon -RRB- 	 1.0
match up 	 0.16666666666666666
The systems 	 0.005208333333333333
perform adaptive 	 0.09090909090909091
faster to 	 0.3333333333333333
like being 	 0.03571428571428571
, Number 	 0.0005614823133071309
gestures , 	 0.5
more input 	 0.010526315789473684
English and 	 0.08108108108108109
<s> Note 	 0.006917755572636433
future developments 	 0.3333333333333333
<s> Harris 	 0.0007686395080707148
insights . 	 1.0
total number 	 0.5
advanced -LRB- 	 0.2
in service 	 0.0018726591760299626
question-answering abilities 	 0.5
use this 	 0.027777777777777776
the primary 	 0.001384083044982699
to apply 	 0.0013280212483399733
from data 	 0.019230769230769232
various attempts 	 0.05555555555555555
vertices ? 	 0.1111111111111111
majority of 	 1.0
fashion , 	 1.0
several words 	 0.045454545454545456
isolated words 	 0.2
reviewed and 	 1.0
for what 	 0.007220216606498195
Direct Voice 	 1.0
'' This 	 0.005154639175257732
^ , 	 0.6666666666666666
intended for 	 0.4
Recognition or 	 0.125
evaluation comes 	 0.018518518518518517
for understanding 	 0.0036101083032490976
input data 	 0.14634146341463414
were rare 	 0.024390243902439025
horizontal mark 	 1.0
Examples are 	 0.6666666666666666
-RRB- words 	 0.0027100271002710027
centroid sentence 	 0.5
BASEBALL and 	 0.5
opposite of 	 1.0
musical notations 	 1.0
machine that 	 0.012658227848101266
quantitatively comparing 	 1.0
role of 	 0.25
<s> Context 	 0.0007686395080707148
pilot to 	 0.4
rich lexicon 	 0.6
Greek , 	 0.3333333333333333
can compensate 	 0.0055248618784530384
generate weather 	 0.05555555555555555
vision . 	 1.0
, Constraint 	 0.0005614823133071309
still largely 	 0.06666666666666667
entropy -LRB- 	 0.2
<s> OCR 	 0.0030745580322828594
is RDF 	 0.0020325203252032522
Symantec Corporation 	 0.5
letter , 	 0.16666666666666666
150 separate 	 0.5
, computational 	 0.0005614823133071309
<s> Ideally 	 0.0015372790161414297
meaning . 	 0.08695652173913043
accepts some 	 0.5
by decomposing 	 0.005714285714285714
all quantitative 	 0.023255813953488372
software tools 	 0.037037037037037035
be statistically 	 0.004219409282700422
, explanation 	 0.0005614823133071309
extraction system 	 0.06451612903225806
precise set 	 0.3333333333333333
both linguistic 	 0.03225806451612903
+ Mobile 	 0.16666666666666666
commands or 	 0.2
programming languages 	 0.6
a high 	 0.0036809815950920245
each observed 	 0.022222222222222223
asked and 	 0.3333333333333333
some detail 	 0.012048192771084338
Labov , 	 1.0
We can 	 0.2857142857142857
increase in 	 0.75
still contains 	 0.06666666666666667
smaller lexical 	 0.14285714285714285
the volume 	 0.0006920415224913495
and exclamation 	 0.001445086705202312
are focused 	 0.004149377593360996
AI than 	 0.3333333333333333
when it 	 0.02857142857142857
the Medical 	 0.0006920415224913495
of scanned 	 0.00089126559714795
votes from 	 1.0
online resource 	 0.125
supplying more 	 1.0
level ; 	 0.1
<s> ﻿Natural 	 0.0007686395080707148
process may 	 0.027777777777777776
has turned 	 0.011904761904761904
a difficult 	 0.001226993865030675
reveal that 	 1.0
big green 	 0.5
Bayes , 	 0.3333333333333333
surrounding consonants 	 0.2
was influenced 	 0.012987012987012988
degree of 	 0.5
-LRB- arguably 	 0.0027100271002710027
in some 	 0.00749063670411985
Dogged '' 	 1.0
, called 	 0.0005614823133071309
into modern 	 0.01282051282051282
the behavior 	 0.0006920415224913495
a form 	 0.001226993865030675
President Biden 	 0.25
person , 	 0.21052631578947367
analysis and 	 0.03076923076923077
those two 	 0.045454545454545456
message understanding 	 0.5
was due 	 0.012987012987012988
that accuracy 	 0.0035460992907801418
<s> Thus 	 0.009223674096848577
blue '' 	 1.0
Word-sense disambiguation 	 1.0
when discussing 	 0.05714285714285714
Analysis of 	 0.2
'' as 	 0.02577319587628866
sometimes ambiguous 	 0.07692307692307693
more and 	 0.021052631578947368
and Thai 	 0.001445086705202312
statistical models 	 0.21212121212121213
the world 	 0.0020761245674740486
With discontinuous 	 0.14285714285714285
the main 	 0.001384083044982699
first primitive 	 0.030303030303030304
be confused 	 0.004219409282700422
is definite 	 0.0020325203252032522
end , 	 0.125
of keyphrases 	 0.00267379679144385
transducers with 	 1.0
surprising popularity 	 1.0
trees . 	 0.3333333333333333
with values 	 0.01639344262295082
Lee Pike 	 1.0
interfaces Symantec 	 0.5
this work 	 0.01098901098901099
might want 	 0.038461538461538464
of 19th 	 0.00089126559714795
to disambiguate 	 0.00398406374501992
or even 	 0.02252252252252252
paper explored 	 0.09090909090909091
increased and 	 0.2
token , 	 0.25
are rarely 	 0.004149377593360996
Sentences ; 	 1.0
better decisions 	 0.1111111111111111
Last level 	 1.0
stationary probability 	 0.14285714285714285
from our 	 0.009615384615384616
allows users 	 0.25
stationary distribution 	 0.2857142857142857
word co-occurrence 	 0.016666666666666666
high at 	 0.05555555555555555
to discrete 	 0.0013280212483399733
wide range 	 0.5
even disappear 	 0.037037037037037035
of hours 	 0.00089126559714795
because there 	 0.06666666666666667
has increasingly 	 0.011904761904761904
systems take 	 0.008928571428571428
fully automatic 	 0.5
word stems 	 0.016666666666666666
systems are 	 0.11607142857142858
feature statistical 	 0.07692307692307693
we think 	 0.022222222222222223
-- are 	 0.08
degree to 	 0.16666666666666666
These waves 	 0.058823529411764705
the challenge 	 0.0006920415224913495
other non-textual 	 0.014285714285714285
polynomial time 	 1.0
news-gathering , 	 1.0
sometimes confused 	 0.07692307692307693
mainly on 	 0.16666666666666666
the parse 	 0.0006920415224913495
news domain 	 0.23076923076923078
other domains 	 0.014285714285714285
specialised document 	 0.5
with certain 	 0.00546448087431694
, turns 	 0.0005614823133071309
several systems 	 0.045454545454545456
taking the 	 0.6
the parts 	 0.0006920415224913495
without it 	 0.07692307692307693
also called 	 0.043478260869565216
Extract subjective 	 1.0
did fail 	 0.2
on to 	 0.009433962264150943
content selection 	 0.08333333333333333
the disfluences 	 0.0006920415224913495
also needs 	 0.014492753623188406
require a 	 0.22727272727272727
-LRB- e.g. 	 0.10298102981029811
intermediary , 	 0.3333333333333333
characters which 	 0.125
a question 	 0.008588957055214725
for disambiguation 	 0.0036101083032490976
word processing 	 0.016666666666666666
format . 	 0.5
are analyzed 	 0.004149377593360996
resolved : 	 1.0
a phrase 	 0.00245398773006135
Dijk , 	 1.0
improvements . 	 0.5
Why unsupervised 	 0.14285714285714285
fastens -LRB- 	 1.0
different groups 	 0.02040816326530612
Tagging Guidelines 	 1.0
CLAWS , 	 0.5
assessing whether 	 1.0
Ingria R. 	 1.0
zero '' 	 1.0
hit you 	 1.0
summers of 	 1.0
the 2006 	 0.0006920415224913495
Makoto Nagao 	 1.0
but most 	 0.029411764705882353
by multiplying 	 0.005714285714285714
tokens , 	 0.14285714285714285
mark was 	 0.3333333333333333
have much 	 0.009615384615384616
in such 	 0.0056179775280898875
useful for 	 0.21428571428571427
spoken words 	 0.14285714285714285
exploits the 	 1.0
meaningless tokens 	 1.0
customer service 	 1.0
that requires 	 0.0070921985815602835
language , 	 0.04054054054054054
overcome these 	 0.5
uses spontaneous 	 0.07142857142857142
words from 	 0.01834862385321101
is felt 	 0.0020325203252032522
overall topics 	 0.16666666666666666
expectations , 	 1.0
previous training 	 0.3333333333333333
a character 	 0.001226993865030675
hand-printed characters 	 0.25
relevant summaries 	 0.14285714285714285
<s> Solving 	 0.0007686395080707148
of grammar 	 0.0017825311942959
tagging or 	 0.08
was declared 	 0.012987012987012988
no assumptions 	 0.07692307692307693
measure , 	 0.09090909090909091
Schiffrin , 	 1.0
to decoding 	 0.0013280212483399733
would somehow 	 0.018867924528301886
perhaps trivial 	 0.16666666666666666
from context 	 0.009615384615384616
was done 	 0.012987012987012988
'' occurs 	 0.005154639175257732
dataset -RRB- 	 1.0
heritage -LRB- 	 1.0
taught the 	 0.6666666666666666
analysis as 	 0.015384615384615385
these , 	 0.047619047619047616
calculator or 	 0.5
their own 	 0.029411764705882353
language use 	 0.02702702702702703
and on 	 0.002890173410404624
are limited 	 0.004149377593360996
to computer 	 0.0013280212483399733
informative with 	 0.5
-LRB- Realtime 	 0.0027100271002710027
English POS-taggers 	 0.02702702702702703
dimensionality reduction 	 1.0
World Health 	 0.14285714285714285
one instance 	 0.015384615384615385
camp '' 	 0.25
in that 	 0.003745318352059925
Greene and 	 1.0
target value 	 0.09090909090909091
algorithm to 	 0.07142857142857142
Royal Australian 	 0.5
mechanical or 	 1.0
novel proposal 	 1.0
, writing 	 0.0011229646266142617
others more 	 0.08333333333333333
and direction 	 0.001445086705202312
Contrary to 	 1.0
keywords or 	 0.5
about the 	 0.2
also terminate 	 0.014492753623188406
word-category disambiguation 	 1.0
is performed 	 0.0040650406504065045
R. Schroeder 	 0.16666666666666666
underpinnings discouraged 	 1.0
, ID 	 0.0005614823133071309
by parser 	 0.005714285714285714
instance text 	 0.07142857142857142
reasoned views 	 1.0
deep are 	 0.14285714285714285
them . 	 0.10526315789473684
text mining 	 0.012578616352201259
score -LRB- 	 0.16666666666666666
The system 	 0.03125
require their 	 0.045454545454545456
+ R 	 0.16666666666666666
creating more 	 0.14285714285714285
real-valued weights 	 0.6666666666666666
into `` 	 0.01282051282051282
ELIZA sometimes 	 0.1111111111111111
negative examples 	 0.125
can effectively 	 0.0055248618784530384
An absorbing 	 0.0625
using precision 	 0.01694915254237288
of rules 	 0.00267379679144385
algebra word 	 0.5
is challenging 	 0.0020325203252032522
<s> Just 	 0.0007686395080707148
generation technology 	 0.1111111111111111
context -- 	 0.030303030303030304
, predicting 	 0.0005614823133071309
and computational 	 0.001445086705202312
are too 	 0.004149377593360996
, grammar 	 0.0005614823133071309
voices or 	 1.0
them automatically 	 0.05263157894736842
just as 	 0.2222222222222222
momentum for 	 1.0
for different 	 0.0036101083032490976
find the 	 0.3076923076923077
not accommodate 	 0.017857142857142856
the relative 	 0.0006920415224913495
phrase begin 	 0.1
words involved 	 0.009174311926605505
, usually 	 0.002807411566535654
developing new 	 0.25
overload has 	 1.0
as length 	 0.003484320557491289
remain high 	 1.0
has fairly 	 0.011904761904761904
latent semantic 	 1.0
CyberEmotions project 	 1.0
about how 	 0.025
-LRB- transcription 	 0.0027100271002710027
Digest , 	 0.3333333333333333
between successive 	 0.02564102564102564
methods QA 	 0.022727272727272728
questions and 	 0.038461538461538464
full comprehension 	 0.2
another verb 	 0.07692307692307693
to rate 	 0.0026560424966799467
all caps 	 0.023255813953488372
DA -RRB- 	 0.6666666666666666
Air Force 	 0.6666666666666666
requires in-depth 	 0.0625
insurance bills 	 1.0
boundary markers 	 0.16666666666666666
each state 	 0.022222222222222223
may vary 	 0.019230769230769232
each other 	 0.13333333333333333
corpora are 	 0.18181818181818182
Savic Naomi 	 1.0
Longacre , 	 1.0
discriminative training 	 1.0
<s> Informally 	 0.0007686395080707148
human-language question 	 1.0
can greatly 	 0.0055248618784530384
-LRB- especially 	 0.005420054200542005
of identifying 	 0.00089126559714795
a nice 	 0.00245398773006135
of January 	 0.00089126559714795
the reverse 	 0.0006920415224913495
code , 	 0.14285714285714285
an automated 	 0.007575757575757576
word blend 	 0.016666666666666666
`` centroid 	 0.005291005291005291
of classifying 	 0.00089126559714795
perform automated 	 0.09090909090909091
to other 	 0.0013280212483399733
psycholinguistics , 	 0.5
accumulation of 	 1.0
system generates 	 0.010752688172043012
other scientific 	 0.014285714285714285
be specified 	 0.004219409282700422
masculine , 	 1.0
language parsing 	 0.013513513513513514
complex task 	 0.041666666666666664
natural -RRB- 	 0.013333333333333334
the adviser 	 0.0006920415224913495
and cross-lingual 	 0.001445086705202312
the exception 	 0.0006920415224913495
associate or 	 0.5
any previous 	 0.03225806451612903
high pollen 	 0.05555555555555555
Putting words 	 1.0
Polar Lander 	 1.0
or formulaic 	 0.0045045045045045045
took Harris 	 1.0
task usually 	 0.023809523809523808
processing tools 	 0.018518518518518517
as phoneme 	 0.003484320557491289
than processing 	 0.022222222222222223
performed more 	 0.1
into separate 	 0.02564102564102564
determine `` 	 0.043478260869565216
been achieved 	 0.029411764705882353
should vertices 	 0.05263157894736842
December 2010 	 1.0
Asian language 	 1.0
'' appears 	 0.005154639175257732
the software 	 0.0020761245674740486
User profiling 	 0.5
unigrams , 	 0.25
decimal point 	 1.0
requires significant 	 0.0625
specific domains 	 0.047619047619047616
the World 	 0.0034602076124567475
<s> Systems 	 0.006149116064565719
modern systems 	 0.2
been superseded 	 0.014705882352941176
an implementation 	 0.007575757575757576
general concepts 	 0.045454545454545456
ARNS system 	 1.0
recognition performance 	 0.03305785123966942
boundary information 	 0.16666666666666666
company Kurzweil 	 0.3333333333333333
that OCR 	 0.0035460992907801418
is at 	 0.0020325203252032522
Manfred R. 	 1.0
cases -- 	 0.05555555555555555
`` fastens 	 0.005291005291005291
Critical Genre 	 0.5
James A. 	 0.5
out of 	 0.07142857142857142
first agree 	 0.030303030303030304
spaces . 	 0.2
by part 	 0.005714285714285714
different grammatical 	 0.02040816326530612
particularly speech 	 0.2
background knowledge 	 0.3333333333333333
International continued 	 1.0
presented to 	 0.16666666666666666
or profession 	 0.0045045045045045045
Norman Fairclough 	 0.5
a shortened 	 0.001226993865030675
50 to 	 0.3333333333333333
ranked adjacent 	 0.2
carefully design 	 1.0
Emanuel Schegloff 	 0.5
set appends 	 0.02564102564102564
categories in 	 0.1111111111111111
simple queries 	 0.038461538461538464
a very 	 0.013496932515337423
as humans 	 0.003484320557491289
Speaker Recognition 	 0.16666666666666666
some local 	 0.012048192771084338
, what 	 0.0011229646266142617
schemata . 	 1.0
or transfer-based 	 0.0045045045045045045
probable answer 	 1.0
no evident 	 0.07692307692307693
character . 	 0.045454545454545456
transformations to 	 0.5
from non 	 0.009615384615384616
aid users 	 0.25
work has 	 0.08333333333333333
Schank , 	 0.2
have tested 	 0.009615384615384616
medial and 	 1.0
mechanized sorting 	 1.0
of hand-written 	 0.00267379679144385
naive Bayes 	 0.5
and form 	 0.001445086705202312
first and 	 0.030303030303030304
picture quality 	 0.25
pertaining to 	 1.0
summarizes that 	 1.0
texts can 	 0.058823529411764705
involves paraphrasing 	 0.1
earliest-used algorithms 	 0.5
is an 	 0.02032520325203252
These range 	 0.058823529411764705
widespread . 	 1.0
Work on 	 0.5
shared-task events 	 1.0
well as 	 0.4642857142857143
individual cursive 	 0.08333333333333333
Paul Chilton 	 0.2
techniques use 	 0.043478260869565216
manner . 	 0.75
Machine Learning 	 0.1111111111111111
many artificial 	 0.019230769230769232
produces less 	 0.25
structure with 	 0.08333333333333333
Mariani J. 	 1.0
data about 	 0.012987012987012988
of size 	 0.00089126559714795
them are 	 0.10526315789473684
the dynamic 	 0.0006920415224913495
backward , 	 1.0
'' in 	 0.03608247422680412
the web 	 0.001384083044982699
the answer 	 0.009688581314878892
manage their 	 1.0
capabilities of 	 0.2
, queries 	 0.0005614823133071309
can prove 	 0.0055248618784530384
A restricted 	 0.02
that part 	 0.0035460992907801418
1982 Gary 	 0.3333333333333333
MCE -RRB- 	 1.0
it easily 	 0.008547008547008548
EVALITA campaign 	 0.5
the -LRB- 	 0.0006920415224913495
such features 	 0.008130081300813009
written without 	 0.038461538461538464
be put 	 0.004219409282700422
-LRB- EBMT 	 0.0027100271002710027
both summarization 	 0.03225806451612903
interim year 	 1.0
in 1989 	 0.0018726591760299626
for POS 	 0.0036101083032490976
but triples 	 0.014705882352941176
department , 	 0.5
that preclude 	 0.0035460992907801418
-LRB- May 	 0.005420054200542005
one . 	 0.03076923076923077
not President 	 0.008928571428571428
was hoping 	 0.012987012987012988
triples and 	 0.3333333333333333
Hybrid MT 	 0.5
given CFG 	 0.041666666666666664
to post-process 	 0.0013280212483399733
vice versa 	 1.0
would only 	 0.018867924528301886
a referring 	 0.001226993865030675
of virtual 	 0.00089126559714795
words were 	 0.01834862385321101
synthesis techniques 	 1.0
-LRB- Sonic 	 0.0027100271002710027
and a 	 0.023121387283236993
best modern 	 0.05555555555555555
structured databases 	 0.16666666666666666
, G 	 0.0005614823133071309
universal language 	 0.3333333333333333
on general 	 0.0047169811320754715
<s> Starting 	 0.0007686395080707148
and 2002 	 0.001445086705202312
level 6 	 0.05
differing contexts 	 1.0
computer input 	 0.022727272727272728
word -RRB- 	 0.016666666666666666
the semantic 	 0.001384083044982699
various ways 	 0.1111111111111111
one sentence 	 0.03076923076923077
commonly tagged 	 0.125
related in 	 0.06666666666666667
keyphrases `` 	 0.02857142857142857
Handbook chapter 	 1.0
`` case 	 0.005291005291005291
we construct 	 0.022222222222222223
of language 	 0.004456327985739751
fighter environment 	 0.16666666666666666
computer performance 	 0.022727272727272728
the extremely 	 0.0006920415224913495
the output 	 0.0020761245674740486
typical sentence 	 0.1111111111111111
the WYSIWYM 	 0.0006920415224913495
which consisted 	 0.007246376811594203
or emotion 	 0.0045045045045045045
provide summaries 	 0.16666666666666666
and competitions 	 0.001445086705202312
this approach 	 0.02197802197802198
ambiguity and 	 0.125
-LRB- Wilensky 	 0.0027100271002710027
would need 	 0.018867924528301886
implemented in 	 0.2
automatic evaluation 	 0.13043478260869565
usage is 	 1.0
wrong fairly 	 1.0
wave as 	 0.1111111111111111
equipment was 	 0.3333333333333333
the title 	 0.0006920415224913495
-LRB- CSR 	 0.005420054200542005
it 's 	 0.008547008547008548
as BLEU 	 0.003484320557491289
In English 	 0.01904761904761905
distances represented 	 0.5
the keyboard 	 0.0006920415224913495
to five 	 0.0013280212483399733
unigrams . 	 0.16666666666666666
intervention : 	 1.0
needed without 	 0.047619047619047616
comprehensive survey 	 0.2
funding of 	 0.125
see what 	 0.05
and may 	 0.002890173410404624
sales reports 	 0.3333333333333333
1,000 parts 	 0.5
Intelligence Corporation 	 0.3333333333333333
accuracy because 	 0.03225806451612903
similarity of 	 0.1
commercial efforts 	 0.18181818181818182
large-vocabulary system 	 0.3333333333333333
build on 	 0.3333333333333333
<s> POS-tagging 	 0.0007686395080707148
industry currently 	 0.3333333333333333
term , 	 0.05555555555555555
measure that 	 0.09090909090909091
for specific 	 0.0036101083032490976
usually in 	 0.09375
grammar rules 	 0.13513513513513514
-RRB- , 	 0.21138211382113822
by Environment 	 0.005714285714285714
are designed 	 0.004149377593360996
a deep 	 0.001226993865030675
robustness of 	 0.25
was used 	 0.05194805194805195
Task and 	 0.6666666666666666
who have 	 0.2
would contain 	 0.018867924528301886
identified the 	 0.2
Many documents 	 0.08333333333333333
not a 	 0.008928571428571428
context-free , 	 0.09090909090909091
of 4 	 0.0017825311942959
equipment would 	 0.3333333333333333
<s> Specifically 	 0.0007686395080707148
Symbian and 	 1.0
Grant ever 	 1.0
early as 	 0.1
successful in 	 0.1111111111111111
below . 	 0.4
digital camera 	 0.14285714285714285
syntactic analysis 	 0.15384615384615385
<s> Realisation 	 0.0007686395080707148
<s> Aided 	 0.0007686395080707148
a nautical 	 0.001226993865030675
subjective sentences 	 0.16666666666666666
multiple approaches 	 0.07692307692307693
approaches the 	 0.03571428571428571
step neural 	 0.06666666666666667
natural and 	 0.02666666666666667
looks , 	 0.25
to explore 	 0.0026560424966799467
implicate `` 	 1.0
computational concerns 	 0.1
Work in 	 0.5
the overall 	 0.0020761245674740486
Early work 	 0.5
as CLAWS 	 0.003484320557491289
of all 	 0.0035650623885918
relationships of 	 0.16666666666666666
but other 	 0.014705882352941176
users . 	 0.2222222222222222
some languages 	 0.024096385542168676
rules and 	 0.023255813953488372
1980s . 	 0.2222222222222222
more unmanageable 	 0.010526315789473684
1966 . 	 0.3333333333333333
; Speech 	 0.02127659574468085
, Winograd 	 0.0005614823133071309
10 parsers 	 0.125
process that 	 0.05555555555555555
The common 	 0.005208333333333333
's house 	 0.0392156862745098
automatically do 	 0.047619047619047616
often difficult 	 0.022727272727272728
and domain 	 0.001445086705202312
during machine 	 0.1
assign targets 	 0.2
theoretical perspectives 	 0.3333333333333333
into canned 	 0.01282051282051282
printed records 	 0.08333333333333333
statistics readily 	 0.125
or larger 	 0.0045045045045045045
possibilities multiply 	 0.2
the written 	 0.0006920415224913495
a function 	 0.001226993865030675
verb . 	 0.15384615384615385
<s> Matches 	 0.0007686395080707148
system-generated summary 	 0.5
assessed mainly 	 1.0
or aspect 	 0.0045045045045045045
Word Error 	 0.14285714285714285
language require 	 0.006756756756756757
`` dogs 	 0.021164021164021163
Before a 	 0.5
be included 	 0.004219409282700422
expanded the 	 1.0
approach . 	 0.05714285714285714
application has 	 0.07142857142857142
slow and 	 0.5
public opinion 	 1.0
http:\/\/arxiv.org\/abs\/1104.2086 -RRB- 	 1.0
<s> All 	 0.0007686395080707148
lexical functional 	 0.07692307692307693
popular strategy 	 0.1111111111111111
<s> Summarization 	 0.0015372790161414297
of complex 	 0.00089126559714795
approach -RRB- 	 0.05714285714285714
when describing 	 0.05714285714285714
by induction 	 0.005714285714285714
is it 	 0.0020325203252032522
Relationship extraction 	 1.0
parser The 	 0.125
answers can 	 0.08333333333333333
and frequency 	 0.001445086705202312
translator need 	 0.14285714285714285
In some 	 0.0380952380952381
Transfer-based machine 	 1.0
using machine 	 0.01694915254237288
exactly how 	 0.3333333333333333
think about 	 0.3333333333333333
the lexical 	 0.0006920415224913495
and simulation 	 0.001445086705202312
to unsupervised 	 0.0026560424966799467
POS tagging 	 0.38461538461538464
paste relevant 	 1.0
high levels 	 0.16666666666666666
unless the 	 1.0
looking waves 	 0.2
, verbs 	 0.0011229646266142617
with any 	 0.00546448087431694
-LRB- Harris 	 0.005420054200542005
The rule-based 	 0.005208333333333333
a reading 	 0.001226993865030675
Narrow but 	 1.0
<s> How 	 0.0030745580322828594
not sound 	 0.008928571428571428
exclamation marks 	 1.0
is `` 	 0.0040650406504065045
translation process 	 0.02702702702702703
As an 	 0.1111111111111111
phrase-structure grammars 	 1.0
reliably -- 	 1.0
are multiple 	 0.004149377593360996
related words 	 0.06666666666666667
mapping each 	 0.5
generate translations 	 0.05555555555555555
any QA 	 0.03225806451612903
are ambiguities 	 0.004149377593360996
Extraction and 	 0.3333333333333333
; By 	 0.02127659574468085
and evaluation 	 0.004335260115606936
prefer to 	 0.5
computational humor 	 0.1
algorithm for 	 0.07142857142857142
function is 	 0.125
the grammar 	 0.006228373702422145
the English-French 	 0.0006920415224913495
most fonts 	 0.017241379310344827
text illustrates 	 0.006289308176100629
recognition problems 	 0.008264462809917356
MySpace -RRB- 	 1.0
it aimed 	 0.008547008547008548
question types 	 0.023809523809523808
negligibly rare 	 1.0
about social 	 0.025
Example-based Main 	 0.3333333333333333
leaving the 	 1.0
risk -LRB- 	 0.5
requires expansion 	 0.0625
network approaches 	 0.3333333333333333
keyphrases , 	 0.05714285714285714
, tag 	 0.0005614823133071309
steered toward 	 1.0
underlies the 	 1.0
within the 	 0.16666666666666666
, candidacies 	 0.0005614823133071309
summarization methods 	 0.02
edit distances 	 1.0
in document 	 0.003745318352059925
other words 	 0.02857142857142857
dynamics and 	 0.5
and relevance 	 0.001445086705202312
the subject 	 0.0034602076124567475
sets ; 	 0.09090909090909091
effort should 	 0.25
Hulth used 	 0.3333333333333333
but deep 	 0.014705882352941176
very common 	 0.04878048780487805
are exploited 	 0.004149377593360996
module uses 	 0.3333333333333333
DeRose and 	 0.2
using `` 	 0.01694915254237288
retrieval -- 	 0.14285714285714285
deterministic problem 	 0.25
power The 	 0.25
Prominent discourse 	 1.0
just validated 	 0.1111111111111111
processed , 	 0.16666666666666666
do . 	 0.038461538461538464
René Descartes 	 1.0
, Wallace 	 0.0005614823133071309
Semantic Orientation 	 0.3333333333333333
what they 	 0.03125
displaced by 	 1.0
-RRB- would 	 0.005420054200542005
Rules post-processed 	 0.3333333333333333
environment as 	 0.16666666666666666
verifying certain 	 1.0
intelligence systems 	 0.125
strategy for 	 0.2
tagging include 	 0.04
Documents '' 	 1.0
of theories 	 0.00089126559714795
algorithms work 	 0.02857142857142857
becoming more 	 1.0
theoretical underpinnings 	 0.3333333333333333
determining the 	 0.6666666666666666
<s> Tags 	 0.0007686395080707148
other English 	 0.014285714285714285
needed . 	 0.09523809523809523
Europe , 	 0.6
frequently used 	 0.5
in speech 	 0.013108614232209739
becomes harder 	 0.25
NLP , 	 0.02127659574468085
's students 	 0.0196078431372549
analysis which 	 0.015384615384615385
assumptions such 	 0.2
field since 	 0.037037037037037035
dictionaries , 	 1.0
reduced set 	 0.25
is true 	 0.0020325203252032522
most NLP 	 0.017241379310344827
with word 	 0.01639344262295082
of sentence 	 0.00089126559714795
method for 	 0.125
It refers 	 0.02631578947368421
usually asked 	 0.03125
`` ask 	 0.005291005291005291
acoustic signal 	 0.16666666666666666
heuristic final 	 0.3333333333333333
, research 	 0.0011229646266142617
-LRB- often 	 0.005420054200542005
`` Statistical 	 0.005291005291005291
reading is 	 0.125
extraction module 	 0.03225806451612903
to accomplish 	 0.0013280212483399733
is part 	 0.0020325203252032522
and possessives 	 0.001445086705202312
Full of 	 1.0
boundary identification 	 0.16666666666666666
To translate 	 0.1111111111111111
offering WebOCR 	 1.0
the expected 	 0.001384083044982699
`` Application-Oriented 	 0.005291005291005291
= accusative 	 0.1111111111111111
PDF to 	 1.0
are used 	 0.03319502074688797
Recall can 	 0.3333333333333333
emotional communication 	 0.25
generated is 	 0.06666666666666667
not necessary 	 0.008928571428571428
the BLEU 	 0.0006920415224913495
`` generalized 	 0.005291005291005291
visit . 	 0.5
: Merging 	 0.00980392156862745
more interest 	 0.010526315789473684
further commercializing 	 0.125
nice beach 	 0.25
constraints Read 	 0.25
and R. 	 0.001445086705202312
very different 	 0.07317073170731707
, comprising 	 0.0005614823133071309
grammar ' 	 0.02702702702702703
is positive 	 0.0020325203252032522
previous Section 	 0.3333333333333333
are ranked 	 0.004149377593360996
syntax is 	 0.09090909090909091
comprehensive theories 	 0.2
becomes . 	 0.25
OCR products 	 0.02040816326530612
text generation 	 0.006289308176100629
her judgement 	 0.5
as statistical 	 0.003484320557491289
develop dedicated 	 0.2
text accordingly 	 0.006289308176100629
80 % 	 1.0
impersonate a 	 1.0
LexRank is 	 0.08333333333333333
word processors 	 0.016666666666666666
and Speech 	 0.001445086705202312
, on 	 0.0039303761931499155
Speereo Voice 	 0.5
people speaking 	 0.125
and maximum 	 0.001445086705202312
Since OCR 	 0.2
More detailed 	 0.1111111111111111
2009 -RRB- 	 0.3333333333333333
with only 	 0.00546448087431694
dictionary or 	 0.14285714285714285
extensive research 	 0.3333333333333333
parameters for 	 0.5
enterprise customers 	 1.0
SPOTLIGHT system 	 1.0
researchers need 	 0.1
`` text 	 0.005291005291005291
classification looks 	 0.058823529411764705
the source 	 0.008304498269896194
Speaker dependence 	 0.16666666666666666
is devoted 	 0.0020325203252032522
-LRB- natural 	 0.0027100271002710027
title concentrates 	 1.0
done with 	 0.18181818181818182
hours of 	 0.5
, Animate 	 0.0005614823133071309
more fine-grained 	 0.010526315789473684
express all 	 0.2
also known 	 0.08695652173913043
longest running 	 1.0
had not 	 0.07142857142857142
potential to 	 0.2857142857142857
and legal 	 0.001445086705202312
some degree 	 0.024096385542168676
of general 	 0.00089126559714795
qualitative manner 	 0.5
the leaders 	 0.0006920415224913495
about speech 	 0.05
Statistics guided 	 0.3333333333333333
W. Handel 	 0.5
, highly-specialized 	 0.0005614823133071309
distinction of 	 0.2
are made 	 0.012448132780082987
an original 	 0.007575757575757576
the fidelity 	 0.0006920415224913495
to existing 	 0.0013280212483399733
on written 	 0.0047169811320754715
is concerned 	 0.0040650406504065045
an approximation 	 0.015151515151515152
text is 	 0.025157232704402517
rates . 	 0.125
question answering 	 0.21428571428571427
research has 	 0.14285714285714285
large corpus 	 0.043478260869565216
set and 	 0.02564102564102564
Continuous speech 	 1.0
symbols defined 	 0.3333333333333333
need for 	 0.14285714285714285
depth '' 	 0.3333333333333333
speech interface 	 0.006578947368421052
are MARGIE 	 0.004149377593360996
Communications -LRB- 	 1.0
NLG applications 	 0.047619047619047616
diverse set 	 0.5
what type 	 0.03125
TextRank algorithm 	 0.07142857142857142
products in 	 0.25
e.g. echoes 	 0.017857142857142856
on sentences 	 0.0047169811320754715
measure for 	 0.09090909090909091
objective evaluation 	 0.2
and searching 	 0.001445086705202312
or as 	 0.009009009009009009
on lower 	 0.0047169811320754715
Adda G. 	 0.5
'' -RRB- 	 0.09278350515463918
<s> Running 	 0.0007686395080707148
: Microsoft 	 0.00980392156862745
also cut 	 0.014492753623188406
of annotated 	 0.00089126559714795
limit to 	 0.25
horoscope machines 	 1.0
disambiguate parts 	 0.3333333333333333
the waves 	 0.0006920415224913495
DOE -RRB- 	 1.0
summarization on 	 0.02
given loss 	 0.041666666666666664
to limit 	 0.0026560424966799467
to an 	 0.0013280212483399733
particular dataset 	 0.07692307692307693
the grammar-based 	 0.0006920415224913495
1946 by 	 1.0
accuracy substantially 	 0.03225806451612903
linguistic typology 	 0.0625
actual text 	 0.2
meaning ; 	 0.043478260869565216
<s> Closed-domain 	 0.0007686395080707148
in fundamentally 	 0.0018726591760299626
service on 	 0.2
Cognitive Process 	 0.3333333333333333
intervals like 	 1.0
as articles 	 0.003484320557491289
needs of 	 0.1
Many Electronic 	 0.08333333333333333
provided by 	 0.4
industry , 	 0.3333333333333333
CLAWS -LRB- 	 0.25
<s> Trained 	 0.0007686395080707148
that adaptation 	 0.0035460992907801418
, supervised 	 0.0005614823133071309
More complex 	 0.1111111111111111
mining have 	 0.2
only unigrams 	 0.02631578947368421
seems to 	 1.0
instructed to 	 1.0
battle management 	 1.0
new complex 	 0.041666666666666664
has to 	 0.05952380952380952
high . 	 0.05555555555555555
same summary 	 0.04
sales data 	 0.3333333333333333
Question answering 	 0.2857142857142857
and generally 	 0.001445086705202312
Penn tag 	 0.2222222222222222
the previous 	 0.001384083044982699
Computers can 	 1.0
superset of 	 1.0
small knowledge 	 0.1111111111111111
hand-written by 	 0.14285714285714285
to adapt 	 0.0013280212483399733
apply increasingly 	 0.2
Sentiment Analysis 	 0.16666666666666666
documents onto 	 0.02631578947368421
abbreviation MT 	 0.5
D. Das 	 0.2
Much effort 	 0.3333333333333333
algorithms which 	 0.02857142857142857
figure out 	 0.5
thought-to-paper communication 	 1.0
approximation to 	 0.3333333333333333
effort , 	 0.25
or phones 	 0.0045045045045045045
summary in 	 0.047619047619047616
human summary 	 0.021739130434782608
as subtasks 	 0.003484320557491289
useful as 	 0.07142857142857142
<s> Imagine 	 0.0007686395080707148
or POST 	 0.0045045045045045045
PageRank , 	 0.16666666666666666
decorrelating the 	 1.0
checked each 	 0.5
du discors 	 1.0
the KEA 	 0.0006920415224913495
refers to 	 1.0
accuracy of 	 0.12903225806451613
or applying 	 0.0045045045045045045
simulators with 	 1.0
automatic methodology 	 0.043478260869565216
but sometimes 	 0.014705882352941176
`` fire 	 0.005291005291005291
entered the 	 0.5
Document structuring 	 0.25
hence need 	 0.5
a modal 	 0.001226993865030675
portion of 	 1.0
significant taggers 	 0.1111111111111111
high-quality weather 	 1.0
beyond which 	 0.16666666666666666
the controller 	 0.001384083044982699
finding non-existent 	 0.2
easier task 	 0.125
editing and 	 0.5
of nouns 	 0.00089126559714795
in Italy 	 0.0018726591760299626
What should 	 0.09090909090909091
translation tasks 	 0.013513513513513514
language being 	 0.013513513513513514
Unsupervised approaches 	 0.16666666666666666
to resort 	 0.0013280212483399733
still quite 	 0.06666666666666667
also many 	 0.014492753623188406
that simple 	 0.0035460992907801418
How , 	 0.14285714285714285
endeavors such 	 1.0
and derive 	 0.001445086705202312
Thompson , 	 1.0
additional citations 	 0.16666666666666666
words -LRB- 	 0.027522935779816515
for simple 	 0.0036101083032490976
small incremental 	 0.1111111111111111
<s> Unlike 	 0.0007686395080707148
and very 	 0.001445086705202312
different way 	 0.02040816326530612
languages is 	 0.02
speech the 	 0.006578947368421052
expended to 	 1.0
erroneous input 	 1.0
<s> Anaphora 	 0.0007686395080707148
improvement to 	 0.25
capitalization at 	 0.3333333333333333
than of 	 0.022222222222222223
parser often 	 0.0625
small set 	 0.1111111111111111
agglutinative languages 	 1.0
Kenneth Lee 	 1.0
easier on 	 0.125
to protect 	 0.0013280212483399733
The idea 	 0.010416666666666666
one another 	 0.015384615384615385
in of 	 0.0018726591760299626
large portion 	 0.043478260869565216
the Gene 	 0.0006920415224913495
maintained by 	 0.5
idea of 	 0.2857142857142857
and an 	 0.004335260115606936
our life 	 0.2
be made 	 0.016877637130801686
tasks have 	 0.03125
domain . 	 0.3
output by 	 0.038461538461538464
overt morphological 	 1.0
approaches and 	 0.03571428571428571
discriminate between 	 0.3333333333333333
improve recognition 	 0.15384615384615385
components that 	 0.2
+ Cloud 	 0.16666666666666666
under construction 	 0.2
phone call 	 0.25
background noise 	 0.3333333333333333
assumptions . 	 0.2
-LRB- titled 	 0.0027100271002710027
cause the 	 0.5
people . 	 0.125
dependency theory 	 0.2
content overlap 	 0.16666666666666666
required for 	 0.14285714285714285
the dynamics 	 0.0006920415224913495
restricted world 	 0.25
evaluation requires 	 0.018518518518518517
genre and 	 0.5
right-hand-sides of 	 1.0
scale rather 	 0.16666666666666666
the right 	 0.0020761245674740486
shifted priorities 	 1.0
name is 	 0.2
To perform 	 0.1111111111111111
computing . 	 0.5
without the 	 0.07692307692307693
RCA product 	 0.2
generate form 	 0.05555555555555555
; but 	 0.0425531914893617
application , 	 0.14285714285714285
-RRB- vs. 	 0.0027100271002710027
describing a 	 0.25
path sentences 	 0.5
help in 	 0.1111111111111111
depths of 	 1.0
methods have 	 0.045454545454545456
, syntactic 	 0.0016844469399213925
have direct 	 0.009615384615384616
learning , 	 0.09302325581395349
Force and 	 0.5
new methods 	 0.041666666666666664
the string 	 0.0006920415224913495
still translates 	 0.06666666666666667
modified in 	 1.0
different possible 	 0.02040816326530612
and nouns 	 0.001445086705202312
video , 	 0.2
the summers 	 0.0006920415224913495
-LRB- at 	 0.0027100271002710027
character error 	 0.045454545454545456
one 10msec 	 0.015384615384615385
are less 	 0.004149377593360996
the corpus 	 0.0006920415224913495
engines to 	 0.3333333333333333
of natural 	 0.0196078431372549
a Standard 	 0.001226993865030675
e.g. space 	 0.017857142857142856
rule that 	 0.3333333333333333
are confusable 	 0.004149377593360996
characters that 	 0.0625
by combining 	 0.005714285714285714
major component 	 0.08333333333333333
answers to 	 0.08333333333333333
narrowest and 	 1.0
-LRB- HMT 	 0.0027100271002710027
text are 	 0.006289308176100629
tokens form 	 0.14285714285714285
algorithm -LRB- 	 0.03571428571428571
complex cognitive 	 0.041666666666666664
texts written 	 0.058823529411764705
defined , 	 0.16666666666666666
including the 	 0.07142857142857142
simplified form 	 0.5
and represented 	 0.001445086705202312
the table 	 0.001384083044982699
simulated the 	 0.5
a fluent 	 0.001226993865030675
based approach 	 0.018518518518518517
which occur 	 0.007246376811594203
<s> Are 	 0.0007686395080707148
not necessarily 	 0.017857142857142856
general principles 	 0.045454545454545456
keyboard and 	 0.3333333333333333
, 3 	 0.0005614823133071309
what a 	 0.09375
such systems 	 0.008130081300813009
to humans 	 0.0013280212483399733
in ` 	 0.003745318352059925
the finite 	 0.0006920415224913495
Another research 	 0.07692307692307693
related languages 	 0.06666666666666667
Brazil , 	 1.0
artificial processes 	 0.18181818181818182
system by 	 0.010752688172043012
but they 	 0.04411764705882353
word boundary 	 0.016666666666666666
not as 	 0.008928571428571428
This analysis 	 0.015873015873015872
to authenticate 	 0.0013280212483399733
, linguistic 	 0.0005614823133071309
the ambiguity 	 0.0006920415224913495
turns and 	 0.3333333333333333
parser generates 	 0.0625
found recognition 	 0.07142857142857142
-RRB- `` 	 0.0027100271002710027
QA -RRB- 	 0.047619047619047616
a database 	 0.0036809815950920245
larger group 	 0.0625
performed an 	 0.1
<s> the 	 0.0007686395080707148
The relations 	 0.026041666666666668
document retrieval 	 0.027777777777777776
call '' 	 0.3333333333333333
Cynthia Hardy 	 1.0
the name 	 0.001384083044982699
those surrounding 	 0.045454545454545456
, impressive 	 0.0005614823133071309
an input-stream 	 0.007575757575757576
, whose 	 0.0011229646266142617
the postal 	 0.0006920415224913495
most positive 	 0.017241379310344827
of computer 	 0.0035650623885918
too similar 	 0.16666666666666666
training a 	 0.03571428571428571
language search 	 0.006756756756756757
rules can 	 0.06976744186046512
Spoken Language 	 1.0
Other measures 	 0.14285714285714285
has used 	 0.011904761904761904
and interactive 	 0.001445086705202312
the late 	 0.005536332179930796
of special 	 0.00089126559714795
semantic parsing 	 0.047619047619047616
discourses , 	 0.5
of Latin-script 	 0.00089126559714795
vector machines 	 0.3333333333333333
worldwide view 	 1.0
closely related 	 0.4
are generated 	 0.004149377593360996
the goals 	 0.0006920415224913495
information , 	 0.043478260869565216
are at 	 0.008298755186721992
if documents 	 0.03571428571428571
summarization and 	 0.02
cases on 	 0.05555555555555555
sense , 	 0.125
producing more 	 0.3333333333333333
as simply 	 0.003484320557491289
tagger proceeds 	 0.1111111111111111
sufficient . 	 0.6
it , 	 0.017094017094017096
in Jones 	 0.0018726591760299626
also help 	 0.014492753623188406
note is 	 1.0
uses several 	 0.07142857142857142
standard corpora 	 0.07142857142857142
Chinese have 	 0.14285714285714285
1965 based 	 0.25
, to 	 0.0072992700729927005
have all 	 0.009615384615384616
consonants , 	 0.3333333333333333
an optimal 	 0.007575757575757576
their answers 	 0.029411764705882353
can also 	 0.04419889502762431
available to 	 0.058823529411764705
data was 	 0.012987012987012988
example through 	 0.012345679012345678
DA began 	 0.3333333333333333
in similar 	 0.0018726591760299626
probably use 	 0.25
'' standards 	 0.005154639175257732
- NC 	 0.0625
fixed static 	 0.5
of regular 	 0.00089126559714795
must first 	 0.07142857142857142
extraction as 	 0.06451612903225806
evaluation metrics 	 0.018518518518518517
now the 	 0.07692307692307693
two waves 	 0.034482758620689655
sets , 	 0.09090909090909091
grammar , 	 0.10810810810810811
machine digitized 	 0.012658227848101266
Harris , 	 0.1111111111111111
reader was 	 0.2
of finite 	 0.00089126559714795
D. Booth 	 0.2
that may 	 0.0070921985815602835
draft is 	 0.5
using unweighted 	 0.01694915254237288
learn explicit 	 0.07692307692307693
form letters 	 0.05
do research 	 0.038461538461538464
capitalization may 	 0.3333333333333333
, search 	 0.0011229646266142617
passages . 	 0.5
Tagger , 	 1.0
sound input 	 0.05
e.g. Chinese 	 0.017857142857142856
new approaches 	 0.041666666666666664
was FoG 	 0.012987012987012988
earliest example 	 0.5
1957 and 	 1.0
great deal 	 0.3333333333333333
which even 	 0.007246376811594203
grammar to 	 0.02702702702702703
of outputs 	 0.00089126559714795
essentially they 	 0.125
ideas , 	 0.25
might generate 	 0.038461538461538464
nodes in 	 0.14285714285714285
this time 	 0.03296703296703297
`` breadth 	 0.005291005291005291
behavior even 	 0.5
evident way 	 0.5
the draft 	 0.0006920415224913495
perspective , 	 0.25
and if 	 0.001445086705202312
parse reranking 	 0.1111111111111111
like in 	 0.03571428571428571
be fully 	 0.008438818565400843
strategy gets 	 0.2
profile feature 	 0.3333333333333333
radios , 	 1.0
Arbor , 	 1.0
expression just 	 0.1
Workshop Hirschman 	 1.0
different left 	 0.02040816326530612
alternative right-hand-sides 	 0.3333333333333333
levels of 	 0.3181818181818182
and how 	 0.004335260115606936
use either 	 0.013888888888888888
open problem 	 0.25
parses -LRB- 	 0.5
The lowest 	 0.005208333333333333
socio-psychological characteristics 	 1.0
words -- 	 0.009174311926605505
ambiguity because 	 0.125
Society , 	 1.0
any other 	 0.06451612903225806
's promise 	 0.0196078431372549
been written 	 0.014705882352941176
shown in 	 0.4
decisions only 	 0.1
you meant 	 0.07692307692307693
by finding 	 0.005714285714285714
multi-document extractive 	 0.25
the naval 	 0.0006920415224913495
learning '' 	 0.11627906976744186
Each frame 	 0.16666666666666666
Blind In 	 0.5
problem involves 	 0.045454545454545456
whereby words 	 1.0
start of 	 0.2857142857142857
Mobile Smartphones 	 0.3333333333333333
processing -LRB- 	 0.07407407407407407
or real 	 0.0045045045045045045
evaluation procedures 	 0.018518518518518517
as Greek 	 0.003484320557491289
process new 	 0.027777777777777776
Models In 	 0.3333333333333333
across their 	 0.2
use or 	 0.027777777777777776
and computer 	 0.001445086705202312
great success 	 0.3333333333333333
such phrases 	 0.008130081300813009
well a 	 0.03571428571428571
parser is 	 0.1875
common for 	 0.08
is about 	 0.0020325203252032522
, dimensions 	 0.0005614823133071309
-- i.e. 	 0.04
read aloud 	 0.14285714285714285
both in 	 0.03225806451612903
digital texts 	 0.14285714285714285
<s> Words 	 0.0015372790161414297
speech caused 	 0.006578947368421052
not consistently 	 0.017857142857142856
<s> Further 	 0.0023059185242121443
some assertive 	 0.012048192771084338
automatic is 	 0.043478260869565216
model temporal 	 0.03333333333333333
binary classification 	 0.5
components operating 	 0.2
that can 	 0.04609929078014184
needs additional 	 0.1
adjacent words 	 0.3333333333333333
and weaknesses 	 0.001445086705202312
the neural-network 	 0.0006920415224913495
scanner and 	 0.3333333333333333
not spend 	 0.008928571428571428
are statistical 	 0.004149377593360996
unsupervised and 	 0.125
Giro , 	 1.0
organizations such 	 1.0
do all 	 0.038461538461538464
same information 	 0.04
infinitive marker 	 1.0
CSIS -RRB- 	 0.5
8 % 	 1.0
resolution , 	 0.25
translator 's 	 0.2857142857142857
; no 	 0.02127659574468085
each known 	 0.022222222222222223
He entered 	 0.125
are certain 	 0.004149377593360996
to keep 	 0.0026560424966799467
your system 	 0.5
without understanding 	 0.07692307692307693
currently . 	 0.14285714285714285
and use 	 0.004335260115606936
text in 	 0.050314465408805034
separate tokens 	 0.1
from any 	 0.009615384615384616
and there 	 0.005780346820809248
recall-based to 	 0.5
can produce 	 0.0055248618784530384
context to 	 0.030303030303030304
analysis Variation 	 0.015384615384615385
is related 	 0.0020325203252032522
regions for 	 0.5
and linear 	 0.001445086705202312
difficult to 	 0.39285714285714285
-RRB- and 	 0.05420054200542006
formally expressed 	 0.5
objective sentences 	 0.2
But then 	 0.16666666666666666
Research Corporation 	 0.125
step -RRB- 	 0.06666666666666667
stochastic taggers 	 0.125
principles which 	 1.0
1950s , 	 0.5
-RRB- in 	 0.01084010840108401
and assess 	 0.001445086705202312
diversity '' 	 0.25
which arise 	 0.007246376811594203
' pyramid 	 0.05263157894736842
linear representation 	 0.14285714285714285
pre-marked . 	 1.0
summaries and 	 0.046511627906976744
of intelligence 	 0.00089126559714795
be unique 	 0.004219409282700422
Loebner prize 	 1.0
engines , 	 0.3333333333333333
related fields 	 0.06666666666666667
absorbing Markov 	 0.3333333333333333
do n't 	 0.07692307692307693
the bridging 	 0.0006920415224913495
questions about 	 0.15384615384615385
recall . 	 0.6666666666666666
that machine 	 0.010638297872340425
Bush '' 	 0.5
were simply 	 0.024390243902439025
The state-of-the-art 	 0.005208333333333333
some variant 	 0.012048192771084338
dispense with 	 1.0
geography , 	 1.0
effect of 	 0.5
-RRB- Acoustical 	 0.0027100271002710027
rather than 	 0.875
SYSTRAN for 	 1.0
more words 	 0.010526315789473684
in sentiment 	 0.0018726591760299626
concurrently with 	 1.0
the inter-texual 	 0.0006920415224913495
by both 	 0.005714285714285714
vs. manual 	 0.08333333333333333
many applications 	 0.038461538461538464
promoting diversity 	 1.0
had a 	 0.2857142857142857
Speereo Software 	 0.5
subtask of 	 1.0
A. Lauriault\/Loriot 	 0.4
means that 	 0.6666666666666666
, some 	 0.0050533408197641775
comparing the 	 0.5
disambiguate the 	 0.3333333333333333
all . 	 0.023255813953488372
was recognized 	 0.012987012987012988
translate spoken 	 0.16666666666666666
several classifiers 	 0.045454545454545456
Teun A. 	 1.0
of off-line 	 0.00089126559714795
necessary to 	 0.2
called the 	 0.1111111111111111
qualitatively The 	 1.0
appropriately , 	 0.5
the Alenia 	 0.0006920415224913495
`` random 	 0.005291005291005291
<s> Why 	 0.0015372790161414297
<s> You 	 0.0007686395080707148
linguistic analysis 	 0.0625
likely another 	 0.0625
then , 	 0.08571428571428572
be more 	 0.02109704641350211
be his 	 0.004219409282700422
act as 	 0.75
focus on 	 0.5714285714285714
the other 	 0.005536332179930796
by Turney 	 0.005714285714285714
word context 	 0.016666666666666666
human judge 	 0.021739130434782608
the very 	 0.0006920415224913495
computer-understandable data 	 1.0
Brill tagger 	 0.3333333333333333
ELIZA , 	 0.3333333333333333
Brill Tagger 	 0.3333333333333333
include Single 	 0.037037037037037035
the strings 	 0.0006920415224913495
combination of 	 0.2
of languages 	 0.00089126559714795
can aid 	 0.0055248618784530384
, POS 	 0.0005614823133071309
tagging could 	 0.04
-LRB- context-free 	 0.0027100271002710027
concept -LRB- 	 0.25
human would 	 0.021739130434782608
available isolated-word 	 0.058823529411764705
constrained , 	 1.0
artificial intelligence 	 0.6363636363636364
called a 	 0.05555555555555555
modeling approach 	 0.14285714285714285
or cross-lingual 	 0.0045045045045045045
Commanders and 	 1.0
most later 	 0.017241379310344827
properties as 	 0.25
, Talmy 	 0.0005614823133071309
materials . 	 0.5
and possibly 	 0.001445086705202312
TextRank While 	 0.07142857142857142
simply using 	 0.08333333333333333
, symbolic 	 0.0005614823133071309
spoken languages 	 0.14285714285714285
PageRank selects 	 0.16666666666666666
until the 	 0.5
with two 	 0.01092896174863388
normalization to 	 0.16666666666666666
i.e. , 	 0.3684210526315789
concerns ; 	 0.5
These methods 	 0.17647058823529413
summarization algorithms 	 0.02
answer reuse 	 0.03333333333333333
25 % 	 1.0
then use 	 0.02857142857142857
being expended 	 0.05555555555555555
his or 	 0.08333333333333333
require that 	 0.045454545454545456
a mixture 	 0.001226993865030675
proved far 	 0.3333333333333333
`` depth 	 0.005291005291005291
very attractive 	 0.024390243902439025
Effective natural 	 1.0
about 25 	 0.025
and end 	 0.001445086705202312
Speech When 	 0.03225806451612903
from a 	 0.11538461538461539
further clues 	 0.125
A large 	 0.02
but that 	 0.04411764705882353
described above 	 0.5
, Robert 	 0.0016844469399213925
tagging Koine 	 0.04
1970s , 	 0.3333333333333333
place in 	 0.5
Austrian emigre 	 1.0
labor intensive 	 0.5
translated as 	 0.25
distance , 	 0.6666666666666666
II -LRB- 	 0.5
early market 	 0.1
output a 	 0.07692307692307693
is Hard 	 0.0020325203252032522
particular words 	 0.07692307692307693
also lead 	 0.014492753623188406
add them 	 1.0
<s> Paul 	 0.0007686395080707148
model , 	 0.1
a multi-way 	 0.001226993865030675
a worldwide 	 0.001226993865030675
Rules are 	 0.6666666666666666
the way 	 0.002768166089965398
reliable hits 	 0.25
e.g. who 	 0.017857142857142856
a year 	 0.001226993865030675
several million 	 0.045454545454545456
specific domain 	 0.14285714285714285
+5 scale 	 1.0
modeling salience 	 0.14285714285714285
grammars are 	 0.07142857142857142
project compared 	 0.15384615384615385
usefully be 	 1.0
<s> Shepard 	 0.0007686395080707148
if uttered 	 0.03571428571428571
-LRB- Nuance 	 0.0027100271002710027
Iraq in 	 0.5
annotation has 	 0.25
rocks returned 	 1.0
networks discussions 	 0.07142857142857142
follow-the-bouncing-ball video 	 1.0
adverb , 	 1.0
decade , 	 0.3333333333333333
thanks to 	 1.0
aspects of 	 0.8571428571428571
Sound Graph 	 0.3333333333333333
subject , 	 0.25
language using 	 0.006756756756756757
providers began 	 1.0
changes which 	 1.0
that perform 	 0.0035460992907801418
Chafe , 	 1.0
non-annotated data 	 1.0
discontinuous speech 	 0.6666666666666666
feature segment 	 0.07692307692307693
whole discourses 	 0.1111111111111111
common when 	 0.04
significantly . 	 1.0
sub-committee is 	 1.0
discrete characters 	 0.3333333333333333
single source 	 0.14285714285714285
least five 	 0.2
claim that 	 1.0
without documents 	 0.07692307692307693
software resources 	 0.037037037037037035
the conversations 	 0.0006920415224913495
to focus 	 0.0013280212483399733
themselves simply 	 0.25
in addition 	 0.0056179775280898875
require some 	 0.045454545454545456
k -RRB- 	 1.0
sometimes , 	 0.07692307692307693
an open 	 0.015151515151515152
might co-occur 	 0.038461538461538464
knowledge representation 	 0.037037037037037035
that words 	 0.0070921985815602835
: Emergent 	 0.00980392156862745
lexical analyser 	 0.07692307692307693
and actual 	 0.001445086705202312
, their 	 0.0005614823133071309
the specific 	 0.0020761245674740486
were published 	 0.024390243902439025
Manual analysis 	 0.3333333333333333
relationship with 	 0.16666666666666666
forecasts . 	 0.2
for editing 	 0.0036101083032490976
a succession 	 0.001226993865030675
the conversational 	 0.0006920415224913495
The experiment 	 0.005208333333333333
of numbers 	 0.0017825311942959
over sixty 	 0.08333333333333333
National Giro 	 0.3333333333333333
a widely-reported 	 0.001226993865030675
summary -RRB- 	 0.023809523809523808
also possible 	 0.043478260869565216
of cases 	 0.00089126559714795
exponential time 	 0.5
word -LRB- 	 0.016666666666666666
sixty Russian 	 1.0
retrieving information 	 1.0
In 1950 	 0.01904761904761905
, Edmund 	 0.0005614823133071309
corpora specifically 	 0.09090909090909091
after removing 	 0.08333333333333333
collection sizes 	 0.2
the holder 	 0.0006920415224913495
: Automatically 	 0.00980392156862745
<s> Winograd 	 0.0007686395080707148
put together 	 0.25
to action 	 0.0013280212483399733
scanned page 	 0.3333333333333333
semantics is 	 0.07142857142857142
data used 	 0.025974025974025976
OCR Since 	 0.02040816326530612
<s> To 	 0.006149116064565719
bites dog 	 0.3333333333333333
based on 	 0.8333333333333334
answers that 	 0.08333333333333333
techniques that 	 0.08695652173913043
-LRB- disambiguation 	 0.0027100271002710027
major database 	 0.08333333333333333
program . 	 0.13636363636363635
accessibility , 	 1.0
languages with 	 0.02
London -RRB- 	 1.0
Google used 	 0.25
`` happy 	 0.005291005291005291
performance on 	 0.05555555555555555
those meanings 	 0.045454545454545456
considerable attention 	 0.2
article contains 	 0.034482758620689655
single language 	 0.07142857142857142
larger tasks 	 0.0625
with initial 	 0.00546448087431694
conference rooms 	 0.5
HMM based 	 0.3333333333333333
of trying 	 0.00089126559714795
into English 	 0.02564102564102564
rules from 	 0.023255813953488372
its context 	 0.02857142857142857
Speaker Independent 	 0.16666666666666666
as hidden 	 0.003484320557491289
<s> Among 	 0.0007686395080707148
Pang showed 	 0.3333333333333333
meaning which 	 0.043478260869565216
was followed 	 0.012987012987012988
words surrounding 	 0.009174311926605505
security process 	 1.0
backgrounds , 	 1.0
-RRB- a 	 0.005420054200542005
Dictionary-based machine 	 0.5
, 10 	 0.0011229646266142617
then we 	 0.05714285714285714
cockpit , 	 0.5
and perhaps 	 0.001445086705202312
about feature 	 0.025
formed Symantec 	 0.2
2000 , 	 0.3333333333333333
was walking 	 0.012987012987012988
first-cut can 	 1.0
understanding . 	 0.06060606060606061
, sentence 	 0.0005614823133071309
often requires 	 0.022727272727272728
producing natural 	 0.3333333333333333
not agree 	 0.008928571428571428
and Jabberwacky 	 0.001445086705202312
do this 	 0.07692307692307693
of phrase 	 0.00089126559714795
customize the 	 0.5
improve readability 	 0.07692307692307693
non-trivial techniques 	 0.5
a '' 	 0.001226993865030675
harder tasks 	 0.14285714285714285
or emails 	 0.0045045045045045045
samples per 	 0.5
this problem 	 0.07692307692307693
to predict 	 0.0026560424966799467
order '' 	 0.07142857142857142
analysis include 	 0.015384615384615385
& OnlineOCR 	 0.25
maximum mutual 	 0.16666666666666666
Harris et 	 0.1111111111111111
might be 	 0.23076923076923078
studies , 	 0.5
size N 	 0.16666666666666666
separate words 	 0.3
context have 	 0.030303030303030304
are unable 	 0.004149377593360996
the Viterbi 	 0.002768166089965398
map to 	 0.5
dried up 	 1.0
arguably -RRB- 	 0.5
ends a 	 0.5
size -RRB- 	 0.16666666666666666
: Creating 	 0.0196078431372549
true only 	 0.5
language will 	 0.006756756756756757
would look 	 0.03773584905660377
a piecewise 	 0.001226993865030675
speech recogniton 	 0.006578947368421052
Rescoring is 	 1.0
; Computed 	 0.02127659574468085
shorter the 	 0.5
A. D. 	 0.2
among named 	 0.125
less time 	 0.08333333333333333
internal semantic 	 0.2
better guide 	 0.1111111111111111
the formal 	 0.001384083044982699
a rule 	 0.001226993865030675
very sophisticated 	 0.024390243902439025
output to 	 0.038461538461538464
use hidden 	 0.013888888888888888
is quite 	 0.0020325203252032522
two sentences 	 0.06896551724137931
can both 	 0.0055248618784530384
of deciding 	 0.00089126559714795
`` I 	 0.005291005291005291
were limited 	 0.024390243902439025
issue . 	 0.25
field are 	 0.037037037037037035
direction . 	 0.3333333333333333
ROUGE-1 only 	 0.2
consider the 	 0.5
four step 	 0.14285714285714285
computerized language 	 0.5
now absorbing 	 0.07692307692307693
only study 	 0.02631578947368421
, spacecraft 	 0.0005614823133071309
on different 	 0.0047169811320754715
control , 	 0.2
be repeated 	 0.004219409282700422
than optimizing 	 0.022222222222222223
symbols , 	 0.3333333333333333
object , 	 0.5
a global 	 0.001226993865030675
the scope 	 0.001384083044982699
- keyphrases 	 0.0625
both query 	 0.03225806451612903
<s> Alternatively 	 0.0015372790161414297
and search 	 0.001445086705202312
core elements 	 0.5
<s> NLP 	 0.0007686395080707148
too expensive 	 0.3333333333333333
desired structure 	 0.2
grammatical gender 	 0.09090909090909091
parsers in 	 0.07692307692307693
-RRB- for 	 0.005420054200542005
Such algorithms 	 0.125
of aircraft 	 0.00089126559714795
Gee , 	 1.0
statistical quantity 	 0.030303030303030304
301 computer 	 1.0
corpus with 	 0.03225806451612903
interest Topics 	 0.09090909090909091
representative of 	 1.0
The attitude 	 0.005208333333333333
structure that 	 0.08333333333333333
information retrieval 	 0.10869565217391304
of other 	 0.0035650623885918
Computing Machinery 	 0.5
abbreviations that 	 0.2
goal is 	 0.42857142857142855
-LRB- and 	 0.013550135501355014
of word-forms 	 0.00089126559714795
search engine 	 0.09090909090909091
automatic summarization 	 0.08695652173913043
the pragmatics 	 0.0006920415224913495
are widely 	 0.004149377593360996
spoke the 	 1.0
: Neural 	 0.00980392156862745
<s> Precision 	 0.0007686395080707148
be detected 	 0.004219409282700422
cards for 	 1.0
consider a 	 0.25
form multi-word 	 0.05
`` Why 	 0.015873015873015872
toolkit is 	 0.5
-LRB- essentially 	 0.0027100271002710027
employs a 	 0.5
Post has 	 0.5
extensive lexicons 	 0.3333333333333333
to mention 	 0.0013280212483399733
ease interoperability 	 1.0
systems included 	 0.008928571428571428
overall system 	 0.16666666666666666
, Gina 	 0.0005614823133071309
of listening 	 0.00089126559714795
language to 	 0.02702702702702703
of tasks 	 0.00089126559714795
necessary therefore 	 0.1
, associating 	 0.0005614823133071309
the idea 	 0.001384083044982699
, beyond 	 0.0005614823133071309
the desired 	 0.002768166089965398
agree about 	 0.3333333333333333
have '' 	 0.009615384615384616
high rank 	 0.05555555555555555
floods '' 	 1.0
ROUGE-1 scores 	 0.2
from script 	 0.009615384615384616
more precise 	 0.010526315789473684
functional languages 	 0.5
same as 	 0.08
the original 	 0.006920415224913495
<s> Prominent 	 0.0007686395080707148
any safety 	 0.03225806451612903
just keeping 	 0.1111111111111111
given a 	 0.16666666666666666
good approximation 	 0.07692307692307693
have difficulty 	 0.009615384615384616
-LRB- perhaps 	 0.0027100271002710027
aircraft Substantial 	 0.14285714285714285
word vector 	 0.016666666666666666
length , 	 0.25
subjectivity used 	 0.5
parsing aims 	 0.03571428571428571
computer-aided translation 	 0.3333333333333333
Xerox , 	 0.5
typically based 	 0.05555555555555555
recognizer , 	 1.0
Word sense 	 0.2857142857142857
by Junqua 	 0.005714285714285714
example where 	 0.012345679012345678
might learn 	 0.038461538461538464
speech single 	 0.006578947368421052
The algorithms 	 0.010416666666666666
grammatical information 	 0.09090909090909091
appears several 	 0.2
% accurate 	 0.05128205128205128
<s> Typically 	 0.0007686395080707148
at characters 	 0.014705882352941176
termed Direct 	 0.25
an 11 	 0.007575757575757576
linguistics -LRB- 	 0.05
Data sources 	 1.0
<s> Formal 	 0.0007686395080707148
also need 	 0.014492753623188406
NLP problems 	 0.0425531914893617
used OCR 	 0.008849557522123894
, Nikolas 	 0.0005614823133071309
or verify 	 0.0045045045045045045
rapid access 	 1.0
a unit 	 0.001226993865030675
either an 	 0.1
The final 	 0.005208333333333333
may require 	 0.038461538461538464
inclusion in 	 1.0
produce more 	 0.09090909090909091
parts of 	 1.0
involve various 	 0.16666666666666666
about a 	 0.025
value is 	 0.3333333333333333
researchers to 	 0.1
the prolific 	 0.0006920415224913495
Speaker Dependence 	 0.16666666666666666
using ATC 	 0.01694915254237288
media . 	 0.16666666666666666
A possible 	 0.02
that serve 	 0.0035460992907801418
hour or 	 1.0
before classifying 	 0.16666666666666666
stock market 	 0.3333333333333333
'' each 	 0.005154639175257732
specific voice 	 0.047619047619047616
<s> Main 	 0.0007686395080707148
additional costs 	 0.16666666666666666
as simple 	 0.006968641114982578
'' continued 	 0.005154639175257732
predict task-effectiveness 	 0.3333333333333333
As a 	 0.1111111111111111
describing graphs 	 0.25
the order 	 0.001384083044982699
, LUNAR 	 0.0005614823133071309
precise ones 	 0.3333333333333333
consistently achieve 	 0.3333333333333333
select the 	 0.3333333333333333
versions for 	 0.3333333333333333
part of 	 0.8148148148148148
Markov model 	 0.4444444444444444
<s> It 	 0.026133743274404306
lessening of 	 1.0
is angry 	 0.0020325203252032522
Fundamentals of 	 1.0
that investigates 	 0.0035460992907801418
in helicopters 	 0.003745318352059925
analysis Genre 	 0.015384615384615385
OnStar , 	 1.0
ways by 	 0.125
other terms 	 0.014285714285714285
<s> ATNs 	 0.0007686395080707148
extraction task 	 0.03225806451612903
others have 	 0.08333333333333333
or her 	 0.009009009009009009
examples ? 	 0.041666666666666664
logical , 	 0.16666666666666666
were repeatedly 	 0.024390243902439025
technology devised 	 0.045454545454545456
least partly 	 0.2
as 10 	 0.003484320557491289
human-like interaction 	 1.0
answer the 	 0.03333333333333333
said with 	 1.0
categories ; 	 0.1111111111111111
but only 	 0.014705882352941176
translation may 	 0.013513513513513514
Based on 	 1.0
when outputting 	 0.02857142857142857
words are 	 0.09174311926605505
be maintained 	 0.004219409282700422
pronunciation , 	 1.0
one distinguishes 	 0.015384615384615385
program focuses 	 0.045454545454545456
to some 	 0.006640106241699867
by its 	 0.005714285714285714
models are 	 0.038461538461538464
and developed 	 0.002890173410404624
Two years 	 0.42857142857142855
features describing 	 0.038461538461538464
papers by 	 0.3333333333333333
make use 	 0.05
of glass-box 	 0.00089126559714795
die or 	 1.0
documents obtained 	 0.02631578947368421
, once 	 0.0005614823133071309
are pulled 	 0.004149377593360996
engine . 	 0.6666666666666666
<s> LR 	 0.0007686395080707148
it learns 	 0.008547008547008548
Peru . 	 0.5
France , 	 0.5
required , 	 0.14285714285714285
texts or 	 0.058823529411764705
evaluating summaries 	 0.4
grown . 	 1.0
you 've 	 0.15384615384615385
an excerpt 	 0.007575757575757576
terms , 	 0.07692307692307693
to meet 	 0.005312084993359893
the unigrams 	 0.001384083044982699
4 star 	 0.2
<s> This 	 0.03996925441967717
12 \* 	 0.4
mainly with 	 0.16666666666666666
can add 	 0.0055248618784530384
corpus - 	 0.03225806451612903
have the 	 0.009615384615384616
information usually 	 0.021739130434782608
level features 	 0.05
-LRB- For 	 0.005420054200542005
difficult than 	 0.10714285714285714
this area 	 0.03296703296703297
LexRank paper 	 0.08333333333333333
on stochastic 	 0.0047169811320754715
will approach 	 0.02857142857142857
algorithms to 	 0.08571428571428572
of freely 	 0.00089126559714795
flying in 	 1.0
would never 	 0.018867924528301886
, fuse 	 0.0005614823133071309
26 letters 	 1.0
Vauquois ' 	 1.0
by these 	 0.005714285714285714
segmentation approaches 	 0.030303030303030304
problem . 	 0.22727272727272727
the partial 	 0.0006920415224913495
They combine 	 0.3333333333333333
RSI became 	 1.0
rate of 	 0.2727272727272727
On-line systems 	 0.3333333333333333
organization , 	 0.2
deterministic decisions 	 0.25
recognition software 	 0.024793388429752067
articles in 	 0.25
standards require 	 0.2
for many 	 0.007220216606498195
inter-annotator agreement 	 1.0
noun phrases 	 0.07142857142857142
on the 	 0.30660377358490565
and\/or aural 	 0.3333333333333333
of allowing 	 0.00089126559714795
, C 	 0.0005614823133071309
, LinguaSys 	 0.0005614823133071309
by receivers 	 0.005714285714285714
multiple possible 	 0.15384615384615385
of 1956 	 0.00089126559714795
-LRB- English 	 0.0027100271002710027
, Invoice 	 0.0005614823133071309
issue in 	 0.125
The sub-committee 	 0.005208333333333333
any domain 	 0.03225806451612903
a letter 	 0.001226993865030675
deemed most 	 0.5
reason why 	 0.25
probabilities , 	 0.09090909090909091
Current machine 	 0.2
doctors -RRB- 	 0.3333333333333333
character groups 	 0.045454545454545456
many researchers 	 0.019230769230769232
of extracting 	 0.00089126559714795
simpler questions 	 0.3333333333333333
Each article 	 0.16666666666666666
Edges are 	 1.0
Paul Hopper 	 0.2
computer -LRB- 	 0.022727272727272728
measure a 	 0.09090909090909091
to allow 	 0.00398406374501992
, Henry 	 0.0005614823133071309
predicting ratings 	 0.5
efforts are 	 0.14285714285714285
Trained linguists 	 1.0
methods In 	 0.022727272727272728
summary . 	 0.2619047619047619
stream is 	 0.5
handmade list 	 1.0
the left 	 0.001384083044982699
from one 	 0.028846153846153848
, pitch 	 0.0005614823133071309
NLP . 	 0.10638297872340426
the object 	 0.0006920415224913495
other modifying 	 0.014285714285714285
results may 	 0.047619047619047616
inaccurate or 	 1.0
e.g. yes-no 	 0.017857142857142856
rank unigrams 	 0.16666666666666666
automatically The 	 0.047619047619047616
because a 	 0.03333333333333333
subjective . 	 0.3333333333333333
3 , 	 0.2
University included 	 0.1111111111111111
Morpholympics compared 	 1.0
places and 	 0.5
<s> Human-machine 	 0.0007686395080707148
for using 	 0.0036101083032490976
have not 	 0.019230769230769232
description and 	 1.0
of DA 	 0.00089126559714795
second step 	 0.2
; e.g. 	 0.0425531914893617
indicate a 	 0.3333333333333333
article quoting 	 0.034482758620689655
component sentences 	 0.2
corporation does 	 1.0
second aim 	 0.1
limited vocabulary 	 0.1
<s> Hybrid 	 0.0007686395080707148
segmentation will 	 0.030303030303030304
language understanding 	 0.0945945945945946
this ostensibly 	 0.01098901098901099
accuracies in 	 1.0
with rules 	 0.00546448087431694
, V 	 0.0005614823133071309
precision and 	 0.4
gracefully with 	 1.0
contain the 	 0.16666666666666666
, Art 	 0.0005614823133071309
analogy and 	 1.0
compared a 	 0.14285714285714285
phases . 	 1.0
proper names 	 0.14285714285714285
knowledge and 	 0.07407407407407407
no spaces 	 0.07692307692307693
likely a 	 0.0625
a number 	 0.024539877300613498
1965 , 	 0.5
of this 	 0.00980392156862745
successful demonstration 	 0.1111111111111111
of shared 	 0.00089126559714795
linguistic resources 	 0.0625
medical data 	 0.3333333333333333
so-called ROUGE 	 0.3333333333333333
of approaches 	 0.00089126559714795
constructs can 	 0.3333333333333333
fueled interest 	 1.0
Imagine you 	 1.0
with part-of-speech 	 0.00546448087431694
conversation or 	 0.25
better translations 	 0.1111111111111111
human -LRB- 	 0.043478260869565216
that do 	 0.0035460992907801418
Pike , 	 1.0
attempts to 	 0.5
textbook of 	 0.5
air -LRB- 	 0.2
translation MAHT 	 0.013513513513513514
sciences , 	 0.5
input , 	 0.07317073170731707
different meanings 	 0.02040816326530612
about democratizing 	 0.025
<s> Manual 	 0.0015372790161414297
the door 	 0.0006920415224913495
actually correct 	 0.3333333333333333
says phrases 	 1.0
to highly 	 0.0013280212483399733
answers might 	 0.08333333333333333
manipulate it 	 0.3333333333333333
farther forward 	 1.0
Some ISO 	 0.047619047619047616
Unsupervised tagging 	 0.16666666666666666
a sublanguage 	 0.001226993865030675
nodes represents 	 0.14285714285714285
a professional 	 0.001226993865030675
further information 	 0.125
not trivial 	 0.008928571428571428
direct hand 	 0.16666666666666666
Janet Kolodner 	 0.5
mainland Scotland 	 1.0
nice side 	 0.25
elaboration , 	 1.0
attractive acoustic 	 0.3333333333333333
contains no 	 0.1
<s> Dictionary-based 	 0.0007686395080707148
would consist 	 0.018867924528301886
effectively utilize 	 0.3333333333333333
moderate should 	 0.2
analytics to 	 1.0
AVRADA tests 	 0.5
grammatical and 	 0.09090909090909091
system and 	 0.03225806451612903
in France 	 0.003745318352059925
of answer 	 0.00089126559714795
7110.65 details 	 1.0
constructions occur 	 1.0
for can 	 0.0036101083032490976
is facing 	 0.0020325203252032522
have assessed 	 0.009615384615384616
information display 	 0.021739130434782608
grammar can 	 0.02702702702702703
original . 	 0.07692307692307693
between 1964 	 0.02564102564102564
if word 	 0.03571428571428571
<s> Its 	 0.0015372790161414297
a routing 	 0.001226993865030675
a chunk 	 0.007361963190184049
doing as 	 0.5
team led 	 1.0
time-consuming . 	 0.3333333333333333
undirected and 	 1.0
`` patient 	 0.005291005291005291
, Racter 	 0.0005614823133071309
POS tagger 	 0.3076923076923077
evaluated in 	 0.14285714285714285
fact that 	 0.45454545454545453
continued research 	 0.2222222222222222
and typical 	 0.001445086705202312
<s> Shallow 	 0.0015372790161414297
entrants companies 	 1.0
with an 	 0.0273224043715847
in Norman 	 0.0018726591760299626
boundary disambiguation 	 0.3333333333333333
abstractive keyphrase 	 0.16666666666666666
A similar 	 0.02
, Inc. 	 0.0011229646266142617
the theory 	 0.0020761245674740486
syntactic parsers 	 0.07692307692307693
representations such 	 0.25
to take 	 0.006640106241699867
any human 	 0.03225806451612903
Deborah Schiffrin 	 0.5
<s> Because 	 0.0015372790161414297
delta-delta coefficients 	 1.0
Aletta Norval 	 1.0
into more 	 0.02564102564102564
of errors 	 0.00089126559714795
in psycholinguistics 	 0.0018726591760299626
categories could 	 0.1111111111111111
Jones Street 	 1.0
where the 	 0.37142857142857144
recogniton by 	 0.5
Paul Drew 	 0.2
the dictator 	 0.0006920415224913495
text printed 	 0.006289308176100629
context-free grammar 	 0.45454545454545453
Journal -RRB- 	 0.3333333333333333
innumerable studies 	 1.0
often argued 	 0.022727272727272728
an example 	 0.03787878787878788
Human sentences 	 0.2
and 1957 	 0.001445086705202312
and QA 	 0.001445086705202312
possible without 	 0.041666666666666664
of large 	 0.0035650623885918
and easily 	 0.001445086705202312
quantity . 	 0.3333333333333333
Treebank -RRB- 	 0.16666666666666666
and model 	 0.001445086705202312
for part-of-speech 	 0.007220216606498195
likely to 	 0.4375
the process 	 0.007612456747404845
seconds , 	 1.0
paraphrases -LRB- 	 1.0
- not 	 0.0625
, mathematical 	 0.0005614823133071309
at a 	 0.058823529411764705
that produce 	 0.0035460992907801418
involve the 	 0.16666666666666666
the extraction 	 0.002768166089965398
research may 	 0.023809523809523808
Page\/Lex\/TextRank that 	 1.0
closed-captioning of 	 1.0
rules similar 	 0.046511627906976744
then the 	 0.05714285714285714
automated language 	 0.14285714285714285
and thus 	 0.004335260115606936
source language 	 0.125
Vocabulary Size 	 0.3333333333333333
University researchers 	 0.1111111111111111
post-process the 	 1.0
labeled keyphrases 	 0.3333333333333333
a translator 	 0.0036809815950920245
work is 	 0.125
be approximately 	 0.004219409282700422
recognition is 	 0.0743801652892562
an associated 	 0.007575757575757576
networks allow 	 0.07142857142857142
might use 	 0.07692307692307693
usually be 	 0.03125
Recognition of 	 0.25
these sounds 	 0.023809523809523808
human review 	 0.021739130434782608
hybrid system 	 0.5
the dialog 	 0.0006920415224913495
, definitional 	 0.0005614823133071309
all governmental 	 0.023255813953488372
in processing 	 0.0018726591760299626
expressivity of 	 1.0
, adverb 	 0.0005614823133071309
graphs and 	 1.0
be resolved 	 0.004219409282700422
set to 	 0.02564102564102564
Speech segmentation 	 0.0967741935483871
must be 	 0.42857142857142855
scripts , 	 0.3333333333333333
to estimate 	 0.00398406374501992
these every 	 0.023809523809523808
these tasks 	 0.047619047619047616
, except 	 0.0005614823133071309
social psychology 	 0.07142857142857142
of mainland 	 0.0017825311942959
as relations 	 0.003484320557491289
whereas speed 	 0.3333333333333333
final letter 	 0.1111111111111111
government and 	 0.3333333333333333
← barmaid 	 1.0
translation has 	 0.02702702702702703
important points 	 0.0625
they create 	 0.025
in classifying 	 0.0018726591760299626
a bank 	 0.001226993865030675
a subsystem 	 0.001226993865030675
all such 	 0.023255813953488372
software for 	 0.037037037037037035
track of 	 1.0
Schools commonly 	 1.0
article deal 	 0.034482758620689655
common strategy 	 0.04
selecting duplicate 	 0.2
the Sparkle 	 0.0006920415224913495
Programming methods 	 0.3333333333333333
computationally feasible 	 0.5
, based 	 0.0011229646266142617
used all 	 0.008849557522123894
sound to 	 0.05
of bottom-up 	 0.00089126559714795
by an 	 0.011428571428571429
Roger Schank 	 0.75
represents an 	 0.25
resources , 	 0.3333333333333333
Comparing these 	 1.0
successively more 	 1.0
`` New 	 0.005291005291005291
entirely committed 	 0.5
In part-of-speech 	 0.009523809523809525
an abstractive 	 0.015151515151515152
% accuracy 	 0.10256410256410256
, Malcolm 	 0.0005614823133071309
was proposed 	 0.025974025974025976
need as 	 0.047619047619047616
coding of 	 1.0
word pronunciations 	 0.016666666666666666
lend well 	 1.0
into segments 	 0.02564102564102564
a storm 	 0.001226993865030675
<s> Research 	 0.0015372790161414297
more consistent 	 0.010526315789473684
input into 	 0.024390243902439025
in October 	 0.0018726591760299626
`` an 	 0.005291005291005291
garden path 	 1.0
of allowable 	 0.00089126559714795
In addition 	 0.02857142857142857
of product 	 0.00089126559714795
their framework 	 0.029411764705882353
on top 	 0.0047169811320754715
of supervised 	 0.00089126559714795
Word error 	 0.14285714285714285
rise of 	 0.5
piece of 	 1.0
the AVRADA 	 0.0006920415224913495
manual evaluation 	 0.5
merge adjacent 	 1.0
create both 	 0.058823529411764705
in this 	 0.018726591760299626
edges ? 	 0.2857142857142857
, René 	 0.0005614823133071309
above techniques 	 0.07692307692307693
translation paradigms 	 0.013513513513513514
the latter 	 0.0006920415224913495
a person 	 0.013496932515337423
speech Adverse 	 0.006578947368421052
Syntactic ; 	 1.0
language Prolog 	 0.006756756756756757
the length 	 0.001384083044982699
run the 	 0.4
keeping the 	 0.5
were compared 	 0.04878048780487805
programs to 	 0.09090909090909091
area of 	 0.45454545454545453
: Word 	 0.00980392156862745
persuasion , 	 1.0
The `` 	 0.010416666666666666
being scanned 	 0.05555555555555555
of FoG 	 0.00089126559714795
world '' 	 0.06666666666666667
relations are 	 0.08333333333333333
examples and 	 0.16666666666666666
revolution in 	 1.0
Web-based OCR 	 0.6666666666666666
requiring all 	 0.5
sentences in 	 0.10526315789473684
G. , 	 0.5
have resulted 	 0.009615384615384616
whether medium 	 0.07692307692307693
possessive , 	 1.0
OCR systems 	 0.08163265306122448
just a 	 0.2222222222222222
or topics 	 0.0045045045045045045
standards . 	 0.4
uses search 	 0.07142857142857142
can achieve 	 0.0055248618784530384
resolution is 	 0.25
-LRB- of 	 0.005420054200542005
want to 	 0.8333333333333334
involved , 	 0.16666666666666666
waves would 	 0.14285714285714285
larger summarization 	 0.0625
documents were 	 0.02631578947368421
meaning but 	 0.043478260869565216
e.g. Phonemes 	 0.017857142857142856
-RRB- Commissioned 	 0.0027100271002710027
competence , 	 1.0
account context 	 0.3333333333333333
results over 	 0.047619047619047616
projects never 	 0.5
has given 	 0.011904761904761904
distance to 	 0.3333333333333333
of semantics 	 0.00089126559714795
its main 	 0.02857142857142857
had similar 	 0.07142857142857142
is 8000 	 0.0020325203252032522
make soft 	 0.2
ATC -RRB- 	 0.2
classifier and 	 0.14285714285714285
or word-category 	 0.0045045045045045045
optimizes parameters 	 1.0
direction of 	 0.3333333333333333
The apple 	 0.010416666666666666
its suitability 	 0.02857142857142857
, OCR 	 0.0011229646266142617
used . 	 0.04424778761061947
, plural 	 0.0005614823133071309
techniques from 	 0.043478260869565216
Research , 	 0.125
Question processing 	 0.14285714285714285
Manual evaluation 	 0.6666666666666666
using conventional 	 0.01694915254237288
times in 	 0.2
Tokens are 	 1.0
Holmes , 	 1.0
be implemented 	 0.008438818565400843
ways in 	 0.125
sources for 	 0.16666666666666666
possibilities must 	 0.2
database queries 	 0.1
classes of 	 0.4
in logical 	 0.0018726591760299626
in rank 	 0.0018726591760299626
<s> Whether 	 0.0015372790161414297
relay services 	 1.0
most of 	 0.08620689655172414
or disease 	 0.0045045045045045045
including web 	 0.07142857142857142
best with 	 0.05555555555555555
these systems 	 0.11904761904761904
President Obama 	 0.25
describe a 	 0.16666666666666666
Independent '' 	 1.0
the higher 	 0.0006920415224913495
while others 	 0.2
learn such 	 0.07692307692307693
Each concept 	 0.16666666666666666
specific contexts 	 0.047619047619047616
rescoring -RRB- 	 1.0
to two 	 0.0013280212483399733
Lehnert , 	 0.6666666666666666
setting steer-point 	 0.2
of similarity 	 0.00089126559714795
information . 	 0.08695652173913043
meaning from 	 0.043478260869565216
done in 	 0.45454545454545453
new insights 	 0.041666666666666664
and recording 	 0.001445086705202312
spoken -RRB- 	 0.07142857142857142
considered . 	 0.1111111111111111
Language Processor 	 0.08333333333333333
LexRank The 	 0.08333333333333333
Pyramid Method 	 1.0
so far 	 0.03333333333333333
this level 	 0.01098901098901099
opportunity rather 	 0.5
sponsored evaluations 	 0.5
named entity 	 0.2857142857142857
result of 	 0.2727272727272727
generation because 	 0.1111111111111111
Harris The 	 0.1111111111111111
To overcome 	 0.1111111111111111
CCD flatbed 	 1.0
Dynamic Programming 	 0.2
`` political 	 0.005291005291005291
by Naomi 	 0.005714285714285714
ambitious projects 	 1.0
helicopter environment 	 0.5
Grishman R. 	 1.0
verb , 	 0.38461538461538464
but this 	 0.058823529411764705
jokes -LRB- 	 1.0
these numbers 	 0.047619047619047616
noise levels 	 0.125
involves visual 	 0.1
-LRB- stationary 	 0.0027100271002710027
are explicitly 	 0.004149377593360996
is necessary 	 0.0040650406504065045
profile may 	 0.3333333333333333
in spoken 	 0.003745318352059925
relative certainty 	 0.3333333333333333
contexts , 	 0.2857142857142857
ranking process 	 0.14285714285714285
parsing have 	 0.03571428571428571
routed through 	 0.5
effort . 	 0.25
years later 	 0.14285714285714285
ambiguity . 	 0.125
and find 	 0.001445086705202312
SRI International 	 1.0
In contrast 	 0.047619047619047616
`` advanced 	 0.005291005291005291
questions pertaining 	 0.038461538461538464
more formal 	 0.010526315789473684
or semantics 	 0.0045045045045045045
1990 -RRB- 	 0.6666666666666666
has 2 	 0.011904761904761904
integrate reasoning 	 1.0
parameters , 	 0.25
This work 	 0.031746031746031744
candidates instead 	 0.2
NLP system 	 0.0851063829787234
automated target 	 0.14285714285714285
<s> N 	 0.0007686395080707148
disagree on 	 0.3333333333333333
have increased 	 0.028846153846153848
to any 	 0.00398406374501992
MT has 	 0.2
poor coverage 	 1.0
<s> Components 	 0.0007686395080707148
Then the 	 0.4
inflectional morphology 	 1.0
ACL , 	 0.5
culture of 	 1.0
individuals that 	 1.0
methods are 	 0.045454545454545456
be around 	 0.004219409282700422
the legends 	 0.0006920415224913495
reduction of 	 0.5
who '' 	 0.1
which itself 	 0.007246376811594203
be directed 	 0.004219409282700422
to . 	 0.00398406374501992
the water 	 0.0006920415224913495
ASR '' 	 0.16666666666666666
a much 	 0.0036809815950920245
that make 	 0.010638297872340425
printed characters 	 0.08333333333333333
statistics : 	 0.125
paraphrase . 	 1.0
<s> Rescoring 	 0.0007686395080707148
Internet and 	 0.5
vowel in 	 1.0
ambiguity than 	 0.125
specific person 	 0.047619047619047616
a QA 	 0.0049079754601227
at all 	 0.07352941176470588
that participate 	 0.0035460992907801418
to human-written 	 0.0013280212483399733
only as 	 0.02631578947368421
Air controller 	 0.3333333333333333
highest ROUGE-1 	 0.3333333333333333
accuracy reported 	 0.03225806451612903
In 1965 	 0.009523809523809525
Medical Records 	 0.5
Many words 	 0.16666666666666666
some improvement 	 0.012048192771084338
generate high-quality 	 0.05555555555555555
sort of 	 0.6666666666666666
been a 	 0.029411764705882353
are presented 	 0.004149377593360996
Apple Newton 	 1.0
input that 	 0.04878048780487805
possible . 	 0.125
opposed to 	 1.0
linked with 	 0.3333333333333333
are accepted 	 0.004149377593360996
and discontinuous 	 0.001445086705202312
question . 	 0.047619047619047616
we can 	 0.06666666666666667
to the 	 0.10225763612217796
Pallet 1998 	 0.5
hyphenation . 	 1.0
writing rules 	 0.1111111111111111
metrics like 	 0.1111111111111111
or by 	 0.0045045045045045045
ends up 	 0.5
sentence . 	 0.14583333333333334
being developed 	 0.05555555555555555
results suggest 	 0.047619047619047616
as Eugene 	 0.003484320557491289
years , 	 0.23809523809523808
difficulty in 	 0.2857142857142857
using some 	 0.03389830508474576
's intrinsic 	 0.0196078431372549
working for 	 0.14285714285714285
Typical stages 	 0.5
taggers are 	 0.14285714285714285
Japanese -RRB- 	 0.125
engine , 	 0.16666666666666666
<s> LexRank 	 0.0015372790161414297
and Dale 	 0.001445086705202312
This makes 	 0.015873015873015872
3rd rev 	 1.0
the intermediary 	 0.0006920415224913495
<s> Several 	 0.0023059185242121443
Fourier Transform 	 0.3333333333333333
Hence the 	 0.5
psychology , 	 0.75
the '' 	 0.0006920415224913495
usually from 	 0.03125
remains to 	 0.25
which used 	 0.007246376811594203
approaches to 	 0.17857142857142858
algorithm optimizes 	 0.03571428571428571
a linear 	 0.00245398773006135
considered a 	 0.1111111111111111
placed in 	 1.0
software Current 	 0.037037037037037035
ALPAC report 	 1.0
amounts of 	 1.0
likely related 	 0.0625
OCR in 	 0.04081632653061224
generally , 	 0.09090909090909091
their hands 	 0.029411764705882353
for male-female 	 0.0036101083032490976
method in 	 0.0625
component . 	 0.2
case . 	 0.17647058823529413
machine at 	 0.012658227848101266
typically undirected 	 0.05555555555555555
platforms . 	 1.0
, ^ 	 0.0011229646266142617
the resulting 	 0.001384083044982699
Administration , 	 1.0
attempted by 	 1.0
sound that 	 0.05
remarkably similar 	 1.0
Psycholinguists prefer 	 1.0
diversity as 	 0.25
are , 	 0.004149377593360996
world 's 	 0.06666666666666667
cartoon animation 	 1.0
a template 	 0.00245398773006135
to finding 	 0.0013280212483399733
indirect left-recursion 	 1.0
2012 -RRB- 	 1.0
provide a 	 0.3333333333333333
damping factor 	 1.0
users sent 	 0.1111111111111111
light . 	 0.3333333333333333
covariance Gaussians 	 0.5
the EHR 	 0.0006920415224913495
be different 	 0.004219409282700422
late 1930s 	 0.1111111111111111
for their 	 0.007220216606498195
different vendors 	 0.02040816326530612
summary is 	 0.047619047619047616
ones already 	 0.1
<s> Despite 	 0.0007686395080707148
are provided 	 0.004149377593360996
relaxed parser 	 1.0
recent development 	 0.125
sometimes called 	 0.07692307692307693
societal problem 	 1.0
: Statistical 	 0.00980392156862745
Dragon Systems 	 1.0
The translator 	 0.005208333333333333
Control -RRB- 	 1.0
sense a 	 0.125
TextRank is 	 0.07142857142857142
software . 	 0.037037037037037035
'' non-linearly 	 0.005154639175257732
strongly to 	 0.5
-LRB- WER 	 0.0027100271002710027
a suitable 	 0.0036809815950920245
known cases 	 0.038461538461538464
gone into 	 1.0
each example 	 0.044444444444444446
: Overall 	 0.00980392156862745
<s> Coreference 	 0.0007686395080707148
based engine 	 0.018518518518518517
objective or 	 0.2
statistical methods 	 0.12121212121212122
library are 	 0.5
and others 	 0.002890173410404624
a sense 	 0.001226993865030675
The success 	 0.005208333333333333
the use 	 0.010380622837370242
representation . 	 0.21052631578947367
Kittredge & 	 0.5
we wanted 	 0.022222222222222223
the known 	 0.0034602076124567475
happy . 	 1.0
and strength 	 0.001445086705202312
6 to 	 0.75
system in 	 0.021505376344086023
, Petrov 	 0.0005614823133071309
way -- 	 0.041666666666666664
we ultimately 	 0.022222222222222223
processes such 	 0.2
be -LRB- 	 0.004219409282700422
for comparison 	 0.0036101083032490976
embedded system 	 0.25
of great 	 0.00089126559714795
ISO\/TC37\/SC4 . 	 1.0
reader processed 	 0.1
recognition using 	 0.008264462809917356
Maximal Marginal 	 1.0
not the 	 0.044642857142857144
of keywords 	 0.00089126559714795
chosen . 	 0.2
, HMM-based 	 0.0005614823133071309
frequency -LRB- 	 0.5
are easier 	 0.004149377593360996
both of 	 0.06451612903225806
Extrinsic evaluations 	 0.5
expensive task 	 0.14285714285714285
approximates the 	 0.5
Flow of 	 1.0
perception that 	 0.5
ratings . 	 0.1111111111111111
reduction , 	 0.5
the implied 	 0.0006920415224913495
language . 	 0.07432432432432433
, sentences 	 0.0011229646266142617
several summarization 	 0.045454545454545456
information extraction 	 0.021739130434782608
<s> Narrow 	 0.0007686395080707148
for Friday 	 0.010830324909747292
internal organization 	 0.2
a rightmost 	 0.00245398773006135
decisions based 	 0.2
This allows 	 0.031746031746031744
interaction Pronunciation 	 0.125
specific theoretical 	 0.047619047619047616
VITO Voice2Go 	 1.0
are connected 	 0.004149377593360996
expression or 	 0.1
-LRB- 99 	 0.0027100271002710027
1993 there 	 0.3333333333333333
were question 	 0.024390243902439025
= common 	 0.1111111111111111
then applies 	 0.02857142857142857
and rule 	 0.001445086705202312
Nunan , 	 1.0
is used 	 0.026422764227642278
summarization system 	 0.06
tag '' 	 0.0625
mark word 	 0.3333333333333333
from the 	 0.21153846153846154
the operation 	 0.0006920415224913495
describe words 	 0.16666666666666666
not sufficient 	 0.008928571428571428
the DUC 	 0.0006920415224913495
uses continuous 	 0.07142857142857142
alternative approach 	 0.3333333333333333
In 1969 	 0.01904761904761905
sponsored by 	 0.5
web , 	 0.125
are good 	 0.004149377593360996
summaries available 	 0.023255813953488372
predefined template 	 1.0
achieved translating 	 0.1
spaces used 	 0.2
timing for 	 1.0
parsers were 	 0.07692307692307693
In 1996 	 0.009523809523809525
perception of 	 0.5
<s> Work 	 0.0015372790161414297
should we 	 0.05263157894736842
The Brill 	 0.005208333333333333
bites man 	 0.3333333333333333
coherence and 	 0.6666666666666666
automated online 	 0.14285714285714285
contain names 	 0.08333333333333333
processing uses 	 0.018518518518518517
signal to 	 0.16666666666666666
reasonable chance 	 0.5
that causes 	 0.0035460992907801418
query these 	 0.3333333333333333
A year 	 0.02
`` Computing 	 0.005291005291005291
technology , 	 0.13636363636363635
visible under 	 0.3333333333333333
The authors 	 0.015625
maintained within 	 0.5
The late 	 0.005208333333333333
be roughly 	 0.004219409282700422
and determining 	 0.001445086705202312
to ask 	 0.0013280212483399733
explicitly mention 	 0.25
with disabilities 	 0.01092896174863388
need at 	 0.047619047619047616
U.S. has 	 0.14285714285714285
it word 	 0.008547008547008548
motion during 	 1.0
Beginning in 	 0.5
, contains 	 0.0005614823133071309
performed through 	 0.1
, Adam 	 0.0005614823133071309
' language 	 0.05263157894736842
ontologies -LRB- 	 0.16666666666666666
occurring ' 	 1.0
is always 	 0.0040650406504065045
house through 	 0.5
machine '' 	 0.012658227848101266
means Category 	 0.16666666666666666
critical that 	 0.25
effectiveness in 	 0.3333333333333333
from left 	 0.009615384615384616
and potential 	 0.001445086705202312
produced systems 	 0.2222222222222222
`` Call 	 0.005291005291005291
still not 	 0.06666666666666667
causing it 	 1.0
required translation 	 0.14285714285714285
and expensive 	 0.002890173410404624
of Turney 	 0.0017825311942959
under stress 	 0.2
inter-texual and 	 0.5
patented and 	 1.0
interactive clarification 	 0.25
an attribute 	 0.007575757575757576
to structured 	 0.0013280212483399733
place at 	 0.25
as sounds 	 0.003484320557491289
-LRB- although 	 0.005420054200542005
into methods 	 0.01282051282051282
the automatic 	 0.001384083044982699
world . 	 0.13333333333333333
- EVALITA 	 0.0625
tractability . 	 1.0
carried out 	 0.5
successful NLP 	 0.1111111111111111
articles or 	 0.25
one human 	 0.015384615384615385
smoothly with 	 0.5
became less 	 0.2
analytical artificial 	 0.5
social work 	 0.07142857142857142
email address 	 0.5
- -RRB- 	 0.0625
multileveled pattern 	 1.0
artificial languages 	 0.09090909090909091
all written 	 0.06976744186046512
input sentence 	 0.024390243902439025
open , 	 0.25
, natural 	 0.0005614823133071309
pilot workload 	 0.2
, paragraphs 	 0.0005614823133071309
the accuracy 	 0.0006920415224913495
creating systems 	 0.14285714285714285
reputations . 	 1.0
the news 	 0.001384083044982699
: Deciding 	 0.00980392156862745
'' sentiment 	 0.005154639175257732
or millions 	 0.0045045045045045045
were found 	 0.04878048780487805
many debates 	 0.019230769230769232
character , 	 0.09090909090909091
intended emotional 	 0.2
This convinced 	 0.015873015873015872
in evaluation 	 0.0018726591760299626
it builds 	 0.008547008547008548
software and 	 0.037037037037037035
two extremes 	 0.034482758620689655
fall between 	 0.25
often continues 	 0.022727272727272728
knowledge on 	 0.037037037037037035
`` diversity 	 0.005291005291005291
identify ambiguities 	 0.08333333333333333
functioning as 	 0.6666666666666666
consonants depends 	 0.3333333333333333
language usually 	 0.006756756756756757
extractor might 	 0.5
<s> Future 	 0.0007686395080707148
as sentence 	 0.006968641114982578
- for 	 0.0625
typically how 	 0.05555555555555555
in 1987 	 0.003745318352059925
individual unigrams 	 0.08333333333333333
the screen 	 0.0006920415224913495
actions . 	 1.0
the time 	 0.004152249134948097
mining . 	 0.2
this particular 	 0.01098901098901099
workshops , 	 0.5
ideas in 	 0.5
cause much 	 0.5
, education 	 0.0005614823133071309
needs a 	 0.2
disagree with 	 0.3333333333333333
patent on 	 0.75
uses the 	 0.07142857142857142
medical records 	 0.3333333333333333
needs the 	 0.1
2004 -RRB- 	 0.3333333333333333
-LRB- 3rd 	 0.0027100271002710027
of software 	 0.00089126559714795
`` recommendation 	 0.010582010582010581
to classify 	 0.0013280212483399733
then applied 	 0.05714285714285714
`` sounds 	 0.005291005291005291
which accommodate 	 0.007246376811594203
common tag 	 0.04
often allows 	 0.022727272727272728
requires a 	 0.0625
approximation . 	 0.16666666666666666
the gold 	 0.0020761245674740486
made available 	 0.0625
below -RRB- 	 0.4
city . 	 1.0
of 1928 	 0.00089126559714795
accomplished in 	 1.0
Text Segmentation 	 0.16666666666666666
that use 	 0.0070921985815602835
playing in 	 1.0
a stream 	 0.001226993865030675
milliseconds . 	 0.5
translating Quechua 	 0.25
specific summarization 	 0.047619047619047616
looking wave 	 0.2
sound really 	 0.05
<s> Glass-box 	 0.0007686395080707148
for businesses 	 0.0036101083032490976
for 5 	 0.0036101083032490976
deployed in 	 0.5
; Compute 	 0.02127659574468085
<s> Telephony 	 0.0007686395080707148
lexicon with 	 0.1111111111111111
the areas 	 0.001384083044982699
text can 	 0.006289308176100629
lexical statistics 	 0.07692307692307693
, grammatical 	 0.0005614823133071309
some time 	 0.024096385542168676
objective True\/False 	 0.2
corpus denote 	 0.03225806451612903
'' -LRB- 	 0.04639175257731959
T vertices\/unigrams 	 0.16666666666666666
these approaches 	 0.047619047619047616
by humans 	 0.011428571428571429
processing part 	 0.018518518518518517
such system 	 0.008130081300813009
in question 	 0.003745318352059925
, E 	 0.0005614823133071309
appears multiple 	 0.2
in any 	 0.0018726591760299626
one video 	 0.015384615384615385
keyboard . 	 0.3333333333333333
fact ambiguous 	 0.09090909090909091
after 30 	 0.08333333333333333
there would 	 0.025
are 9 	 0.004149377593360996
the discourse 	 0.0020761245674740486
printed by 	 0.08333333333333333
modeling are 	 0.14285714285714285
word `` 	 0.016666666666666666
analysis , 	 0.1076923076923077
and some 	 0.002890173410404624
have proposed 	 0.009615384615384616
stochastic purposes 	 0.125
`` AI-complete 	 0.010582010582010581
right-most derivations 	 1.0
explicit by 	 0.2
simple implementations 	 0.038461538461538464
& Hollenbach 	 0.125
process -LRB- 	 0.027777777777777776
between adjacent 	 0.05128205128205128
captioned telephone 	 1.0
who co-founded 	 0.1
they rely 	 0.025
, sociology 	 0.0005614823133071309
their training 	 0.029411764705882353
language are 	 0.006756756756756757
used on 	 0.008849557522123894
otherwise achieves 	 0.5
Vocabulary size 	 0.3333333333333333
: Interlingual 	 0.00980392156862745
` summary 	 0.0625
which draws 	 0.007246376811594203
very widely 	 0.024390243902439025
screen of 	 1.0
strong correlation 	 0.25
on-line recognition 	 0.3333333333333333
M. , 	 0.5
been learned 	 0.029411764705882353
corresponding summaries 	 0.16666666666666666
done using 	 0.09090909090909091
left-most derivations 	 0.5
user-specified or 	 0.5
humanities and 	 1.0
or analysis 	 0.0045045045045045045
speakers might 	 0.25
split into 	 0.5
any condition 	 0.03225806451612903
nodes to 	 0.14285714285714285
summarization It 	 0.02
as multiple 	 0.003484320557491289
speech-to-text processing 	 0.5
differences in 	 0.3333333333333333
this is 	 0.0989010989010989
Jelinek and 	 0.5
randomly chosen 	 1.0
groups , 	 0.2
complex problem 	 0.041666666666666664
Tigrinya among 	 1.0
methods , 	 0.09090909090909091
address field 	 0.25
bridge the 	 1.0
parsing of 	 0.07142857142857142
languages or 	 0.02
and left 	 0.001445086705202312
rules based 	 0.023255813953488372
stopwords . 	 1.0
application of 	 0.2857142857142857
reached . 	 0.5
Technology -LRB- 	 0.3333333333333333
tag-sets . 	 1.0
successful systems 	 0.1111111111111111
-RRB- from 	 0.0027100271002710027
-RRB- If 	 0.005420054200542005
highly structured 	 0.1111111111111111
Many different 	 0.16666666666666666
was developed 	 0.012987012987012988
This includes 	 0.015873015873015872
knowledge or 	 0.037037037037037035
question into 	 0.023809523809523808
was made 	 0.012987012987012988
These edges 	 0.058823529411764705
that there 	 0.0070921985815602835
usually do 	 0.03125
affected by 	 1.0
steady accumulation 	 0.5
The human 	 0.005208333333333333
common . 	 0.08
NLP Main 	 0.02127659574468085
WordNet , 	 0.5
be based 	 0.004219409282700422
recognition of 	 0.0743801652892562
allows movement 	 0.125
function -LRB- 	 0.125
Closed-domain question 	 1.0
man-hours worked 	 1.0
to progress 	 0.0013280212483399733
success of 	 0.6
domain -LRB- 	 0.05
100 million 	 0.3333333333333333
been operated 	 0.014705882352941176
typically from 	 0.05555555555555555
significant -RRB- 	 0.1111111111111111
classes : 	 0.2
normalized by 	 1.0
conveyed via 	 1.0
paragraphs -RRB- 	 0.25
scores as 	 0.2
discourse analysts 	 0.05555555555555555
language for 	 0.02027027027027027
Several research 	 0.3333333333333333
are examples 	 0.012448132780082987
requires its 	 0.0625
have some 	 0.009615384615384616
or knowledge 	 0.0045045045045045045
of many 	 0.0017825311942959
domains such 	 0.125
textual summaries 	 0.2
algorithms use 	 0.02857142857142857
parsing written 	 0.03571428571428571
An intrinsic 	 0.125
, rushing 	 0.0005614823133071309
the abbreviation 	 0.0006920415224913495
carry out 	 1.0
features are 	 0.11538461538461539
\/ target-language-independent 	 0.3333333333333333
<s> Knowing 	 0.0007686395080707148
or English 	 0.0045045045045045045
a small 	 0.00245398773006135
spacecraft , 	 1.0
changing information 	 1.0
iteration , 	 1.0
Center , 	 1.0
high quality 	 0.05555555555555555
estimating the 	 1.0
learning As 	 0.023255813953488372
-LRB- Recall-Oriented 	 0.005420054200542005
and non-annotated 	 0.001445086705202312
represent analog 	 0.1111111111111111
user could 	 0.07142857142857142
LILOG , 	 0.5
automatically generate 	 0.047619047619047616
Bar-Hillel . 	 1.0
summary 's 	 0.047619047619047616
the tasks 	 0.0006920415224913495
by relying 	 0.005714285714285714
next stage 	 0.2857142857142857
, speed 	 0.0005614823133071309
as separate 	 0.003484320557491289
of accent 	 0.00089126559714795
most commonly 	 0.017241379310344827
reading text 	 0.125
semantic equivalence 	 0.047619047619047616
An ongoing 	 0.0625
and classification 	 0.001445086705202312
+4 -RRB- 	 1.0
but weaker 	 0.014705882352941176
Who is 	 0.5
as normalization 	 0.003484320557491289
temporal dependencies 	 0.5
DeRose 1990 	 0.2
the letter 	 0.0006920415224913495
`` create 	 0.005291005291005291
not in 	 0.008928571428571428
containing words 	 0.125
identifying trends 	 0.16666666666666666
user can 	 0.07142857142857142
Size Grows 	 1.0
simple data 	 0.038461538461538464
, resolve 	 0.0005614823133071309
specific trade 	 0.047619047619047616
Microphone on 	 1.0
Software OCR 	 0.5
interpretation . 	 0.5
's blocks 	 0.0196078431372549
<s> Intrinsic 	 0.0015372790161414297
coined the 	 1.0
based only 	 0.018518518518518517
Unlike PageRank 	 1.0
task is 	 0.14285714285714285
training . 	 0.03571428571428571
Caldas-Coulthard , 	 1.0
full stop 	 0.4
applications seek 	 0.04
shows , 	 1.0
displayed on-line 	 0.5
is doing 	 0.0020325203252032522
as of 	 0.006968641114982578
are particularly 	 0.004149377593360996
approaches used 	 0.03571428571428571
designed for 	 0.14285714285714285
calling for 	 1.0
consecutive words 	 0.5
rules defining 	 0.023255813953488372
depends on 	 0.875
a corporation 	 0.001226993865030675
DARPA Speech 	 0.25
other applications 	 0.014285714285714285
and pasted 	 0.001445086705202312
repetitive stress 	 0.5
IR and 	 0.3333333333333333
described here 	 0.16666666666666666
the edges 	 0.001384083044982699
, selecting 	 0.0005614823133071309
range of 	 0.5714285714285714
ongoing as 	 0.5
difficulties discussed 	 0.5
Keyphrases have 	 1.0
and researchers 	 0.001445086705202312
`` Computational 	 0.005291005291005291
of NLG 	 0.00089126559714795
is formed 	 0.0020325203252032522
, Type 	 0.0005614823133071309
based therapy 	 0.018518518518518517
edge between 	 0.3333333333333333
-RRB- recognize 	 0.0027100271002710027
worth noting 	 0.5
to assist 	 0.0013280212483399733
<s> Ensemble 	 0.0007686395080707148
copied and 	 0.5
generators . 	 0.5
document reader 	 0.027777777777777776
can often 	 0.0055248618784530384
false starts 	 0.5
medicine or 	 1.0
user about 	 0.07142857142857142
practice of 	 0.5
a reader 	 0.001226993865030675
skip the 	 1.0
real-world information 	 0.16666666666666666
NLP is 	 0.02127659574468085
as pseudo-pilot 	 0.003484320557491289
project -LRB- 	 0.07692307692307693
related . 	 0.06666666666666667
<s> Like 	 0.0007686395080707148
the printed 	 0.0006920415224913495
and hearings 	 0.001445086705202312
Systems -RRB- 	 0.08333333333333333
tagging words 	 0.04
a background 	 0.001226993865030675
method -LRB- 	 0.0625
is non-trivial 	 0.0020325203252032522
which consists 	 0.007246376811594203
sometimes be 	 0.07692307692307693
very likely 	 0.024390243902439025
paper Shipibo 	 0.09090909090909091
well human-ratings 	 0.03571428571428571
removes the 	 1.0
, Jelinek 	 0.0005614823133071309
, machine-aided 	 0.0005614823133071309
Francis , 	 1.0
look-up tables 	 1.0
more corpus 	 0.010526315789473684
commercially available 	 1.0
human speech 	 0.021739130434782608
Grammatical dependency 	 1.0
a keyphrase 	 0.00245398773006135
this point 	 0.01098901098901099
powerful grammars 	 1.0
extracting and 	 0.2
this aim 	 0.01098901098901099
-RRB- Parsing 	 0.0027100271002710027
'' such 	 0.005154639175257732
receipts , 	 1.0
finalized . 	 1.0
suggest that 	 0.3333333333333333
, models 	 0.0005614823133071309
of whether 	 0.00089126559714795
Improved output 	 1.0
reference summary 	 0.375
-LRB- ending 	 0.0027100271002710027
approximately 200 	 0.5
creation of 	 1.0
soft decisions 	 0.5
<s> Isolated 	 0.0007686395080707148
Interactive voice 	 0.5
type of 	 0.5714285714285714
, syntax 	 0.0011229646266142617
information about 	 0.043478260869565216
Scotland with 	 0.2
<s> metrics 	 0.0007686395080707148
referred to 	 1.0
underlie the 	 1.0
tokens that 	 0.14285714285714285
<s> Advanced 	 0.0023059185242121443
modifying words 	 1.0
applied , 	 0.06666666666666667
as often 	 0.003484320557491289
beach , 	 1.0
smaller . 	 0.14285714285714285
on a 	 0.10849056603773585
produces all 	 0.25
rejecting `` 	 0.6666666666666666
early 20th-century 	 0.1
appear as 	 0.0625
documents with 	 0.05263157894736842
belong to 	 1.0
for single 	 0.0036101083032490976
one that 	 0.015384615384615385
the Ge'ez 	 0.0006920415224913495
problems colloquially 	 0.11764705882352941
seen -RRB- 	 0.1
speaker can 	 0.05555555555555555
, that 	 0.0022459292532285235
, ICASSP 	 0.0005614823133071309
break sentences 	 0.5
a paper 	 0.001226993865030675
4 letters 	 0.2
History Some 	 0.5
following : 	 0.13333333333333333
decision trees 	 1.0
answered , 	 0.2
and whether 	 0.001445086705202312
For sentiment 	 0.01639344262295082
ELIZA might 	 0.1111111111111111
and controversial 	 0.001445086705202312
Did Marilyn 	 1.0
cockpit functions 	 0.5
no knowledge 	 0.07692307692307693
ranking task 	 0.14285714285714285
are measured 	 0.004149377593360996
learns a 	 1.0
, ranging 	 0.0011229646266142617
: Why 	 0.00980392156862745
corresponding systems 	 0.16666666666666666
phrases that 	 0.0625
and gets 	 0.001445086705202312
acts , 	 0.3333333333333333
algorithm or 	 0.03571428571428571
writing custom 	 0.1111111111111111
result in 	 0.09090909090909091
2009 to 	 0.3333333333333333
`` understanding 	 0.005291005291005291
common , 	 0.08
quite weak 	 0.125
T. 1991 	 1.0
called morphological 	 0.05555555555555555
the distinctive 	 0.0006920415224913495
subdivided into 	 1.0
given sentence 	 0.08333333333333333
other commercial 	 0.014285714285714285
GRASSHOPPER incorporates 	 0.3333333333333333
that , 	 0.0035460992907801418
people to 	 0.125
at Brown 	 0.029411764705882353
large number 	 0.08695652173913043
a long-time 	 0.001226993865030675
lexicons with 	 0.5
taking a 	 0.2
response Mobile 	 0.5
heavily inflected 	 1.0
Spanish do 	 0.5
parsing In 	 0.03571428571428571
isolated NLP 	 0.2
= Machine 	 0.1111111111111111
the additional 	 0.0006920415224913495
by concatenating 	 0.005714285714285714
significant effort 	 0.1111111111111111
machines by 	 0.25
problem was 	 0.022727272727272728
of their 	 0.0017825311942959
1996 , 	 1.0
II in 	 0.5
Importance of 	 1.0
serial numbers 	 1.0
appears in 	 0.2
largely an 	 0.2
parse computationally 	 0.1111111111111111
more subjective 	 0.010526315789473684
benchmark tests 	 1.0
's quality 	 0.0196078431372549
NLP as 	 0.02127659574468085
among others 	 0.125
term first 	 0.05555555555555555
that ten 	 0.0035460992907801418
summaries humans 	 0.023255813953488372
-- indeed 	 0.04
Rhetoric Stylistics 	 1.0
calls instead 	 1.0
a topic 	 0.001226993865030675
accusative , 	 1.0
breathing was 	 1.0
Speech and 	 0.16129032258064516
often quoted 	 0.022727272727272728
Language Processing 	 0.25
possible semantics 	 0.041666666666666664
some new 	 0.012048192771084338
ISRI -RRB- 	 1.0
hand printing 	 0.07142857142857142
Some scholars 	 0.047619047619047616
Technology Center 	 0.3333333333333333
Instead of 	 1.0
Human Aided 	 0.2
dictionary entry 	 0.14285714285714285
and natural 	 0.005780346820809248
them out 	 0.05263157894736842
`` supervised 	 0.026455026455026454
to merge 	 0.0013280212483399733
country . 	 0.5
' properties 	 0.05263157894736842
machine speech 	 0.012658227848101266
what information 	 0.03125
challenged and 	 1.0
developed by 	 0.038461538461538464
extractive methods 	 0.14285714285714285
issue , 	 0.125
launched the 	 1.0
may be 	 0.40384615384615385
that access 	 0.0035460992907801418
= 2PR 	 0.1111111111111111
for example 	 0.06498194945848375
report -LRB- 	 0.25
isolated word 	 0.2
the same 	 0.01522491349480969
had failed 	 0.14285714285714285
system working 	 0.010752688172043012
performance is 	 0.1111111111111111
dynamic motion 	 0.2
used similar 	 0.008849557522123894
avoiding some 	 0.5
created , 	 0.2857142857142857
of abbreviations 	 0.0017825311942959
not words 	 0.026785714285714284
as consideration 	 0.003484320557491289
Beatrice Santorini 	 1.0
a Fourier 	 0.001226993865030675
application to 	 0.07142857142857142
vocabulary , 	 0.125
good insight 	 0.07692307692307693
document level 	 0.027777777777777776
chosen domain 	 0.2
avoid confusion 	 1.0
what categories 	 0.03125
surrounding the 	 0.2
documents might 	 0.02631578947368421
resource such 	 0.2
Corpus developed 	 0.0625
statistics -LRB- 	 0.125
the stationary 	 0.0006920415224913495
Every acoustic 	 1.0
kind to 	 0.09090909090909091
of real-world 	 0.00089126559714795
as it 	 0.003484320557491289
angry . 	 0.5
tools for 	 0.16666666666666666
and aircraft 	 0.001445086705202312
I would 	 1.0
specification . 	 0.5
Cognitive psychology 	 0.3333333333333333
and news 	 0.001445086705202312
Kurzweil Computer 	 0.2857142857142857
contain densely 	 0.08333333333333333
usually a 	 0.03125
deemed the 	 0.5
questions in 	 0.038461538461538464
psychologist . 	 1.0
AT&T libraries 	 1.0
harmonic mean 	 1.0
The sentences 	 0.005208333333333333
verb : 	 0.07692307692307693
there were 	 0.075
Iraq and 	 0.5
individual morphemes 	 0.08333333333333333
Such a 	 0.125
words or 	 0.06422018348623854
short commands 	 0.125
general with 	 0.045454545454545456
Genres of 	 1.0
generally amenable 	 0.09090909090909091
be computed 	 0.004219409282700422
use software 	 0.013888888888888888
of various 	 0.00089126559714795
of text 	 0.0213903743315508
Reading the 	 0.5
, Harrison 	 0.0005614823133071309
span several 	 1.0
, DeRose 	 0.0005614823133071309
first statistical 	 0.06060606060606061
normalization and 	 0.16666666666666666
to as 	 0.005312084993359893
last decade 	 0.4
understanding involves 	 0.030303030303030304
with matching 	 0.00546448087431694
Speech-to-text reporter 	 1.0
applies directly 	 0.14285714285714285
, determine 	 0.003368893879842785
Adriana Bolivar 	 1.0
, our 	 0.0005614823133071309
article , 	 0.10344827586206896
little any 	 0.3333333333333333
accurate program 	 0.14285714285714285
In 1987 	 0.009523809523809525
different angle 	 0.02040816326530612
place to 	 0.25
correctly . 	 1.0
this basic 	 0.01098901098901099
several variables 	 0.045454545454545456
people create 	 0.0625
areas , 	 0.16666666666666666
other automatic 	 0.014285714285714285
`` corpora 	 0.005291005291005291
evaluation looks 	 0.018518518518518517
the platform 	 0.0006920415224913495
and Re-encoding 	 0.001445086705202312
boundaries in 	 0.09090909090909091
campaigns within 	 0.5
USA in 	 1.0
, handling 	 0.0011229646266142617
pre-determined when 	 1.0
were printed 	 0.024390243902439025
templates may 	 1.0
German taggers 	 0.25
The AT&T 	 0.005208333333333333
languages words 	 0.02
might vary 	 0.038461538461538464
and system 	 0.001445086705202312
about 95 	 0.025
are exceptions 	 0.004149377593360996
on-line , 	 0.3333333333333333
MAHT and 	 1.0
human raters 	 0.021739130434782608
Convert information 	 0.5
module that 	 0.3333333333333333
words immediately 	 0.009174311926605505
`` Naturally 	 0.005291005291005291
consumer , 	 1.0
Tom Clancy 	 1.0
run each 	 0.2
been annotated 	 0.014705882352941176
2000 . 	 0.3333333333333333
documents where 	 0.02631578947368421
extractive approach 	 0.14285714285714285
informal exchange 	 0.5
translation Transfer-based 	 0.013513513513513514
A well-known 	 0.02
closely tied 	 0.2
produces usable 	 0.25
meaning in 	 0.08695652173913043
the provider 	 0.001384083044982699
a choice 	 0.001226993865030675
units , 	 0.14285714285714285
words just 	 0.009174311926605505
Lawrence Rabiner 	 1.0
FAA as 	 0.5
<s> Significant 	 0.0007686395080707148
decision-making , 	 1.0
, dynamic 	 0.0005614823133071309
assist human 	 1.0
though it 	 0.2
the desktop 	 0.0006920415224913495
two possibilities 	 0.034482758620689655
on linguistic 	 0.0047169811320754715
education , 	 1.0
aspects such 	 0.14285714285714285
been believed 	 0.014705882352941176
the jet 	 0.0006920415224913495
LexisNexis was 	 1.0
written conversation 	 0.038461538461538464
issues were 	 0.2
trees , 	 0.5
The popular 	 0.005208333333333333
rudimentary way 	 0.5
and abstraction 	 0.002890173410404624
which of 	 0.007246376811594203
Marcus M. 	 1.0
understanding can 	 0.030303030303030304
for evaluating 	 0.010830324909747292
Scope and 	 1.0
analysis : 	 0.06153846153846154
that involve 	 0.0035460992907801418
have several 	 0.009615384615384616
letters are 	 0.1
interlingua . 	 1.0
With sufficient 	 0.14285714285714285
why applying 	 0.14285714285714285
User Interface 	 0.5
tackles each 	 1.0
main '' 	 0.125
Document reader 	 0.25
Essentially , 	 1.0
perform as 	 0.09090909090909091
considered or 	 0.1111111111111111
sizes of 	 0.6666666666666666
on Text 	 0.0047169811320754715
effects of 	 1.0
season , 	 1.0
The approaches 	 0.005208333333333333
restaurant reviews 	 0.5
any number 	 0.03225806451612903
than polarity 	 0.022222222222222223
Although these 	 0.125
may fail 	 0.019230769230769232
variation across 	 1.0
be approximated 	 0.004219409282700422
dissertation -LRB- 	 0.3333333333333333
, pruned 	 0.0005614823133071309
an RCA 	 0.015151515151515152
accuracy and 	 0.03225806451612903
prisoner of 	 1.0
classification for 	 0.11764705882352941
instead of 	 0.5714285714285714
account how 	 0.3333333333333333
To decode 	 0.1111111111111111
of analysis 	 0.00089126559714795
, Z 	 0.0005614823133071309
performance of 	 0.1111111111111111
two steps 	 0.034482758620689655
discourse -LRB- 	 0.05555555555555555
requires six 	 0.0625
tagging has 	 0.04
simplification Text-to-speech 	 1.0
text or 	 0.018867924528301886
are saying 	 0.004149377593360996
Text-to-speech Text-proofing 	 1.0
others assign 	 0.08333333333333333
critics claim 	 1.0
is farther 	 0.0020325203252032522
field . 	 0.14814814814814814
-LRB- greater 	 0.0027100271002710027
anywhere on 	 1.0
`` book 	 0.005291005291005291
which sentences 	 0.007246376811594203
Projects Agency 	 1.0
and development 	 0.002890173410404624
200 , 	 0.5
styles itself 	 1.0
recursive productions 	 1.0
transcriptions is 	 0.5
, isolated 	 0.0005614823133071309
RCA Drum 	 0.2
shapes . 	 0.3333333333333333
so-called delta 	 0.3333333333333333
using an 	 0.03389830508474576
on this 	 0.014150943396226415
characters themselves 	 0.0625
corpora in 	 0.09090909090909091
methodology to 	 0.5
entities , 	 0.2857142857142857
, text-to-speech 	 0.0011229646266142617
, recently 	 0.0005614823133071309
of input 	 0.00267379679144385
some systems 	 0.024096385542168676
HAMS = 	 1.0
Applied Intelligence 	 0.5
high , 	 0.05555555555555555
nearly perfect 	 0.5
of front-end 	 0.00089126559714795
, Computer 	 0.0005614823133071309
research teams 	 0.023809523809523808
techniques on 	 0.043478260869565216
attached , 	 0.5
the sequence 	 0.0006920415224913495
or moderate 	 0.0045045045045045045
such an 	 0.016260162601626018
EHR . 	 0.3333333333333333
align recorded 	 1.0
that aid 	 0.0035460992907801418
from spelling 	 0.009615384615384616
further condensation 	 0.125
to documents 	 0.0013280212483399733
the answers 	 0.0006920415224913495
a sub-field 	 0.001226993865030675
Bayes risk 	 0.3333333333333333
sets to 	 0.09090909090909091
FoG triggered 	 0.5
readers processed 	 0.5
, since 	 0.002807411566535654
or most 	 0.0045045045045045045
a set 	 0.01717791411042945
representation , 	 0.15789473684210525
that much 	 0.0035460992907801418
's estimate 	 0.0196078431372549
constructs -LRB- 	 0.3333333333333333
associated with 	 0.25
of Pennsylvania 	 0.00089126559714795
of semantic 	 0.004456327985739751
sample is 	 0.3333333333333333
contractions like 	 0.5
select whole 	 0.16666666666666666
1998 -RRB- 	 0.5
is coherent 	 0.0020325203252032522
of simpler 	 0.0017825311942959
the summaries 	 0.002768166089965398
to seize 	 0.0013280212483399733
Lander used 	 0.5
'' by 	 0.02577319587628866
additional features 	 0.16666666666666666
elements containing 	 0.25
the training 	 0.002768166089965398
he went 	 0.2857142857142857
with recognition 	 0.00546448087431694
of domain 	 0.00089126559714795
10 milliseconds 	 0.25
nested one 	 1.0
and even 	 0.008670520231213872
a user-specified 	 0.001226993865030675
units are 	 0.2857142857142857
opinion expressed 	 0.2
OCR technology 	 0.1836734693877551
marking abbreviations 	 0.5
claiming to 	 1.0
accurate simply 	 0.14285714285714285
wingmen with 	 1.0
of images 	 0.00089126559714795
open-ended questions 	 1.0
English-French record 	 1.0
starts , 	 0.5
Thus , 	 0.9166666666666666
communicative goal 	 0.6666666666666666
and 1970s 	 0.001445086705202312
descriptive tags 	 0.3333333333333333
different strategies 	 0.02040816326530612
explained by 	 1.0
done by 	 0.18181818181818182
2.0 The 	 0.5
documents containing 	 0.02631578947368421
is described 	 0.0020325203252032522
arguably function 	 0.5
which led 	 0.007246376811594203
token is 	 0.5
exercises on 	 1.0
statistics , 	 0.125
started trying 	 0.25
even in 	 0.07407407407407407
is much 	 0.0040650406504065045
several methods 	 0.045454545454545456
a professor 	 0.001226993865030675
Jabberwacky . 	 1.0
coherent discourse 	 0.2
major design 	 0.08333333333333333
Such systems 	 0.125
acts in 	 0.3333333333333333
2,026,329 -RRB- 	 1.0
periods or 	 0.3333333333333333
opinion has 	 0.2
flood-control pumps 	 1.0
as `` 	 0.04878048780487805
merely assigning 	 0.5
typewritten messages 	 0.2
<s> Speech 	 0.011529592621060722
all these 	 0.023255813953488372
Before getting 	 0.5
aid in 	 0.75
matching up 	 0.2
-LRB- not 	 0.0027100271002710027
reporter -LRB- 	 1.0
on Reader 	 0.0047169811320754715
showing comparative 	 0.5
assistant providing 	 1.0
dividing a 	 0.3333333333333333
networks have 	 0.07142857142857142
summary -LRB- 	 0.023809523809523808
stutering , 	 1.0
it offered 	 0.008547008547008548
about specific 	 0.025
a cryptanalyst 	 0.001226993865030675
coefficients and 	 0.25
Methods such 	 0.25
But unfortunately 	 0.16666666666666666
language during 	 0.006756756756756757
the action 	 0.0006920415224913495
more probabilistic 	 0.010526315789473684
tried . 	 0.3333333333333333
between those 	 0.02564102564102564
in principle 	 0.0018726591760299626
to different 	 0.0013280212483399733
corresponding text 	 0.16666666666666666
to parse 	 0.005312084993359893
-LRB- Carbonell 	 0.0027100271002710027
this context 	 0.02197802197802198
, Brenton 	 0.0005614823133071309
vocabularies , 	 1.0
summaries depending 	 0.046511627906976744
glass-box evaluation 	 1.0
consecutively and 	 1.0
theory -RRB- 	 0.07692307692307693
some of 	 0.1566265060240964
1982 -RRB- 	 0.3333333333333333
judgement , 	 0.3333333333333333
This phenomenon 	 0.031746031746031744
considerable commercial 	 0.2
Applications include 	 0.5
translation . 	 0.05405405405405406
we would 	 0.06666666666666667
logic -RRB- 	 0.25
-RRB- into 	 0.005420054200542005
<s> Basically 	 0.0007686395080707148
text of 	 0.006289308176100629
, propositions 	 0.0011229646266142617
patents . 	 1.0
is processed 	 0.0020325203252032522
language in 	 0.006756756756756757
vectors would 	 0.3333333333333333
blocks , 	 0.25
an AI-complete 	 0.007575757575757576
for multiple 	 0.0036101083032490976
successfully adapted 	 0.3333333333333333
States . 	 0.2857142857142857
Some of 	 0.19047619047619047
meantime , 	 1.0
Stubbs , 	 1.0
in many 	 0.0149812734082397
taking only 	 0.2
States Postal 	 0.14285714285714285
showing evidence 	 0.5
allowing for 	 0.3333333333333333
automate sentiment 	 0.3333333333333333
Potentially , 	 1.0
subsequent concepts 	 0.5
Besides the 	 1.0
and Tigrinya 	 0.001445086705202312
found that 	 0.35714285714285715
pronouns and 	 0.5
on OCR 	 0.0047169811320754715
Postal Service 	 1.0
to serve 	 0.0013280212483399733
process include 	 0.027777777777777776
in research 	 0.0018726591760299626
Star Trek 	 1.0
Italian -RRB- 	 0.5
useful work 	 0.07142857142857142
the CKY 	 0.0006920415224913495
<s> Behind 	 0.0007686395080707148
search . 	 0.09090909090909091
-RRB- Video 	 0.0027100271002710027
block of 	 1.0
topic boundaries 	 0.125
`` proper 	 0.005291005291005291
that in 	 0.0070921985815602835
Language Input 	 0.08333333333333333
British National 	 0.3333333333333333
Intrinsic vs. 	 0.3333333333333333
and LexRank 	 0.004335260115606936
some other 	 0.08433734939759036
and here 	 0.001445086705202312
was only 	 0.012987012987012988
against any 	 0.2
taught to 	 0.3333333333333333
is entirely 	 0.0020325203252032522
strength of 	 0.6
term applies 	 0.1111111111111111
parsing comes 	 0.03571428571428571
numbers on 	 0.14285714285714285
relationship mentions 	 0.16666666666666666
term artificial 	 0.05555555555555555
to get 	 0.005312084993359893
market their 	 0.3333333333333333
typewritten or 	 0.2
improvement of 	 0.5
Standardization in 	 1.0
IBM . 	 0.3333333333333333
manually assigned 	 0.25
improve results 	 0.07692307692307693
<s> n 	 0.0007686395080707148
of candidates 	 0.00089126559714795
the language 	 0.005536332179930796
The main 	 0.015625
models -LRB- 	 0.11538461538461539
dog to 	 0.3333333333333333
the separate 	 0.0006920415224913495
words '' 	 0.01834862385321101
beginning to 	 0.5
, communication 	 0.0005614823133071309
essentially two 	 0.125
2004 , 	 0.3333333333333333
have focused 	 0.019230769230769232
differently on 	 1.0
English into 	 0.02702702702702703
systems based 	 0.017857142857142856
copied despite 	 0.5
entities -LRB- 	 0.14285714285714285
`` A 	 0.005291005291005291
sources or 	 0.16666666666666666
groups submit 	 0.2
by context-free 	 0.005714285714285714
generate text 	 0.05555555555555555
some writing 	 0.012048192771084338
visual detection 	 0.5
knowledge that 	 0.037037037037037035
E. Brill 	 0.25
This comparison 	 0.015873015873015872
to infer 	 0.0013280212483399733
utterance and 	 0.3333333333333333
as to 	 0.013937282229965157
and final 	 0.001445086705202312
standard random 	 0.07142857142857142
Gary Hendrix 	 1.0
running Palm 	 0.3333333333333333
this genre 	 0.01098901098901099
on developing 	 0.0047169811320754715
translate between 	 0.16666666666666666
levels first 	 0.045454545454545456
Guy Cook 	 1.0
out a 	 0.07142857142857142
are remarkably 	 0.004149377593360996
summarization involves 	 0.02
find a 	 0.15384615384615385
efforts have 	 0.5714285714285714
Computer Speech 	 0.3333333333333333
intended meaning 	 0.2
10 digits 	 0.125
larger system 	 0.125
so meaningless 	 0.03333333333333333
early text-to-speech 	 0.1
as there 	 0.003484320557491289
and do 	 0.001445086705202312
involves both 	 0.1
questions are 	 0.07692307692307693
a multileveled 	 0.001226993865030675
Evaluation of 	 0.1111111111111111
its utility 	 0.02857142857142857
`` B 	 0.005291005291005291
parsers is 	 0.07692307692307693
entering a 	 0.5
simple natural 	 0.038461538461538464
projects in 	 0.5
quoted in 	 1.0
of context 	 0.0017825311942959
there as 	 0.025
, statistics 	 0.0011229646266142617
in texts 	 0.0018726591760299626
some kind 	 0.04819277108433735
word divider 	 0.016666666666666666
particular , 	 0.23076923076923078
matching , 	 0.2
own expert 	 0.16666666666666666
2,000 or 	 0.5
resources such 	 0.16666666666666666
vs. glass-box 	 0.08333333333333333
converted them 	 0.3333333333333333
wrote ELIZA 	 0.16666666666666666
literature are 	 1.0
its immediate 	 0.02857142857142857
applies both 	 0.2857142857142857
referring expressions 	 0.5
and support 	 0.001445086705202312
of heuristics 	 0.00089126559714795
single word 	 0.07142857142857142
system comprising 	 0.010752688172043012
photographing data 	 1.0
defined as 	 0.16666666666666666
uses . 	 0.07142857142857142
using general 	 0.01694915254237288
passages to 	 0.5
sailor '' 	 0.4
words accidentally 	 0.009174311926605505
Stages The 	 1.0
limited type 	 0.1
in predicate 	 0.0018726591760299626
case , 	 0.17647058823529413
differ in 	 0.3333333333333333
for parse 	 0.0036101083032490976
keyphrases available 	 0.02857142857142857
we normally 	 0.022222222222222223
: expanded 	 0.00980392156862745
included a 	 0.125
popular media 	 0.1111111111111111
a `` 	 0.007361963190184049
Gismo . 	 0.5
into the 	 0.10256410256410256
what linguistic 	 0.03125
rule-based and 	 0.14285714285714285
more data 	 0.021052631578947368
15-20 million 	 1.0
steps of 	 0.5
, Hafiz 	 0.0005614823133071309
system exhibited 	 0.010752688172043012
Woods introduced 	 1.0
either user-specified 	 0.1
a psychologist 	 0.001226993865030675
differences It 	 0.3333333333333333
sentences have 	 0.02631578947368421
are those 	 0.004149377593360996
running English 	 0.3333333333333333
and Canada 	 0.001445086705202312
tasks defined 	 0.03125
look at 	 0.4
higher error 	 0.14285714285714285
reliability -RRB- 	 0.5
unit , 	 0.3333333333333333
In theory 	 0.01904761904761905
trivial , 	 0.25
the stock 	 0.0006920415224913495
composing Braille 	 1.0
not an 	 0.008928571428571428
not rely 	 0.017857142857142856
and obtained 	 0.001445086705202312
a sensible 	 0.001226993865030675
of taking 	 0.00089126559714795
Deese , 	 1.0
D. Faber 	 0.2
general software 	 0.045454545454545456
judge its 	 0.25
noise problem 	 0.125
innovative Web-based 	 1.0
Rabinow . 	 1.0
patent for 	 0.25
Englund -LRB- 	 1.0
be performed 	 0.008438818565400843
easily be 	 0.1111111111111111
programer 's 	 1.0
the centers 	 0.0006920415224913495
humans , 	 0.16666666666666666
, emoticons 	 0.0005614823133071309
OCR machines 	 0.02040816326530612
very optimistic 	 0.024390243902439025
model of 	 0.03333333333333333
most natural 	 0.034482758620689655
dependence vs. 	 1.0
in conference 	 0.0018726591760299626
characters rather 	 0.0625
air is 	 0.2
vertices , 	 0.1111111111111111
-RRB- provides 	 0.0027100271002710027
assumptions , 	 0.2
, why 	 0.0011229646266142617
Due to 	 1.0
such input 	 0.008130081300813009
Commercial research 	 0.5
and Zacharov 	 0.001445086705202312
of rule-based 	 0.00089126559714795
recognition conferences 	 0.008264462809917356
some form 	 0.04819277108433735
most often 	 0.017241379310344827
automatically the 	 0.047619047619047616
tried , 	 0.3333333333333333
keyphrases and 	 0.02857142857142857
Functional grammar 	 1.0
QA The 	 0.047619047619047616
Retrieval Conference 	 1.0
SHRDLU simulated 	 0.16666666666666666
only Wikipedia 	 0.02631578947368421
even lower 	 0.037037037037037035
does the 	 0.1
from computer 	 0.009615384615384616
1969 , 	 0.5
research were 	 0.023809523809523808
supervised extractive 	 0.0625
-LRB- semi 	 0.0027100271002710027
vertices be 	 0.1111111111111111
own sentence 	 0.16666666666666666
scientific and 	 0.5
area is 	 0.18181818181818182
patterns , 	 0.2
to search 	 0.0013280212483399733
Variation analysis 	 1.0
-LRB- ARRA 	 0.0027100271002710027
behavior of 	 0.5
many systems 	 0.019230769230769232
is need 	 0.0020325203252032522
user interfaces 	 0.14285714285714285
task-based evaluations 	 0.75
Kurzweil sold 	 0.14285714285714285
has included 	 0.011904761904761904
sentence importance 	 0.020833333333333332
QA computer 	 0.047619047619047616
Lehnert 1981 	 0.3333333333333333
equivalent ideas 	 0.2
Cullingford , 	 1.0
% or 	 0.02564102564102564
the majority 	 0.0006920415224913495
Howarth , 	 1.0
each whole 	 0.022222222222222223
<s> `` 	 0.003843197540353574
fire '' 	 0.5
the ANR-Passage 	 0.0006920415224913495
distinct from 	 0.42857142857142855
takes the 	 0.3333333333333333
interaction Genres 	 0.125
and treat 	 0.001445086705202312
speech naturally 	 0.006578947368421052
are hardly 	 0.004149377593360996
answering a 	 0.08333333333333333
fuse with 	 1.0
with C4 	 0.00546448087431694
build an 	 0.6666666666666666
no pauses 	 0.07692307692307693
parametric values 	 1.0
, although 	 0.0022459292532285235
out from 	 0.07142857142857142
mentioned earlier 	 0.3333333333333333
H. Levinsohn 	 0.5
Voice Command 	 0.2
words by 	 0.009174311926605505
<s> English 	 0.0007686395080707148
-LRB- among 	 0.005420054200542005
Mirage aircraft 	 1.0
an understanding 	 0.015151515151515152
well-known application 	 1.0
discourse is 	 0.08333333333333333
captures data 	 1.0
the multiple 	 0.0006920415224913495
software Annotate 	 0.037037037037037035
bill payment 	 0.5
While there 	 0.2
be decided 	 0.004219409282700422
decomposing it 	 1.0
products , 	 0.25
and processing 	 0.001445086705202312
unsupervised summarization 	 0.25
at TWA 	 0.014705882352941176
science , 	 0.4
the most 	 0.01314878892733564
the part-of-speech 	 0.0006920415224913495
EndWar and 	 1.0
diagonal covariance 	 1.0
the nouns 	 0.0006920415224913495
one needs 	 0.015384615384615385
humans in 	 0.08333333333333333
<s> Compare 	 0.0007686395080707148
involve learning 	 0.16666666666666666
and writing 	 0.001445086705202312
Stef Slembrouck 	 1.0
learned is 	 0.2
Languages like 	 0.3333333333333333
been successfully 	 0.014705882352941176
highly by 	 0.1111111111111111
chance of 	 1.0
situation where 	 0.5
of 80 	 0.00089126559714795
design and 	 0.25
1-July-2005 , 	 1.0
phones and 	 0.5
the RCA 	 0.0006920415224913495
when summarizing 	 0.02857142857142857
in USA 	 0.0018726591760299626
tied to 	 1.0
thereof -RRB- 	 1.0
the noise 	 0.0006920415224913495
procedure still 	 0.3333333333333333
language -RRB- 	 0.013513513513513514
Su , 	 1.0
sub-field of 	 1.0
perform complex 	 0.09090909090909091
The method 	 0.005208333333333333
In any 	 0.01904761904761905
, minimum 	 0.0005614823133071309
walk , 	 0.2
preliminary recognition 	 0.3333333333333333
Summarization -RRB- 	 0.5
numerous approaches 	 1.0
and architecture 	 0.001445086705202312
breaking , 	 0.5
embedded . 	 0.25
van Leeuwen 	 0.5
a generated 	 0.001226993865030675
mean word 	 0.5
a precise 	 0.001226993865030675
original source 	 0.07692307692307693
industry . 	 0.3333333333333333
translation requires 	 0.013513513513513514
<s> Extractive 	 0.0007686395080707148
and captioned 	 0.001445086705202312
programs , 	 0.18181818181818182
important by 	 0.0625
of what 	 0.0035650623885918
<s> WebOCR 	 0.0015372790161414297
U.S. program 	 0.14285714285714285
available from 	 0.058823529411764705
program by 	 0.045454545454545456
from limited 	 0.009615384615384616
processing step 	 0.018518518518518517
Patent 2,026,329 	 0.3333333333333333
loud . 	 1.0
punctuation marks 	 0.2857142857142857
computer in 	 0.045454545454545456
QA systems 	 0.2857142857142857
a digital 	 0.00245398773006135
database tables 	 0.1
<s> Sound 	 0.0015372790161414297
relevance or 	 0.3333333333333333
Xuedong Huang 	 1.0
Canada and 	 0.16666666666666666
by standard 	 0.005714285714285714
over an 	 0.08333333333333333
new , 	 0.041666666666666664
us to 	 0.5
is easily 	 0.0020325203252032522
approaches . 	 0.10714285714285714
cell phone 	 1.0
`` a 	 0.005291005291005291
include : 	 0.1111111111111111
automatic analysis 	 0.043478260869565216
A random 	 0.02
untagged corpus 	 1.0
or sentences 	 0.0045045045045045045
analysis systems 	 0.015384615384615385
-LRB- Loriot 	 0.0027100271002710027
emotional states 	 0.25
about unigrams 	 0.025
Goodwin , 	 1.0
particularly difficult 	 0.2
available for 	 0.11764705882352941
<s> Inclusive 	 0.0007686395080707148
pre-defined by 	 0.5
are by 	 0.004149377593360996
prepare formal 	 1.0
Another key 	 0.07692307692307693
created rules 	 0.14285714285714285
into machine-encoded 	 0.01282051282051282
limits -LRB- 	 1.0
the algorithms 	 0.001384083044982699
of language-processing 	 0.00089126559714795
this issue 	 0.01098901098901099
performance had 	 0.05555555555555555
typical parser 	 0.1111111111111111
essentially a 	 0.25
report finalized 	 0.25
stemming or 	 0.5
-LRB- ATC 	 0.0027100271002710027
such statistical 	 0.008130081300813009
keyphrase , 	 0.05263157894736842
measured by 	 0.5
include overt 	 0.037037037037037035
complicated because 	 0.3333333333333333
example , 	 0.6666666666666666
1980s , 	 0.5555555555555556
has more 	 0.011904761904761904
granted a 	 1.0
After 30 	 0.3333333333333333
while parsing 	 0.05
, June 	 0.0005614823133071309
for machine-translation 	 0.0036101083032490976
not typically 	 0.017857142857142856
they require 	 0.05
require significant 	 0.045454545454545456
<s> When 	 0.004611837048424289
speech there 	 0.006578947368421052
therapy -LRB- 	 1.0
sentences but 	 0.02631578947368421
using will 	 0.01694915254237288
and controlling 	 0.001445086705202312
the Apollo 	 0.0006920415224913495
small integer 	 0.1111111111111111
relations , 	 0.16666666666666666
a societal 	 0.001226993865030675
keep track 	 0.3333333333333333
phones . 	 0.5
amount of 	 1.0
typically grouped 	 0.05555555555555555
-LRB- c 	 0.0027100271002710027
index entries 	 1.0
need context 	 0.047619047619047616
for Greek 	 0.0036101083032490976
phonemes in 	 0.16666666666666666
syntactic . 	 0.07692307692307693
remains another 	 0.25
Navy , 	 1.0
sentence , 	 0.125
approximation was 	 0.16666666666666666
theory for 	 0.07692307692307693
David Nunan 	 0.25
technologies -- 	 0.25
Voice Control 	 0.2
the program 	 0.0020761245674740486
accurate results 	 0.14285714285714285
to maintain 	 0.0013280212483399733
news release 	 0.07692307692307693
in very 	 0.0056179775280898875
Australia . 	 1.0
to automated 	 0.0013280212483399733
word separators 	 0.016666666666666666
the equipment 	 0.0006920415224913495
can assign 	 0.0055248618784530384
phrases and 	 0.1875
logical representation 	 0.16666666666666666
font . 	 0.6666666666666666
first of 	 0.030303030303030304
exist . 	 1.0
one language 	 0.03076923076923077
most difficult 	 0.017241379310344827
on context 	 0.0047169811320754715
processed documents 	 0.16666666666666666
and document 	 0.002890173410404624
'' implicate 	 0.005154639175257732
Canadian parliament 	 0.5
because of 	 0.2
analysts not 	 0.5
as keeping 	 0.003484320557491289
human judgments 	 0.021739130434782608
in pattern 	 0.0018726591760299626
software Desktop 	 0.037037037037037035
USAF , 	 1.0
linguistic term 	 0.0625
Australian Air 	 0.5
the Senseval 	 0.0006920415224913495
has interest 	 0.011904761904761904
glue text 	 1.0
own assumptions 	 0.16666666666666666
A somewhat 	 0.02
algorithm like 	 0.03571428571428571
-LRB- Journal 	 0.0027100271002710027
tell whether 	 0.3333333333333333
Wikipedia and 	 0.5
spontaneous speech 	 1.0
than a 	 0.1111111111111111
fully articulated 	 0.16666666666666666
common term 	 0.04
conference headed 	 0.5
the degree 	 0.0006920415224913495
, consider 	 0.0005614823133071309
, words 	 0.0011229646266142617
question , 	 0.2619047619047619
interpreted as 	 1.0
produce numeric 	 0.045454545454545456
analysis could 	 0.015384615384615385
the SIGGEN 	 0.0006920415224913495
can get 	 0.0055248618784530384
derive part-of-speech 	 0.5
Du Bois 	 1.0
records and 	 0.25
real time 	 0.1111111111111111
of Chinese 	 0.00089126559714795
Harvey Sacks 	 1.0
individual characters 	 0.08333333333333333
contain subjective 	 0.08333333333333333
of discourses 	 0.00089126559714795
Michel Foucault 	 1.0
equivalence relations 	 0.5
the sample 	 0.0006920415224913495
Wide Web 	 1.0
humor -RRB- 	 1.0
but BLEU 	 0.014705882352941176
interactive use 	 0.25
Why does 	 0.2857142857142857
Bois , 	 1.0
different relationships 	 0.02040816326530612
wanted to 	 1.0
feasibility study 	 0.5
only want 	 0.02631578947368421
pilots flying 	 0.5
in non-Western 	 0.0018726591760299626
studies and 	 0.25
speeds . 	 0.5
less expensive 	 0.08333333333333333
<s> Automatic 	 0.005380476556495004
theory it 	 0.07692307692307693
and actioning 	 0.001445086705202312
As well 	 0.05555555555555555
tags , 	 0.3333333333333333
to artificial 	 0.0026560424966799467
services . 	 0.6666666666666666
duplicate or 	 0.5
purely statistical 	 1.0
<s> Profile 	 0.0007686395080707148
networks make 	 0.07142857142857142
that did 	 0.0070921985815602835
are much 	 0.008298755186721992
Grammar , 	 1.0
use , 	 0.05555555555555555
how the 	 0.034482758620689655
, Stephen 	 0.0005614823133071309
Yet ELIZA 	 1.0
customers , 	 0.5
also : 	 0.028985507246376812
speech-enabled Symbian 	 1.0
speech in 	 0.013157894736842105
formulation The 	 1.0
covariance transform 	 0.5
OCR accuracy 	 0.02040816326530612
is transformed 	 0.0020325203252032522
judge fluency 	 0.25
Eastern Peru 	 1.0
which to 	 0.007246376811594203
Dec. 2011 	 1.0
and which 	 0.001445086705202312
by taking 	 0.005714285714285714
have helped 	 0.009615384615384616
have explicit 	 0.009615384615384616
language like 	 0.006756756756756757
became the 	 0.2
answers -RRB- 	 0.08333333333333333
've seen 	 0.5
operated successfully 	 0.5
speech full 	 0.006578947368421052
In information 	 0.009523809523809525
in general 	 0.009363295880149813
at its 	 0.014705882352941176
Flickinger D. 	 1.0
which soon 	 0.007246376811594203
's seminal 	 0.0196078431372549
conceptual ontologies 	 0.5
especially statistical 	 0.06666666666666667
goal -RRB- 	 0.14285714285714285
for QA 	 0.010830324909747292
techniques used 	 0.043478260869565216
Supervised text 	 1.0
is vital 	 0.0020325203252032522
`` Tell 	 0.005291005291005291
meaningful way 	 0.125
network to 	 0.16666666666666666
2006 , 	 0.3333333333333333
In computer 	 0.009523809523809525
systems on 	 0.008928571428571428
different from 	 0.12244897959183673
of vertices 	 0.00089126559714795
more easily 	 0.010526315789473684
negative , 	 0.125
shallow approaches 	 0.16666666666666666
proper declaration 	 0.14285714285714285
<s> Intuitively 	 0.0007686395080707148
characters and 	 0.0625
dialog that 	 0.5
Piron , 	 0.3333333333333333
being able 	 0.05555555555555555
area -RRB- 	 0.09090909090909091
signing off 	 1.0
among sentences 	 0.125
features '' 	 0.038461538461538464
evaluate an 	 0.25
by giving 	 0.005714285714285714
sufficient include 	 0.2
Wayne Ratliff 	 1.0
, morphology 	 0.0011229646266142617
why he 	 0.2857142857142857
often characterised 	 0.022727272727272728
in several 	 0.003745318352059925
from 50 	 0.009615384615384616
challenges -RRB- 	 0.5
are further 	 0.004149377593360996
to government 	 0.0013280212483399733
their part 	 0.029411764705882353
been carried 	 0.014705882352941176
that you 	 0.0035460992907801418
-LRB- ME 	 0.0027100271002710027
Writing -RRB- 	 1.0
'' sentence 	 0.005154639175257732
four different 	 0.2857142857142857
OCR vendors 	 0.02040816326530612
seem completely 	 0.5
easily as 	 0.1111111111111111
? -RRB- 	 0.125
answering The 	 0.08333333333333333
computer OCR 	 0.022727272727272728
dynamic programming 	 0.2
access to 	 1.0
it requires 	 0.017094017094017096
user interface 	 0.07142857142857142
given data 	 0.041666666666666664
99 % 	 1.0
<s> Jump 	 0.0007686395080707148
human ratings 	 0.08695652173913043
RAF employs 	 1.0
to wreck 	 0.0013280212483399733
to erroneous 	 0.0013280212483399733
depended on 	 1.0
when reading 	 0.02857142857142857
nouns , 	 0.6666666666666666
and allows 	 0.002890173410404624
using CSIS 	 0.01694915254237288
document -LRB- 	 0.05555555555555555
complex images 	 0.041666666666666664
conversations , 	 0.3333333333333333
1990 dissertation 	 0.3333333333333333
-LRB- counselling 	 0.0027100271002710027
overriding issue 	 1.0
examples where 	 0.041666666666666664
create edges 	 0.058823529411764705
, along 	 0.0005614823133071309
of yesterday 	 0.00267379679144385
desired -RRB- 	 0.2
Constraints may 	 0.3333333333333333
of speech 	 0.040998217468805706
poetry passages 	 1.0
type is 	 0.14285714285714285
objects and 	 0.2
is available 	 0.0020325203252032522
loss function 	 1.0
for Italian 	 0.0036101083032490976
The model 	 0.005208333333333333
Margaret Wetherell 	 1.0
knowledge bases 	 0.037037037037037035
to natural 	 0.0013280212483399733
but IR 	 0.014705882352941176
`` still 	 0.005291005291005291
information deemed 	 0.021739130434782608
abbreviations -RRB- 	 0.2
system-generated summaries 	 0.5
and identify 	 0.002890173410404624
reasoning approach 	 0.14285714285714285
ways to 	 0.125
: an 	 0.00980392156862745
summarization systems 	 0.1
currently the 	 0.14285714285714285
judge , 	 0.25
are dealing 	 0.004149377593360996
missions . 	 1.0
generally without 	 0.09090909090909091
inference algorithms 	 0.25
, shallow 	 0.0005614823133071309
involve grammar 	 0.16666666666666666
useful review 	 0.07142857142857142
it quite 	 0.008547008547008548
argued that 	 1.0
and speaker 	 0.001445086705202312
the concept 	 0.001384083044982699
rules generated 	 0.023255813953488372
Language Constraints 	 0.08333333333333333
You are 	 1.0
document . 	 0.1388888888888889
`` Army 	 0.005291005291005291
individual sentences 	 0.08333333333333333
quite distinct 	 0.125
of years 	 0.00089126559714795
by people 	 0.011428571428571429
Automatic segmentation 	 0.3333333333333333
<s> Full 	 0.0007686395080707148
referenced . 	 1.0
or 4-gram 	 0.0045045045045045045
<s> Psycholinguists 	 0.0007686395080707148
on an 	 0.014150943396226415
production has 	 0.3333333333333333
in its 	 0.003745318352059925
immediate neighbors 	 1.0
5 % 	 0.5
This system 	 0.015873015873015872
the class 	 0.0006920415224913495
` nice 	 0.0625
the opinion 	 0.0006920415224913495
character stream 	 0.045454545454545456
<s> Similarly 	 0.0007686395080707148
constituents such 	 0.5
`` training 	 0.005291005291005291
the next 	 0.004152249134948097
-LRB- Microsoft 	 0.0027100271002710027
special types 	 0.2
idea but 	 0.14285714285714285
as 50 	 0.003484320557491289
tagging techniques 	 0.04
if indeed 	 0.03571428571428571
-RRB- -- 	 0.0027100271002710027
The acoustic 	 0.005208333333333333
probabilistic and 	 0.14285714285714285
would still 	 0.018867924528301886
of good 	 0.00089126559714795
practically available 	 1.0
indicate speech 	 0.3333333333333333
typical large-vocabulary 	 0.1111111111111111
, GRASSHOPPER 	 0.0005614823133071309
limit is 	 0.25
the true 	 0.0006920415224913495
its application 	 0.02857142857142857
NLP and 	 0.02127659574468085
be located 	 0.004219409282700422
more or 	 0.031578947368421054
been made 	 0.014705882352941176
content and 	 0.16666666666666666
gradually reduced 	 1.0
Marilyn Monroe 	 1.0
or deferred 	 0.0045045045045045045
complex recognition 	 0.041666666666666664
is analyzed 	 0.006097560975609756
<s> Encouraging 	 0.0007686395080707148
silence are 	 1.0
of generating 	 0.00089126559714795
of people 	 0.00089126559714795
of modern 	 0.00089126559714795
to HMMs 	 0.0013280212483399733
by DARPA 	 0.005714285714285714
cares about 	 1.0
Sager , 	 0.5
for clarification 	 0.0036101083032490976
authoritative of 	 1.0
separators -RRB- 	 1.0
<s> Helicopters 	 0.0007686395080707148
the characters 	 0.0006920415224913495
potentially more 	 0.3333333333333333
implemented , 	 0.2
they superimpose 	 0.025
recognition tasks 	 0.01652892561983471
units -- 	 0.14285714285714285
45 % 	 1.0
product became 	 0.14285714285714285
collection -RRB- 	 0.2
isolated speech 	 0.4
a dog 	 0.001226993865030675
feedback . 	 0.5
and Rubin 	 0.001445086705202312
inserts those 	 1.0
smaller sub-sounds 	 0.14285714285714285
salience . 	 1.0
draft document 	 0.5
practical systems 	 0.5
basis of 	 0.6666666666666666
in translating 	 0.0018726591760299626
generated . 	 0.2
nodes should 	 0.14285714285714285
capabilities were 	 0.2
Conference Evaluation 	 0.5
<s> Dragon 	 0.0007686395080707148
example text 	 0.012345679012345678
complex system 	 0.08333333333333333
dealing with 	 1.0
`` not 	 0.005291005291005291
a compiler 	 0.0036809815950920245
reflect a 	 1.0
within that 	 0.05555555555555555
identify . 	 0.08333333333333333
a Japanese 	 0.001226993865030675
can select 	 0.0055248618784530384
<s> Part-of-speech 	 0.0007686395080707148
reasons . 	 0.5
Ochs , 	 1.0
this information 	 0.01098901098901099
Chinese characters 	 0.14285714285714285
V , 	 1.0
`` Speech 	 0.005291005291005291
Typically features 	 1.0
1960s . 	 0.3333333333333333
an arithmetic 	 0.007575757575757576
2000 -RRB- 	 0.3333333333333333
the techniques 	 0.0006920415224913495
device required 	 0.5
possible task 	 0.041666666666666664
identify keyphrases 	 0.08333333333333333
Parsers may 	 0.5
user , 	 0.07142857142857142
-LRB- GPO 	 0.0027100271002710027
meet larger 	 0.25
the programs 	 0.0006920415224913495
evaluation in 	 0.05555555555555555
recall-based measure 	 0.5
speakers to 	 0.25
ROUGE measures 	 0.2
Drew , 	 1.0
is on 	 0.0040650406504065045
classifier so 	 0.14285714285714285
In 1978 	 0.009523809523809525
approximating sentence 	 1.0
caught ' 	 1.0
merged with 	 1.0
which items 	 0.007246376811594203
test document 	 0.1
or XML 	 0.0045045045045045045
readability and 	 1.0
with images 	 0.00546448087431694
children , 	 0.5
all unknowns 	 0.023255813953488372
convey intended 	 0.3333333333333333
Robyn Carston 	 1.0
is using 	 0.0040650406504065045
has little 	 0.011904761904761904
helped overall 	 0.3333333333333333
then noun 	 0.02857142857142857
analyzed using 	 0.2
translation methodologies 	 0.013513513513513514
and answers 	 0.001445086705202312
reference for 	 0.125
select keyphrases 	 0.16666666666666666
of certain 	 0.00089126559714795
gold standard 	 0.8333333333333334
be to 	 0.008438818565400843
grammars . 	 0.14285714285714285
filtered from 	 0.3333333333333333
workday to 	 1.0
text normally 	 0.006289308176100629
to which 	 0.006640106241699867
, relative 	 0.0005614823133071309
green fire 	 1.0
May 2012 	 0.5
the dBase 	 0.0006920415224913495
and algorithms 	 0.001445086705202312
get this 	 0.14285714285714285
The Unicode 	 0.005208333333333333
built a 	 0.3333333333333333
away unlikely 	 0.5
, phrases 	 0.0005614823133071309
or nature 	 0.0045045045045045045
machine-learning approach 	 0.25
linguistic knowledge 	 0.0625
N-best list 	 1.0
a strong 	 0.00245398773006135
of unstructured 	 0.00089126559714795
in essentially 	 0.0018726591760299626
then extrapolate 	 0.02857142857142857
and merging 	 0.001445086705202312
design of 	 0.25
they join 	 0.025
quality criteria 	 0.1
system such 	 0.010752688172043012
independent systems 	 0.5
software to 	 0.07407407407407407
into readable 	 0.01282051282051282
project . 	 0.07692307692307693
SVOX . 	 1.0
minimum classification 	 0.5
computer programs 	 0.045454545454545456
summarization -RRB- 	 0.02
-RRB- has 	 0.008130081300813009
otherwise , 	 0.5
it aims 	 0.008547008547008548
processed Airline 	 0.16666666666666666
the context 	 0.004152249134948097
subproblem of 	 1.0
ambiguity '' 	 0.125
entropy-based summarization 	 1.0
adjectives , 	 0.3333333333333333
disambiguation : 	 0.1
space character 	 0.2
answer -LRB- 	 0.03333333333333333
like `` 	 0.10714285714285714
to English 	 0.0013280212483399733
in overall 	 0.0018726591760299626
% correct 	 0.02564102564102564
general speaker 	 0.045454545454545456
multiple topics 	 0.07692307692307693
keyphrase using 	 0.05263157894736842
at helping 	 0.014705882352941176
choices -LRB- 	 0.2
how air 	 0.034482758620689655
suggest valuable 	 0.3333333333333333
system for 	 0.021505376344086023
need to 	 0.47619047619047616
Automated essay 	 0.5
construction , 	 0.3333333333333333
of edge 	 0.00089126559714795
images . 	 0.16666666666666666
Context and 	 1.0
the Lancaster-Oslo-Bergen 	 0.0006920415224913495
conversion of 	 0.6666666666666666
by periods 	 0.005714285714285714
to consult 	 0.0013280212483399733
`` learning 	 0.021164021164021163
on integrating 	 0.0047169811320754715
speech recognizers 	 0.006578947368421052
As businesses 	 0.05555555555555555
political discourse 	 0.3333333333333333
some classification-related 	 0.012048192771084338
or program 	 0.0045045045045045045
therefore it 	 0.6
appropriate number 	 0.25
Mobile devices 	 0.3333333333333333
texts . 	 0.17647058823529413
and trigrams 	 0.002890173410404624
foreign word 	 0.5
Several MT 	 0.3333333333333333
hidden parts 	 0.125
neighbors are 	 0.3333333333333333
noun in 	 0.07142857142857142
ushered in 	 1.0
now rely 	 0.07692307692307693
-LRB- DOE 	 0.0027100271002710027
even several 	 0.037037037037037035
learning procedures 	 0.046511627906976744
conversations . 	 0.3333333333333333
be needed 	 0.004219409282700422
Organization , 	 1.0
are domain-independent 	 0.004149377593360996
an underlying 	 0.007575757575757576
human-generated model 	 0.5
volume and 	 0.25
-LRB- Kittredge 	 0.0027100271002710027
elements from 	 0.25
actual NLP 	 0.2
posed in 	 0.6666666666666666
C4 .5 	 1.0
Turn around 	 1.0
the optical 	 0.0006920415224913495
conferences held 	 1.0
annotation and 	 0.25
two distinct 	 0.034482758620689655
, proper 	 0.0005614823133071309
would identify 	 0.018867924528301886
lies a 	 0.5
difficult . 	 0.07142857142857142
problem for 	 0.022727272727272728
supervised learning 	 0.3125
been created 	 0.029411764705882353
presume a 	 1.0
were called 	 0.024390243902439025
often using 	 0.022727272727272728
like sentence 	 0.03571428571428571
bought the 	 1.0
a dissertation 	 0.001226993865030675
semantic model 	 0.047619047619047616
a frame 	 0.001226993865030675
demonstrates the 	 1.0
large amounts 	 0.043478260869565216
methods of 	 0.045454545454545456
parsers will 	 0.15384615384615385
techniques fall 	 0.043478260869565216
, symbols 	 0.0005614823133071309
the biomedical 	 0.0006920415224913495
abbreviations , 	 0.4
continuous speech 	 0.5
technology development 	 0.045454545454545456
a predefined 	 0.001226993865030675
pairs , 	 1.0
nautical term 	 0.5
The poor 	 0.005208333333333333
the use\/mention 	 0.0006920415224913495
units as 	 0.14285714285714285
1990s , 	 0.3333333333333333
vector of 	 0.3333333333333333
structure The 	 0.08333333333333333
allow spoken 	 0.2
tasks in 	 0.09375
on neat 	 0.0047169811320754715
text corresponds 	 0.006289308176100629
use directly 	 0.013888888888888888
considered an 	 0.1111111111111111
`` Fundamentals 	 0.010582010582010581
involved in 	 0.16666666666666666
a slide 	 0.001226993865030675
for his 	 0.007220216606498195
an upper-case 	 0.007575757575757576
method and 	 0.0625
a user 	 0.00245398773006135
volume , 	 0.25
correct part 	 0.2
both be 	 0.03225806451612903
algorithms currently 	 0.02857142857142857
Pennsylvania in 	 1.0
action is 	 0.2
Conferences , 	 0.5
the robot 	 0.0006920415224913495
`` My 	 0.005291005291005291
, more 	 0.0022459292532285235
summaries -LRB- 	 0.046511627906976744
other country 	 0.014285714285714285
many chatterbots 	 0.019230769230769232
to very 	 0.0013280212483399733
F-16 aircraft 	 0.5
be run 	 0.004219409282700422
only relief 	 0.02631578947368421
-LRB- creating 	 0.0027100271002710027
any supervised 	 0.03225806451612903
NP-complete . 	 1.0
the extent 	 0.001384083044982699
-LRB- http:\/\/haydn.isi.edu\/ROUGE\/ 	 0.0027100271002710027
the end 	 0.001384083044982699
from closely 	 0.009615384615384616
design feature 	 0.25
A parser 	 0.02
ParaEval -RRB- 	 1.0
Technologies that 	 1.0
experts of 	 1.0
change focus 	 1.0
went in 	 0.2
Martin , 	 0.5
Corpus tag 	 0.125
about 12 	 0.025
as Turkish 	 0.003484320557491289
<s> For 	 0.043812451960030745
Extractive methods 	 1.0
rule-based machine 	 0.14285714285714285
differ from 	 0.3333333333333333
a roadmap 	 0.001226993865030675
, Abney 	 0.0005614823133071309
necessary . 	 0.1
authors decide 	 0.2
, identify 	 0.0011229646266142617
recognition refers 	 0.008264462809917356
correlate best 	 0.3333333333333333
Biden visited 	 0.3333333333333333
guide the 	 1.0
dictionary is 	 0.14285714285714285
E. , 	 0.25
payment systems 	 1.0
about machine 	 0.025
intelligent character 	 1.0
and would 	 0.001445086705202312
He pointed 	 0.125
, political 	 0.0005614823133071309
CLAWS pioneered 	 0.25
to solve 	 0.005312084993359893
and Plot 	 0.001445086705202312
convinced many 	 1.0
Vietnamese , 	 1.0
the diagramming 	 0.001384083044982699
extractive summarization 	 0.42857142857142855
calculates n-gram 	 1.0
having ` 	 0.2
contextual polarity 	 0.5
approaches , 	 0.03571428571428571
not just 	 0.008928571428571428
each phoneme 	 0.022222222222222223
A high 	 0.02
address - 	 0.25
represents a 	 0.75
it may 	 0.017094017094017096
<s> Referring 	 0.0007686395080707148
even the 	 0.07407407407407407
-RRB- This 	 0.0027100271002710027
release beyond 	 0.3333333333333333
Wodak , 	 1.0
database look-up 	 0.1
and have 	 0.001445086705202312
apparent from 	 1.0
Advanced , 	 0.2
can even 	 0.0055248618784530384
Svenka Savic 	 1.0
or people 	 0.009009009009009009
algorithm , 	 0.10714285714285714
criteria is 	 0.25
computational linguistics 	 0.6
into French 	 0.02564102564102564
see List 	 0.05
's OCR 	 0.0196078431372549
looking to 	 0.4
the sentiment 	 0.001384083044982699
translating speech 	 0.25
one natural 	 0.03076923076923077
complexity , 	 0.16666666666666666
are important 	 0.004149377593360996
or English-like 	 0.0045045045045045045
a skilled 	 0.001226993865030675
centrality '' 	 0.5
Corps of 	 1.0
many languages 	 0.019230769230769232
could co-occur 	 0.0625
'' - 	 0.010309278350515464
negligence '' 	 1.0
reasoning schemes 	 0.14285714285714285
statically beforehand 	 1.0
is brought 	 0.0020325203252032522
for . 	 0.0036101083032490976
semantic similarity 	 0.047619047619047616
transfer-based , 	 0.3333333333333333
hub '' 	 1.0
metric such 	 0.3333333333333333
paradigm of 	 0.3333333333333333
what knowledge 	 0.03125
do not 	 0.5
are implemented 	 0.004149377593360996
In 1982 	 0.009523809523809525
various aspects 	 0.05555555555555555
a simple 	 0.001226993865030675
meet President 	 0.25
and patented 	 0.001445086705202312
from floods 	 0.009615384615384616
less than 	 0.25
, answered 	 0.0005614823133071309
Northern areas 	 0.3333333333333333
proceedings into 	 1.0
demonstration of 	 0.4
well . 	 0.07142857142857142
unstructured text 	 1.0
reducing training 	 0.5
-LRB- for 	 0.018970189701897018
addition , 	 0.3333333333333333
, Hindle 	 0.0005614823133071309
is recall-based 	 0.0020325203252032522
Efficient algorithms 	 1.0
e.g. Noise 	 0.017857142857142856
phonetic-based categories 	 1.0
look to 	 0.2
NLP Handbook 	 0.02127659574468085
emails and 	 0.5
widely applied 	 0.125
markers over 	 0.3333333333333333
1989 -RRB- 	 0.5
start symbol 	 0.2857142857142857
used is 	 0.008849557522123894
which means 	 0.028985507246376812
, leaving 	 0.0005614823133071309
intensive as 	 1.0
separate lexical 	 0.1
teletype typewritten 	 1.0
use ` 	 0.013888888888888888
are beyond 	 0.004149377593360996
between Internet 	 0.02564102564102564
Open-domain question 	 1.0
thus reducing 	 0.1
-LRB- most 	 0.01084010840108401
knowledge source 	 0.037037037037037035
usefulness of 	 1.0
at Yale 	 0.029411764705882353
Italian ; 	 0.5
ISO sub-committee 	 0.5
rule-based methods 	 0.14285714285714285
in those 	 0.0018726591760299626
combinations of 	 1.0
e.g. `` 	 0.017857142857142856
corrected by 	 1.0
is reported 	 0.0020325203252032522
Mellon University 	 1.0
a specialised 	 0.001226993865030675
games , 	 1.0
-RRB- break 	 0.0027100271002710027
for document 	 0.0036101083032490976
contrast to 	 0.25
past-tense verb 	 1.0
meaningful portions 	 0.125
this shifting 	 0.01098901098901099
machine-translation research 	 0.5
converse on 	 1.0
conducted the 	 0.2
efficient however 	 0.3333333333333333
also being 	 0.014492753623188406
document , 	 0.1388888888888889
corpora '' 	 0.09090909090909091
main underlying 	 0.125
Scotland . 	 0.4
commonly associated 	 0.125
answers from 	 0.16666666666666666
Information -LRB- 	 0.2
a computer 	 0.018404907975460124
potential parses 	 0.14285714285714285
the bi-directional 	 0.0006920415224913495
presented with 	 0.16666666666666666
<s> POS 	 0.0007686395080707148
instances , 	 0.3333333333333333
billion words 	 1.0
get the 	 0.2857142857142857
step the 	 0.06666666666666667
may pick 	 0.019230769230769232
shallow-transfer machine 	 1.0
new odd 	 0.041666666666666664
the book 	 0.001384083044982699
optimal match 	 1.0
Why , 	 0.14285714285714285
One could 	 0.07692307692307693
grouped into 	 0.5
worth remembering 	 0.5
possibilities . 	 0.2
publishing , 	 1.0
l'assignation des 	 1.0
would use 	 0.018867924528301886
a test 	 0.0036809815950920245
character-by-character OCR 	 1.0
ten years 	 1.0
Art Graesser 	 1.0
-LRB- orally 	 0.0027100271002710027
to 150 	 0.0013280212483399733
recognition -RRB- 	 0.008264462809917356
predict -RRB- 	 0.16666666666666666
Another reason 	 0.07692307692307693
answers . 	 0.08333333333333333
rely on 	 0.8571428571428571
funding Measuring 	 0.125
multiple subtasks 	 0.07692307692307693
of substantial 	 0.00089126559714795
was historically 	 0.012987012987012988
DCD library 	 1.0
, but 	 0.02695115103874228
a natural 	 0.006134969325153374
The FAA 	 0.005208333333333333
keyphrases are 	 0.05714285714285714
the UK 	 0.002768166089965398
American Recovery 	 0.2
are different 	 0.004149377593360996
decide when 	 0.25
continues to 	 1.0
preselects small 	 1.0
Lao , 	 1.0
brought together 	 1.0
linguistically meaningful 	 1.0
with learning 	 0.00546448087431694
right information 	 0.1
along with 	 1.0
<s> Extracted 	 0.0007686395080707148
so that 	 0.2
<s> DARPA 	 0.0007686395080707148
adaptive summarization 	 0.6666666666666666
commercial OCR 	 0.18181818181818182
evaluation , 	 0.05555555555555555
incorrect assignment 	 0.3333333333333333
sentences that 	 0.06578947368421052
the identity 	 0.002768166089965398
ratings on 	 0.1111111111111111
ones that 	 0.1
at emotional 	 0.014705882352941176
kick ' 	 1.0
is Reiter 	 0.0020325203252032522
more severe 	 0.010526315789473684
news article 	 0.15384615384615385
main drawback 	 0.125
on democratizing 	 0.0047169811320754715
of discourse 	 0.00980392156862745
but far 	 0.014705882352941176
which the 	 0.057971014492753624
any answer 	 0.03225806451612903
grammar of 	 0.05405405405405406
produced by 	 0.3333333333333333
its definition 	 0.02857142857142857
NLP using 	 0.02127659574468085
<s> human 	 0.0007686395080707148
instance some 	 0.07142857142857142
estimated probability 	 1.0
surrounding words 	 0.4
machine-learning systems 	 0.25
in . 	 0.0018726591760299626
computer vision 	 0.022727272727272728
the Northern 	 0.001384083044982699
with one 	 0.00546448087431694
the Defense 	 0.0006920415224913495
to correct 	 0.0013280212483399733
issue is 	 0.125
of Arabic 	 0.00089126559714795
TextRank uses 	 0.14285714285714285
`` central 	 0.010582010582010581
different aspects 	 0.02040816326530612
the memory 	 0.0006920415224913495
because many 	 0.03333333333333333
Question Answering 	 0.14285714285714285
identify and 	 0.08333333333333333
Halliday , 	 1.0
input a 	 0.024390243902439025
technology Sensory 	 0.045454545454545456
program is 	 0.045454545454545456
This way 	 0.015873015873015872
, i.e. 	 0.0039303761931499155
OCR -RRB- 	 0.02040816326530612
ICR . 	 0.3333333333333333
collections vary 	 0.25
first evaluation 	 0.030303030303030304
Penicillin '' 	 1.0
text itself 	 0.006289308176100629
<s> Conferences 	 0.0007686395080707148
interpretable rules 	 1.0
of four 	 0.00089126559714795
Chinese , 	 0.2857142857142857
The problem 	 0.015625
context can 	 0.030303030303030304
discourse analyst 	 0.027777777777777776
<s> With 	 0.003843197540353574
are keyphrase 	 0.004149377593360996
as ME 	 0.003484320557491289
method can 	 0.0625
it is 	 0.20512820512820512
Compute features 	 1.0
term parsing 	 0.05555555555555555
reading machine 	 0.125
a cell 	 0.00245398773006135
examples produces 	 0.041666666666666664
a radiology 	 0.001226993865030675
different co-occurring 	 0.02040816326530612
still be 	 0.06666666666666667
slowly but 	 0.5
were able 	 0.024390243902439025
sequential lines 	 1.0
; it 	 0.02127659574468085
learning -RRB- 	 0.023255813953488372
journal article 	 0.3333333333333333
best single 	 0.05555555555555555
different speaking 	 0.02040816326530612
sound in 	 0.05
except some 	 1.0
more readily 	 0.010526315789473684
choice : 	 0.125
Marc Angenot 	 1.0
the Vocabulary 	 0.0006920415224913495
`` The 	 0.015873015873015872
short time-scales 	 0.125
<s> Back-End 	 0.0007686395080707148
highly ranked 	 0.1111111111111111
examples have 	 0.041666666666666664
indeed answer 	 0.3333333333333333
a top-down 	 0.001226993865030675
a knowledge 	 0.001226993865030675
builds a 	 0.5
redundancy . 	 0.3333333333333333
= singular 	 0.1111111111111111
responding to 	 1.0
Applied linguistics 	 0.5
many more 	 0.019230769230769232
ways , 	 0.25
related to 	 0.26666666666666666
, MySpace 	 0.0005614823133071309
to adjust\/correct 	 0.0013280212483399733
by increasing 	 0.005714285714285714
sentence also 	 0.020833333333333332
been more 	 0.029411764705882353
generally more 	 0.18181818181818182
` caught 	 0.0625
several qualities 	 0.045454545454545456
a cosine 	 0.001226993865030675
phrase -LRB- 	 0.1
in errata 	 0.0018726591760299626
begin with 	 0.6666666666666666
lowest level 	 1.0
some linguistic 	 0.012048192771084338
keyphrases your 	 0.02857142857142857
isolation . 	 0.5
continued with 	 0.1111111111111111
adjectives . 	 0.3333333333333333
to capture 	 0.0013280212483399733
review as 	 0.3333333333333333
restricted-domain QA 	 1.0
the derived 	 0.0006920415224913495
large collections 	 0.043478260869565216
considered for 	 0.1111111111111111
two enabling 	 0.034482758620689655
level . 	 0.05
of extractive 	 0.00089126559714795
and methods 	 0.001445086705202312
and classifying 	 0.001445086705202312
in accordance 	 0.0018726591760299626
asked for 	 0.3333333333333333
orange in 	 1.0
<s> SHRDLU 	 0.0015372790161414297
adaptation greatly 	 0.3333333333333333
in natural 	 0.013108614232209739
MT Hybrid 	 0.2
obvious at 	 1.0
classifying short-time 	 0.2
the machine 	 0.0006920415224913495
considered good 	 0.1111111111111111
printed documents 	 0.08333333333333333
an interest 	 0.007575757575757576
the Romance 	 0.0006920415224913495
worse if 	 1.0
less accurate 	 0.08333333333333333
University by 	 0.1111111111111111
and visible 	 0.001445086705202312
Often a 	 0.3333333333333333
Online , 	 0.5
'' exceeded 	 0.005154639175257732
Pollen counts 	 1.0
complex NLG 	 0.041666666666666664
reporting on 	 0.3333333333333333
about as 	 0.025
answer 90 	 0.03333333333333333
returning a 	 1.0
on computational 	 0.0047169811320754715
would . 	 0.018867924528301886
learning problem 	 0.023255813953488372
deterministic rule 	 0.25
given approach 	 0.08333333333333333
, deciding 	 0.0022459292532285235
, discourse 	 0.0005614823133071309
that were 	 0.014184397163120567
`` warped 	 0.005291005291005291
Document summarization 	 0.25
fast-evolving field 	 1.0
testing for 	 0.2
As mentioned 	 0.16666666666666666
challenging because 	 1.0
-LRB- Black 	 0.0027100271002710027
a camera 	 0.001226993865030675
paper is 	 0.09090909090909091
this meaning 	 0.01098901098901099
chosen publications 	 0.2
source materials 	 0.041666666666666664
assigns large 	 1.0
used English 	 0.008849557522123894
called evaluation 	 0.1111111111111111
areas with 	 0.3333333333333333
commands issued 	 0.2
pragmatics of 	 0.3333333333333333
meaning part 	 0.043478260869565216
Japanese prisoners 	 0.125
all nouns 	 0.023255813953488372
retail sales 	 1.0
other . 	 0.02857142857142857
The difference 	 0.005208333333333333
meanings of 	 0.25
sense in 	 0.125
of logical 	 0.00089126559714795
, Ford 	 0.0005614823133071309
-LRB- icon 	 0.0027100271002710027
short intervals 	 0.125
data , 	 0.12987012987012986
on any 	 0.018867924528301886
with different 	 0.01092896174863388
for descriptive 	 0.0036101083032490976
Generally speaking 	 0.4
substantial ambiguity 	 0.2
Das , 	 1.0
For more 	 0.03278688524590164
This is 	 0.2698412698412698
given an 	 0.041666666666666664
`` Red 	 0.005291005291005291
segmentation may 	 0.030303030303030304
billing purposes 	 1.0
all rules 	 0.023255813953488372
the cosine 	 0.0006920415224913495
first order 	 0.030303030303030304
quality and 	 0.1
capital letters 	 0.3333333333333333
boundaries of 	 0.09090909090909091
not pursued 	 0.008928571428571428
neighbors . 	 0.3333333333333333
Much remains 	 0.3333333333333333
AI-complete '' 	 0.6666666666666666
text based 	 0.006289308176100629
and funding 	 0.001445086705202312
that is 	 0.05673758865248227
window of 	 1.0
ratings : 	 0.1111111111111111
, enables 	 0.0005614823133071309
EHR will 	 0.3333333333333333
with misspelled 	 0.00546448087431694
rules that 	 0.09302325581395349
did exactly 	 0.2
random surfer 	 0.14285714285714285
the culture 	 0.0006920415224913495
aspect is 	 0.5
LUNAR , 	 0.6666666666666666
<s> User 	 0.0007686395080707148
measuring similarity 	 1.0
epidemic which 	 1.0
it Contains 	 0.008547008547008548
computed as 	 0.5
regards to 	 1.0
are grouped 	 0.004149377593360996
's instructions 	 0.0196078431372549
central `` 	 0.3333333333333333
linear discriminant 	 0.2857142857142857
applications Aerospace 	 0.04
Speech recogniton 	 0.03225806451612903
been done 	 0.029411764705882353
handwritten cursive 	 0.5
<s> Virtually 	 0.0007686395080707148
<s> Markov 	 0.0007686395080707148
idioms , 	 1.0
as input 	 0.006968641114982578
; An 	 0.02127659574468085
a probabilistic 	 0.001226993865030675
consumed from 	 1.0
formal language 	 0.2222222222222222
in a 	 0.09363295880149813
then it 	 0.05714285714285714
can deal 	 0.0055248618784530384
OnlineOCR practically 	 0.3333333333333333
consistently use 	 0.3333333333333333
revolutionized bill 	 1.0
system would 	 0.021505376344086023
errata , 	 1.0
smoothly or 	 0.5
all been 	 0.023255813953488372
insight into 	 1.0
, displayed 	 0.0005614823133071309
what extent 	 0.03125
-RRB- case 	 0.0027100271002710027
should be 	 0.47368421052631576
processing of 	 0.037037037037037035
<s> Evaluation 	 0.003843197540353574
be necessary 	 0.008438818565400843
recursively . 	 0.5
May 2009 	 0.5
a better 	 0.00245398773006135
deep parsing 	 0.14285714285714285
every 10msec 	 0.3333333333333333
handwriting , 	 0.5
understanding the 	 0.12121212121212122
not describe 	 0.008928571428571428
describing language 	 0.25
chunk of 	 1.0
language can 	 0.006756756756756757
summaries can 	 0.046511627906976744
Bhatia , 	 1.0
compute the 	 0.5
of surrounding 	 0.00089126559714795
technology would 	 0.045454545454545456
Keyphrase Extraction 	 0.25
global ` 	 0.3333333333333333
right . 	 0.3
, pronunciation 	 0.0005614823133071309
as weapon 	 0.003484320557491289
are confirmed 	 0.004149377593360996
the Pyramid 	 0.0006920415224913495
, online 	 0.0016844469399213925
registry . 	 1.0
simply ranks 	 0.08333333333333333
system on 	 0.010752688172043012
are under 	 0.004149377593360996
person to 	 0.10526315789473684
be represented 	 0.008438818565400843
various fine 	 0.05555555555555555
words such 	 0.01834862385321101
file to 	 1.0
verbal unit 	 1.0
is whether 	 0.0020325203252032522
two meanings 	 0.034482758620689655
using journal 	 0.01694915254237288
, OnStar 	 0.0005614823133071309
, texts 	 0.0005614823133071309
summaries helps 	 0.023255813953488372
American Bible 	 0.2
disabilities People 	 0.25
, disturbed 	 0.0005614823133071309
developing Q&A 	 0.25
Training for 	 0.5
for various 	 0.0036101083032490976
Parsing can 	 0.2
, Oklahoma 	 0.0005614823133071309
document browsing 	 0.027777777777777776
long-time translator 	 1.0
random walk 	 0.5714285714285714
mainly evaluation 	 0.16666666666666666
of different 	 0.0017825311942959
resources it 	 0.16666666666666666
vs. objective 	 0.08333333333333333
2 '' 	 0.2
Message Understanding 	 1.0
startlingly human-like 	 1.0
verb or 	 0.23076923076923078
waves and 	 0.14285714285714285
& Lehrberger 	 0.125
included as 	 0.125
the production 	 0.0006920415224913495
software -LRB- 	 0.037037037037037035
, could 	 0.0005614823133071309
is as 	 0.0020325203252032522
or from 	 0.0045045045045045045
visit Iraq 	 0.5
example generic 	 0.012345679012345678
, NP 	 0.0005614823133071309
using paraphrases 	 0.01694915254237288
and languages 	 0.001445086705202312
or she 	 0.0045045045045045045
to carefully 	 0.0013280212483399733
current commercial 	 0.14285714285714285
on several 	 0.0047169811320754715
universities around 	 1.0
of original 	 0.00089126559714795
contains the 	 0.2
2008 -RRB- 	 1.0
now we 	 0.07692307692307693
alignment method 	 0.5
weather reports 	 0.2857142857142857
are broader 	 0.004149377593360996
the sort 	 0.0006920415224913495
backup methods 	 1.0
tagging was 	 0.08
, we 	 0.009545199326221224
relative probability 	 0.3333333333333333
of several 	 0.00267379679144385
OS or 	 0.5
letter ? 	 0.16666666666666666
in various 	 0.0056179775280898875
rhetoric , 	 1.0
high ranks 	 0.05555555555555555
certain patterns 	 0.14285714285714285
and many 	 0.001445086705202312
measure one 	 0.09090909090909091
breaks exist 	 0.5
fact , 	 0.45454545454545453
understanding and 	 0.030303030303030304
are very 	 0.016597510373443983
ambiguous and 	 0.16666666666666666
on automatically 	 0.0047169811320754715
, multilingual 	 0.0005614823133071309
necessary subtask 	 0.1
GPO -RRB- 	 1.0
automating abstractive 	 1.0
models have 	 0.038461538461538464
during the 	 0.4
chapter , 	 1.0
large multilingual 	 0.043478260869565216
which focuses 	 0.007246376811594203
some cases 	 0.04819277108433735
analysis -RRB- 	 0.03076923076923077
the mechanism 	 0.0006920415224913495
ways : 	 0.25
, How 	 0.0005614823133071309
<s> Based 	 0.0007686395080707148
those languages 	 0.09090909090909091
, contain 	 0.0005614823133071309
user are 	 0.07142857142857142
affine and 	 1.0
help determine 	 0.1111111111111111
both LexRank 	 0.03225806451612903
in simple 	 0.0018726591760299626
work derived 	 0.041666666666666664
performance only 	 0.05555555555555555
been popular 	 0.014705882352941176
`` diverse 	 0.005291005291005291
testing the 	 0.2
syntactic parser 	 0.07692307692307693
the polarity 	 0.0006920415224913495
with Optical 	 0.00546448087431694
discrete phonetic 	 0.3333333333333333
power increased 	 0.25
built in 	 0.3333333333333333
generic machine-generated 	 0.3333333333333333
lexical exigencies 	 0.07692307692307693
or , 	 0.0045045045045045045
, social 	 0.0011229646266142617
an individual 	 0.007575757575757576
generating too 	 0.2
application . 	 0.07142857142857142
microphone . 	 1.0
Latin-script , 	 1.0
and introducing 	 0.001445086705202312
his book 	 0.08333333333333333
Its results 	 0.5
sociolinguistics , 	 0.5
the Brown 	 0.005536332179930796
flexibility and 	 1.0
tools starts 	 0.16666666666666666
as supervised 	 0.003484320557491289
history -RRB- 	 0.25
are marked 	 0.004149377593360996
deaf telephony 	 1.0
an image 	 0.007575757575757576
ranking -LRB- 	 0.14285714285714285
`` He 	 0.005291005291005291
opinionated , 	 1.0
paper legal 	 0.09090909090909091
latter as 	 1.0
extraction algorithm 	 0.03225806451612903
from mild 	 0.009615384615384616
the financial 	 0.0006920415224913495
For individuals 	 0.01639344262295082
depending what 	 0.25
for multi-document 	 0.0036101083032490976
Eagles Guidelines 	 1.0
<s> Application-Oriented 	 0.0007686395080707148
might expect 	 0.038461538461538464
a corresponding 	 0.001226993865030675
legal documents 	 0.3333333333333333
tools require 	 0.16666666666666666
elements of 	 0.25
Battle Management 	 0.5
conduct with 	 1.0
in fighter 	 0.0056179775280898875
procedure for 	 0.3333333333333333
simple world 	 0.038461538461538464
is probably 	 0.0020325203252032522
performance has 	 0.05555555555555555
rarely the 	 0.3333333333333333
'' <s/> 	 0.041237113402061855
of overlap 	 0.00089126559714795
NYU , 	 1.0
In 1629 	 0.009523809523809525
support vector 	 0.25
depth understanding 	 0.3333333333333333
entire words 	 0.3333333333333333
semantics . 	 0.07142857142857142
Much of 	 0.3333333333333333
by inputting 	 0.005714285714285714
Document Understanding 	 0.25
about nearly 	 0.025
-LRB- as 	 0.018970189701897018
Braille texts 	 1.0
of typical 	 0.00089126559714795
ID card 	 1.0
many person-years 	 0.019230769230769232
database available 	 0.1
been characterized 	 0.014705882352941176
NLG systems 	 0.23809523809523808
tag `` 	 0.0625
object `` 	 0.5
opinion in 	 0.4
translation programs 	 0.013513513513513514
companies to 	 0.5
it to 	 0.042735042735042736
realizations as 	 1.0
titled Natural 	 1.0
Speech '' 	 0.03225806451612903
approaches assume 	 0.03571428571428571
material may 	 0.5
structured resources 	 0.16666666666666666
query-biased summaries 	 1.0
the semantics 	 0.001384083044982699
strategy to 	 0.6
human kind 	 0.021739130434782608
a patent 	 0.00245398773006135
-RRB- Some 	 0.0027100271002710027
of formalisms\/languages 	 0.00089126559714795
In about 	 0.009523809523809525
'' it 	 0.005154639175257732
technology providers 	 0.045454545454545456
precision because 	 0.2
to dozens 	 0.0013280212483399733
extraction Answer 	 0.03225806451612903
the photos 	 0.0006920415224913495
continued more 	 0.1111111111111111
not before 	 0.008928571428571428
these T 	 0.023809523809523808
Chilton , 	 1.0
more complicated 	 0.010526315789473684
more sophisticated 	 0.021052631578947368
code readers 	 0.14285714285714285
machines , 	 0.25
, stutering 	 0.0005614823133071309
Text grammar 	 0.16666666666666666
WYSIWYM framework 	 1.0
numbers that 	 0.14285714285714285
classify a 	 0.5
lead-in fighter 	 1.0
that builds 	 0.0035460992907801418
understanding is 	 0.06060606060606061
was reading 	 0.012987012987012988
field because 	 0.037037037037037035
results can 	 0.047619047619047616
: Extract 	 0.00980392156862745
scanned images 	 0.3333333333333333
and graphics 	 0.001445086705202312
Recognizing the 	 1.0
is phonetically 	 0.0020325203252032522
no information 	 0.07692307692307693
98.5 % 	 1.0
Civil Aviation 	 1.0
data -RRB- 	 0.03896103896103896
and both 	 0.001445086705202312
may well 	 0.019230769230769232
all possibilities 	 0.023255813953488372
in-depth analysis 	 0.3333333333333333
wrote an 	 0.3333333333333333
separate parts 	 0.1
is responsible 	 0.0020325203252032522
Given the 	 0.07142857142857142
indicating important 	 1.0
how summarization 	 0.034482758620689655
Application-Oriented OCR 	 1.0
a consumer 	 0.001226993865030675
'' http:\/\/arxiv.org\/abs\/1104.2086 	 0.005154639175257732
automatically learn 	 0.09523809523809523
alone -- 	 0.25
analysts This 	 0.5
, stored 	 0.0005614823133071309
lines segments 	 0.3333333333333333
' structures 	 0.10526315789473684
read not 	 0.14285714285714285
language-processing tasks 	 1.0
longer sentences 	 1.0
of continuous 	 0.0017825311942959
nautical context 	 0.5
first commercial 	 0.06060606060606061
question processing 	 0.07142857142857142
, evaluation 	 0.0005614823133071309
human judges 	 0.043478260869565216
, Teun 	 0.0005614823133071309
their closest 	 0.029411764705882353
i.e. the 	 0.2631578947368421
, ratings 	 0.0005614823133071309
and reasoning 	 0.002890173410404624
the appropriate 	 0.0020761245674740486
-- computer 	 0.04
language into 	 0.006756756756756757
the steady 	 0.001384083044982699
at processing 	 0.014705882352941176
<s> Early 	 0.0015372790161414297
letters . 	 0.4
them into 	 0.05263157894736842
value . 	 0.3333333333333333
as high 	 0.003484320557491289
to another 	 0.00398406374501992
The Apple 	 0.005208333333333333
by trying 	 0.005714285714285714
-LRB- `` 	 0.02168021680216802
the requirements 	 0.0006920415224913495
measure summary 	 0.09090909090909091
more reliable 	 0.031578947368421054
times a 	 0.2
being psychotherapy 	 0.05555555555555555
length . 	 0.125
electronic medical 	 0.5
find only 	 0.07692307692307693
A speaker 	 0.04
commercial perspective 	 0.09090909090909091
detected , 	 0.5
edge cases 	 0.3333333333333333
decode the 	 1.0
eliminate redundancy 	 0.5
Other tasks 	 0.14285714285714285
short time 	 0.125
may happen 	 0.019230769230769232
by IMR 	 0.005714285714285714
and proper 	 0.001445086705202312
and SpeechTEK 	 0.001445086705202312
to densely 	 0.0013280212483399733
units ; 	 0.14285714285714285
identified in 	 0.2
basis for 	 0.3333333333333333
language production 	 0.006756756756756757
text corpora 	 0.006289308176100629
topic -RRB- 	 0.125
e.g. WordNet 	 0.017857142857142856
' -RRB- 	 0.05263157894736842
outputting one 	 0.5
one way 	 0.015384615384615385
the gap 	 0.0006920415224913495
because translation 	 0.03333333333333333
instead optimize 	 0.14285714285714285
-RRB- Telematics 	 0.0027100271002710027
but we 	 0.014705882352941176
<s> Today 	 0.0007686395080707148
Chinese and 	 0.14285714285714285
` kit 	 0.125
camp with 	 0.5
analyzed and 	 0.2
NN for 	 1.0
tasks such 	 0.0625
much of 	 0.09090909090909091
work by 	 0.08333333333333333
<s> Lexical 	 0.0015372790161414297
to TextRank 	 0.0013280212483399733
Project , 	 1.0
of his 	 0.00267379679144385
were started 	 0.024390243902439025
networks can 	 0.07142857142857142
for top-down 	 0.0036101083032490976
automatically to 	 0.047619047619047616
create '' 	 0.058823529411764705
versa first-cut 	 1.0
Features might 	 1.0
systems which 	 0.017857142857142856
in picture 	 0.0018726591760299626
in following 	 0.0018726591760299626
page scanner 	 0.14285714285714285
technology are 	 0.045454545454545456
1997 -LRB- 	 0.5
systems sold 	 0.008928571428571428
, linear-time 	 0.0005614823133071309
a web 	 0.001226993865030675
PC can 	 0.25
, Discontinuous 	 0.0005614823133071309
field which 	 0.037037037037037035
automatic summaries 	 0.13043478260869565
that combine 	 0.0035460992907801418
Each sample 	 0.16666666666666666
ROUGE-1 -LRB- 	 0.2
is specifically 	 0.0020325203252032522
-RRB- of 	 0.018970189701897018
<s> Correct 	 0.0007686395080707148
dry up 	 1.0
been automatic 	 0.014705882352941176
create tokens 	 0.058823529411764705
potential redundancy 	 0.14285714285714285
has a 	 0.047619047619047616
its communicative 	 0.02857142857142857
a sequence 	 0.007361963190184049
connected regions 	 0.2
important subproblem 	 0.0625
inference within 	 0.25
general , 	 0.2727272727272727
critical or 	 0.25
Information Science 	 0.2
from natural 	 0.009615384615384616
up-to-date research 	 1.0
and mapping 	 0.001445086705202312
QA It 	 0.047619047619047616
the syntactic 	 0.0006920415224913495
discourse , 	 0.08333333333333333
have multiple 	 0.009615384615384616
right -LRB- 	 0.1
recognition rates 	 0.01652892561983471
sources , 	 0.16666666666666666
itself as 	 0.2
research articles 	 0.023809523809523808
or FST 	 0.0045045045045045045
if it 	 0.07142857142857142
` discourse 	 0.0625
and echoes 	 0.001445086705202312
be possible 	 0.008438818565400843
by Greene 	 0.005714285714285714
its performance 	 0.02857142857142857
first few 	 0.030303030303030304
<s> Additional 	 0.0007686395080707148
communication radios 	 0.2
Associated Press 	 1.0
classifier is 	 0.14285714285714285
are created 	 0.012448132780082987
with such 	 0.00546448087431694
learner , 	 0.5
of 2007 	 0.0017825311942959
of laws 	 0.00089126559714795
Computer Science 	 0.16666666666666666
and retrieving 	 0.001445086705202312
principle , 	 1.0
Faber , 	 1.0
ASR is 	 0.16666666666666666
documents in 	 0.02631578947368421
process Flow 	 0.027777777777777776
to school-age 	 0.0013280212483399733
term frequencies 	 0.05555555555555555
interest . 	 0.09090909090909091
with Swedish 	 0.00546448087431694
<s> Acoustical 	 0.0007686395080707148
the parsing 	 0.001384083044982699
we create 	 0.044444444444444446
and rule-based 	 0.001445086705202312
which do 	 0.007246376811594203
linguistic rules 	 0.0625
extraction of 	 0.0967741935483871
software libraries 	 0.037037037037037035
that was 	 0.010638297872340425
speech acts 	 0.019736842105263157
getting published 	 0.25
left-most and 	 0.5
text rather 	 0.006289308176100629
words as 	 0.009174311926605505
are fields 	 0.004149377593360996
word with 	 0.016666666666666666
taggers can 	 0.14285714285714285
is permuted 	 0.0020325203252032522
compiler , 	 0.3333333333333333
IEEE Transactions 	 0.6666666666666666
laughter -RRB- 	 1.0
given restaurant 	 0.041666666666666664
computers . 	 0.2222222222222222
But the 	 0.16666666666666666
NLP tasks 	 0.0425531914893617
experimented with 	 1.0
induction . 	 1.0
The European 	 0.005208333333333333
words occur 	 0.009174311926605505
meaningful relationships 	 0.125
historically -RRB- 	 0.5
retrieval module 	 0.14285714285714285
word segmentation 	 0.05
up In 	 0.045454545454545456
assessment , 	 1.0
opened , 	 1.0
example type 	 0.012345679012345678
also require 	 0.014492753623188406
discourse analysis 	 0.2222222222222222
January , 	 0.25
If a 	 0.1
<s> Adverse 	 0.0007686395080707148
<s> Yet 	 0.0007686395080707148
inference -- 	 0.25
a trend 	 0.001226993865030675
Conversational analysis 	 1.0
of developing 	 0.00089126559714795
are germane 	 0.004149377593360996
- and 	 0.125
, Nuance 	 0.0005614823133071309
previously prepared 	 0.5
algorithm essentially 	 0.03571428571428571
non-linearly to 	 1.0
Reader 's 	 1.0
symbolic representation 	 1.0
Company for 	 0.5
similar measure 	 0.037037037037037035
of incorrect 	 0.00089126559714795
system was 	 0.053763440860215055
Brown University 	 0.14285714285714285
one must 	 0.015384615384615385
concept is 	 0.25
see references 	 0.05
's input 	 0.0196078431372549
sound signal 	 0.05
evaluation criteria 	 0.037037037037037035
broken in 	 0.2
for this 	 0.018050541516245487
between positive 	 0.02564102564102564
evaluation process 	 0.018518518518518517
coverage of 	 0.3333333333333333
three or 	 0.3333333333333333
has unambiguously 	 0.011904761904761904
will mention 	 0.02857142857142857
runs PageRank 	 1.0
have any 	 0.009615384615384616
research , 	 0.07142857142857142
questioner 's 	 0.25
of cursive 	 0.00089126559714795
correct result 	 0.06666666666666667
personal computer 	 0.25
states are 	 0.25
return ? 	 0.5
, there 	 0.006176305446378439
<s> Contrary 	 0.0015372790161414297
memory of 	 0.5
can occur 	 0.0055248618784530384
assessments of 	 1.0
conveniently as 	 1.0
ambiguities or 	 0.25
adapted to 	 1.0
, see 	 0.0011229646266142617
, Reukos 	 0.0005614823133071309
-LRB- unigram 	 0.0027100271002710027
Alenia Aermacchi 	 1.0
continuous similarity 	 0.16666666666666666
can still 	 0.0055248618784530384
and ATIS 	 0.001445086705202312
: Given 	 0.09803921568627451
and paragraph 	 0.001445086705202312
technologies to 	 0.25
vary from 	 0.16666666666666666
using OCR 	 0.05084745762711865
Wikipedia 's 	 0.5
inputting approximately 	 1.0
merging the 	 0.5
left-to-right . 	 1.0
cheque -LRB- 	 1.0
widely used 	 0.875
Statistics derived 	 0.3333333333333333
from multiple 	 0.009615384615384616
generating index 	 0.2
and Japanese 	 0.001445086705202312
the similarity 	 0.0006920415224913495
metrics used 	 0.1111111111111111
person who 	 0.05263157894736842
would check 	 0.018867924528301886
input can 	 0.024390243902439025
product was 	 0.14285714285714285
well known 	 0.03571428571428571
the envelope 	 0.0006920415224913495
formal grammar 	 0.2222222222222222
graph-based ranking 	 1.0
angle . 	 1.0
So an 	 0.3333333333333333
evaluations such 	 0.16666666666666666
in terms 	 0.011235955056179775
limitation in 	 1.0
basic categories 	 0.07692307692307693
techniques to 	 0.17391304347826086
the conceptual 	 0.0006920415224913495
this vocabulary 	 0.01098901098901099
account several 	 0.3333333333333333
various boolean 	 0.05555555555555555
late 1960s 	 0.1111111111111111
mentions within 	 0.3333333333333333
rules engine 	 0.023255813953488372
correct values 	 0.06666666666666667
: A 	 0.00980392156862745
understanding of 	 0.15151515151515152
a categorical 	 0.001226993865030675
Emergent grammar 	 1.0
emerge that 	 1.0
13 parsers 	 0.5
Parsing algorithms 	 0.2
many other 	 0.09615384615384616
programs often 	 0.09090909090909091
a horizontal 	 0.001226993865030675
Pointwise Mutual 	 1.0
resources and 	 0.16666666666666666
original voice 	 0.07692307692307693
Decoding the 	 0.5
scores that 	 0.2
and performance 	 0.001445086705202312
as HMM 	 0.003484320557491289
neutral . 	 0.5
evaluation methods 	 0.018518518518518517
speed . 	 0.42857142857142855
<s> DeRose 	 0.0015372790161414297
summarization . 	 0.12
efficiently . 	 1.0
complex sound 	 0.125
that involves 	 0.0035460992907801418
, recommendations 	 0.0005614823133071309
these tools 	 0.023809523809523808
Latin alphabet 	 0.25
, Edward 	 0.0005614823133071309
that handles 	 0.0035460992907801418
NLG -RRB- 	 0.047619047619047616
major degradation 	 0.08333333333333333
fair gold-standard 	 1.0
unlike brain 	 1.0
provided significant 	 0.2
process of 	 0.3333333333333333
NLG to 	 0.14285714285714285
algorithms optimized 	 0.02857142857142857
have approached 	 0.009615384615384616
Birkbeck College 	 1.0
evidence of 	 0.5
linguistics Cognitive 	 0.05
Current QA 	 0.2
together , 	 0.125
Jaworski , 	 1.0
management of 	 0.2857142857142857
the LexRank 	 0.0006920415224913495
as Penn 	 0.003484320557491289
ATNs '' 	 0.3333333333333333
many consecutive 	 0.019230769230769232
<s> Computationally 	 0.0007686395080707148
start with 	 0.14285714285714285
of commercial 	 0.00089126559714795
use splicing 	 0.013888888888888888
<s> Most 	 0.0015372790161414297
noun 40 	 0.07142857142857142
sentiment words 	 0.04
approach would 	 0.02857142857142857
calculator program 	 0.5
speech -LRB- 	 0.02631578947368421
methods based 	 0.022727272727272728
other sentences 	 0.014285714285714285
subjectivity\/objectivity identification 	 1.0
semi-supervised '' 	 0.5
intended semantic 	 0.2
each one 	 0.044444444444444446
automatizing the 	 1.0
The results 	 0.005208333333333333
Web pages 	 0.2222222222222222
Text linguistics 	 0.16666666666666666
funding was 	 0.125
aimed at 	 1.0
of connected 	 0.00089126559714795
as : 	 0.003484320557491289
le français 	 1.0
many times 	 0.019230769230769232
generally achieved 	 0.09090909090909091
the first 	 0.010380622837370242
and -RRB- 	 0.001445086705202312
parsers . 	 0.07692307692307693
times , 	 0.2
local ' 	 0.3333333333333333
deciding on 	 0.16666666666666666
web may 	 0.125
view -LRB- 	 0.3333333333333333
know document 	 0.5
robotic arm 	 1.0
to error 	 0.0013280212483399733
1991 -RRB- 	 0.6666666666666666
is focused 	 0.0020325203252032522
Multilingual -LRB- 	 1.0
, -RRB- 	 0.0005614823133071309
human -- 	 0.021739130434782608
1975 -RRB- 	 1.0
, too 	 0.0005614823133071309
parsing can 	 0.07142857142857142
field within 	 0.037037037037037035
classroom lectures 	 1.0
takes into 	 0.3333333333333333
but vocabulary 	 0.014705882352941176
and French 	 0.001445086705202312
vocal tract 	 1.0
low agreement 	 0.3333333333333333
captured by 	 1.0
harder and 	 0.14285714285714285
successes occurred 	 1.0
system depend 	 0.010752688172043012
for developing 	 0.007220216606498195
<s> Xerox 	 0.0007686395080707148
both speech 	 0.03225806451612903
is again 	 0.0020325203252032522
and conversations 	 0.001445086705202312
summarization : 	 0.02
urgent early 	 1.0
answer to 	 0.16666666666666666
syntactic and 	 0.15384615384615385
, combined 	 0.0005614823133071309
as ACL 	 0.003484320557491289
consider sequences 	 0.25
translator must 	 0.14285714285714285
segmentation that 	 0.030303030303030304
linguists would 	 0.3333333333333333
is produced 	 0.0040650406504065045
of being 	 0.00089126559714795
algorithm known 	 0.03571428571428571
due especially 	 0.2
<s> Parsing 	 0.0030745580322828594
hierarchy of 	 1.0
large quantities 	 0.08695652173913043
model that 	 0.13333333333333333
extraction Task 	 0.03225806451612903
from weather 	 0.009615384615384616
especially to 	 0.06666666666666667
are traditionally 	 0.008298755186721992
-LRB- 2005 	 0.0027100271002710027
to evaluate 	 0.005312084993359893
<s> Later 	 0.0007686395080707148
find that 	 0.07692307692307693
segmentation systems 	 0.030303030303030304
Number = 	 1.0
to their 	 0.0026560424966799467
compared phrase-structure 	 0.14285714285714285
This section 	 0.031746031746031744
English , 	 0.16216216216216217
for the 	 0.11191335740072202
instead for 	 0.14285714285714285
usually with 	 0.0625
to analyze 	 0.0013280212483399733
succession of 	 1.0
speeds made 	 0.5
not hear 	 0.008928571428571428
`` can 	 0.005291005291005291
sequences of 	 0.3333333333333333
included in 	 0.125
of these 	 0.00980392156862745
users sometimes 	 0.1111111111111111
linguistic nuances 	 0.0625
in linguistic 	 0.003745318352059925
been taken 	 0.014705882352941176
between lexical 	 0.05128205128205128
NLG system 	 0.09523809523809523
learn to 	 0.07692307692307693
especially because 	 0.06666666666666667
been used 	 0.07352941176470588
When punctuation 	 0.14285714285714285
any learning 	 0.03225806451612903
feedback on 	 0.5
a piece 	 0.00245398773006135
help improve 	 0.1111111111111111
an attempt 	 0.022727272727272728
UMLS -RRB- 	 1.0
and turns 	 0.001445086705202312
Rose , 	 1.0
NLG summaries 	 0.047619047619047616
element of 	 1.0
an utterance 	 0.015151515151515152
the summary 	 0.005536332179930796
can serve 	 0.011049723756906077
<s> Subsequently 	 0.0007686395080707148
modern approaches 	 0.2
and unexpected 	 0.001445086705202312
science that 	 0.1
still '' 	 0.06666666666666667
as those 	 0.017421602787456445
an interactive 	 0.007575757575757576
abruptly at 	 1.0
the ROUGE-1 	 0.0006920415224913495
, reliability 	 0.0005614823133071309
<s> Then 	 0.003843197540353574
, inter-texual 	 0.0005614823133071309
position and 	 0.25
its use 	 0.02857142857142857
, taking 	 0.0005614823133071309
system involves 	 0.010752688172043012
very distant 	 0.024390243902439025
watertight barmaid 	 1.0
for inclusion 	 0.0036101083032490976
been changed 	 0.014705882352941176
of document\/text 	 0.00089126559714795
associating a 	 1.0
are often 	 0.016597510373443983
Navigation Systems 	 1.0
-LRB- b 	 0.0027100271002710027
; Given 	 0.02127659574468085
named IEEE 	 0.14285714285714285
that speech 	 0.0035460992907801418
established within 	 1.0
include . 	 0.037037037037037035
As the 	 0.05555555555555555
preferred computer-generated 	 1.0
analyzing the 	 0.2
important memorandum 	 0.0625
determines the 	 0.6666666666666666
precursor to 	 1.0
fonts used 	 0.3333333333333333
contains embedded 	 0.1
-RRB- \/ 	 0.0027100271002710027
morphemes and 	 0.3333333333333333
a first 	 0.00245398773006135
Typical questions 	 0.5
importance is 	 0.16666666666666666
that should 	 0.0070921985815602835
to enumerate 	 0.0013280212483399733
continuous recognition 	 0.16666666666666666
evaluations . 	 0.16666666666666666
-RRB- MorphoChallenge 	 0.0027100271002710027
diversity during 	 0.25
similarity score 	 0.1
a substantial 	 0.001226993865030675
began offering 	 0.14285714285714285
relevance theory 	 0.3333333333333333
modern statistically-based 	 0.2
making up 	 0.14285714285714285
should represent 	 0.10526315789473684
quite expensive 	 0.125
could imagine 	 0.0625
Gender = 	 1.0
tokens from 	 0.14285714285714285
task requires 	 0.023809523809523808
researchers undertake 	 0.1
more successful 	 0.031578947368421054
A problem 	 0.02
topic , 	 0.25
Components and 	 1.0
and semi-supervised 	 0.001445086705202312
discourse Political 	 0.027777777777777776
action applied 	 0.2
form an 	 0.05
effectively launched 	 0.3333333333333333
involved fully 	 0.3333333333333333
in Germany 	 0.003745318352059925
Management command 	 1.0
Knowledge of 	 0.5
Australian physician 	 0.5
features -LRB- 	 0.038461538461538464
between IR 	 0.02564102564102564
text to 	 0.0440251572327044
who utilize 	 0.1
on discourse 	 0.0047169811320754715
to analyzing 	 0.0013280212483399733
yes-no question 	 1.0
be referenced 	 0.004219409282700422
undercarriage , 	 1.0
, electrical 	 0.0005614823133071309
different languages 	 0.02040816326530612
Maximum entropy-based 	 0.3333333333333333
time warping 	 0.12121212121212122
corpus and 	 0.03225806451612903
which includes 	 0.014492753623188406
published but 	 0.14285714285714285
analysis algorithms 	 0.015384615384615385
abilities . 	 1.0
trivial -RRB- 	 0.25
to use 	 0.013280212483399735
since they 	 0.1
adjust\/correct the 	 1.0
boundaries are 	 0.09090909090909091
Given basic 	 0.07142857142857142
social interaction 	 0.07142857142857142
topic were 	 0.125
QA Before 	 0.047619047619047616
to expand 	 0.0013280212483399733
truck A 	 1.0
interface for 	 0.25
States ? 	 0.14285714285714285
product or 	 0.14285714285714285
the undercarriage 	 0.0006920415224913495
properly . 	 0.5
negative sentiment 	 0.125
Turing test 	 0.5
NLP ranking 	 0.02127659574468085
phoneme classification 	 0.5
rules for 	 0.046511627906976744
currently in 	 0.14285714285714285
to proper 	 0.0013280212483399733
, organization 	 0.0011229646266142617
, from 	 0.0005614823133071309
for creating 	 0.0036101083032490976
T to 	 0.16666666666666666
approach in 	 0.02857142857142857
British General 	 0.3333333333333333
-RRB- Current 	 0.0027100271002710027
1978 -RRB- 	 0.6666666666666666
significant momentum 	 0.1111111111111111
free of 	 0.25
<s> Issues 	 0.0015372790161414297
computer automated 	 0.022727272727272728
-RRB- Around 	 0.0027100271002710027
split it 	 0.25
by resorting 	 0.005714285714285714
ratings , 	 0.1111111111111111
the Wordnet 	 0.0006920415224913495
pieces as 	 1.0
in color 	 0.0018726591760299626
high level 	 0.16666666666666666
possible transcriptions 	 0.08333333333333333
terminology , 	 1.0
strength at 	 0.2
analyzed for 	 0.2
objective document 	 0.2
beer , 	 1.0
, aspect 	 0.0005614823133071309
sentences instead 	 0.013157894736842105
science convention 	 0.1
ratings usually 	 0.1111111111111111
R. Harris 	 0.16666666666666666
or function 	 0.0045045045045045045
operation of 	 0.5
polarity of 	 0.25
triples or 	 0.3333333333333333
waves are 	 0.14285714285714285
methods require 	 0.022727272727272728
it might 	 0.008547008547008548
effectiveness . 	 0.3333333333333333
and must 	 0.001445086705202312
to enhance 	 0.0013280212483399733
20,000 words 	 1.0
over 95 	 0.08333333333333333
data-to-text systems 	 1.0
wife of 	 1.0
questions under 	 0.038461538461538464
what is 	 0.09375
development , 	 0.16666666666666666
researchers wrote 	 0.1
name and 	 0.2
the broken 	 0.0006920415224913495
chain random 	 1.0
leads , 	 1.0
Creating the 	 0.5
check for 	 0.5
on functional 	 0.0047169811320754715
the Blind 	 0.001384083044982699
SAM -LRB- 	 1.0
state automata 	 0.07142857142857142
reader designed 	 0.2
quality can 	 0.1
be labeled 	 0.004219409282700422
more basic 	 0.021052631578947368
short summary 	 0.125
adding citations 	 0.5
accent , 	 1.0
how big 	 0.034482758620689655
two given 	 0.034482758620689655
The readers 	 0.005208333333333333
the personal 	 0.0006920415224913495
fusion techniques 	 1.0
data which 	 0.012987012987012988
The parser 	 0.005208333333333333
also considered 	 0.014492753623188406
formalisms\/languages . 	 1.0
a restricted 	 0.001226993865030675
representation into 	 0.05263157894736842
we try 	 0.022222222222222223
confusions with 	 1.0
particularly effective 	 0.2
to form 	 0.005312084993359893
and computationally 	 0.001445086705202312
be thought 	 0.004219409282700422
of disassembling 	 0.00089126559714795
extractor follows 	 0.5
similarity metrics 	 0.1
speech a 	 0.006578947368421052
, standard 	 0.0011229646266142617
tones that 	 1.0
ranging from 	 1.0
unigram `` 	 0.2
e.g. elaboration 	 0.017857142857142856
Vocalizations vary 	 1.0
technology also 	 0.045454545454545456
easily , 	 0.1111111111111111
nice ' 	 0.25
category registry 	 0.5
of high 	 0.00089126559714795
or Latin 	 0.0045045045045045045
evaluation step 	 0.037037037037037035
January 2010 	 0.5
statistics of 	 0.125
objects . 	 0.2
model '' 	 0.03333333333333333
political negligence 	 0.3333333333333333
a discussion 	 0.001226993865030675
affects the 	 1.0
's first 	 0.0196078431372549
Hansard corpus 	 1.0
they relate 	 0.025
descriptive and 	 0.3333333333333333
are BASEBALL 	 0.004149377593360996
the continuously 	 0.0006920415224913495
than 98 	 0.022222222222222223
their products 	 0.029411764705882353
consistently available 	 0.3333333333333333
machine-translation approaches 	 0.5
Hopper , 	 1.0
On-line character 	 0.6666666666666666
to create 	 0.01195219123505976
communicative event 	 0.3333333333333333
is a 	 0.10975609756097561
2 -RRB- 	 0.2
applications include 	 0.08
off as 	 0.5
approach of 	 0.02857142857142857
the NLP 	 0.0006920415224913495
classify properly 	 0.5
forecasts to 	 0.2
were ambiguous 	 0.024390243902439025
The software 	 0.005208333333333333
considered separately 	 0.1111111111111111
natural '' 	 0.02666666666666667
or the 	 0.04054054054054054
will never 	 0.02857142857142857
step and 	 0.06666666666666667
to judge 	 0.0026560424966799467
an F-score 	 0.007575757575757576
it difficult 	 0.017094017094017096
to tag 	 0.0013280212483399733
summarization program 	 0.02
traditionally made 	 0.5
some NLP 	 0.012048192771084338
<s> Features 	 0.0007686395080707148
, Strzalkowski 	 0.0005614823133071309
as categories 	 0.003484320557491289
Bolivar , 	 1.0
each template 	 0.022222222222222223
this task 	 0.04395604395604396
scanning solution 	 0.5
the phenomenon 	 0.001384083044982699
a well-defined 	 0.001226993865030675
subtypes of 	 1.0
used a 	 0.02654867256637168
about 30 	 0.025
consonants and 	 0.3333333333333333
translation -RRB- 	 0.02702702702702703
since ROUGE-1 	 0.1
correlate with 	 0.6666666666666666
are difficult 	 0.004149377593360996
as needed 	 0.003484320557491289
them for 	 0.05263157894736842
SHRDLU provided 	 0.16666666666666666
but such 	 0.014705882352941176
e.g. , 	 0.4642857142857143
LexRank score 	 0.08333333333333333
infer where 	 1.0
describe the 	 0.3333333333333333
human-generated summaries 	 0.5
recognize speech 	 0.1111111111111111
capture speech 	 0.5
human capabilities 	 0.021739130434782608
selling a 	 1.0
false positives 	 0.5
by computer 	 0.017142857142857144
a threshold 	 0.0036809815950920245
High-performance fighter 	 1.0
compare various 	 0.14285714285714285
by creating 	 0.005714285714285714
is little 	 0.0020325203252032522
SR while 	 0.3333333333333333
software was 	 0.037037037037037035
using logical 	 0.01694915254237288
what features 	 0.03125
To find 	 0.1111111111111111
1949 RCA 	 0.5
the inherent 	 0.0006920415224913495
termed coarticulation 	 0.25
intelligence , 	 0.125
examples of 	 0.20833333333333334
parser proposes 	 0.0625
<s> Types 	 0.0015372790161414297
not become 	 0.008928571428571428
Peru , 	 0.5
ask him 	 0.25
PageRank . 	 0.16666666666666666
healthcare is 	 1.0
the addressee 	 0.0006920415224913495
submit their 	 0.5
a wide 	 0.00245398773006135
but instead 	 0.014705882352941176
<s> Each 	 0.003843197540353574
across a 	 0.2
syntax effectively 	 0.09090909090909091
1984 . 	 1.0
networks emerged 	 0.07142857142857142
H. Shepard 	 0.5
create texts 	 0.058823529411764705
, its 	 0.0005614823133071309
Liu 's 	 1.0
Wilensky , 	 1.0
had been 	 0.07142857142857142
the talk 	 0.0006920415224913495
one on 	 0.015384615384615385
Style Studies 	 1.0
grammars -RRB- 	 0.14285714285714285
<s> Design 	 0.0023059185242121443
Schober , 	 1.0
surprisingly disruptive 	 0.3333333333333333
textual weather 	 0.2
metamodel and 	 1.0
around 6 	 0.375
usually rated 	 0.03125
assumptions about 	 0.2
standard expression 	 0.07142857142857142
for real-world 	 0.007220216606498195
grid & 	 1.0
documents and 	 0.02631578947368421
5 consecutive 	 0.5
the labor 	 0.0006920415224913495
pragmatics to 	 0.3333333333333333
to many 	 0.005312084993359893
<s> Apart 	 0.0007686395080707148
, clean 	 0.0005614823133071309
it was 	 0.02564102564102564
simply apply 	 0.08333333333333333
on Mandarin 	 0.0047169811320754715
the quantitative 	 0.0006920415224913495
a clarification 	 0.001226993865030675
doctors , 	 0.3333333333333333
most basic 	 0.017241379310344827
and answer 	 0.001445086705202312
N words 	 0.3333333333333333
Act of 	 1.0
, improving 	 0.0005614823133071309
an OCR 	 0.007575757575757576
contexts and 	 0.14285714285714285
also programs 	 0.014492753623188406
searched , 	 1.0
to upload 	 0.0013280212483399733
alphabet , 	 0.6666666666666666
speech-recognition engine 	 0.6666666666666666
dependencies . 	 1.0
, have 	 0.0011229646266142617
both time 	 0.03225806451612903
Ann Arbor 	 1.0
wide variance 	 0.25
EMR -LRB- 	 0.3333333333333333
evaluation would 	 0.05555555555555555
Paroubek P. 	 1.0
Fournier d'Albe 	 1.0
summary based 	 0.023809523809523808
<s> After 	 0.0023059185242121443
only some 	 0.05263157894736842
pollen level 	 0.15384615384615385
speech signal 	 0.006578947368421052
is far 	 0.0040650406504065045
Generation Challenges 	 0.5
give a 	 0.25
brain is 	 0.3333333333333333
and achieves 	 0.001445086705202312
lot more 	 0.3333333333333333
was quite 	 0.012987012987012988
social science 	 0.07142857142857142
OCR Accuracy 	 0.02040816326530612
breadth '' 	 0.5
participate in 	 1.0
relations -LRB- 	 0.08333333333333333
equivalent information 	 0.2
or Auto 	 0.0045045045045045045
web pages 	 0.125
software often 	 0.037037037037037035
500,000 . 	 1.0
'' tagging 	 0.005154639175257732
enough to 	 0.2
OS . 	 0.5
understand that 	 0.14285714285714285
are SHRDLU 	 0.004149377593360996
answers rather 	 0.08333333333333333
overlap , 	 0.25
analysis system 	 0.015384615384615385
hoping to 	 1.0
become more 	 0.25
listed on 	 1.0
was CANDIDE 	 0.012987012987012988
are some 	 0.004149377593360996
expanding all 	 1.0
query relevant 	 0.6666666666666666
without inter-word 	 0.07692307692307693
writing SHRDLU 	 0.1111111111111111
upper level 	 1.0
permuted automatically 	 1.0
Aermacchi M-346 	 1.0
The performance 	 0.005208333333333333
increasingly focused 	 0.3333333333333333
language data 	 0.006756756756756757
as references 	 0.003484320557491289
or some 	 0.013513513513513514
are summaries 	 0.004149377593360996
are outside 	 0.004149377593360996
mathematical framework 	 0.5
that match 	 0.0035460992907801418
ATNs used 	 0.3333333333333333
for statistical 	 0.007220216606498195
that characterize 	 0.0035460992907801418
The vectors 	 0.005208333333333333
originally as 	 0.5
to enable 	 0.0013280212483399733
technology that 	 0.045454545454545456
Speech is 	 0.06451612903225806
sentences . 	 0.10526315789473684
e.g. SCU 	 0.017857142857142856
used the 	 0.008849557522123894
unigrams to 	 0.08333333333333333
languages , 	 0.22
they all 	 0.05
data available 	 0.03896103896103896
any speaker 	 0.03225806451612903
one font 	 0.015384615384615385
desired answers 	 0.2
complex question 	 0.041666666666666664
ending at 	 1.0
topic by 	 0.125
scoring function 	 0.5
make up 	 0.1
helped improve 	 0.3333333333333333
bi-directional inference 	 1.0
inter-word spaces 	 1.0
representation of 	 0.10526315789473684
the reason 	 0.0006920415224913495
where only 	 0.02857142857142857
selects the 	 0.5
assumptions on 	 0.2
1990s . 	 0.3333333333333333
task -LRB- 	 0.023809523809523808
using punctuation 	 0.01694915254237288
in two 	 0.0018726591760299626
linguistic informational 	 0.0625
bilingual corpus 	 0.5
reviews , 	 0.5
devices take 	 0.25
document may 	 0.05555555555555555
be `` 	 0.004219409282700422
if a 	 0.03571428571428571
lessons learned 	 1.0
not using 	 0.008928571428571428
refined score 	 1.0
began selling 	 0.14285714285714285
interaction . 	 0.25
is rarely 	 0.0020325203252032522
ambiguities in 	 0.25
European group 	 0.3333333333333333
less complex 	 0.08333333333333333
or paragraph 	 0.0045045045045045045
, MT 	 0.0011229646266142617
in NLP 	 0.0149812734082397
because they 	 0.13333333333333333
each sentence 	 0.022222222222222223
boards and 	 1.0
Also , 	 1.0
synopsis like 	 1.0
systems explore 	 0.008928571428571428
the sizes 	 0.0006920415224913495
day did 	 1.0
and decelerations 	 0.001445086705202312
achieving fully 	 0.5
component of 	 0.6
degree . 	 0.16666666666666666
and example 	 0.001445086705202312
<s> These 	 0.012298232129131437
to machine-learning 	 0.0013280212483399733
HMMs underlie 	 0.125
2006 hurricane 	 0.3333333333333333
been produced 	 0.014705882352941176
<s> DTW 	 0.0007686395080707148
Some systems 	 0.09523809523809523
to end 	 0.0026560424966799467
had the 	 0.07142857142857142
higher degree 	 0.14285714285714285
early successes 	 0.1
Alternatively , 	 1.0
system at 	 0.010752688172043012
2001 and 	 0.5
detail but 	 0.5
phonemes with 	 0.16666666666666666
determining whether 	 0.16666666666666666
about pronouns 	 0.025
for these 	 0.0036101083032490976
main difficulty 	 0.25
most important 	 0.034482758620689655
to '' 	 0.0026560424966799467
Research Projects 	 0.125
Analysis Although 	 0.2
however , 	 0.9230769230769231
have taken 	 0.009615384615384616
<s> Users 	 0.0007686395080707148
of tourism 	 0.00089126559714795
as first 	 0.003484320557491289
finding the 	 0.4
than when 	 0.022222222222222223
psycholinguistics when 	 0.5
-LRB- probabilistic 	 0.0027100271002710027
Verbyx VRX 	 1.0
methods parse 	 0.022727272727272728
for air 	 0.0036101083032490976
Man dog 	 0.5
Europe was 	 0.2
has focused 	 0.047619047619047616
use . 	 0.041666666666666664
both appear 	 0.03225806451612903
word being 	 0.03333333333333333
services , 	 0.3333333333333333
and singular 	 0.001445086705202312
as is 	 0.013937282229965157
The extrinsic 	 0.005208333333333333
learning that 	 0.023255813953488372
heteroscedastic linear 	 1.0
ROUGE -LRB- 	 0.2
book does 	 0.125
-RRB- evaluation 	 0.0027100271002710027
The umbrella 	 0.005208333333333333
other structure 	 0.014285714285714285
-RRB- securely 	 0.0027100271002710027
lexical units 	 0.07692307692307693
summary , 	 0.14285714285714285
reviews and 	 0.16666666666666666
step -- 	 0.13333333333333333
was conducted 	 0.025974025974025976
, individual 	 0.0005614823133071309
frequently formalized 	 0.5
, automatic 	 0.0016844469399213925
grammar Rhetoric 	 0.02702702702702703
centrality . 	 0.5
The grammar 	 0.010416666666666666
method is 	 0.0625
the EMR 	 0.0006920415224913495
Digital Syphon 	 1.0
then be 	 0.02857142857142857
an application 	 0.015151515151515152
there general 	 0.025
generated readable 	 0.06666666666666667
aim is 	 0.5
: Information 	 0.00980392156862745
Once examples 	 0.2
Unfortunately , 	 1.0
e.g. Known 	 0.017857142857142856
Known word 	 1.0
and data 	 0.005780346820809248
quantity of 	 0.3333333333333333
two ways 	 0.034482758620689655
that cause 	 0.0035460992907801418
war camp 	 1.0
a technique 	 0.001226993865030675
Treebank . 	 0.3333333333333333
why HMMs 	 0.14285714285714285
to text 	 0.0026560424966799467
<s> Methods 	 0.0023059185242121443
Driver-license OCR 	 1.0
decided without 	 0.3333333333333333
including recognition 	 0.07142857142857142
processing text 	 0.018518518518518517
financial section 	 0.25
paper-to-computer text 	 1.0
time-scales -LRB- 	 1.0
context or 	 0.030303030303030304
explored using 	 0.5
, typical 	 0.0005614823133071309
warnings from 	 1.0
be either 	 0.004219409282700422
-RRB- output 	 0.0027100271002710027
level , 	 0.2
Nations materials 	 0.5
approach which 	 0.05714285714285714
that appears 	 0.0035460992907801418
Adam Jaworski 	 1.0
A popular 	 0.02
knowledge sources 	 0.037037037037037035
best application 	 0.05555555555555555
NLG output 	 0.047619047619047616
relations between 	 0.4166666666666667
-RRB- involved 	 0.0027100271002710027
in spirit 	 0.0018726591760299626
systems will 	 0.008928571428571428
even level 	 0.037037037037037035
context dependency 	 0.030303030303030304
, neural 	 0.0022459292532285235
the standard 	 0.001384083044982699
while on-line 	 0.05
, Brazil 	 0.0005614823133071309
Competing semantic 	 1.0
is , 	 0.018292682926829267
at University 	 0.014705882352941176
to right 	 0.0013280212483399733
during verbalization 	 0.1
spectrum using 	 1.0
Nuance Voice 	 0.3333333333333333
<s> Bottom-up 	 0.0007686395080707148
approach , 	 0.05714285714285714
adjective -LRB- 	 0.14285714285714285
offered the 	 1.0
expected . 	 0.14285714285714285
are names 	 0.004149377593360996
requirements of 	 0.5
usually faster 	 0.03125
forecast -LRB- 	 1.0
technique is 	 0.14285714285714285
the mean 	 0.0006920415224913495
<s> Algorithms 	 0.0015372790161414297
relations to 	 0.08333333333333333
recogniton vary 	 0.5
phrase `` 	 0.1
prune away 	 1.0
-LRB- HMMs 	 0.005420054200542005
of N 	 0.00089126559714795
, find 	 0.0011229646266142617
handles both 	 1.0
time or 	 0.030303030303030304
blind people 	 0.75
of about 	 0.00089126559714795
or insufficient 	 0.0045045045045045045
; that 	 0.02127659574468085
a gradually 	 0.001226993865030675
disfluences -LRB- 	 1.0
reasons that 	 0.5
given text 	 0.041666666666666664
an idea 	 0.007575757575757576
Viterbi algorithm 	 1.0
those East 	 0.045454545454545456
we might 	 0.022222222222222223
be '' 	 0.008438818565400843
was attempted 	 0.012987012987012988
the needs 	 0.0006920415224913495
Canada ? 	 0.16666666666666666
dozens of 	 1.0
the keyphrases 	 0.001384083044982699
-LRB- see 	 0.032520325203252036
was dramatically 	 0.012987012987012988
structure rules 	 0.08333333333333333
, time 	 0.0005614823133071309
Such models 	 0.25
used about 	 0.008849557522123894
pattern recognition 	 0.6666666666666666
corresponding to 	 0.3333333333333333
algorithms for 	 0.11428571428571428
more robust 	 0.021052631578947368
hard if-then 	 0.3333333333333333
of John 	 0.00089126559714795
, Elinor 	 0.0005614823133071309
languages -LRB- 	 0.04
target-language-independent representation 	 1.0
or identical 	 0.0045045045045045045
because analyzing 	 0.03333333333333333
in turn 	 0.009363295880149813
-LRB- MMI 	 0.0027100271002710027
in computers 	 0.0018726591760299626
On the 	 0.3333333333333333
, Svenka 	 0.0005614823133071309
entries , 	 0.5
Red is 	 1.0
the random 	 0.0006920415224913495
Company and 	 0.5
purposes . 	 0.5
the systems 	 0.0020761245674740486
for use 	 0.007220216606498195
more descriptive 	 0.010526315789473684
include automatic 	 0.037037037037037035
is not 	 0.03861788617886179
highest level 	 0.3333333333333333
to what 	 0.005312084993359893
Conference Technolangue\/Easy 	 0.5
represented by 	 0.3333333333333333
obstacles to 	 1.0
pages concluded 	 0.14285714285714285
boundaries between 	 0.18181818181818182
parsing and 	 0.03571428571428571
sentences when 	 0.013157894736842105
candidates can 	 0.2
A technique 	 0.02
decade to 	 0.3333333333333333
to 90 	 0.0013280212483399733
is easier 	 0.0040650406504065045
's NLP 	 0.0196078431372549
the token 	 0.0006920415224913495
consideration neural 	 0.3333333333333333
is 2,000 	 0.0020325203252032522
mathematical rules 	 0.5
<s> Shared 	 0.0007686395080707148
Kurzweil Applied 	 0.14285714285714285
very small 	 0.04878048780487805
we rank 	 0.022222222222222223
the basic 	 0.001384083044982699
In 1994 	 0.009523809523809525
online reviews 	 0.25
<verb> ← 	 1.0
with regards 	 0.00546448087431694
have a 	 0.125
to clarify 	 0.0013280212483399733
well to 	 0.03571428571428571
that NLG 	 0.0070921985815602835
The various 	 0.005208333333333333
of ways 	 0.0017825311942959
the implications 	 0.0006920415224913495
late 1980s 	 0.4444444444444444
More up 	 0.1111111111111111
Michael Halliday 	 0.25
broad , 	 0.25
transcriptions -LRB- 	 0.5
and intra-texual 	 0.001445086705202312
is edited 	 0.0020325203252032522
linguistic cues 	 0.0625
the sentences 	 0.005536332179930796
-RRB- vibration 	 0.0027100271002710027
a bilingual 	 0.001226993865030675
sequences . 	 0.2222222222222222
search Sentence 	 0.09090909090909091
proper noun 	 0.14285714285714285
can make 	 0.016574585635359115
, widely 	 0.0005614823133071309
a speech-recognition 	 0.0036809815950920245
shown that 	 0.2
to discriminate 	 0.0026560424966799467
meaningful information 	 0.125
be sequences 	 0.004219409282700422
the stage 	 0.0006920415224913495
, anthropology 	 0.0005614823133071309
when there 	 0.02857142857142857
coughing , 	 1.0
shift-reduce algorithm 	 1.0
The loss 	 0.005208333333333333
but is 	 0.029411764705882353
whether each 	 0.07692307692307693
previously unseen 	 0.5
Treebank project 	 0.16666666666666666
slot represents 	 1.0
and ELIZA 	 0.002890173410404624
quite different 	 0.375
movie together 	 0.3333333333333333
both the 	 0.06451612903225806
indifferent to 	 1.0
schematic organization 	 1.0
therefore to 	 0.2
to mental 	 0.0013280212483399733
medical informatics 	 0.16666666666666666
deliberately inserts 	 1.0
towards this 	 1.0
in testing 	 0.0018726591760299626
prize . 	 1.0
documents . 	 0.13157894736842105
domotic appliance 	 1.0
By 1985 	 0.3333333333333333
example above 	 0.012345679012345678
culminating in 	 1.0
results . 	 0.09523809523809523
human vocabularies 	 0.021739130434782608
Rubin , 	 1.0
additional clues 	 0.16666666666666666
a unified 	 0.001226993865030675
the technique 	 0.0006920415224913495
new set 	 0.041666666666666664
supervised keyphrase 	 0.125
and commercial 	 0.001445086705202312
sales receipts 	 0.3333333333333333
positive -RRB- 	 0.14285714285714285
in titles 	 0.0018726591760299626
: David 	 0.00980392156862745
a trivial 	 0.001226993865030675
they disagree 	 0.025
voice file 	 0.07692307692307693
a vocabulary 	 0.001226993865030675
fighter aircraft 	 0.5
French , 	 0.125
conducted until 	 0.2
those of 	 0.045454545454545456
's Stilstudien 	 0.0196078431372549
Context-free grammars 	 1.0
developed for 	 0.038461538461538464
weaker . 	 1.0
`` ASR 	 0.005291005291005291
and speed 	 0.002890173410404624
sound creates 	 0.05
Therefore , 	 1.0
about an 	 0.025
lacks pre-existing 	 1.0
coarticulation , 	 1.0
drawback of 	 1.0
sentence and 	 0.020833333333333332
expressed in 	 0.16666666666666666
Monroe and 	 1.0
robot in 	 0.5
for tense 	 0.0036101083032490976
the removal 	 0.0006920415224913495
be found 	 0.012658227848101266
generate . 	 0.05555555555555555
lectures , 	 1.0
direct and 	 0.16666666666666666
turn requires 	 0.16666666666666666
of dividing 	 0.00267379679144385
information request 	 0.021739130434782608
named entities 	 0.42857142857142855
far more 	 0.25
The essential 	 0.005208333333333333
the choice 	 0.0006920415224913495
is exactly 	 0.0020325203252032522
using votes 	 0.01694915254237288
filtered out 	 0.3333333333333333
developing a 	 0.25
to break 	 0.0013280212483399733
Case = 	 1.0
be faster 	 0.004219409282700422
boolean syntactic 	 1.0
then spoke 	 0.02857142857142857
information are 	 0.021739130434782608
processes of 	 0.2
overall task 	 0.16666666666666666
as language 	 0.003484320557491289
keyphrases will 	 0.02857142857142857
that they 	 0.024822695035460994
on paper 	 0.0047169811320754715
HLT , 	 1.0
<s> svg 	 0.0007686395080707148
of unlabeled 	 0.00089126559714795
about which 	 0.025
might appear 	 0.038461538461538464
process , 	 0.027777777777777776
strong and 	 0.25
task often 	 0.023809523809523808
What are 	 0.36363636363636365
tasks that 	 0.03125
messages into 	 0.5
develop in 	 0.2
respect to 	 1.0
to move 	 0.0013280212483399733
involve working 	 0.16666666666666666
user . 	 0.21428571428571427
what Biden 	 0.03125
see is 	 0.05
T final 	 0.16666666666666666
, adverbs 	 0.0005614823133071309
of linguistics 	 0.00089126559714795
overfitting and 	 0.5
burden on 	 1.0
created . 	 0.14285714285714285
been applied 	 0.08823529411764706
concerning coherence 	 1.0
Recent applications 	 0.3333333333333333
contain enough 	 0.08333333333333333
ME is 	 0.5
<s> Examples 	 0.0023059185242121443
identities of 	 1.0
general purpose 	 0.09090909090909091
broadcast news 	 1.0
, so 	 0.00673778775968557
incorrect letters 	 0.3333333333333333
, does 	 0.0011229646266142617
Health Record 	 0.5
periods can 	 0.3333333333333333
documents at 	 0.02631578947368421
relate to 	 1.0
occurs in 	 1.0
million words 	 0.3333333333333333
for identifying 	 0.0036101083032490976
it should 	 0.008547008547008548
a date 	 0.001226993865030675
insufficient . 	 1.0
electrical characteristics 	 1.0
popular evaluation 	 0.1111111111111111
field of 	 0.4444444444444444
Cohesion and 	 1.0
its vocabulary 	 0.02857142857142857
usually evaluated 	 0.03125
Sentiment analysis 	 0.8333333333333334
system answers 	 0.010752688172043012
morphological analysis 	 0.3333333333333333
the University 	 0.0006920415224913495
an English-like 	 0.007575757575757576
control when 	 0.2
only 10 	 0.02631578947368421
human geography 	 0.021739130434782608
and phrases 	 0.002890173410404624
co-occurrence in 	 0.3333333333333333
basic level 	 0.07692307692307693
style and 	 0.5
Abney S. 	 1.0
Aided summarization 	 0.3333333333333333
will have 	 0.05714285714285714
BLEU measure 	 0.3333333333333333
such formal 	 0.008130081300813009
Foucault became 	 0.3333333333333333
evaluation is 	 0.07407407407407407
fighter trainer 	 0.16666666666666666
and slang 	 0.001445086705202312
100000 may 	 1.0
One step 	 0.07692307692307693
pattern has 	 0.16666666666666666
of war 	 0.00089126559714795
processing : 	 0.018518518518518517
SWER -RRB- 	 1.0
to mimic 	 0.0013280212483399733
absorbing random 	 0.3333333333333333
<s> Document 	 0.0015372790161414297
of criteria 	 0.00089126559714795
-LRB- which 	 0.008130081300813009
Computer Problem 	 0.16666666666666666
extracting sentences 	 0.2
a -5 	 0.001226993865030675
and in 	 0.010115606936416185
Convert chunks 	 0.5
models ... 	 0.038461538461538464
like Japanese 	 0.03571428571428571
gives examples 	 0.5
and report 	 0.001445086705202312
feasibility demonstration 	 0.5
reported for 	 0.2
achieved accuracy 	 0.2
`` recommend 	 0.010582010582010581
same type 	 0.04
most notoriously 	 0.017241379310344827
efficient parsers 	 0.3333333333333333
the recognition 	 0.001384083044982699
syntactic relations 	 0.07692307692307693
data to 	 0.012987012987012988
groups : 	 0.2
This approach 	 0.031746031746031744
translation studies 	 0.013513513513513514
might ask 	 0.038461538461538464
denote abbreviations 	 0.5
Isles and 	 1.0
earlier Brown 	 0.25
model will 	 0.03333333333333333
develop innovative 	 0.2
Today there 	 1.0
acquiring coarse-grained 	 1.0
Engineers '' 	 0.5
identity of 	 1.0
on content 	 0.0047169811320754715
Communication . 	 1.0
more difficult 	 0.07368421052631578
with implicit 	 0.00546448087431694
understanding machine 	 0.030303030303030304
a science 	 0.00245398773006135
discussion groups 	 0.5
question classifier 	 0.023809523809523808
than instances 	 0.022222222222222223
through the 	 0.5
keyphrases for 	 0.05714285714285714
scale . 	 0.16666666666666666
end up 	 0.25
many neighbors 	 0.019230769230769232
pointed out 	 1.0
QA is 	 0.047619047619047616
serve as 	 0.8
multiple documents 	 0.15384615384615385
Junqua and 	 1.0
Interlingual machine 	 0.6666666666666666
, TaleSpin 	 0.0005614823133071309
works It 	 0.5
type validity 	 0.07142857142857142
introducing models 	 1.0
best algorithms 	 0.05555555555555555
<s> Evaluating 	 0.0015372790161414297
detailed discussions 	 0.5
than other 	 0.022222222222222223
to turn 	 0.0013280212483399733
covers the 	 0.5
with other 	 0.00546448087431694
same time 	 0.12
would expect 	 0.018867924528301886
question marks 	 0.023809523809523808
with having 	 0.00546448087431694
and substitution 	 0.001445086705202312
essentially to 	 0.125
and their 	 0.008670520231213872
expression and 	 0.2
of disparate 	 0.00089126559714795
processing natural 	 0.018518518518518517
approximation thereof 	 0.16666666666666666
IMR -RRB- 	 0.5
approach using 	 0.02857142857142857
example -LRB- 	 0.012345679012345678
, Ernesto 	 0.0005614823133071309
Dale -LRB- 	 1.0
comprehension of 	 0.2857142857142857
It sometimes 	 0.02631578947368421
fail for 	 0.3333333333333333
-RRB- have 	 0.005420054200542005
typical real-world 	 0.1111111111111111
answer type 	 0.06666666666666667
found in 	 0.21428571428571427
on what 	 0.009433962264150943
<s> Open-domain 	 0.0007686395080707148
's opinions 	 0.0196078431372549
Gisting Evaluation 	 1.0
famous article 	 0.3333333333333333
often of 	 0.022727272727272728
also that 	 0.014492753623188406
is constructed 	 0.0040650406504065045
analysis of 	 0.18461538461538463
disambiguate sentence 	 0.3333333333333333
<s> According 	 0.0007686395080707148
Cleave and 	 1.0
decided a 	 0.3333333333333333
the differences 	 0.0006920415224913495
<s> QA 	 0.0007686395080707148
; these 	 0.0425531914893617
also marked 	 0.014492753623188406
, style 	 0.0005614823133071309
usually has 	 0.03125
, many 	 0.0039303761931499155
with many 	 0.00546448087431694
for acquiring 	 0.0036101083032490976
It was 	 0.05263157894736842
ink -LRB- 	 1.0
to 150,000 	 0.0013280212483399733
finite state 	 0.8
, pre-defined 	 0.0005614823133071309
to automate 	 0.0026560424966799467
current major 	 0.14285714285714285
are delimited 	 0.012448132780082987
phrases may 	 0.0625
rates of 	 0.375
assertions , 	 0.5
defines the 	 0.5
associated number 	 0.25
that had 	 0.0035460992907801418
developed and 	 0.038461538461538464
these represent 	 0.023809523809523808
year despite 	 0.16666666666666666
judgement often 	 0.3333333333333333
individual users 	 0.08333333333333333
scientists , 	 1.0
marker vs. 	 1.0
is measured 	 0.006097560975609756
-LRB- ōrātiōnis 	 0.0027100271002710027
its grammatical 	 0.02857142857142857
discriminate keyphrases 	 0.3333333333333333
Linguistics defines 	 0.3333333333333333
achieved , 	 0.2
As with 	 0.05555555555555555
and practical 	 0.001445086705202312
algorithms used 	 0.02857142857142857
more recent 	 0.010526315789473684
model information 	 0.03333333333333333
primarily by 	 0.5
The United 	 0.005208333333333333
of interest 	 0.00267379679144385
For nouns 	 0.01639344262295082
further speaker 	 0.125
the big 	 0.0006920415224913495
be asked 	 0.004219409282700422
POS categories 	 0.07692307692307693
to guide 	 0.0013280212483399733
demonstrate . 	 1.0
represent a 	 0.2222222222222222
the OCR-A 	 0.0006920415224913495
`` speech 	 0.005291005291005291
, on-line 	 0.0005614823133071309
Cognitive Systems 	 0.3333333333333333
summarization works 	 0.02
these actions 	 0.023809523809523808
recommendation '' 	 1.0
and indirect 	 0.001445086705202312
take the 	 0.2
linked in 	 0.3333333333333333
and Development 	 0.001445086705202312
showed how 	 0.25
languages text 	 0.02
the measurement 	 0.0006920415224913495
atmosphere -LRB- 	 1.0
some context 	 0.012048192771084338
`` Sentiment 	 0.005291005291005291
a relative 	 0.001226993865030675
in system 	 0.0018726591760299626
any arbitrary 	 0.06451612903225806
Law and 	 1.0
company to 	 0.3333333333333333
the type 	 0.002768166089965398
text structure 	 0.006289308176100629
aim of 	 0.5
-RRB- to 	 0.01084010840108401
VTLN -RRB- 	 1.0
system had 	 0.010752688172043012
but IE 	 0.014705882352941176
use it 	 0.027777777777777776
to pauses 	 0.0013280212483399733
spoken sentences 	 0.07142857142857142
mail , 	 0.5
be poorly 	 0.004219409282700422
interlingual machine 	 0.75
apply the 	 0.2
Gina Poncini 	 1.0
degree -LRB- 	 0.16666666666666666
event . 	 0.3333333333333333
minimize the 	 1.0
in computational 	 0.003745318352059925
One such 	 0.07692307692307693
any font 	 0.03225806451612903
entry from 	 0.25
at by 	 0.014705882352941176
, conversation 	 0.0005614823133071309
tasks . 	 0.125
Few assumptions 	 1.0
MLLR -RRB- 	 1.0
and linguistics 	 0.002890173410404624
-LRB- in 	 0.005420054200542005
article on 	 0.034482758620689655
to news-gathering 	 0.0013280212483399733
IMR during 	 0.5
article by 	 0.034482758620689655
document classification 	 0.05555555555555555
represented themselves 	 0.16666666666666666
-LRB- role 	 0.0027100271002710027
He decided 	 0.125
, then 	 0.006176305446378439
-RRB- Hands-free 	 0.0027100271002710027
weighted to 	 0.3333333333333333
QA research 	 0.047619047619047616
can indeed 	 0.0055248618784530384
breadth and 	 0.5
<s> Consider 	 0.0015372790161414297
1 -RRB- 	 0.25
reconfiguring them 	 1.0
Yale which 	 0.5
France installing 	 0.25
and transmitting 	 0.001445086705202312
\/ -LRB- 	 0.3333333333333333
noise -LRB- 	 0.125
, document 	 0.0005614823133071309
analysis has 	 0.015384615384615385
vowels and 	 0.3333333333333333
to fulfill 	 0.0026560424966799467
much easier 	 0.045454545454545456
also general 	 0.014492753623188406
Speech Technology 	 0.03225806451612903
million books 	 0.3333333333333333
times they 	 0.2
converting the 	 0.5
2004 . 	 0.3333333333333333
, artificial 	 0.0011229646266142617
centroid '' 	 0.5
was shown 	 0.025974025974025976
features of 	 0.15384615384615385
learn about 	 0.07692307692307693
what class 	 0.03125
ranks the 	 0.5
first sentence-end 	 0.030303030303030304
, recorded 	 0.0005614823133071309
That is 	 1.0
newspaper . 	 0.3333333333333333
using NLG 	 0.05084745762711865
discourse -RRB- 	 0.027777777777777776
any capitalization 	 0.03225806451612903
enhance accessibility 	 1.0
could read 	 0.0625
a feature 	 0.00245398773006135
Optical character 	 0.6666666666666666
domain posed 	 0.05
-LRB- December 	 0.0027100271002710027
statistical distribution 	 0.030303030303030304
an internal 	 0.022727272727272728
`` mentions 	 0.005291005291005291
Charniak points 	 1.0
incremental improvements 	 1.0
rules to 	 0.06976744186046512
training organizations 	 0.03571428571428571
corpus , 	 0.06451612903225806
`` Conversational 	 0.005291005291005291
obtained . 	 0.14285714285714285
model taggers 	 0.03333333333333333
be very 	 0.012658227848101266
, Guy 	 0.0005614823133071309
within their 	 0.05555555555555555
as social 	 0.003484320557491289
classification task 	 0.058823529411764705
of more 	 0.0035650623885918
the application 	 0.0006920415224913495
text may 	 0.006289308176100629
models can 	 0.038461538461538464
decomposition into 	 1.0
not remember 	 0.008928571428571428
Bobrow 's 	 1.0
uttered before 	 0.3333333333333333
L'action GRACE 	 1.0
learn parameters 	 0.07692307692307693
Lemke , 	 1.0
the testing 	 0.0006920415224913495
an interlingual 	 0.007575757575757576
input is 	 0.024390243902439025
different times 	 0.02040816326530612
unreferenced section 	 1.0
phrases supported 	 0.0625
but those 	 0.014705882352941176
have different 	 0.019230769230769232
<s> Extrinsic 	 0.0015372790161414297
up the 	 0.045454545454545456
extent of 	 0.25
too -RRB- 	 0.16666666666666666
extraction : 	 0.06451612903225806
the closest 	 0.0006920415224913495
virtual currency 	 1.0
areas -- 	 0.16666666666666666
representation framework 	 0.05263157894736842
the shift-reduce 	 0.0006920415224913495
used during 	 0.008849557522123894
, while 	 0.007860752386299831
local collection 	 0.3333333333333333
-LRB- parsed 	 0.0027100271002710027
into linguistically 	 0.01282051282051282
management task 	 0.14285714285714285
or highly 	 0.0045045045045045045
are given 	 0.012448132780082987
The successful 	 0.005208333333333333
determine which 	 0.08695652173913043
, meaning 	 0.0005614823133071309
four words 	 0.14285714285714285
see the 	 0.1
bill stub 	 0.5
the sampling 	 0.0006920415224913495
yesterday with 	 0.6666666666666666
a tonal 	 0.001226993865030675
are harder 	 0.004149377593360996
talk page 	 1.0
The examples 	 0.005208333333333333
English . 	 0.13513513513513514
of named 	 0.00089126559714795
that minimizes 	 0.0070921985815602835
metrics , 	 0.1111111111111111
with `` 	 0.01639344262295082
far , 	 0.125
a central 	 0.001226993865030675
, task-based 	 0.0005614823133071309
in software 	 0.0018726591760299626
media , 	 0.5
set on 	 0.02564102564102564
service with 	 0.2
solve a 	 0.25
, Air 	 0.0005614823133071309
answer questions 	 0.03333333333333333
similarly effective 	 1.0
A typical 	 0.04
components -RRB- 	 0.2
methods try 	 0.022727272727272728
but these 	 0.014705882352941176
individual vertices 	 0.08333333333333333
paragraphs in 	 0.25
structuring : 	 1.0
A good 	 0.02
usually are 	 0.03125
and shorter 	 0.001445086705202312
Their methods 	 0.5
corresponded to 	 1.0
estimate sentence 	 0.25
table lookup 	 0.14285714285714285
to himself 	 0.0013280212483399733
monetary value 	 1.0
<s> Military 	 0.0007686395080707148
identifying the 	 0.6666666666666666
of part-of-speech 	 0.0017825311942959
evaluation . 	 0.05555555555555555
World Wide 	 0.5714285714285714
meaningful symbols 	 0.125
developments of 	 0.3333333333333333
Alan Turing 	 1.0
this example 	 0.01098901098901099
posts and 	 1.0
translation -LRB- 	 0.02702702702702703
merges highly 	 1.0
networks Neural 	 0.07142857142857142
keyphrases formed 	 0.02857142857142857
Since this 	 0.2
leaders of 	 1.0
this regard 	 0.01098901098901099
retrained to 	 1.0
digitalized : 	 1.0
table '' 	 0.14285714285714285
such cases 	 0.016260162601626018
HTML and 	 1.0
more compactly 	 0.010526315789473684
being said 	 0.05555555555555555
training and 	 0.03571428571428571
were `` 	 0.024390243902439025
accuracy under 	 0.03225806451612903
universal encyclopedia 	 0.3333333333333333
representation can 	 0.05263157894736842
standardised text 	 1.0
successful finished 	 0.1111111111111111
by Lawrence 	 0.005714285714285714
unverified or 	 1.0
since it 	 0.2
-LRB- Cullingford 	 0.0027100271002710027
problem can 	 0.022727272727272728
built that 	 0.3333333333333333
that converts 	 0.0035460992907801418
been opinionated 	 0.014705882352941176
Topic segmentation 	 1.0
selecting a 	 0.4
`` natural 	 0.015873015873015872
reads sections 	 0.5
Where such 	 1.0
earliest such 	 0.5
new wave 	 0.041666666666666664
encyclopedia '' 	 1.0
anthropology , 	 1.0
course be 	 0.3333333333333333
methods already 	 0.022727272727272728
domain-independent and 	 1.0
human language 	 0.06521739130434782
rates can 	 0.125
accuracy in 	 0.03225806451612903
, Alan 	 0.0005614823133071309
in common 	 0.003745318352059925
steer-point coordinates 	 1.0
made with 	 0.0625
as division 	 0.003484320557491289
Turney 's 	 0.4444444444444444
domain-specific keyphrase 	 0.5
context -LRB- 	 0.06060606060606061
occurrence of 	 0.5
of characters 	 0.0017825311942959
examples as 	 0.041666666666666664
generated summary 	 0.06666666666666667
least historically 	 0.2
barmaid . 	 0.16666666666666666
<s> The 	 0.11222136817832437
many sentences 	 0.019230769230769232
allows a 	 0.125
Treebank Project 	 0.16666666666666666
from 71 	 0.009615384615384616
a positive 	 0.001226993865030675
communication studies 	 0.2
different types 	 0.04081632653061224
setting radio 	 0.2
DARPA -LRB- 	 0.25
has specialized 	 0.011904761904761904
and post-secondary 	 0.001445086705202312
have included 	 0.009615384615384616
by Jurafsky 	 0.005714285714285714
methods presented 	 0.022727272727272728
Different types 	 1.0
all be 	 0.023255813953488372
analyzing it 	 0.2
Moore 's 	 1.0
Type = 	 1.0
special fonts 	 0.2
many different 	 0.07692307692307693
surprisingly , 	 0.3333333333333333
soon . 	 0.3333333333333333
of reviews 	 0.00089126559714795
Statistics are 	 0.3333333333333333
summaries There 	 0.023255813953488372
out the 	 0.21428571428571427
sentiment with 	 0.04
by Joseph 	 0.005714285714285714
model is 	 0.1
concentrates on 	 1.0
a prior 	 0.001226993865030675
, 1979 	 0.0005614823133071309
discourse are 	 0.027777777777777776
factors which 	 0.3333333333333333
character for 	 0.045454545454545456
into sentences 	 0.01282051282051282
printed pages 	 0.08333333333333333
web 2.0 	 0.125
recognition within 	 0.008264462809917356
score . 	 0.5
transition network 	 1.0
analysis can 	 0.03076923076923077
page count 	 0.14285714285714285
bore similarities 	 1.0
been built 	 0.014705882352941176
do you 	 0.038461538461538464
Human languages 	 0.2
tested in 	 0.5
similarity scores 	 0.1
VRX and 	 1.0
very similar 	 0.0975609756097561
scanner that 	 0.3333333333333333
entirety , 	 1.0
are : 	 0.008298755186721992
-LRB- NER 	 0.0027100271002710027
system is 	 0.0967741935483871
training ; 	 0.03571428571428571
application requirements 	 0.07142857142857142
increases in 	 1.0
language learning 	 0.006756756756756757
identify objects 	 0.08333333333333333
be electronically 	 0.004219409282700422
70s the 	 1.0
that we 	 0.010638297872340425
all , 	 0.06976744186046512
than the 	 0.08888888888888889
highly and 	 0.1111111111111111
Schank 's 	 0.2
, approach 	 0.0005614823133071309
rely are 	 0.14285714285714285
1970s and 	 0.6666666666666666
forward than 	 1.0
the topic 	 0.001384083044982699
capitalized . 	 0.3333333333333333
successfully for 	 0.3333333333333333
from MUC 	 0.009615384615384616
Paul W. 	 0.2
return a 	 0.5
tries to 	 1.0
choose from 	 0.5
exceptions -RRB- 	 1.0
so-called discriminative 	 0.3333333333333333
difficulty is 	 0.14285714285714285
connects to 	 1.0
was Pollen 	 0.012987012987012988
Callaghan which 	 1.0
Information retrieval 	 0.2
human user 	 0.043478260869565216
verbs , 	 0.6
-LRB- IR 	 0.0027100271002710027
task of 	 0.21428571428571427
which makes 	 0.021739130434782608
considerable variation 	 0.2
been encouraging 	 0.014705882352941176
important sentences 	 0.125
Barbara Johnstone 	 1.0
, US 	 0.0011229646266142617
is needed 	 0.0020325203252032522
USMC , 	 1.0
common components 	 0.04
to specific 	 0.0013280212483399733
possibly others 	 0.5
they rephrase 	 0.025
to machine 	 0.00398406374501992
and right 	 0.002890173410404624
an equivalent 	 0.007575757575757576
hand it 	 0.07142857142857142
the best 	 0.008996539792387544
bites '' 	 0.3333333333333333
on ; 	 0.0047169811320754715
one of 	 0.2153846153846154
Blommaert , 	 1.0
kind appears 	 0.09090909090909091
, Maximum 	 0.0005614823133071309
length using 	 0.125
performance . 	 0.16666666666666666
punctuation characters 	 0.14285714285714285
one summary 	 0.015384615384615385
similar application 	 0.037037037037037035
, possibly 	 0.0005614823133071309
1985 , 	 1.0
MT performs 	 0.2
division rather 	 0.5
keyboard a 	 0.3333333333333333
informational structures 	 0.5
annotation process 	 0.25
fire truck 	 0.5
when such 	 0.05714285714285714
feature transformation 	 0.07692307692307693
is speaking 	 0.0020325203252032522
word forms 	 0.016666666666666666
a window 	 0.001226993865030675
happens when 	 1.0
basic sound 	 0.15384615384615385
its speakers 	 0.02857142857142857
option . 	 1.0
historical data 	 1.0
identified is 	 0.2
to 1966 	 0.0013280212483399733
levels will 	 0.09090909090909091
, follow-the-bouncing-ball 	 0.0005614823133071309
in-depth knowledge 	 0.6666666666666666
entity recognition 	 0.4
actually more 	 0.3333333333333333
validated and 	 1.0
match certain 	 0.16666666666666666
as Robert 	 0.003484320557491289
A feature 	 0.02
methods Some 	 0.022727272727272728
typical sentences 	 0.1111111111111111
another . 	 0.23076923076923078
there may 	 0.025
special ink 	 0.2
included speech 	 0.125
references . 	 0.25
<s> Anaphor 	 0.0007686395080707148
The management 	 0.005208333333333333
organization -RRB- 	 0.2
and assessing 	 0.001445086705202312
output -RRB- 	 0.038461538461538464
science disciplines 	 0.1
Alessandro Duranti 	 1.0
are pre-marked 	 0.004149377593360996
States and 	 0.14285714285714285
new text 	 0.08333333333333333
way . 	 0.041666666666666664
more principled 	 0.010526315789473684
Drum printer 	 1.0
Biden visit 	 0.3333333333333333
CSR -RRB- 	 0.6666666666666666
Kittredge , 	 0.5
summarise financial 	 0.3333333333333333
the inter-word 	 0.0006920415224913495
nouns or 	 0.1111111111111111
two ? 	 0.034482758620689655
, speeches 	 0.0005614823133071309
translation : 	 0.02702702702702703
of as 	 0.0017825311942959
8000 samples 	 1.0
in each 	 0.0018726591760299626
generalizes as 	 1.0
of how 	 0.00089126559714795
thus returning 	 0.1
Text Retrieval 	 0.16666666666666666
or discourse 	 0.009009009009009009
of subjectivity 	 0.00089126559714795
including sentiment 	 0.07142857142857142
G-loads . 	 1.0
later the 	 0.1
has already 	 0.011904761904761904
MAHS = 	 1.0
to know 	 0.0013280212483399733
American camp 	 0.2
order logic 	 0.07142857142857142
of words 	 0.013368983957219251
the example 	 0.0020761245674740486
this kind 	 0.01098901098901099
Schegloff , 	 1.0
program would 	 0.045454545454545456
-LRB- P 	 0.0027100271002710027
Because ROUGE 	 0.5
increasingly complex 	 0.3333333333333333
the company 	 0.0006920415224913495
-LRB- extrinsic 	 0.0027100271002710027
not always 	 0.008928571428571428
tables with 	 0.3333333333333333
to submit 	 0.0013280212483399733
an eyes-busy 	 0.007575757575757576
because two 	 0.03333333333333333
Harrison P. 	 1.0
<s> Discourse 	 0.0023059185242121443
other potential 	 0.014285714285714285
-LRB- MEAD 	 0.0027100271002710027
how many 	 0.10344827586206896
resources are 	 0.16666666666666666
and Nearest-neighbor 	 0.001445086705202312
better than 	 0.1111111111111111
creating pre-defined 	 0.14285714285714285
show the 	 1.0
expression . 	 0.2
has proven 	 0.011904761904761904
Disambiguation Main 	 1.0
Carston , 	 1.0
provided a 	 0.2
ask the 	 0.5
senses , 	 0.5
of corpora 	 0.00089126559714795
proposed photographing 	 0.1111111111111111
Approaches which 	 0.3333333333333333
agrees with 	 1.0
What learning 	 0.09090909090909091
Aerospace -LRB- 	 0.5
basic technology 	 0.07692307692307693
systems read 	 0.008928571428571428
the parser 	 0.002768166089965398
<s> Languages 	 0.0023059185242121443
arbitrary new 	 0.3333333333333333
in restricted 	 0.0018726591760299626
21 taggers 	 1.0
specification is 	 0.5
approaches based 	 0.03571428571428571
teams in 	 0.5
, classroom 	 0.0005614823133071309
length normalization 	 0.125
Norman , 	 0.5
is ambiguous 	 0.0020325203252032522
to converse 	 0.0013280212483399733
train their 	 1.0
these words 	 0.047619047619047616
the HTK 	 0.0006920415224913495
world and 	 0.06666666666666667
from Latin 	 0.009615384615384616
keep the 	 0.3333333333333333
like PDF 	 0.03571428571428571
In practice 	 0.009523809523809525
Prior implementations 	 1.0
crossed below 	 1.0
or produce 	 0.0045045045045045045
Two of 	 0.2857142857142857
would generate 	 0.018867924528301886
Lisp hence 	 1.0
many stochastic 	 0.019230769230769232
as some 	 0.003484320557491289
count T 	 0.2
and alignment 	 0.001445086705202312
systems for 	 0.017857142857142856
attempt to 	 1.0
the functioning 	 0.0006920415224913495
the creation 	 0.0006920415224913495
system developed 	 0.010752688172043012
approximately divided 	 0.5
Some attempts 	 0.047619047619047616
controller would 	 0.25
from Fully 	 0.009615384615384616
, Wendy 	 0.0005614823133071309
Please improve 	 0.3333333333333333
computer science 	 0.09090909090909091
detecting the 	 1.0
exploited ; 	 1.0
hits than 	 1.0
called ISO\/TC37\/SC4 	 0.05555555555555555
pseudo-pilot , 	 0.5
greatly improved 	 0.14285714285714285
search for 	 0.09090909090909091
those utility 	 0.045454545454545456
Energy -LRB- 	 1.0
and often 	 0.004335260115606936
useful only 	 0.07142857142857142
Henry Kucera 	 0.5
for instance 	 0.018050541516245487
the unwanted 	 0.0006920415224913495
had an 	 0.07142857142857142
convention in 	 1.0
Piron mentions 	 0.3333333333333333
get high 	 0.14285714285714285
step -LRB- 	 0.06666666666666667
with -LRB- 	 0.00546448087431694
, mail 	 0.0005614823133071309
custom software 	 0.5
, grounded 	 0.0005614823133071309
meet a 	 0.25
impact of 	 0.5
resource management 	 0.4
the management 	 0.0006920415224913495
or indiscriminate 	 0.0045045045045045045
were surprisingly 	 0.024390243902439025
in their 	 0.00749063670411985
algorithm -RRB- 	 0.03571428571428571
some glue 	 0.012048192771084338
news stories 	 0.07692307692307693
of ability 	 0.00089126559714795
`` language 	 0.010582010582010581
characterized by 	 0.5
larger sequences 	 0.0625
results demonstrate 	 0.047619047619047616
and can 	 0.011560693641618497
of standard 	 0.00089126559714795
supervised '' 	 0.1875
framework based 	 0.25
nonexistent in 	 1.0
also important 	 0.014492753623188406
grammatical structure 	 0.09090909090909091
famous QA 	 0.3333333333333333
by larger 	 0.005714285714285714
trigram found 	 0.3333333333333333
usable output 	 1.0
different classes 	 0.02040816326530612
Online software 	 0.5
turn , 	 0.16666666666666666
ultraviolet light 	 1.0
1980s saw 	 0.1111111111111111
However sentence 	 0.02702702702702703
and duplicate 	 0.001445086705202312
discourse . 	 0.027777777777777776
from IBM 	 0.009615384615384616
checked after 	 0.5
This sequence 	 0.015873015873015872
writer with 	 1.0
a common 	 0.00245398773006135
output simply 	 0.038461538461538464
