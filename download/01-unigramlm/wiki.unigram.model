(<class 'UnicodeDecodeError'>, UnicodeDecodeError('gbk', b"istinctive groups : rule-based and stochastic .\r\nE. Brill 's tagger , one of the first and widely used English POS-taggers , employs rule-based algorithms .\r\nThis is not rare -- in natural languages -LRB- as opposed to many artificial languages -RRB- , a large percentage of word-forms are ambiguous .\r\nFor example , even `` dogs '' , which is usually thought of as just a plural noun , can also be a verb : The sailor dogs the barmaid .\r\nPerforming grammatical tagging will indicate that `` dogs '' is a verb , and not the more common plural noun , since one of the words must be the main verb , and the noun reading is less likely following `` sailor '' -LRB- sailor !\r\n\xe2\x86\x92 dogs -RRB- .\r\nSemantic analysis can then extrapolate that `` sailor '' and `` barmaid '' implicate `` dogs '' as 1 -RRB- in the nautical context -LRB- sailor \xe2\x86\x92 <verb> \xe2\x86\x90 barmaid -RRB- and 2 -RRB- an action applied to the object `` barmaid '' -LRB- -LRB- subject -RRB- dogs \xe2\x86\x92 barmaid -RRB- .\r\nIn this context , `` dogs '' is a nautical term meaning `` fastens -LRB- a watertight barmaid -RRB- securely ; applies a dog to '' .\r\n`` Dogged '' , on the other hand , can be either an adjective or a past-tense verb .\r\nJust which parts of speech a word can represent varies greatly .\r\nTrained linguists can identify the grammatical parts of speech to various fine degrees depending on the tagging system .\r\nSchools commonly teach that there are 9 parts of speech in English : noun , verb , article , adjective , preposition , pronoun , adverb , conjunction , and interjection .\r\nHowever , there are clearly many more categories and sub-categories .\r\nFor nouns , plural , possessive , and singular forms can be distinguished .\r\nIn many languages words are also marked for their `` case '' -LRB- role as subject , object , etc. -RRB- , grammatical gender , and so on ; while verbs are marked for tense , aspect , and other things .\r\nIn part-of-speech tagging by computer , it is typical to distinguish from 50 to 150 separate parts of speech for English , for example , NN for singular common nouns , NNS for plural common nouns , NP for singular proper nouns -LRB- see the POS tags used in the Brown Corpus -RRB- .\r\nWork on stochastic methods for tagging Koine Greek -LRB- DeRose 1990 -RRB- has used over 1,000 parts of speech , and found that about as many words were ambiguous there as in English .\r\nA morphosyntactic descriptor in the case of morphologically rich languages can be expressed like Ncmsan , which means Category = Noun , Type = common , Gender = masculine , Number = singular , Case = accusative , Animate = no. .\r\nHistory The Brown Corpus Research on part-of-speech tagging has been closely tied to corpus linguistics .\r\nThe first major corpus of English for computer analysis was the Brown Corpus developed at Brown University by Henry Kucera and Nelson Francis , in the mid-1960s .\r\nIt consists of about 1,000,000 words of running English prose text , made up of 500 samples from randomly chosen publications .\r\nEach sample is 2,000 or more words -LRB- ending at the first sentence-end after 2,000 words , so that the corpus contains only complete sentences -RRB- .\r\nThe Brown Corpus was painstakingly `` tagged '' with part-of-speech markers over many years .\r\nA first approximation was done with a program by Greene and Rubin , which consisted of a huge handmade list of what categories could co-occur at all .\r\nFor example , article then noun can occur , but article verb -LRB- arguably -RRB- can not .\r\nThe program got about 70 % correct .\r\nIts results were repeatedly reviewed and corrected by hand , and later users sent in errata , so that by the late 70s the tagging was nearly perfect -LRB- allowing for some cases on which even human speakers might not agree -RRB- .\r\nThis corpus has been used for innumerable studies of word-frequency and of part-of-speech , and inspired the development of similar `` tagged '' corpora in many other languages .\r\nStatistics derived by analyzing it formed the basis for most later part-of-speech tagging systems , such as CLAWS -LRB- linguistics -RRB- and VOLSUNGA .\r\nHowever , by this time -LRB- 2005 -RRB- it has been superseded by larger corpora such as the 100 million word British National Corpus .\r\nFor some time , part-of-speech tagging was considered an inseparable part of natural language processing , because there are certain cases where the correct part of speech can not be decided without understanding the semantics or even the pragmatics of the context .\r\nThis is extremely expensive , especially because analyzing the higher levels is much harder when multiple part-of-speech possibilities must be considered for each word .\r\nUse of Hidden Markov Models In the mid 1980s , researchers in Europe began to use hidden Markov models -LRB- HMMs -RRB- to disambiguate parts of speech , when working to tag the Lancaster-Oslo-Bergen Corpus of British English .\r\nHMMs involve counting cases -LRB- such as from the Brown Corpus -RRB- , and making a table of the probabilities of certain sequences .\r\nFor example , once you 've seen an article such as ` the ' , perhaps the next word is a noun 40 % of the time , an adjective 40 % , and a number 20 % .\r\nKnowing this , a program can decide that `` can '' in `` the can '' is far more likely to be a noun than a verb or a modal .\r\nThe same method can of course be used to benefit from knowledge about following words .\r\nMore advanced -LRB- `` higher order '' -RRB- HMMs learn the probabilities not only of pairs , but triples or even larger sequences .\r\nSo , for example , if you 've just seen an article and a verb , the next item may be very likely a preposition , article , or noun , but much less likely another verb .\r\nWhen several ambiguous words occur together , the possibilities multiply .\r\nHowever , it is easy to enumerate every combination and to assign a relative probability to each one , by multiplying together the probabilities of each choice in turn .\r\nThe combination with highest probability is then chosen .\r\nThe European group developed CLAWS , a tagging program that did exactly this , and achieved accuracy in the 93-95 % range .\r\nIt is worth remembering , as Eugene Charniak points out in Statistical techniques for natural language parsing , that merely assigning the most common tag to each known word and the tag `` proper noun '' to all unknowns , will approach 90 % accuracy because many words are unambiguous .\r\nCLAWS pioneered the field of HMM-based part of speech tagging , but was quite expensive since it enumerated all possibilities .\r\nIt sometimes had to resort to backup methods when there were simply too many -LRB- the Brown Corpus contains a case with 17 ambiguous words in a row , and there are words such as `` still '' that can represent as many as 7 distinct parts of speech -RRB- .\r\nHMMs underlie the functioning of stochastic taggers and are used in various algorithms one of the most widely used being the bi-directional inference algorithm .\r\nDynamic Programming methods In 1987 , Steven DeRose and Ken Church independently developed dynamic programming algorithms to solve the same problem in vastly less time .\r\nTheir methods were similar to the Viterbi algorithm known for some time in other fields .\r\nDeRose used a table of pairs , while Church used a table of triples and a method of estimating the values for triples that were rare or nonexistent in the Brown Corpus -LRB- actual measurement of triple probabilities would require a much larger corpus -RRB- .\r\nBoth methods achieved accuracy over 95 % .\r\nDeRose 's 1990 dissertation at Brown University included analyses of the specific error types , probabilities , and other related data , and replicated his work for Greek , where it proved similarly effective .\r\nThese findings were surprisingly disruptive to the field of natural language processing .\r\nThe accuracy reported was higher than the typical accuracy of very sophisticated algorithms that integrated part of speech choice with many higher levels of linguistic analysis : syntax , morphology , semantics , and so on .\r\nCLAWS , DeRose 's and Church 's methods did fail for some of the known cases where semantics is required , but", 674, 675, 'illegal multibyte sequence'), <traceback object at 0x0000014FA0274FC8>)
